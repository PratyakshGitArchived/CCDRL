{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep_Q_Learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96LHSolI5lr-",
        "colab_type": "text"
      },
      "source": [
        "**Downgrade to Tensorflow 1.14 for compatibality**\n",
        "\n",
        "**Install keras-rl**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esYfXv__5FSD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 939
        },
        "outputId": "392010cb-13ed-4878-e4d6-399aa0c33547"
      },
      "source": [
        "!pip install keras_rl==0.4.2\n",
        "!pip3 install tensorflow==1.14"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras_rl==0.4.2 in /usr/local/lib/python3.6/dist-packages (0.4.2)\n",
            "Requirement already satisfied: keras>=2.0.7 in /usr/local/lib/python3.6/dist-packages (from keras_rl==0.4.2) (2.3.1)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras_rl==0.4.2) (1.4.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras_rl==0.4.2) (1.1.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras_rl==0.4.2) (1.12.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras_rl==0.4.2) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras_rl==0.4.2) (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras_rl==0.4.2) (1.18.5)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras_rl==0.4.2) (1.0.8)\n",
            "Collecting tensorflow==1.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/f0/96fb2e0412ae9692dbf400e5b04432885f677ad6241c088ccc5fe7724d69/tensorflow-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (109.2MB)\n",
            "\u001b[K     |████████████████████████████████| 109.2MB 49kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.18.5)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.29.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (3.10.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.1.2)\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 47.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.3.3)\n",
            "Collecting tensorboard<1.15.0,>=1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 45.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.12.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.12.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.9.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.8.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.2.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.34.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.14) (47.1.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14) (2.10.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (1.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.1.0)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 2.2.0\n",
            "    Uninstalling tensorflow-estimator-2.2.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0\n",
            "  Found existing installation: tensorboard 2.2.2\n",
            "    Uninstalling tensorboard-2.2.2:\n",
            "      Successfully uninstalled tensorboard-2.2.2\n",
            "  Found existing installation: tensorflow 2.2.0\n",
            "    Uninstalling tensorflow-2.2.0:\n",
            "      Successfully uninstalled tensorflow-2.2.0\n",
            "Successfully installed tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CvwI7Y6c6zmR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "outputId": "edcff5c2-a66e-4927-cfc9-0b83ba7ab8a7"
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import random\n",
        "\n",
        "ENV_NAME = \"Taxi-v3\"\n",
        "env = gym.make(ENV_NAME)\n",
        "env.render()\n",
        "\n",
        "\n",
        "print(\"Number of actions: %d\" % env.action_space.n)\n",
        "print(\"Number of states: %d\" % env.observation_space.n)\n",
        "\n",
        "action_size = env.action_space.n\n",
        "state_size = env.observation_space.n\n",
        "\n",
        "np.random.seed(6)\n",
        "env.seed(6)\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten, Embedding, Reshape\n",
        "from keras.optimizers import Adam\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import EpsGreedyQPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "\n",
        "\n",
        "env.reset()\n",
        "env.step(env.action_space.sample())[0]"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : :\u001b[43m \u001b[0m: : |\n",
            "| | : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "\n",
            "Number of actions: 6\n",
            "Number of states: 500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "169"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jgjF81hWV5DN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "outputId": "08046904-a5fb-43b3-ef34-6528f77e7961"
      },
      "source": [
        "model_only_embedding = Sequential()\n",
        "model_only_embedding.add(Embedding(500, 6, input_length=1))\n",
        "model_only_embedding.add(Reshape((6,)))\n",
        "print(model_only_embedding.summary())\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(500, 10, input_length=1))\n",
        "model.add(Reshape((10,)))\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(action_size, activation='linear'))\n",
        "print(model.summary())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 1, 6)              3000      \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 6)                 0         \n",
            "=================================================================\n",
            "Total params: 3,000\n",
            "Trainable params: 3,000\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 1, 10)             5000      \n",
            "_________________________________________________________________\n",
            "reshape_2 (Reshape)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 50)                550       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 6)                 306       \n",
            "=================================================================\n",
            "Total params: 10,956\n",
            "Trainable params: 10,956\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Aywe4VCeVs5b",
        "colab": {},
        "outputId": "ec6cbe36-5c4e-4cea-85e3-0b59a8e3dd87"
      },
      "source": [
        "#####################################################\n",
        "memory = SequentialMemory(limit=50000, window_length=1)\n",
        "policy = EpsGreedyQPolicy()\n",
        "dqn_only_embedding = DQNAgent(model=model, nb_actions=action_size, memory=memory, nb_steps_warmup=500, target_model_update=1e-2, policy=policy)\n",
        "dqn_only_embedding.compile(Adam(lr=1e-3), metrics=['mae'])\n",
        "dqn_only_embedding.fit(env, nb_steps=1000000, visualize=False, verbose=1, nb_max_episode_steps=99, log_interval=10)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for 1000000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "WARNING:tensorflow:From /home/citymaas/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -10.0000\n",
            "Interval 2 (10 steps performed)\n",
            "10/10 [==============================] - 0s 600us/step - reward: -9.1000\n",
            "Interval 3 (20 steps performed)\n",
            "10/10 [==============================] - 0s 531us/step - reward: -10.0000\n",
            "Interval 4 (30 steps performed)\n",
            "10/10 [==============================] - 0s 582us/step - reward: -8.2000\n",
            "Interval 5 (40 steps performed)\n",
            "10/10 [==============================] - 0s 627us/step - reward: -10.0000\n",
            "Interval 6 (50 steps performed)\n",
            "10/10 [==============================] - 0s 569us/step - reward: -10.0000\n",
            "Interval 7 (60 steps performed)\n",
            "10/10 [==============================] - 0s 607us/step - reward: -9.1000\n",
            "Interval 8 (70 steps performed)\n",
            "10/10 [==============================] - 0s 680us/step - reward: -9.1000\n",
            "Interval 9 (80 steps performed)\n",
            "10/10 [==============================] - 0s 738us/step - reward: -9.1000\n",
            "Interval 10 (90 steps performed)\n",
            "10/10 [==============================] - 0s 717us/step - reward: -9.1000\n",
            "1 episodes - episode_reward: -927.000 [-927.000, -927.000] - prob: 1.000\n",
            "\n",
            "Interval 11 (100 steps performed)\n",
            "10/10 [==============================] - 0s 722us/step - reward: -10.0000\n",
            "Interval 12 (110 steps performed)\n",
            "10/10 [==============================] - 0s 699us/step - reward: -8.2000\n",
            "Interval 13 (120 steps performed)\n",
            "10/10 [==============================] - 0s 706us/step - reward: -10.0000\n",
            "Interval 14 (130 steps performed)\n",
            "10/10 [==============================] - 0s 696us/step - reward: -9.1000\n",
            "Interval 15 (140 steps performed)\n",
            "10/10 [==============================] - 0s 676us/step - reward: -7.3000\n",
            "Interval 16 (150 steps performed)\n",
            "10/10 [==============================] - 0s 685us/step - reward: -9.1000\n",
            "Interval 17 (160 steps performed)\n",
            "10/10 [==============================] - 0s 751us/step - reward: -10.0000\n",
            "Interval 18 (170 steps performed)\n",
            "10/10 [==============================] - 0s 679us/step - reward: -9.1000\n",
            "Interval 19 (180 steps performed)\n",
            "10/10 [==============================] - 0s 676us/step - reward: -10.0000\n",
            "Interval 20 (190 steps performed)\n",
            "10/10 [==============================] - 0s 774us/step - reward: -7.3000\n",
            "1 episodes - episode_reward: -900.000 [-900.000, -900.000] - prob: 1.000\n",
            "\n",
            "Interval 21 (200 steps performed)\n",
            "10/10 [==============================] - 0s 778us/step - reward: -10.0000\n",
            "Interval 22 (210 steps performed)\n",
            "10/10 [==============================] - 0s 787us/step - reward: -10.0000\n",
            "Interval 23 (220 steps performed)\n",
            "10/10 [==============================] - 0s 785us/step - reward: -10.0000\n",
            "Interval 24 (230 steps performed)\n",
            "10/10 [==============================] - 0s 671us/step - reward: -10.0000\n",
            "Interval 25 (240 steps performed)\n",
            "10/10 [==============================] - 0s 710us/step - reward: -8.2000\n",
            "Interval 26 (250 steps performed)\n",
            "10/10 [==============================] - 0s 648us/step - reward: -7.3000\n",
            "Interval 27 (260 steps performed)\n",
            "10/10 [==============================] - 0s 666us/step - reward: -10.0000\n",
            "Interval 28 (270 steps performed)\n",
            "10/10 [==============================] - 0s 637us/step - reward: -8.2000\n",
            "Interval 29 (280 steps performed)\n",
            "10/10 [==============================] - 0s 638us/step - reward: -10.0000\n",
            "Interval 30 (290 steps performed)\n",
            "10/10 [==============================] - 0s 784us/step - reward: -9.1000\n",
            "1 episodes - episode_reward: -918.000 [-918.000, -918.000] - prob: 1.000\n",
            "\n",
            "Interval 31 (300 steps performed)\n",
            "10/10 [==============================] - 0s 1ms/step - reward: -10.0000\n",
            "Interval 32 (310 steps performed)\n",
            "10/10 [==============================] - 0s 675us/step - reward: -9.1000\n",
            "Interval 33 (320 steps performed)\n",
            "10/10 [==============================] - 0s 698us/step - reward: -8.2000\n",
            "Interval 34 (330 steps performed)\n",
            "10/10 [==============================] - 0s 674us/step - reward: -8.2000\n",
            "Interval 35 (340 steps performed)\n",
            "10/10 [==============================] - 0s 613us/step - reward: -9.1000\n",
            "Interval 36 (350 steps performed)\n",
            "10/10 [==============================] - 0s 613us/step - reward: -9.1000\n",
            "Interval 37 (360 steps performed)\n",
            "10/10 [==============================] - 0s 579us/step - reward: -8.2000\n",
            "Interval 38 (370 steps performed)\n",
            "10/10 [==============================] - 0s 619us/step - reward: -10.0000\n",
            "Interval 39 (380 steps performed)\n",
            "10/10 [==============================] - 0s 591us/step - reward: -9.1000\n",
            "Interval 40 (390 steps performed)\n",
            "10/10 [==============================] - 0s 741us/step - reward: -8.2000\n",
            "1 episodes - episode_reward: -891.000 [-891.000, -891.000] - prob: 1.000\n",
            "\n",
            "Interval 41 (400 steps performed)\n",
            "10/10 [==============================] - 0s 651us/step - reward: -8.2000\n",
            "Interval 42 (410 steps performed)\n",
            "10/10 [==============================] - 0s 666us/step - reward: -10.0000\n",
            "Interval 43 (420 steps performed)\n",
            "10/10 [==============================] - 0s 585us/step - reward: -10.0000\n",
            "Interval 44 (430 steps performed)\n",
            "10/10 [==============================] - 0s 590us/step - reward: -8.2000\n",
            "Interval 45 (440 steps performed)\n",
            "10/10 [==============================] - 0s 572us/step - reward: -9.1000\n",
            "Interval 46 (450 steps performed)\n",
            "10/10 [==============================] - 0s 649us/step - reward: -10.0000\n",
            "Interval 47 (460 steps performed)\n",
            "10/10 [==============================] - 0s 600us/step - reward: -9.1000\n",
            "Interval 48 (470 steps performed)\n",
            "10/10 [==============================] - 0s 617us/step - reward: -10.0000\n",
            "Interval 49 (480 steps performed)\n",
            "10/10 [==============================] - 0s 664us/step - reward: -10.0000\n",
            "Interval 50 (490 steps performed)\n",
            "10/10 [==============================] - 0s 725us/step - reward: -6.4000\n",
            "1 episodes - episode_reward: -918.000 [-918.000, -918.000] - prob: 1.000\n",
            "\n",
            "Interval 51 (500 steps performed)\n",
            "10/10 [==============================] - 1s 87ms/step - reward: -3.7000\n",
            "Interval 52 (510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 53 (520 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 54 (530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 55 (540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 56 (550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 57 (560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 58 (570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 59 (580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 60 (590 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -153.000 [-153.000, -153.000] - loss: 1.230 - mae: 1.421 - mean_q: -0.915 - prob: 1.000\n",
            "\n",
            "Interval 61 (600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 62 (610 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 63 (620 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 64 (630 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 65 (640 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 66 (650 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 67 (660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 68 (670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 69 (680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 70 (690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "1 episodes - episode_reward: -117.000 [-117.000, -117.000] - loss: 0.171 - mae: 2.655 - mean_q: -1.616 - prob: 1.000\n",
            "\n",
            "Interval 71 (700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 72 (710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 73 (720 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 74 (730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 75 (740 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 76 (750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 77 (760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 78 (770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 79 (780 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 80 (790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -162.000 [-162.000, -162.000] - loss: 0.049 - mae: 3.786 - mean_q: -2.084 - prob: 1.000\n",
            "\n",
            "Interval 81 (800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 82 (810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 83 (820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 84 (830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 85 (840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 86 (850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 87 (860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 88 (870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 89 (880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 90 (890 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -144.000 [-144.000, -144.000] - loss: 0.027 - mae: 4.669 - mean_q: -2.855 - prob: 1.000\n",
            "\n",
            "Interval 91 (900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 92 (910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 93 (920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94 (930 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 95 (940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 96 (950 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 97 (960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 98 (970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 99 (980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -117.000 [-117.000, -117.000] - loss: 0.046 - mae: 5.240 - mean_q: -3.449 - prob: 1.000\n",
            "\n",
            "Interval 100 (990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 101 (1000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 102 (1010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 103 (1020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 104 (1030 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 105 (1040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 106 (1050 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 107 (1060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 108 (1070 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 109 (1080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -117.000 [-117.000, -117.000] - loss: 0.070 - mae: 5.845 - mean_q: -4.185 - prob: 1.000\n",
            "\n",
            "Interval 110 (1090 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 111 (1100 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 112 (1110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 113 (1120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 114 (1130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 115 (1140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 116 (1150 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 117 (1160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 118 (1170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 119 (1180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -117.000 [-117.000, -117.000] - loss: 0.071 - mae: 6.422 - mean_q: -4.701 - prob: 1.000\n",
            "\n",
            "Interval 120 (1190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 121 (1200 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 122 (1210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 123 (1220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 124 (1230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 125 (1240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 126 (1250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 127 (1260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 128 (1270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 129 (1280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "1 episodes - episode_reward: -171.000 [-171.000, -171.000] - loss: 0.090 - mae: 7.219 - mean_q: -5.750 - prob: 1.000\n",
            "\n",
            "Interval 130 (1290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 131 (1300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 132 (1310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 133 (1320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 134 (1330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 135 (1340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 136 (1350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 137 (1360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 138 (1370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 139 (1380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -144.000 [-144.000, -144.000] - loss: 0.106 - mae: 7.951 - mean_q: -6.485 - prob: 1.000\n",
            "\n",
            "Interval 140 (1390 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 141 (1400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 142 (1410 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 143 (1420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 144 (1430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 145 (1440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 146 (1450 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 147 (1460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 148 (1470 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 149 (1480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -144.000 [-144.000, -144.000] - loss: 0.122 - mae: 8.457 - mean_q: -7.152 - prob: 1.000\n",
            "\n",
            "Interval 150 (1490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 151 (1500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 152 (1510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 153 (1520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 154 (1530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 155 (1540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 156 (1550 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 157 (1560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 158 (1570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 159 (1580 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -108.000 [-108.000, -108.000] - loss: 0.347 - mae: 9.009 - mean_q: -7.626 - prob: 1.000\n",
            "\n",
            "Interval 160 (1590 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 161 (1600 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 162 (1610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 163 (1620 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 164 (1630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 165 (1640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 166 (1650 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 167 (1660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 168 (1670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 169 (1680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -126.000 [-126.000, -126.000] - loss: 0.295 - mae: 9.568 - mean_q: -8.422 - prob: 1.000\n",
            "\n",
            "Interval 170 (1690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 171 (1700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 172 (1710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 173 (1720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 174 (1730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 175 (1740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 176 (1750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 177 (1760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 178 (1770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 179 (1780 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "1 episodes - episode_reward: -135.000 [-135.000, -135.000] - loss: 0.308 - mae: 10.100 - mean_q: -8.927 - prob: 1.000\n",
            "\n",
            "Interval 180 (1790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 181 (1800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 182 (1810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 183 (1820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 184 (1830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 185 (1840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 186 (1850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 187 (1860 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 188 (1870 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 189 (1880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "1 episodes - episode_reward: -126.000 [-126.000, -126.000] - loss: 0.343 - mae: 10.861 - mean_q: -9.951 - prob: 1.000\n",
            "\n",
            "Interval 190 (1890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 191 (1900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 192 (1910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 193 (1920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 194 (1930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 195 (1940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 196 (1950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 197 (1960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 198 (1970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -153.000 [-153.000, -153.000] - loss: 0.411 - mae: 11.357 - mean_q: -10.278 - prob: 1.000\n",
            "\n",
            "Interval 199 (1980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 200 (1990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 201 (2000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 202 (2010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 203 (2020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 204 (2030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 205 (2040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 206 (2050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 207 (2060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 208 (2070 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -117.000 [-117.000, -117.000] - loss: 0.217 - mae: 11.915 - mean_q: -11.079 - prob: 1.000\n",
            "\n",
            "Interval 209 (2080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 210 (2090 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 211 (2100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 212 (2110 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 213 (2120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 214 (2130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 215 (2140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 216 (2150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 217 (2160 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 218 (2170 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -144.000 [-144.000, -144.000] - loss: 0.459 - mae: 12.310 - mean_q: -11.584 - prob: 1.000\n",
            "\n",
            "Interval 219 (2180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 220 (2190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 221 (2200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 222 (2210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 223 (2220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 224 (2230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 225 (2240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 226 (2250 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 227 (2260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 228 (2270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -117.000 [-117.000, -117.000] - loss: 1.738 - mae: 12.759 - mean_q: -11.910 - prob: 1.000\n",
            "\n",
            "Interval 229 (2280 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 230 (2290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 231 (2300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 232 (2310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 233 (2320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 234 (2330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 235 (2340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 236 (2350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 237 (2360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 238 (2370 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -108.000 [-108.000, -108.000] - loss: 1.096 - mae: 13.547 - mean_q: -12.786 - prob: 1.000\n",
            "\n",
            "Interval 239 (2380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 240 (2390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 241 (2400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 242 (2410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 243 (2420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 244 (2430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 245 (2440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 246 (2450 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 247 (2460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 248 (2470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "1 episodes - episode_reward: -108.000 [-108.000, -108.000] - loss: 0.454 - mae: 14.196 - mean_q: -13.595 - prob: 1.000\n",
            "\n",
            "Interval 249 (2480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 250 (2490 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 251 (2500 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 252 (2510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 253 (2520 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 254 (2530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 255 (2540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 256 (2550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 257 (2560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 258 (2570 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "1 episodes - episode_reward: -117.000 [-117.000, -117.000] - loss: 0.437 - mae: 14.611 - mean_q: -13.966 - prob: 1.000\n",
            "\n",
            "Interval 259 (2580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 260 (2590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 261 (2600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 262 (2610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 263 (2620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 264 (2630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 265 (2640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 266 (2650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 267 (2660 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 268 (2670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -126.000 [-126.000, -126.000] - loss: 0.767 - mae: 15.014 - mean_q: -14.709 - prob: 1.000\n",
            "\n",
            "Interval 269 (2680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 270 (2690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 271 (2700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 272 (2710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 273 (2720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 274 (2730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 275 (2740 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 276 (2750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 277 (2760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 278 (2770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "1 episodes - episode_reward: -153.000 [-153.000, -153.000] - loss: 0.621 - mae: 15.612 - mean_q: -15.110 - prob: 1.000\n",
            "\n",
            "Interval 279 (2780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 280 (2790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 281 (2800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 282 (2810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 283 (2820 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 284 (2830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 285 (2840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 286 (2850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 287 (2860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 288 (2870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "1 episodes - episode_reward: -126.000 [-126.000, -126.000] - loss: 1.082 - mae: 16.262 - mean_q: -15.831 - prob: 1.000\n",
            "\n",
            "Interval 289 (2880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 290 (2890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 291 (2900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 292 (2910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 293 (2920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 294 (2930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 295 (2940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 296 (2950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 297 (2960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -117.000 [-117.000, -117.000] - loss: 1.016 - mae: 16.715 - mean_q: -16.123 - prob: 1.000\n",
            "\n",
            "Interval 298 (2970 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 299 (2980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 300 (2990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 301 (3000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 302 (3010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 303 (3020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 304 (3030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 305 (3040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 306 (3050 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 307 (3060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -153.000 [-153.000, -153.000] - loss: 1.445 - mae: 16.395 - mean_q: -16.106 - prob: 1.000\n",
            "\n",
            "Interval 308 (3070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 309 (3080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 310 (3090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 311 (3100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 312 (3110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 313 (3120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 314 (3130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 315 (3140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 316 (3150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 317 (3160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -144.000 [-144.000, -144.000] - loss: 1.889 - mae: 17.049 - mean_q: -16.512 - prob: 1.000\n",
            "\n",
            "Interval 318 (3170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 319 (3180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 320 (3190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 321 (3200 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 322 (3210 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 323 (3220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 324 (3230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 325 (3240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 326 (3250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 327 (3260 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "1 episodes - episode_reward: -135.000 [-135.000, -135.000] - loss: 0.594 - mae: 17.694 - mean_q: -17.373 - prob: 1.000\n",
            "\n",
            "Interval 328 (3270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 329 (3280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 330 (3290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 331 (3300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 332 (3310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 333 (3320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 334 (3330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 335 (3340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 336 (3350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 337 (3360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "1 episodes - episode_reward: -117.000 [-117.000, -117.000] - loss: 1.412 - mae: 17.725 - mean_q: -17.733 - prob: 1.000\n",
            "\n",
            "Interval 338 (3370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 339 (3380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 340 (3390 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 341 (3400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 342 (3410 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 343 (3420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 344 (3430 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 345 (3440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 346 (3450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 347 (3460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -126.000 [-126.000, -126.000] - loss: 2.864 - mae: 18.093 - mean_q: -17.680 - prob: 1.000\n",
            "\n",
            "Interval 348 (3470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 349 (3480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 350 (3490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 351 (3500 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 352 (3510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 353 (3520 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 354 (3530 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 355 (3540 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 356 (3550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 357 (3560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -144.000 [-144.000, -144.000] - loss: 1.501 - mae: 18.756 - mean_q: -18.776 - prob: 1.000\n",
            "\n",
            "Interval 358 (3570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 359 (3580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 360 (3590 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 361 (3600 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: -1.9000\n",
            "Interval 362 (3610 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.0000\n",
            "Interval 363 (3620 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.9000\n",
            "Interval 364 (3630 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 365 (3640 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 366 (3650 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 367 (3660 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -117.000 [-117.000, -117.000] - loss: 1.361 - mae: 19.009 - mean_q: -18.842 - prob: 1.000\n",
            "\n",
            "Interval 368 (3670 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 369 (3680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 370 (3690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 371 (3700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 372 (3710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 373 (3720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 374 (3730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 375 (3740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 376 (3750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 377 (3760 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -126.000 [-126.000, -126.000] - loss: 2.632 - mae: 19.309 - mean_q: -19.101 - prob: 1.000\n",
            "\n",
            "Interval 378 (3770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 379 (3780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 380 (3790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 381 (3800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 382 (3810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 383 (3820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 384 (3830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 385 (3840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 386 (3850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 387 (3860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -126.000 [-126.000, -126.000] - loss: 2.168 - mae: 19.386 - mean_q: -19.229 - prob: 1.000\n",
            "\n",
            "Interval 388 (3870 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 389 (3880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 390 (3890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 391 (3900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 392 (3910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 393 (3920 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -2.8000\n",
            "Interval 394 (3930 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 395 (3940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 396 (3950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "1 episodes - episode_reward: -144.000 [-144.000, -144.000] - loss: 1.275 - mae: 19.612 - mean_q: -19.489 - prob: 1.000\n",
            "\n",
            "Interval 397 (3960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 398 (3970 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 399 (3980 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 400 (3990 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 401 (4000 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 402 (4010 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 403 (4020 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 404 (4030 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 405 (4040 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 406 (4050 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -99.000 [-99.000, -99.000] - loss: 0.352 - mae: 20.171 - mean_q: -20.400 - prob: 1.000\n",
            "\n",
            "Interval 407 (4060 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 408 (4070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 409 (4080 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 410 (4090 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 411 (4100 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 412 (4110 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 413 (4120 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 414 (4130 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 415 (4140 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 416 (4150 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -117.000 [-117.000, -117.000] - loss: 1.016 - mae: 20.011 - mean_q: -20.221 - prob: 1.000\n",
            "\n",
            "Interval 417 (4160 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 418 (4170 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 419 (4180 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 420 (4190 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 421 (4200 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 422 (4210 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 423 (4220 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -2.8000\n",
            "Interval 424 (4230 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -2.8000\n",
            "Interval 425 (4240 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 426 (4250 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -153.000 [-153.000, -153.000] - loss: 2.119 - mae: 20.497 - mean_q: -20.229 - prob: 1.000\n",
            "\n",
            "Interval 427 (4260 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 428 (4270 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 429 (4280 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 430 (4290 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 431 (4300 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 432 (4310 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 433 (4320 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 434 (4330 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 435 (4340 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 436 (4350 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -108.000 [-108.000, -108.000] - loss: 0.442 - mae: 20.853 - mean_q: -21.478 - prob: 1.000\n",
            "\n",
            "Interval 437 (4360 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 438 (4370 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 439 (4380 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.0000\n",
            "Interval 440 (4390 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 441 (4400 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 442 (4410 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 443 (4420 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 444 (4430 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 445 (4440 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 446 (4450 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -108.000 [-108.000, -108.000] - loss: 2.824 - mae: 21.413 - mean_q: -21.531 - prob: 1.000\n",
            "\n",
            "Interval 447 (4460 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 448 (4470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 449 (4480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 450 (4490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 451 (4500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 452 (4510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 453 (4520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 454 (4530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 455 (4540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 456 (4550 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -126.000 [-126.000, -126.000] - loss: 3.151 - mae: 21.225 - mean_q: -21.295 - prob: 1.000\n",
            "\n",
            "Interval 457 (4560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 458 (4570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 459 (4580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 460 (4590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 461 (4600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 462 (4610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 463 (4620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 464 (4630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 465 (4640 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 466 (4650 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -153.000 [-153.000, -153.000] - loss: 0.664 - mae: 21.643 - mean_q: -21.974 - prob: 1.000\n",
            "\n",
            "Interval 467 (4660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 468 (4670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 469 (4680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 470 (4690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 471 (4700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 472 (4710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 473 (4720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 474 (4730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 475 (4740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 476 (4750 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -135.000 [-135.000, -135.000] - loss: 3.258 - mae: 22.025 - mean_q: -22.304 - prob: 1.000\n",
            "\n",
            "Interval 477 (4760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 478 (4770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 479 (4780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 480 (4790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 481 (4800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 482 (4810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 483 (4820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 484 (4830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 485 (4840 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 486 (4850 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -153.000 [-153.000, -153.000] - loss: 2.813 - mae: 22.518 - mean_q: -22.288 - prob: 1.000\n",
            "\n",
            "Interval 487 (4860 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 488 (4870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 489 (4880 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -3.7000\n",
            "Interval 490 (4890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 491 (4900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 492 (4910 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 493 (4920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 494 (4930 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 495 (4940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -135.000 [-135.000, -135.000] - loss: 2.789 - mae: 22.145 - mean_q: -22.393 - prob: 1.000\n",
            "\n",
            "Interval 496 (4950 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 497 (4960 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 498 (4970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 499 (4980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 500 (4990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 501 (5000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 502 (5010 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 503 (5020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 504 (5030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 505 (5040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -117.000 [-117.000, -117.000] - loss: 3.880 - mae: 22.829 - mean_q: -23.296 - prob: 1.000\n",
            "\n",
            "Interval 506 (5050 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 507 (5060 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 508 (5070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 509 (5080 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 510 (5090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 511 (5100 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 512 (5110 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 513 (5120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 514 (5130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 515 (5140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -117.000 [-117.000, -117.000] - loss: 3.334 - mae: 22.392 - mean_q: -22.744 - prob: 1.000\n",
            "\n",
            "Interval 516 (5150 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 517 (5160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 518 (5170 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 519 (5180 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 520 (5190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 521 (5200 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 522 (5210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 523 (5220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 524 (5230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 525 (5240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -117.000 [-117.000, -117.000] - loss: 0.811 - mae: 23.167 - mean_q: -23.728 - prob: 1.000\n",
            "\n",
            "Interval 526 (5250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 527 (5260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 528 (5270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 529 (5280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 530 (5290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 531 (5300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 532 (5310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 533 (5320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 534 (5330 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 535 (5340 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "1 episodes - episode_reward: -144.000 [-144.000, -144.000] - loss: 1.825 - mae: 22.784 - mean_q: -23.585 - prob: 1.000\n",
            "\n",
            "Interval 536 (5350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 537 (5360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 538 (5370 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 539 (5380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 540 (5390 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 541 (5400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 542 (5410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 543 (5420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 544 (5430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 545 (5440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -135.000 [-135.000, -135.000] - loss: 2.291 - mae: 23.114 - mean_q: -23.700 - prob: 1.000\n",
            "\n",
            "Interval 546 (5450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 547 (5460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 548 (5470 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 549 (5480 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 550 (5490 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 551 (5500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 552 (5510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 553 (5520 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 554 (5530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 555 (5540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -126.000 [-126.000, -126.000] - loss: 1.644 - mae: 23.766 - mean_q: -24.484 - prob: 1.000\n",
            "\n",
            "Interval 556 (5550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 557 (5560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 558 (5570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 559 (5580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 560 (5590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 561 (5600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 562 (5610 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 563 (5620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 564 (5630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 565 (5640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -108.000 [-108.000, -108.000] - loss: 0.209 - mae: 23.824 - mean_q: -24.659 - prob: 1.000\n",
            "\n",
            "Interval 566 (5650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 567 (5660 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 568 (5670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 569 (5680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 570 (5690 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 571 (5700 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 572 (5710 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 573 (5720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 574 (5730 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 575 (5740 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "1 episodes - episode_reward: -108.000 [-108.000, -108.000] - loss: 1.073 - mae: 24.229 - mean_q: -25.264 - prob: 1.000\n",
            "\n",
            "Interval 576 (5750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 577 (5760 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 578 (5770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 579 (5780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 580 (5790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 581 (5800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 582 (5810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 583 (5820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -3.7000\n",
            "Interval 584 (5830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 585 (5840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "1 episodes - episode_reward: -153.000 [-153.000, -153.000] - loss: 4.760 - mae: 23.954 - mean_q: -24.596 - prob: 1.000\n",
            "\n",
            "Interval 586 (5850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 587 (5860 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 588 (5870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 589 (5880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 590 (5890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 591 (5900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 592 (5910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 593 (5920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 594 (5930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -126.000 [-126.000, -126.000] - loss: 3.825 - mae: 24.246 - mean_q: -25.215 - prob: 1.000\n",
            "\n",
            "Interval 595 (5940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 596 (5950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 597 (5960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 598 (5970 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 599 (5980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 600 (5990 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 601 (6000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 602 (6010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 603 (6020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 604 (6030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -135.000 [-135.000, -135.000] - loss: 1.832 - mae: 24.536 - mean_q: -25.250 - prob: 1.000\n",
            "\n",
            "Interval 605 (6040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 606 (6050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 607 (6060 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 608 (6070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 609 (6080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 610 (6090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 611 (6100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 612 (6110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 613 (6120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 614 (6130 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -108.000 [-108.000, -108.000] - loss: 2.591 - mae: 24.794 - mean_q: -25.792 - prob: 1.000\n",
            "\n",
            "Interval 615 (6140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 616 (6150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 617 (6160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 618 (6170 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 619 (6180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 620 (6190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 621 (6200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 622 (6210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 623 (6220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 624 (6230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -108.000 [-108.000, -108.000] - loss: 1.282 - mae: 24.879 - mean_q: -25.982 - prob: 1.000\n",
            "\n",
            "Interval 625 (6240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 626 (6250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 627 (6260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 628 (6270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 629 (6280 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 630 (6290 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 631 (6300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 632 (6310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 633 (6320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 634 (6330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -117.000 [-117.000, -117.000] - loss: 4.851 - mae: 25.184 - mean_q: -26.087 - prob: 1.000\n",
            "\n",
            "Interval 635 (6340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 636 (6350 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 637 (6360 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 638 (6370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 639 (6380 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 640 (6390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 641 (6400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 642 (6410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 643 (6420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 644 (6430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -135.000 [-135.000, -135.000] - loss: 4.711 - mae: 25.381 - mean_q: -26.426 - prob: 1.000\n",
            "\n",
            "Interval 645 (6440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 646 (6450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 647 (6460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 648 (6470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 649 (6480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 650 (6490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 651 (6500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 652 (6510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 653 (6520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 654 (6530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "1 episodes - episode_reward: -153.000 [-153.000, -153.000] - loss: 3.635 - mae: 25.704 - mean_q: -27.241 - prob: 1.000\n",
            "\n",
            "Interval 655 (6540 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 656 (6550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 657 (6560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 658 (6570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 659 (6580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 660 (6590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 661 (6600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 662 (6610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 663 (6620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 664 (6630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -108.000 [-108.000, -108.000] - loss: 2.363 - mae: 26.070 - mean_q: -27.117 - prob: 1.000\n",
            "\n",
            "Interval 665 (6640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 666 (6650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 667 (6660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 668 (6670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 669 (6680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 670 (6690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 671 (6700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 672 (6710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 673 (6720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 674 (6730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -153.000 [-153.000, -153.000] - loss: 2.037 - mae: 26.110 - mean_q: -27.247 - prob: 1.000\n",
            "\n",
            "Interval 675 (6740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 676 (6750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 677 (6760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 678 (6770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 679 (6780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 680 (6790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 681 (6800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 682 (6810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 683 (6820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 684 (6830 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -108.000 [-108.000, -108.000] - loss: 9.414 - mae: 26.160 - mean_q: -26.972 - prob: 1.000\n",
            "\n",
            "Interval 685 (6840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 686 (6850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 687 (6860 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 688 (6870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 689 (6880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 690 (6890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 691 (6900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 692 (6910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 693 (6920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -135.000 [-135.000, -135.000] - loss: 0.469 - mae: 26.118 - mean_q: -27.489 - prob: 1.000\n",
            "\n",
            "Interval 694 (6930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 695 (6940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 696 (6950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 697 (6960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 698 (6970 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 699 (6980 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 700 (6990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 701 (7000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 702 (7010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 703 (7020 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -108.000 [-108.000, -108.000] - loss: 3.367 - mae: 25.902 - mean_q: -26.997 - prob: 1.000\n",
            "\n",
            "Interval 704 (7030 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 705 (7040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 706 (7050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 707 (7060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 708 (7070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 709 (7080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 710 (7090 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 711 (7100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 712 (7110 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 713 (7120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "1 episodes - episode_reward: -144.000 [-144.000, -144.000] - loss: 2.989 - mae: 26.158 - mean_q: -27.208 - prob: 1.000\n",
            "\n",
            "Interval 714 (7130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 715 (7140 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 716 (7150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 717 (7160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 718 (7170 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 719 (7180 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 720 (7190 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -2.8000\n",
            "Interval 721 (7200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 722 (7210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 723 (7220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "1 episodes - episode_reward: -144.000 [-144.000, -144.000] - loss: 2.817 - mae: 26.516 - mean_q: -27.460 - prob: 1.000\n",
            "\n",
            "Interval 724 (7230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 725 (7240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 726 (7250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 727 (7260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 728 (7270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 729 (7280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 730 (7290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 731 (7300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 732 (7310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 733 (7320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -117.000 [-117.000, -117.000] - loss: 1.280 - mae: 26.722 - mean_q: -28.020 - prob: 1.000\n",
            "\n",
            "Interval 734 (7330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 735 (7340 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 736 (7350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 737 (7360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 738 (7370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 739 (7380 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 740 (7390 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 741 (7400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 742 (7410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 743 (7420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -108.000 [-108.000, -108.000] - loss: 6.031 - mae: 26.539 - mean_q: -27.570 - prob: 1.000\n",
            "\n",
            "Interval 744 (7430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 745 (7440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 746 (7450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 747 (7460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 748 (7470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 749 (7480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 750 (7490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 751 (7500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 752 (7510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 753 (7520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "1 episodes - episode_reward: -126.000 [-126.000, -126.000] - loss: 4.009 - mae: 26.784 - mean_q: -27.793 - prob: 1.000\n",
            "\n",
            "Interval 754 (7530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 755 (7540 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 756 (7550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 757 (7560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 758 (7570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 759 (7580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 760 (7590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 761 (7600 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 762 (7610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 763 (7620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "1 episodes - episode_reward: -126.000 [-126.000, -126.000] - loss: 2.714 - mae: 26.839 - mean_q: -28.111 - prob: 1.000\n",
            "\n",
            "Interval 764 (7630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 765 (7640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 766 (7650 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 767 (7660 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 768 (7670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 769 (7680 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 770 (7690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -3.7000\n",
            "Interval 771 (7700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 772 (7710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 773 (7720 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -171.000 [-171.000, -171.000] - loss: 1.432 - mae: 26.991 - mean_q: -28.091 - prob: 1.000\n",
            "\n",
            "Interval 774 (7730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 775 (7740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 776 (7750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 777 (7760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 778 (7770 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 779 (7780 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 780 (7790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 781 (7800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 782 (7810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 783 (7820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -108.000 [-108.000, -108.000] - loss: 3.757 - mae: 27.310 - mean_q: -28.612 - prob: 1.000\n",
            "\n",
            "Interval 784 (7830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 785 (7840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 786 (7850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 787 (7860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 788 (7870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 789 (7880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 790 (7890 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 791 (7900 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 792 (7910 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "1 episodes - episode_reward: -126.000 [-126.000, -126.000] - loss: 6.641 - mae: 26.829 - mean_q: -28.097 - prob: 1.000\n",
            "\n",
            "Interval 793 (7920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 794 (7930 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 795 (7940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 796 (7950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 797 (7960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 798 (7970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 799 (7980 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 800 (7990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 801 (8000 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 802 (8010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -135.000 [-135.000, -135.000] - loss: 3.508 - mae: 27.159 - mean_q: -28.262 - prob: 1.000\n",
            "\n",
            "Interval 803 (8020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 804 (8030 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 805 (8040 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 806 (8050 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 807 (8060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 808 (8070 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 809 (8080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 810 (8090 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 811 (8100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 812 (8110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "1 episodes - episode_reward: -117.000 [-117.000, -117.000] - loss: 1.741 - mae: 26.890 - mean_q: -28.412 - prob: 1.000\n",
            "\n",
            "Interval 813 (8120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 814 (8130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 815 (8140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 816 (8150 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 817 (8160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 818 (8170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 819 (8180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 820 (8190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 821 (8200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 822 (8210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -126.000 [-126.000, -126.000] - loss: 4.189 - mae: 27.674 - mean_q: -28.895 - prob: 1.000\n",
            "\n",
            "Interval 823 (8220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 824 (8230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 825 (8240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 826 (8250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 827 (8260 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 828 (8270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 829 (8280 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 830 (8290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 831 (8300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 832 (8310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -117.000 [-117.000, -117.000] - loss: 5.042 - mae: 27.228 - mean_q: -28.645 - prob: 1.000\n",
            "\n",
            "Interval 833 (8320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 834 (8330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 835 (8340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 836 (8350 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 837 (8360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 838 (8370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 839 (8380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 840 (8390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 841 (8400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 842 (8410 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -117.000 [-117.000, -117.000] - loss: 3.315 - mae: 27.829 - mean_q: -29.315 - prob: 1.000\n",
            "\n",
            "Interval 843 (8420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 844 (8430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 845 (8440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 846 (8450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 847 (8460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 848 (8470 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 849 (8480 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 850 (8490 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 851 (8500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 852 (8510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -135.000 [-135.000, -135.000] - loss: 2.953 - mae: 27.698 - mean_q: -29.127 - prob: 1.000\n",
            "\n",
            "Interval 853 (8520 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 854 (8530 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 855 (8540 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 856 (8550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 857 (8560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 858 (8570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 859 (8580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 860 (8590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 861 (8600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 862 (8610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -117.000 [-117.000, -117.000] - loss: 1.648 - mae: 28.285 - mean_q: -29.793 - prob: 1.000\n",
            "\n",
            "Interval 863 (8620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 864 (8630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 865 (8640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 866 (8650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 867 (8660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 868 (8670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 869 (8680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 870 (8690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 871 (8700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 872 (8710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -108.000 [-108.000, -108.000] - loss: 1.899 - mae: 28.246 - mean_q: -29.659 - prob: 1.000\n",
            "\n",
            "Interval 873 (8720 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 874 (8730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 875 (8740 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 876 (8750 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 877 (8760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 878 (8770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 879 (8780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 880 (8790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 881 (8800 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 882 (8810 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -3.7000\n",
            "1 episodes - episode_reward: -126.000 [-126.000, -126.000] - loss: 5.322 - mae: 28.217 - mean_q: -30.149 - prob: 1.000\n",
            "\n",
            "Interval 883 (8820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 884 (8830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 885 (8840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 886 (8850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 887 (8860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 888 (8870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 889 (8880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 890 (8890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 891 (8900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -144.000 [-144.000, -144.000] - loss: 0.420 - mae: 28.718 - mean_q: -30.309 - prob: 1.000\n",
            "\n",
            "Interval 892 (8910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 893 (8920 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 894 (8930 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 895 (8940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 896 (8950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 897 (8960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 898 (8970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 899 (8980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 900 (8990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 901 (9000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -126.000 [-126.000, -126.000] - loss: 7.092 - mae: 27.740 - mean_q: -29.212 - prob: 1.000\n",
            "\n",
            "Interval 902 (9010 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 903 (9020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 904 (9030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 905 (9040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 906 (9050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 907 (9060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 908 (9070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 909 (9080 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 910 (9090 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 911 (9100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -99.000 [-99.000, -99.000] - loss: 4.256 - mae: 28.596 - mean_q: -30.161 - prob: 1.000\n",
            "\n",
            "Interval 912 (9110 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 913 (9120 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 914 (9130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 915 (9140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 916 (9150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 917 (9160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 918 (9170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 919 (9180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 920 (9190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 921 (9200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "1 episodes - episode_reward: -135.000 [-135.000, -135.000] - loss: 4.889 - mae: 27.932 - mean_q: -29.400 - prob: 1.000\n",
            "\n",
            "Interval 922 (9210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 923 (9220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 924 (9230 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 925 (9240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 926 (9250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 927 (9260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 928 (9270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 929 (9280 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 930 (9290 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 931 (9300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -108.000 [-108.000, -108.000] - loss: 6.327 - mae: 27.899 - mean_q: -29.531 - prob: 1.000\n",
            "\n",
            "Interval 932 (9310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 933 (9320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 934 (9330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 935 (9340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 936 (9350 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 937 (9360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 938 (9370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 939 (9380 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 940 (9390 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 941 (9400 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -108.000 [-108.000, -108.000] - loss: 0.494 - mae: 29.100 - mean_q: -30.693 - prob: 1.000\n",
            "\n",
            "Interval 942 (9410 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.0000\n",
            "Interval 943 (9420 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 944 (9430 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: -1.9000\n",
            "Interval 945 (9440 steps performed)\n",
            "10/10 [==============================] - 0s 14ms/step - reward: -1.0000\n",
            "Interval 946 (9450 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: -1.0000\n",
            "Interval 947 (9460 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: -1.0000\n",
            "Interval 948 (9470 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: -1.0000\n",
            "Interval 949 (9480 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: -1.9000\n",
            "Interval 950 (9490 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 951 (9500 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.9000\n",
            "1 episodes - episode_reward: -126.000 [-126.000, -126.000] - loss: 4.236 - mae: 29.437 - mean_q: -31.129 - prob: 1.000\n",
            "\n",
            "Interval 952 (9510 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 953 (9520 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 954 (9530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 955 (9540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 956 (9550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 957 (9560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 958 (9570 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 959 (9580 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 960 (9590 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 961 (9600 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -126.000 [-126.000, -126.000] - loss: 5.743 - mae: 28.523 - mean_q: -29.824 - prob: 1.000\n",
            "\n",
            "Interval 962 (9610 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 963 (9620 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 964 (9630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 965 (9640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 966 (9650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 967 (9660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 968 (9670 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 969 (9680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 970 (9690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 971 (9700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -135.000 [-135.000, -135.000] - loss: 3.958 - mae: 28.854 - mean_q: -30.220 - prob: 1.000\n",
            "\n",
            "Interval 972 (9710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 973 (9720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 974 (9730 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 975 (9740 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 976 (9750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 977 (9760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 978 (9770 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 979 (9780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 980 (9790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 981 (9800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -144.000 [-144.000, -144.000] - loss: 4.918 - mae: 29.253 - mean_q: -30.986 - prob: 1.000\n",
            "\n",
            "Interval 982 (9810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 983 (9820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 984 (9830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 985 (9840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 986 (9850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 987 (9860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 988 (9870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 989 (9880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 990 (9890 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.9000\n",
            "1 episodes - episode_reward: -126.000 [-126.000, -126.000] - loss: 10.125 - mae: 28.561 - mean_q: -29.907 - prob: 1.000\n",
            "\n",
            "Interval 991 (9900 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: -1.0000\n",
            "Interval 992 (9910 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 993 (9920 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 994 (9930 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 995 (9940 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 996 (9950 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 997 (9960 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 998 (9970 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 999 (9980 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 1000 (9990 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.9000\n",
            "1 episodes - episode_reward: -126.000 [-126.000, -126.000] - loss: 1.868 - mae: 28.831 - mean_q: -30.555 - prob: 1.000\n",
            "\n",
            "Interval 1001 (10000 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 1002 (10010 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 1003 (10020 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 1004 (10030 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 1005 (10040 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 1006 (10050 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 1007 (10060 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 1008 (10070 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.9000\n",
            "Interval 1009 (10080 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.9000\n",
            "Interval 1010 (10090 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -135.000 [-135.000, -135.000] - loss: 6.355 - mae: 29.352 - mean_q: -30.768 - prob: 1.000\n",
            "\n",
            "Interval 1011 (10100 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.0000\n",
            "Interval 1012 (10110 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.9000\n",
            "Interval 1013 (10120 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: -1.0000\n",
            "Interval 1014 (10130 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 1015 (10140 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 1016 (10150 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 1017 (10160 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 1018 (10170 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 1019 (10180 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 1020 (10190 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -117.000 [-117.000, -117.000] - loss: 5.834 - mae: 29.169 - mean_q: -30.686 - prob: 1.000\n",
            "\n",
            "Interval 1021 (10200 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 1022 (10210 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 1023 (10220 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 1024 (10230 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 1025 (10240 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 1026 (10250 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 1027 (10260 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 1028 (10270 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: -1.0000\n",
            "Interval 1029 (10280 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.9000\n",
            "Interval 1030 (10290 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -126.000 [-126.000, -126.000] - loss: 3.902 - mae: 28.963 - mean_q: -30.690 - prob: 1.000\n",
            "\n",
            "Interval 1031 (10300 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 1032 (10310 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 1033 (10320 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 1034 (10330 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.9000\n",
            "Interval 1035 (10340 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 1036 (10350 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 1037 (10360 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.0000\n",
            "Interval 1038 (10370 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 1039 (10380 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 1040 (10390 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -126.000 [-126.000, -126.000] - loss: 3.039 - mae: 29.399 - mean_q: -31.226 - prob: 1.000\n",
            "\n",
            "Interval 1041 (10400 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 1042 (10410 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 1043 (10420 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 1044 (10430 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 1045 (10440 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 1046 (10450 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 1047 (10460 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: -1.0000\n",
            "Interval 1048 (10470 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 1049 (10480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1050 (10490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -108.000 [-108.000, -108.000] - loss: 9.361 - mae: 29.038 - mean_q: -30.818 - prob: 1.000\n",
            "\n",
            "Interval 1051 (10500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1052 (10510 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1053 (10520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1054 (10530 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1055 (10540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1056 (10550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1057 (10560 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1058 (10570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1059 (10580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1060 (10590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -108.000 [-108.000, -108.000] - loss: 2.339 - mae: 29.591 - mean_q: -31.546 - prob: 1.000\n",
            "\n",
            "Interval 1061 (10600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 1062 (10610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1063 (10620 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1064 (10630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1065 (10640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1066 (10650 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 1067 (10660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1068 (10670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1069 (10680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1070 (10690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -126.000 [-126.000, -126.000] - loss: 6.726 - mae: 29.562 - mean_q: -31.071 - prob: 1.000\n",
            "\n",
            "Interval 1071 (10700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1072 (10710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1073 (10720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1074 (10730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1075 (10740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 1076 (10750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1077 (10760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1078 (10770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1079 (10780 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1080 (10790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -135.000 [-135.000, -135.000] - loss: 5.894 - mae: 29.945 - mean_q: -31.691 - prob: 1.000\n",
            "\n",
            "Interval 1081 (10800 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1082 (10810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1083 (10820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1084 (10830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1085 (10840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1086 (10850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1087 (10860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1088 (10870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1089 (10880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -108.000 [-108.000, -108.000] - loss: 8.080 - mae: 29.571 - mean_q: -30.940 - prob: 1.000\n",
            "\n",
            "Interval 1090 (10890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1091 (10900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1092 (10910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1093 (10920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1094 (10930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1095 (10940 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1096 (10950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1097 (10960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -4.6000\n",
            "Interval 1098 (10970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1099 (10980 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -144.000 [-144.000, -144.000] - loss: 7.336 - mae: 30.120 - mean_q: -31.708 - prob: 1.000\n",
            "\n",
            "Interval 1100 (10990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1101 (11000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 1102 (11010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1103 (11020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1104 (11030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1105 (11040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1106 (11050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1107 (11060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1108 (11070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1109 (11080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "1 episodes - episode_reward: -153.000 [-153.000, -153.000] - loss: 6.708 - mae: 29.734 - mean_q: -31.401 - prob: 1.000\n",
            "\n",
            "Interval 1110 (11090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1111 (11100 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1112 (11110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1113 (11120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1114 (11130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1115 (11140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1116 (11150 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 1117 (11160 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 1118 (11170 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 1119 (11180 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -3.7000\n",
            "1 episodes - episode_reward: -162.000 [-162.000, -162.000] - loss: 3.351 - mae: 29.572 - mean_q: -31.331 - prob: 1.000\n",
            "\n",
            "Interval 1120 (11190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1121 (11200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1122 (11210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1123 (11220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1124 (11230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1125 (11240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1126 (11250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1127 (11260 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1128 (11270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1129 (11280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "1 episodes - episode_reward: -144.000 [-144.000, -144.000] - loss: 10.895 - mae: 29.792 - mean_q: -31.555 - prob: 1.000\n",
            "\n",
            "Interval 1130 (11290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1131 (11300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1132 (11310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1133 (11320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1134 (11330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1135 (11340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1136 (11350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1137 (11360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1138 (11370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1139 (11380 steps performed)\n",
            "10/10 [==============================] - 0s 15ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -144.000 [-144.000, -144.000] - loss: 3.064 - mae: 29.578 - mean_q: -31.120 - prob: 1.000\n",
            "\n",
            "Interval 1140 (11390 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 1141 (11400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1142 (11410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1143 (11420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1144 (11430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1145 (11440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1146 (11450 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1147 (11460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1148 (11470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1149 (11480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -108.000 [-108.000, -108.000] - loss: 1.994 - mae: 29.655 - mean_q: -31.907 - prob: 1.000\n",
            "\n",
            "Interval 1150 (11490 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1151 (11500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1152 (11510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1153 (11520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1154 (11530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1155 (11540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1156 (11550 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 1157 (11560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1158 (11570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1159 (11580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -117.000 [-117.000, -117.000] - loss: 4.597 - mae: 29.820 - mean_q: -31.419 - prob: 1.000\n",
            "\n",
            "Interval 1160 (11590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1161 (11600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1162 (11610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1163 (11620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1164 (11630 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1165 (11640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1166 (11650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1167 (11660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1168 (11670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1169 (11680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -126.000 [-126.000, -126.000] - loss: 2.347 - mae: 29.388 - mean_q: -30.968 - prob: 1.000\n",
            "\n",
            "Interval 1170 (11690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1171 (11700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1172 (11710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1173 (11720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1174 (11730 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1175 (11740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1176 (11750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1177 (11760 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1178 (11770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1179 (11780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -126.000 [-126.000, -126.000] - loss: 1.027 - mae: 29.155 - mean_q: -30.635 - prob: 1.000\n",
            "\n",
            "Interval 1180 (11790 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1181 (11800 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1182 (11810 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1183 (11820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1184 (11830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 1185 (11840 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1186 (11850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1187 (11860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1188 (11870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -135.000 [-135.000, -135.000] - loss: 2.236 - mae: 29.090 - mean_q: -30.840 - prob: 1.000\n",
            "\n",
            "Interval 1189 (11880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1190 (11890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1191 (11900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1192 (11910 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1193 (11920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1194 (11930 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1195 (11940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1196 (11950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1197 (11960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1198 (11970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "1 episodes - episode_reward: -171.000 [-171.000, -171.000] - loss: 2.874 - mae: 29.464 - mean_q: -31.029 - prob: 1.000\n",
            "\n",
            "Interval 1199 (11980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1200 (11990 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1201 (12000 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1202 (12010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1203 (12020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1204 (12030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1205 (12040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1206 (12050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1207 (12060 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1208 (12070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -117.000 [-117.000, -117.000] - loss: 3.145 - mae: 29.471 - mean_q: -30.978 - prob: 1.000\n",
            "\n",
            "Interval 1209 (12080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1210 (12090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1211 (12100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1212 (12110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1213 (12120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1214 (12130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1215 (12140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1216 (12150 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1217 (12160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1218 (12170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -99.000 [-99.000, -99.000] - loss: 5.894 - mae: 29.552 - mean_q: -31.004 - prob: 1.000\n",
            "\n",
            "Interval 1219 (12180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1220 (12190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1221 (12200 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1222 (12210 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1223 (12220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1224 (12230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1225 (12240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1226 (12250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1227 (12260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1228 (12270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -126.000 [-126.000, -126.000] - loss: 7.055 - mae: 29.220 - mean_q: -30.630 - prob: 1.000\n",
            "\n",
            "Interval 1229 (12280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1230 (12290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1231 (12300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1232 (12310 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1233 (12320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1234 (12330 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1235 (12340 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1236 (12350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1237 (12360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1238 (12370 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "1 episodes - episode_reward: -117.000 [-117.000, -117.000] - loss: 6.129 - mae: 29.069 - mean_q: -30.991 - prob: 1.000\n",
            "\n",
            "Interval 1239 (12380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1240 (12390 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 1241 (12400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1242 (12410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1243 (12420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1244 (12430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1245 (12440 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1246 (12450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1247 (12460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1248 (12470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "1 episodes - episode_reward: -117.000 [-117.000, -117.000] - loss: 8.783 - mae: 29.010 - mean_q: -30.377 - prob: 1.000\n",
            "\n",
            "Interval 1249 (12480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1250 (12490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1251 (12500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1252 (12510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1253 (12520 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1254 (12530 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1255 (12540 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1256 (12550 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1257 (12560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1258 (12570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "1 episodes - episode_reward: -144.000 [-144.000, -144.000] - loss: 3.423 - mae: 28.890 - mean_q: -30.152 - prob: 1.000\n",
            "\n",
            "Interval 1259 (12580 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1260 (12590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1261 (12600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1262 (12610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1263 (12620 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1264 (12630 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1265 (12640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1266 (12650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1267 (12660 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1268 (12670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "1 episodes - episode_reward: -135.000 [-135.000, -135.000] - loss: 2.825 - mae: 29.395 - mean_q: -30.983 - prob: 1.000\n",
            "\n",
            "Interval 1269 (12680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1270 (12690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1271 (12700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1272 (12710 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1273 (12720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1274 (12730 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1275 (12740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1276 (12750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1277 (12760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1278 (12770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -126.000 [-126.000, -126.000] - loss: 3.182 - mae: 28.634 - mean_q: -30.194 - prob: 1.000\n",
            "\n",
            "Interval 1279 (12780 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1280 (12790 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1281 (12800 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1282 (12810 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1283 (12820 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1284 (12830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1285 (12840 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1286 (12850 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1287 (12860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "1 episodes - episode_reward: -144.000 [-144.000, -144.000] - loss: 6.811 - mae: 28.853 - mean_q: -30.336 - prob: 1.000\n",
            "\n",
            "Interval 1288 (12870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1289 (12880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1290 (12890 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1291 (12900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1292 (12910 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1293 (12920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1294 (12930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1295 (12940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1296 (12950 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1297 (12960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -108.000 [-108.000, -108.000] - loss: 4.341 - mae: 28.926 - mean_q: -30.216 - prob: 1.000\n",
            "\n",
            "Interval 1298 (12970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1299 (12980 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1300 (12990 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1301 (13000 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1302 (13010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1303 (13020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1304 (13030 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1305 (13040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1306 (13050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1307 (13060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "1 episodes - episode_reward: -126.000 [-126.000, -126.000] - loss: 6.496 - mae: 29.300 - mean_q: -30.429 - prob: 1.000\n",
            "\n",
            "Interval 1308 (13070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1309 (13080 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1310 (13090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1311 (13100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1312 (13110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1313 (13120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1314 (13130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1315 (13140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1316 (13150 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 1317 (13160 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "1 episodes - episode_reward: -153.000 [-153.000, -153.000] - loss: 4.777 - mae: 29.311 - mean_q: -30.877 - prob: 1.000\n",
            "\n",
            "Interval 1318 (13170 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 1319 (13180 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 1320 (13190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1321 (13200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1322 (13210 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 1323 (13220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1324 (13230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1325 (13240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1326 (13250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1327 (13260 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -144.000 [-144.000, -144.000] - loss: 2.535 - mae: 28.471 - mean_q: -30.031 - prob: 1.000\n",
            "\n",
            "Interval 1328 (13270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1329 (13280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1330 (13290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1331 (13300 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1332 (13310 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1333 (13320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1334 (13330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1335 (13340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1336 (13350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1337 (13360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -117.000 [-117.000, -117.000] - loss: 5.119 - mae: 28.364 - mean_q: -30.354 - prob: 1.000\n",
            "\n",
            "Interval 1338 (13370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1339 (13380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1340 (13390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1341 (13400 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1342 (13410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1343 (13420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1344 (13430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1345 (13440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1346 (13450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1347 (13460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -135.000 [-135.000, -135.000] - loss: 3.067 - mae: 28.685 - mean_q: -29.940 - prob: 1.000\n",
            "\n",
            "Interval 1348 (13470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1349 (13480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1350 (13490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1351 (13500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1352 (13510 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1353 (13520 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1354 (13530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1355 (13540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1356 (13550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1357 (13560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -99.000 [-99.000, -99.000] - loss: 4.206 - mae: 28.783 - mean_q: -30.232 - prob: 1.000\n",
            "\n",
            "Interval 1358 (13570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1359 (13580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1360 (13590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1361 (13600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1362 (13610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1363 (13620 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 1364 (13630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1365 (13640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1366 (13650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1367 (13660 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -99.000 [-99.000, -99.000] - loss: 3.884 - mae: 28.283 - mean_q: -29.811 - prob: 1.000\n",
            "\n",
            "Interval 1368 (13670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1369 (13680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1370 (13690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1371 (13700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1372 (13710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1373 (13720 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1374 (13730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1375 (13740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1376 (13750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1377 (13760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -117.000 [-117.000, -117.000] - loss: 5.266 - mae: 29.228 - mean_q: -30.687 - prob: 1.000\n",
            "\n",
            "Interval 1378 (13770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1379 (13780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1380 (13790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1381 (13800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1382 (13810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1383 (13820 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1384 (13830 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1385 (13840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1386 (13850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -108.000 [-108.000, -108.000] - loss: 8.253 - mae: 29.050 - mean_q: -30.567 - prob: 1.000\n",
            "\n",
            "Interval 1387 (13860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1388 (13870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1389 (13880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1390 (13890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1391 (13900 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1392 (13910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1393 (13920 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1394 (13930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1395 (13940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1396 (13950 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -108.000 [-108.000, -108.000] - loss: 3.071 - mae: 28.719 - mean_q: -30.314 - prob: 1.000\n",
            "\n",
            "Interval 1397 (13960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1398 (13970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1399 (13980 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1400 (13990 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1401 (14000 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1402 (14010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1403 (14020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1404 (14030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1405 (14040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1406 (14050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "1 episodes - episode_reward: -135.000 [-135.000, -135.000] - loss: 3.670 - mae: 28.349 - mean_q: -29.534 - prob: 1.000\n",
            "\n",
            "Interval 1407 (14060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1408 (14070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1409 (14080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1410 (14090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1411 (14100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1412 (14110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1413 (14120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1414 (14130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1415 (14140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1416 (14150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -117.000 [-117.000, -117.000] - loss: 4.329 - mae: 28.595 - mean_q: -30.110 - prob: 1.000\n",
            "\n",
            "Interval 1417 (14160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1418 (14170 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1419 (14180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1420 (14190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1421 (14200 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1422 (14210 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1423 (14220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1424 (14230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1425 (14240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1426 (14250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -135.000 [-135.000, -135.000] - loss: 7.717 - mae: 28.173 - mean_q: -29.514 - prob: 1.000\n",
            "\n",
            "Interval 1427 (14260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1428 (14270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1429 (14280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1430 (14290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1431 (14300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1432 (14310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1433 (14320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1434 (14330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1435 (14340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1436 (14350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -126.000 [-126.000, -126.000] - loss: 5.829 - mae: 27.966 - mean_q: -29.099 - prob: 1.000\n",
            "\n",
            "Interval 1437 (14360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1438 (14370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1439 (14380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1440 (14390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1441 (14400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1442 (14410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1443 (14420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1444 (14430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1445 (14440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1446 (14450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -117.000 [-117.000, -117.000] - loss: 8.712 - mae: 27.950 - mean_q: -29.281 - prob: 1.000\n",
            "\n",
            "Interval 1447 (14460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1448 (14470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1449 (14480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1450 (14490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1451 (14500 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 1452 (14510 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1453 (14520 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1454 (14530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1455 (14540 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1456 (14550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -117.000 [-117.000, -117.000] - loss: 4.184 - mae: 28.141 - mean_q: -29.620 - prob: 1.000\n",
            "\n",
            "Interval 1457 (14560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1458 (14570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1459 (14580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1460 (14590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1461 (14600 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1462 (14610 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1463 (14620 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1464 (14630 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1465 (14640 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 1466 (14650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "1 episodes - episode_reward: -126.000 [-126.000, -126.000] - loss: 3.524 - mae: 28.693 - mean_q: -29.990 - prob: 1.000\n",
            "\n",
            "Interval 1467 (14660 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1468 (14670 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1469 (14680 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1470 (14690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 1471 (14700 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1472 (14710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1473 (14720 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1474 (14730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1475 (14740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1476 (14750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -144.000 [-144.000, -144.000] - loss: 11.235 - mae: 27.632 - mean_q: -28.928 - prob: 1.000\n",
            "\n",
            "Interval 1477 (14760 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1478 (14770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1479 (14780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1480 (14790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1481 (14800 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1482 (14810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1483 (14820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1484 (14830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1485 (14840 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "1 episodes - episode_reward: -117.000 [-117.000, -117.000] - loss: 7.396 - mae: 28.516 - mean_q: -29.899 - prob: 1.000\n",
            "\n",
            "Interval 1486 (14850 steps performed)\n",
            "10/10 [==============================] - 0s 15ms/step - reward: -1.0000\n",
            "Interval 1487 (14860 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 1488 (14870 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 1489 (14880 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 1490 (14890 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 1491 (14900 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 1492 (14910 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 1493 (14920 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 1494 (14930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1495 (14940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "1 episodes - episode_reward: -117.000 [-117.000, -117.000] - loss: 0.442 - mae: 28.052 - mean_q: -29.477 - prob: 1.000\n",
            "\n",
            "Interval 1496 (14950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1497 (14960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1498 (14970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1499 (14980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1500 (14990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1501 (15000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1502 (15010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1503 (15020 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1504 (15030 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 1505 (15040 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -99.000 [-99.000, -99.000] - loss: 4.234 - mae: 28.253 - mean_q: -29.841 - prob: 1.000\n",
            "\n",
            "Interval 1506 (15050 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 1507 (15060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1508 (15070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1509 (15080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1510 (15090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 1511 (15100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1512 (15110 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 1513 (15120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1514 (15130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1515 (15140 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "1 episodes - episode_reward: -153.000 [-153.000, -153.000] - loss: 2.584 - mae: 28.420 - mean_q: -29.867 - prob: 1.000\n",
            "\n",
            "Interval 1516 (15150 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 1517 (15160 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 1518 (15170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1519 (15180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1520 (15190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1521 (15200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1522 (15210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1523 (15220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1524 (15230 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1525 (15240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "1 episodes - episode_reward: -117.000 [-117.000, -117.000] - loss: 5.032 - mae: 28.147 - mean_q: -29.837 - prob: 1.000\n",
            "\n",
            "Interval 1526 (15250 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1527 (15260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1528 (15270 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1529 (15280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1530 (15290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 1531 (15300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1532 (15310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1533 (15320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1534 (15330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1535 (15340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -135.000 [-135.000, -135.000] - loss: 2.005 - mae: 28.333 - mean_q: -29.684 - prob: 1.000\n",
            "\n",
            "Interval 1536 (15350 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1537 (15360 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1538 (15370 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1539 (15380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1540 (15390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1541 (15400 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1542 (15410 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1543 (15420 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1544 (15430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1545 (15440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -117.000 [-117.000, -117.000] - loss: 5.143 - mae: 28.842 - mean_q: -30.288 - prob: 1.000\n",
            "\n",
            "Interval 1546 (15450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1547 (15460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1548 (15470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1549 (15480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1550 (15490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1551 (15500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1552 (15510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1553 (15520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1554 (15530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1555 (15540 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -108.000 [-108.000, -108.000] - loss: 1.711 - mae: 28.158 - mean_q: -29.732 - prob: 1.000\n",
            "\n",
            "Interval 1556 (15550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1557 (15560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1558 (15570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1559 (15580 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1560 (15590 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -3.7000\n",
            "Interval 1561 (15600 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1562 (15610 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1563 (15620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1564 (15630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1565 (15640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "1 episodes - episode_reward: -135.000 [-135.000, -135.000] - loss: 3.643 - mae: 27.743 - mean_q: -29.435 - prob: 1.000\n",
            "\n",
            "Interval 1566 (15650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1567 (15660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1568 (15670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1569 (15680 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1570 (15690 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1571 (15700 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1572 (15710 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1573 (15720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1574 (15730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1575 (15740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -135.000 [-135.000, -135.000] - loss: 1.761 - mae: 29.089 - mean_q: -30.766 - prob: 1.000\n",
            "\n",
            "Interval 1576 (15750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1577 (15760 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1578 (15770 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1579 (15780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1580 (15790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1581 (15800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1582 (15810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1583 (15820 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 1584 (15830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -126.000 [-126.000, -126.000] - loss: 7.507 - mae: 28.178 - mean_q: -29.359 - prob: 1.000\n",
            "\n",
            "Interval 1585 (15840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1586 (15850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1587 (15860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1588 (15870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1589 (15880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1590 (15890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1591 (15900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1592 (15910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1593 (15920 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1594 (15930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -126.000 [-126.000, -126.000] - loss: 1.335 - mae: 28.453 - mean_q: -29.819 - prob: 1.000\n",
            "\n",
            "Interval 1595 (15940 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1596 (15950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1597 (15960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1598 (15970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1599 (15980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1600 (15990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1601 (16000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1602 (16010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1603 (16020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1604 (16030 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -108.000 [-108.000, -108.000] - loss: 4.715 - mae: 28.272 - mean_q: -29.580 - prob: 1.000\n",
            "\n",
            "Interval 1605 (16040 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 1606 (16050 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 1607 (16060 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -2.8000\n",
            "Interval 1608 (16070 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 1609 (16080 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 1610 (16090 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 1611 (16100 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 1612 (16110 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -2.8000\n",
            "Interval 1613 (16120 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 1614 (16130 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -153.000 [-153.000, -153.000] - loss: 3.298 - mae: 28.344 - mean_q: -29.970 - prob: 1.000\n",
            "\n",
            "Interval 1615 (16140 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 1616 (16150 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 1617 (16160 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.9000\n",
            "Interval 1618 (16170 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 1619 (16180 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 1620 (16190 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 1621 (16200 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 1622 (16210 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 1623 (16220 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 1624 (16230 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -108.000 [-108.000, -108.000] - loss: 7.047 - mae: 28.300 - mean_q: -29.255 - prob: 1.000\n",
            "\n",
            "Interval 1625 (16240 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 1626 (16250 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 1627 (16260 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 1628 (16270 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 1629 (16280 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 1630 (16290 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 1631 (16300 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 1632 (16310 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 1633 (16320 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 1634 (16330 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -117.000 [-117.000, -117.000] - loss: 6.013 - mae: 28.196 - mean_q: -29.290 - prob: 1.000\n",
            "\n",
            "Interval 1635 (16340 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 1636 (16350 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 1637 (16360 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 1638 (16370 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -2.8000\n",
            "Interval 1639 (16380 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 1640 (16390 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 1641 (16400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1642 (16410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1643 (16420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1644 (16430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -162.000 [-162.000, -162.000] - loss: 3.474 - mae: 28.692 - mean_q: -30.157 - prob: 1.000\n",
            "\n",
            "Interval 1645 (16440 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1646 (16450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1647 (16460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1648 (16470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1649 (16480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1650 (16490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1651 (16500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1652 (16510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1653 (16520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1654 (16530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -108.000 [-108.000, -108.000] - loss: 3.860 - mae: 28.921 - mean_q: -30.078 - prob: 1.000\n",
            "\n",
            "Interval 1655 (16540 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1656 (16550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1657 (16560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1658 (16570 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1659 (16580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1660 (16590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1661 (16600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 1662 (16610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1663 (16620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1664 (16630 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "1 episodes - episode_reward: -135.000 [-135.000, -135.000] - loss: 4.880 - mae: 28.517 - mean_q: -29.931 - prob: 1.000\n",
            "\n",
            "Interval 1665 (16640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1666 (16650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1667 (16660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1668 (16670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1669 (16680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1670 (16690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1671 (16700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1672 (16710 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 1673 (16720 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1674 (16730 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -144.000 [-144.000, -144.000] - loss: 4.088 - mae: 29.056 - mean_q: -31.082 - prob: 1.000\n",
            "\n",
            "Interval 1675 (16740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1676 (16750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1677 (16760 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1678 (16770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 1679 (16780 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1680 (16790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1681 (16800 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1682 (16810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 1683 (16820 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -153.000 [-153.000, -153.000] - loss: 5.417 - mae: 29.019 - mean_q: -30.203 - prob: 1.000\n",
            "\n",
            "Interval 1684 (16830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1685 (16840 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 1686 (16850 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 1687 (16860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1688 (16870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1689 (16880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1690 (16890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1691 (16900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1692 (16910 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 1693 (16920 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "1 episodes - episode_reward: -108.000 [-108.000, -108.000] - loss: 4.938 - mae: 29.175 - mean_q: -30.768 - prob: 1.000\n",
            "\n",
            "Interval 1694 (16930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 1695 (16940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1696 (16950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1697 (16960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1698 (16970 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 1699 (16980 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 1700 (16990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1701 (17000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1702 (17010 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1703 (17020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -135.000 [-135.000, -135.000] - loss: 2.008 - mae: 28.903 - mean_q: -30.608 - prob: 1.000\n",
            "\n",
            "Interval 1704 (17030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1705 (17040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1706 (17050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1707 (17060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1708 (17070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1709 (17080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1710 (17090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1711 (17100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1712 (17110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1713 (17120 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -3.7000\n",
            "1 episodes - episode_reward: -126.000 [-126.000, -126.000] - loss: 1.912 - mae: 29.178 - mean_q: -30.905 - prob: 1.000\n",
            "\n",
            "Interval 1714 (17130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1715 (17140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1716 (17150 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1717 (17160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1718 (17170 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1719 (17180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1720 (17190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1721 (17200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1722 (17210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1723 (17220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -144.000 [-144.000, -144.000] - loss: 8.065 - mae: 29.262 - mean_q: -30.920 - prob: 1.000\n",
            "\n",
            "Interval 1724 (17230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1725 (17240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1726 (17250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1727 (17260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1728 (17270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1729 (17280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1730 (17290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1731 (17300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1732 (17310 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 1733 (17320 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -117.000 [-117.000, -117.000] - loss: 4.597 - mae: 28.490 - mean_q: -29.697 - prob: 1.000\n",
            "\n",
            "Interval 1734 (17330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1735 (17340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1736 (17350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1737 (17360 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 1738 (17370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1739 (17380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1740 (17390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1741 (17400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1742 (17410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1743 (17420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "1 episodes - episode_reward: -144.000 [-144.000, -144.000] - loss: 5.502 - mae: 28.840 - mean_q: -30.170 - prob: 1.000\n",
            "\n",
            "Interval 1744 (17430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1745 (17440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1746 (17450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1747 (17460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1748 (17470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1749 (17480 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1750 (17490 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 1751 (17500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1752 (17510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1753 (17520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -117.000 [-117.000, -117.000] - loss: 5.797 - mae: 28.421 - mean_q: -29.789 - prob: 1.000\n",
            "\n",
            "Interval 1754 (17530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1755 (17540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1756 (17550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1757 (17560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1758 (17570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1759 (17580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1760 (17590 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1761 (17600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1762 (17610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1763 (17620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "1 episodes - episode_reward: -126.000 [-126.000, -126.000] - loss: 3.300 - mae: 28.748 - mean_q: -30.256 - prob: 1.000\n",
            "\n",
            "Interval 1764 (17630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 1765 (17640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1766 (17650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1767 (17660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1768 (17670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1769 (17680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1770 (17690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1771 (17700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1772 (17710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1773 (17720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -144.000 [-144.000, -144.000] - loss: 1.435 - mae: 28.334 - mean_q: -29.755 - prob: 1.000\n",
            "\n",
            "Interval 1774 (17730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1775 (17740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1776 (17750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1777 (17760 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1778 (17770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1779 (17780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1780 (17790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1781 (17800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1782 (17810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "1 episodes - episode_reward: -135.000 [-135.000, -135.000] - loss: 2.008 - mae: 29.348 - mean_q: -30.828 - prob: 1.000\n",
            "\n",
            "Interval 1783 (17820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1784 (17830 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 1785 (17840 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 1786 (17850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1787 (17860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1788 (17870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1789 (17880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1790 (17890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1791 (17900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1792 (17910 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -117.000 [-117.000, -117.000] - loss: 3.846 - mae: 29.205 - mean_q: -30.586 - prob: 1.000\n",
            "\n",
            "Interval 1793 (17920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1794 (17930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1795 (17940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1796 (17950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1797 (17960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1798 (17970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1799 (17980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1800 (17990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1801 (18000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1802 (18010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -117.000 [-117.000, -117.000] - loss: 7.572 - mae: 28.708 - mean_q: -29.860 - prob: 1.000\n",
            "\n",
            "Interval 1803 (18020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1804 (18030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1805 (18040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1806 (18050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1807 (18060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 1808 (18070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1809 (18080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1810 (18090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1811 (18100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1812 (18110 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "1 episodes - episode_reward: -144.000 [-144.000, -144.000] - loss: 3.349 - mae: 29.218 - mean_q: -31.013 - prob: 1.000\n",
            "\n",
            "Interval 1813 (18120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1814 (18130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1815 (18140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1816 (18150 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1817 (18160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 1818 (18170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1819 (18180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1820 (18190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1821 (18200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1822 (18210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "1 episodes - episode_reward: -144.000 [-144.000, -144.000] - loss: 3.096 - mae: 28.573 - mean_q: -30.029 - prob: 1.000\n",
            "\n",
            "Interval 1823 (18220 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1824 (18230 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1825 (18240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1826 (18250 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1827 (18260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1828 (18270 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1829 (18280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1830 (18290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1831 (18300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1832 (18310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "1 episodes - episode_reward: -108.000 [-108.000, -108.000] - loss: 6.155 - mae: 28.489 - mean_q: -29.841 - prob: 1.000\n",
            "\n",
            "Interval 1833 (18320 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1834 (18330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1835 (18340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1836 (18350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1837 (18360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1838 (18370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1839 (18380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1840 (18390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1841 (18400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 1842 (18410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "1 episodes - episode_reward: -171.000 [-171.000, -171.000] - loss: 7.019 - mae: 27.837 - mean_q: -28.972 - prob: 1.000\n",
            "\n",
            "Interval 1843 (18420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1844 (18430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1845 (18440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1846 (18450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1847 (18460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1848 (18470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1849 (18480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1850 (18490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1851 (18500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1852 (18510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -117.000 [-117.000, -117.000] - loss: 1.575 - mae: 28.192 - mean_q: -29.739 - prob: 1.000\n",
            "\n",
            "Interval 1853 (18520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 1854 (18530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1855 (18540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1856 (18550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1857 (18560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1858 (18570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1859 (18580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1860 (18590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1861 (18600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1862 (18610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "1 episodes - episode_reward: -153.000 [-153.000, -153.000] - loss: 9.155 - mae: 28.838 - mean_q: -30.175 - prob: 1.000\n",
            "\n",
            "Interval 1863 (18620 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1864 (18630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1865 (18640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1866 (18650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1867 (18660 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1868 (18670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1869 (18680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1870 (18690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1871 (18700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1872 (18710 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -135.000 [-135.000, -135.000] - loss: 5.692 - mae: 29.030 - mean_q: -30.510 - prob: 1.000\n",
            "\n",
            "Interval 1873 (18720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1874 (18730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1875 (18740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1876 (18750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1877 (18760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1878 (18770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1879 (18780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 1880 (18790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1881 (18800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -135.000 [-135.000, -135.000] - loss: 9.515 - mae: 28.575 - mean_q: -29.803 - prob: 1.000\n",
            "\n",
            "Interval 1882 (18810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1883 (18820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1884 (18830 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1885 (18840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1886 (18850 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 1887 (18860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1888 (18870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1889 (18880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1890 (18890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1891 (18900 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -117.000 [-117.000, -117.000] - loss: 3.130 - mae: 28.800 - mean_q: -30.563 - prob: 1.000\n",
            "\n",
            "Interval 1892 (18910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1893 (18920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1894 (18930 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 1895 (18940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1896 (18950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1897 (18960 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 1898 (18970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1899 (18980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 1900 (18990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1901 (19000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -126.000 [-126.000, -126.000] - loss: 6.570 - mae: 28.033 - mean_q: -29.875 - prob: 1.000\n",
            "\n",
            "Interval 1902 (19010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1903 (19020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1904 (19030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1905 (19040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1906 (19050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1907 (19060 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1908 (19070 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1909 (19080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1910 (19090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1911 (19100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -117.000 [-117.000, -117.000] - loss: 7.239 - mae: 28.520 - mean_q: -29.919 - prob: 1.000\n",
            "\n",
            "Interval 1912 (19110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1913 (19120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1914 (19130 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1915 (19140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1916 (19150 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1917 (19160 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1918 (19170 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1919 (19180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1920 (19190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1921 (19200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -126.000 [-126.000, -126.000] - loss: 2.418 - mae: 29.183 - mean_q: -30.730 - prob: 1.000\n",
            "\n",
            "Interval 1922 (19210 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1923 (19220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1924 (19230 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1925 (19240 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1926 (19250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1927 (19260 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1928 (19270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1929 (19280 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1930 (19290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1931 (19300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -126.000 [-126.000, -126.000] - loss: 4.793 - mae: 28.894 - mean_q: -30.608 - prob: 1.000\n",
            "\n",
            "Interval 1932 (19310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1933 (19320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1934 (19330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 1935 (19340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1936 (19350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1937 (19360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1938 (19370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1939 (19380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1940 (19390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1941 (19400 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "1 episodes - episode_reward: -135.000 [-135.000, -135.000] - loss: 4.725 - mae: 29.441 - mean_q: -31.005 - prob: 1.000\n",
            "\n",
            "Interval 1942 (19410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1943 (19420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1944 (19430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1945 (19440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1946 (19450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1947 (19460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1948 (19470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1949 (19480 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1950 (19490 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1951 (19500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "1 episodes - episode_reward: -126.000 [-126.000, -126.000] - loss: 6.156 - mae: 28.836 - mean_q: -30.482 - prob: 1.000\n",
            "\n",
            "Interval 1952 (19510 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1953 (19520 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1954 (19530 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1955 (19540 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1956 (19550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1957 (19560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1958 (19570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1959 (19580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1960 (19590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1961 (19600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -117.000 [-117.000, -117.000] - loss: 3.904 - mae: 29.229 - mean_q: -30.952 - prob: 1.000\n",
            "\n",
            "Interval 1962 (19610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1963 (19620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1964 (19630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1965 (19640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1966 (19650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1967 (19660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1968 (19670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1969 (19680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1970 (19690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1971 (19700 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -153.000 [-153.000, -153.000] - loss: 6.463 - mae: 29.178 - mean_q: -30.840 - prob: 1.000\n",
            "\n",
            "Interval 1972 (19710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1973 (19720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1974 (19730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1975 (19740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1976 (19750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1977 (19760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1978 (19770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1979 (19780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1980 (19790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -108.000 [-108.000, -108.000] - loss: 2.760 - mae: 28.756 - mean_q: -30.488 - prob: 1.000\n",
            "\n",
            "Interval 1981 (19800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1982 (19810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 1983 (19820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1984 (19830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1985 (19840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1986 (19850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1987 (19860 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 1988 (19870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1989 (19880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1990 (19890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -144.000 [-144.000, -144.000] - loss: 5.140 - mae: 29.776 - mean_q: -31.501 - prob: 1.000\n",
            "\n",
            "Interval 1991 (19900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1992 (19910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1993 (19920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1994 (19930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1995 (19940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1996 (19950 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1997 (19960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1998 (19970 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 1999 (19980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2000 (19990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -108.000 [-108.000, -108.000] - loss: 5.293 - mae: 29.225 - mean_q: -30.869 - prob: 1.000\n",
            "\n",
            "Interval 2001 (20000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2002 (20010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2003 (20020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2004 (20030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2005 (20040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2006 (20050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2007 (20060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2008 (20070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2009 (20080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2010 (20090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -117.000 [-117.000, -117.000] - loss: 3.391 - mae: 28.886 - mean_q: -30.343 - prob: 1.000\n",
            "\n",
            "Interval 2011 (20100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2012 (20110 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 2013 (20120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2014 (20130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2015 (20140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 2016 (20150 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 2017 (20160 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 2018 (20170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2019 (20180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2020 (20190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -135.000 [-135.000, -135.000] - loss: 1.663 - mae: 29.533 - mean_q: -31.175 - prob: 1.000\n",
            "\n",
            "Interval 2021 (20200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2022 (20210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2023 (20220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2024 (20230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2025 (20240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2026 (20250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2027 (20260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2028 (20270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2029 (20280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2030 (20290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -135.000 [-135.000, -135.000] - loss: 0.623 - mae: 30.253 - mean_q: -32.239 - prob: 1.000\n",
            "\n",
            "Interval 2031 (20300 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 2032 (20310 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 2033 (20320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2034 (20330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2035 (20340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2036 (20350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2037 (20360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2038 (20370 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 2039 (20380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2040 (20390 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -99.000 [-99.000, -99.000] - loss: 0.437 - mae: 28.899 - mean_q: -30.399 - prob: 1.000\n",
            "\n",
            "Interval 2041 (20400 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 2042 (20410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2043 (20420 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 2044 (20430 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 2045 (20440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2046 (20450 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 2047 (20460 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 2048 (20470 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 2049 (20480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2050 (20490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -135.000 [-135.000, -135.000] - loss: 7.549 - mae: 28.830 - mean_q: -30.400 - prob: 1.000\n",
            "\n",
            "Interval 2051 (20500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2052 (20510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2053 (20520 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 2054 (20530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2055 (20540 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 2056 (20550 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 2057 (20560 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 2058 (20570 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 2059 (20580 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.0000\n",
            "Interval 2060 (20590 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -117.000 [-117.000, -117.000] - loss: 1.722 - mae: 29.335 - mean_q: -30.929 - prob: 1.000\n",
            "\n",
            "Interval 2061 (20600 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 2062 (20610 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 2063 (20620 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 2064 (20630 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 2065 (20640 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 2066 (20650 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 2067 (20660 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.9000\n",
            "Interval 2068 (20670 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 2069 (20680 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 2070 (20690 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -108.000 [-108.000, -108.000] - loss: 8.035 - mae: 29.118 - mean_q: -30.875 - prob: 1.000\n",
            "\n",
            "Interval 2071 (20700 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.9000\n",
            "Interval 2072 (20710 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 2073 (20720 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -2.8000\n",
            "Interval 2074 (20730 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 2075 (20740 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 2076 (20750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2077 (20760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2078 (20770 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -2.8000\n",
            "Interval 2079 (20780 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "1 episodes - episode_reward: -162.000 [-162.000, -162.000] - loss: 2.405 - mae: 28.307 - mean_q: -30.192 - prob: 1.000\n",
            "\n",
            "Interval 2080 (20790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2081 (20800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2082 (20810 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 2083 (20820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2084 (20830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2085 (20840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2086 (20850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2087 (20860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2088 (20870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2089 (20880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -99.000 [-99.000, -99.000] - loss: 4.197 - mae: 28.499 - mean_q: -30.046 - prob: 1.000\n",
            "\n",
            "Interval 2090 (20890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2091 (20900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2092 (20910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2093 (20920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2094 (20930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2095 (20940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2096 (20950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 2097 (20960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2098 (20970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2099 (20980 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -162.000 [-162.000, -162.000] - loss: 4.118 - mae: 28.991 - mean_q: -30.291 - prob: 1.000\n",
            "\n",
            "Interval 2100 (20990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2101 (21000 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 2102 (21010 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 2103 (21020 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 2104 (21030 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 2105 (21040 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 2106 (21050 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 2107 (21060 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 2108 (21070 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 2109 (21080 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -135.000 [-135.000, -135.000] - loss: 3.449 - mae: 29.795 - mean_q: -31.362 - prob: 1.000\n",
            "\n",
            "Interval 2110 (21090 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: -1.9000\n",
            "Interval 2111 (21100 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 2112 (21110 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 2113 (21120 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: -1.0000\n",
            "Interval 2114 (21130 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.0000\n",
            "Interval 2115 (21140 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 2116 (21150 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 2117 (21160 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.0000\n",
            "Interval 2118 (21170 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.0000\n",
            "Interval 2119 (21180 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: -1.9000\n",
            "1 episodes - episode_reward: -117.000 [-117.000, -117.000] - loss: 6.558 - mae: 29.412 - mean_q: -30.905 - prob: 1.000\n",
            "\n",
            "Interval 2120 (21190 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.9000\n",
            "Interval 2121 (21200 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 2122 (21210 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: -1.0000\n",
            "Interval 2123 (21220 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: -1.0000\n",
            "Interval 2124 (21230 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: -1.9000\n",
            "Interval 2125 (21240 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: -1.0000\n",
            "Interval 2126 (21250 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: -1.9000\n",
            "Interval 2127 (21260 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 2128 (21270 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 2129 (21280 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -126.000 [-126.000, -126.000] - loss: 3.849 - mae: 28.974 - mean_q: -30.212 - prob: 1.000\n",
            "\n",
            "Interval 2130 (21290 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 2131 (21300 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: -2.8000\n",
            "Interval 2132 (21310 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: -1.0000\n",
            "Interval 2133 (21320 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.9000\n",
            "Interval 2134 (21330 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: -1.0000\n",
            "Interval 2135 (21340 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 2136 (21350 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 2137 (21360 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.9000\n",
            "Interval 2138 (21370 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 2139 (21380 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -135.000 [-135.000, -135.000] - loss: 3.903 - mae: 28.912 - mean_q: -30.831 - prob: 1.000\n",
            "\n",
            "Interval 2140 (21390 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.9000\n",
            "Interval 2141 (21400 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 2142 (21410 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 2143 (21420 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 2144 (21430 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 2145 (21440 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -2.8000\n",
            "Interval 2146 (21450 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 2147 (21460 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -2.8000\n",
            "Interval 2148 (21470 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 2149 (21480 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -153.000 [-153.000, -153.000] - loss: 6.407 - mae: 29.803 - mean_q: -31.494 - prob: 1.000\n",
            "\n",
            "Interval 2150 (21490 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 2151 (21500 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 2152 (21510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2153 (21520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2154 (21530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2155 (21540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2156 (21550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2157 (21560 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 2158 (21570 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 2159 (21580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -153.000 [-153.000, -153.000] - loss: 9.791 - mae: 29.208 - mean_q: -30.648 - prob: 1.000\n",
            "\n",
            "Interval 2160 (21590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2161 (21600 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 2162 (21610 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 2163 (21620 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 2164 (21630 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 2165 (21640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2166 (21650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2167 (21660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 2168 (21670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2169 (21680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -144.000 [-144.000, -144.000] - loss: 0.584 - mae: 29.748 - mean_q: -31.632 - prob: 1.000\n",
            "\n",
            "Interval 2170 (21690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2171 (21700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2172 (21710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2173 (21720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2174 (21730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2175 (21740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2176 (21750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2177 (21760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2178 (21770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -135.000 [-135.000, -135.000] - loss: 3.470 - mae: 29.714 - mean_q: -31.389 - prob: 1.000\n",
            "\n",
            "Interval 2179 (21780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2180 (21790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2181 (21800 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 2182 (21810 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 2183 (21820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2184 (21830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2185 (21840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2186 (21850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2187 (21860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2188 (21870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -108.000 [-108.000, -108.000] - loss: 2.514 - mae: 29.339 - mean_q: -31.411 - prob: 1.000\n",
            "\n",
            "Interval 2189 (21880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2190 (21890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 2191 (21900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2192 (21910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2193 (21920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2194 (21930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2195 (21940 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 2196 (21950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2197 (21960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2198 (21970 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "1 episodes - episode_reward: -126.000 [-126.000, -126.000] - loss: 9.129 - mae: 29.671 - mean_q: -31.262 - prob: 1.000\n",
            "\n",
            "Interval 2199 (21980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2200 (21990 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 2201 (22000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2202 (22010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2203 (22020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2204 (22030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2205 (22040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2206 (22050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2207 (22060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2208 (22070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -135.000 [-135.000, -135.000] - loss: 3.046 - mae: 30.002 - mean_q: -31.811 - prob: 1.000\n",
            "\n",
            "Interval 2209 (22080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2210 (22090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2211 (22100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2212 (22110 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 2213 (22120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2214 (22130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2215 (22140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2216 (22150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2217 (22160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2218 (22170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -126.000 [-126.000, -126.000] - loss: 2.747 - mae: 29.478 - mean_q: -31.447 - prob: 1.000\n",
            "\n",
            "Interval 2219 (22180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2220 (22190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2221 (22200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2222 (22210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2223 (22220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2224 (22230 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 2225 (22240 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 2226 (22250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2227 (22260 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 2228 (22270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -144.000 [-144.000, -144.000] - loss: 5.865 - mae: 29.937 - mean_q: -31.793 - prob: 1.000\n",
            "\n",
            "Interval 2229 (22280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2230 (22290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2231 (22300 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 2232 (22310 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 2233 (22320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2234 (22330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2235 (22340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 2236 (22350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2237 (22360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2238 (22370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -144.000 [-144.000, -144.000] - loss: 0.685 - mae: 29.949 - mean_q: -31.904 - prob: 1.000\n",
            "\n",
            "Interval 2239 (22380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2240 (22390 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 2241 (22400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2242 (22410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2243 (22420 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 2244 (22430 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 2245 (22440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2246 (22450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2247 (22460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2248 (22470 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -108.000 [-108.000, -108.000] - loss: 3.415 - mae: 29.669 - mean_q: -31.406 - prob: 1.000\n",
            "\n",
            "Interval 2249 (22480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2250 (22490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2251 (22500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2252 (22510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2253 (22520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2254 (22530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2255 (22540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2256 (22550 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 2257 (22560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2258 (22570 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -108.000 [-108.000, -108.000] - loss: 1.951 - mae: 30.598 - mean_q: -32.748 - prob: 1.000\n",
            "\n",
            "Interval 2259 (22580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2260 (22590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2261 (22600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2262 (22610 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 2263 (22620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2264 (22630 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 2265 (22640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2266 (22650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2267 (22660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2268 (22670 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -126.000 [-126.000, -126.000] - loss: 5.997 - mae: 29.644 - mean_q: -31.211 - prob: 1.000\n",
            "\n",
            "Interval 2269 (22680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2270 (22690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2271 (22700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2272 (22710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2273 (22720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2274 (22730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2275 (22740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2276 (22750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2277 (22760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -117.000 [-117.000, -117.000] - loss: 5.259 - mae: 30.727 - mean_q: -32.524 - prob: 1.000\n",
            "\n",
            "Interval 2278 (22770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2279 (22780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2280 (22790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2281 (22800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2282 (22810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2283 (22820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2284 (22830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2285 (22840 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 2286 (22850 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 2287 (22860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -135.000 [-135.000, -135.000] - loss: 11.495 - mae: 30.567 - mean_q: -32.418 - prob: 1.000\n",
            "\n",
            "Interval 2288 (22870 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 2289 (22880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2290 (22890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2291 (22900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2292 (22910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2293 (22920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2294 (22930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2295 (22940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2296 (22950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2297 (22960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -126.000 [-126.000, -126.000] - loss: 7.229 - mae: 30.334 - mean_q: -31.868 - prob: 1.000\n",
            "\n",
            "Interval 2298 (22970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2299 (22980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2300 (22990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2301 (23000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2302 (23010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2303 (23020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2304 (23030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2305 (23040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2306 (23050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2307 (23060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -99.000 [-99.000, -99.000] - loss: 8.001 - mae: 29.600 - mean_q: -31.254 - prob: 1.000\n",
            "\n",
            "Interval 2308 (23070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2309 (23080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2310 (23090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2311 (23100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2312 (23110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2313 (23120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2314 (23130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2315 (23140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2316 (23150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2317 (23160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: -99.000 [-99.000, -99.000] - loss: 5.263 - mae: 29.861 - mean_q: -31.642 - prob: 1.000\n",
            "\n",
            "Interval 2318 (23170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2319 (23180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2320 (23190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2321 (23200 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 2322 (23210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.004 - mae: 7.238 - mean_q: 12.450 - prob: 1.000\n",
            "\n",
            "Interval 27015 (270140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.004 - mae: 7.499 - mean_q: 12.736 - prob: 1.000\n",
            "\n",
            "Interval 27016 (270150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.003 - mae: 7.457 - mean_q: 12.788 - prob: 1.000\n",
            "\n",
            "Interval 27017 (270160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27018 (270170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.003 - mae: 7.355 - mean_q: 12.610 - prob: 1.000\n",
            "\n",
            "Interval 27019 (270180 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.006 - mae: 7.457 - mean_q: 12.750 - prob: 1.000\n",
            "\n",
            "Interval 27020 (270190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.005 - mae: 7.809 - mean_q: 13.333 - prob: 1.000\n",
            "\n",
            "Interval 27021 (270200 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 27022 (270210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.004 - mae: 7.358 - mean_q: 12.771 - prob: 1.000\n",
            "\n",
            "Interval 27023 (270220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.005 - mae: 7.526 - mean_q: 12.837 - prob: 1.000\n",
            "\n",
            "Interval 27024 (270230 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -4.6000\n",
            "Interval 27025 (270240 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -42.000 [-42.000, -42.000] - loss: 0.004 - mae: 7.516 - mean_q: 12.996 - prob: 1.000\n",
            "\n",
            "Interval 27026 (270250 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 27027 (270260 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.210 - mean_q: 12.449 - prob: 1.000\n",
            "\n",
            "Interval 27028 (270270 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 27029 (270280 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.002 - mae: 7.290 - mean_q: 12.642 - prob: 1.000\n",
            "\n",
            "Interval 27030 (270290 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.9000\n",
            "Interval 27031 (270300 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.004 - mae: 7.486 - mean_q: 12.867 - prob: 1.000\n",
            "\n",
            "Interval 27032 (270310 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.003 - mae: 7.225 - mean_q: 12.561 - prob: 1.000\n",
            "\n",
            "Interval 27033 (270320 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.498 - mean_q: 12.887 - prob: 1.000\n",
            "\n",
            "Interval 27034 (270330 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 27035 (270340 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.003 - mae: 7.431 - mean_q: 12.839 - prob: 1.000\n",
            "\n",
            "Interval 27036 (270350 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.003 - mae: 7.291 - mean_q: 12.583 - prob: 1.000\n",
            "\n",
            "Interval 27037 (270360 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.626 - mean_q: 13.047 - prob: 1.000\n",
            "\n",
            "Interval 27038 (270370 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.9000\n",
            "Interval 27039 (270380 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.004 - mae: 7.260 - mean_q: 12.499 - prob: 1.000\n",
            "\n",
            "Interval 27040 (270390 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.003 - mae: 7.497 - mean_q: 12.916 - prob: 1.000\n",
            "\n",
            "Interval 27041 (270400 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 27042 (270410 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.002 - mae: 7.419 - mean_q: 12.743 - prob: 1.000\n",
            "\n",
            "Interval 27043 (270420 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 27044 (270430 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.232 - mean_q: 12.477 - prob: 1.000\n",
            "\n",
            "Interval 27045 (270440 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.252 - mean_q: 12.543 - prob: 1.000\n",
            "\n",
            "Interval 27046 (270450 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 27047 (270460 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.003 - mae: 7.522 - mean_q: 13.084 - prob: 1.000\n",
            "\n",
            "Interval 27048 (270470 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.430 - mean_q: 12.787 - prob: 1.000\n",
            "\n",
            "Interval 27049 (270480 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.004 - mae: 7.393 - mean_q: 12.656 - prob: 1.000\n",
            "\n",
            "Interval 27050 (270490 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.9000\n",
            "Interval 27051 (270500 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.005 - mae: 7.360 - mean_q: 12.823 - prob: 1.000\n",
            "\n",
            "Interval 27052 (270510 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 27053 (270520 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.004 - mae: 7.180 - mean_q: 12.363 - prob: 1.000\n",
            "\n",
            "Interval 27054 (270530 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.003 - mae: 7.192 - mean_q: 12.492 - prob: 1.000\n",
            "\n",
            "Interval 27055 (270540 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.003 - mae: 7.208 - mean_q: 12.554 - prob: 1.000\n",
            "\n",
            "Interval 27056 (270550 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.0000\n",
            "Interval 27057 (270560 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.148 - mean_q: 12.447 - prob: 1.000\n",
            "\n",
            "Interval 27058 (270570 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 27059 (270580 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.003 - mae: 7.555 - mean_q: 12.921 - prob: 1.000\n",
            "\n",
            "Interval 27060 (270590 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.639 - mean_q: 13.079 - prob: 1.000\n",
            "\n",
            "Interval 27061 (270600 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.447 - mean_q: 12.835 - prob: 1.000\n",
            "\n",
            "Interval 27062 (270610 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 27063 (270620 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.002 - mae: 7.180 - mean_q: 12.458 - prob: 1.000\n",
            "\n",
            "Interval 27064 (270630 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 27065 (270640 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.003 - mae: 7.641 - mean_q: 13.065 - prob: 1.000\n",
            "\n",
            "Interval 27066 (270650 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.003 - mae: 7.358 - mean_q: 12.586 - prob: 1.000\n",
            "\n",
            "Interval 27067 (270660 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.0000\n",
            "Interval 27068 (270670 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.005 - mae: 7.421 - mean_q: 12.722 - prob: 1.000\n",
            "\n",
            "Interval 27069 (270680 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.004 - mae: 6.927 - mean_q: 12.029 - prob: 1.000\n",
            "\n",
            "Interval 27070 (270690 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.004 - mae: 7.471 - mean_q: 12.974 - prob: 1.000\n",
            "\n",
            "Interval 27071 (270700 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 27072 (270710 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.005 - mae: 7.588 - mean_q: 12.994 - prob: 1.000\n",
            "\n",
            "Interval 27073 (270720 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.007 - mae: 7.145 - mean_q: 12.391 - prob: 1.000\n",
            "\n",
            "Interval 27074 (270730 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: -1.0000\n",
            "Interval 27075 (270740 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.009 - mae: 7.239 - mean_q: 12.535 - prob: 1.000\n",
            "\n",
            "Interval 27076 (270750 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.008 - mae: 7.405 - mean_q: 12.800 - prob: 1.000\n",
            "\n",
            "Interval 27077 (270760 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.010 - mae: 7.465 - mean_q: 12.813 - prob: 1.000\n",
            "\n",
            "Interval 27078 (270770 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: -1.9000\n",
            "Interval 27079 (270780 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.006 - mae: 7.161 - mean_q: 12.451 - prob: 1.000\n",
            "\n",
            "Interval 27080 (270790 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.0000\n",
            "Interval 27081 (270800 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.006 - mae: 7.205 - mean_q: 12.673 - prob: 1.000\n",
            "\n",
            "Interval 27082 (270810 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.004 - mae: 7.076 - mean_q: 12.312 - prob: 1.000\n",
            "\n",
            "Interval 27083 (270820 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.004 - mae: 7.422 - mean_q: 12.859 - prob: 1.000\n",
            "\n",
            "Interval 27084 (270830 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 27085 (270840 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.003 - mae: 7.281 - mean_q: 12.624 - prob: 1.000\n",
            "\n",
            "Interval 27086 (270850 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 27087 (270860 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.004 - mae: 7.380 - mean_q: 12.880 - prob: 1.000\n",
            "\n",
            "Interval 27088 (270870 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.004 - mae: 7.561 - mean_q: 12.936 - prob: 1.000\n",
            "\n",
            "Interval 27089 (270880 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 27090 (270890 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 7.370 - mean_q: 12.700 - prob: 1.000\n",
            "\n",
            "Interval 27091 (270900 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.004 - mae: 7.171 - mean_q: 12.278 - prob: 1.000\n",
            "\n",
            "Interval 27092 (270910 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 27093 (270920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.003 - mae: 7.597 - mean_q: 13.142 - prob: 1.000\n",
            "\n",
            "Interval 27094 (270930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.316 - mean_q: 12.591 - prob: 1.000\n",
            "\n",
            "Interval 27095 (270940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 27096 (270950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.004 - mae: 7.240 - mean_q: 12.332 - prob: 1.000\n",
            "\n",
            "Interval 27097 (270960 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.004 - mae: 7.235 - mean_q: 12.519 - prob: 1.000\n",
            "\n",
            "Interval 27098 (270970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.002 - mae: 7.227 - mean_q: 12.471 - prob: 1.000\n",
            "\n",
            "Interval 27099 (270980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 27100 (270990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.004 - mae: 7.280 - mean_q: 12.433 - prob: 1.000\n",
            "\n",
            "Interval 27101 (271000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.004 - mae: 7.279 - mean_q: 12.577 - prob: 1.000\n",
            "\n",
            "Interval 27102 (271010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27103 (271020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.371 - mean_q: 12.695 - prob: 1.000\n",
            "\n",
            "Interval 27104 (271030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 7.274 - mean_q: 12.556 - prob: 1.000\n",
            "\n",
            "Interval 27105 (271040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 27106 (271050 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.003 - mae: 7.253 - mean_q: 12.443 - prob: 1.000\n",
            "\n",
            "Interval 27107 (271060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.003 - mae: 7.351 - mean_q: 12.723 - prob: 1.000\n",
            "\n",
            "Interval 27108 (271070 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.149 - mean_q: 12.518 - prob: 1.000\n",
            "\n",
            "Interval 27109 (271080 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 27110 (271090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 27111 (271100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27112 (271110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -29.000 [-29.000, -29.000] - loss: 0.008 - mae: 7.524 - mean_q: 12.951 - prob: 1.000\n",
            "\n",
            "Interval 27113 (271120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27114 (271130 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.005 - mae: 6.969 - mean_q: 12.253 - prob: 1.000\n",
            "\n",
            "Interval 27115 (271140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27116 (271150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.004 - mae: 7.277 - mean_q: 12.545 - prob: 1.000\n",
            "\n",
            "Interval 27117 (271160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.285 - mean_q: 12.595 - prob: 1.000\n",
            "\n",
            "Interval 27118 (271170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27119 (271180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.401 - mean_q: 12.664 - prob: 1.000\n",
            "\n",
            "Interval 27120 (271190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.002 - mae: 7.298 - mean_q: 12.540 - prob: 1.000\n",
            "\n",
            "Interval 27121 (271200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 27122 (271210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.007 - mae: 7.663 - mean_q: 13.198 - prob: 1.000\n",
            "\n",
            "Interval 27123 (271220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.007 - mae: 7.230 - mean_q: 12.415 - prob: 1.000\n",
            "\n",
            "Interval 27124 (271230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27125 (271240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.006 - mae: 7.520 - mean_q: 12.842 - prob: 1.000\n",
            "\n",
            "Interval 27126 (271250 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.008 - mae: 7.350 - mean_q: 12.716 - prob: 1.000\n",
            "\n",
            "Interval 27127 (271260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27128 (271270 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.006 - mae: 7.225 - mean_q: 12.498 - prob: 1.000\n",
            "\n",
            "Interval 27129 (271280 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.006 - mae: 7.209 - mean_q: 12.474 - prob: 1.000\n",
            "\n",
            "Interval 27130 (271290 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 27131 (271300 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.005 - mae: 7.278 - mean_q: 12.650 - prob: 1.000\n",
            "\n",
            "Interval 27132 (271310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.005 - mae: 7.348 - mean_q: 12.647 - prob: 1.000\n",
            "\n",
            "Interval 27133 (271320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27134 (271330 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.006 - mae: 7.631 - mean_q: 13.092 - prob: 1.000\n",
            "\n",
            "Interval 27135 (271340 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.004 - mae: 7.488 - mean_q: 12.748 - prob: 1.000\n",
            "\n",
            "Interval 27136 (271350 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.007 - mae: 7.340 - mean_q: 12.566 - prob: 1.000\n",
            "\n",
            "Interval 27137 (271360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27138 (271370 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.004 - mae: 7.317 - mean_q: 12.604 - prob: 1.000\n",
            "\n",
            "Interval 27139 (271380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27140 (271390 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.004 - mae: 7.233 - mean_q: 12.691 - prob: 1.000\n",
            "\n",
            "Interval 27141 (271400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.004 - mae: 7.406 - mean_q: 12.777 - prob: 1.000\n",
            "\n",
            "Interval 27142 (271410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 27143 (271420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27144 (271430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.004 - mae: 6.908 - mean_q: 11.819 - prob: 1.000\n",
            "\n",
            "Interval 27145 (271440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 27146 (271450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -17.000 [-17.000, -17.000] - loss: 0.003 - mae: 7.009 - mean_q: 12.282 - prob: 1.000\n",
            "\n",
            "Interval 27147 (271460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27148 (271470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.003 - mae: 7.434 - mean_q: 12.941 - prob: 1.000\n",
            "\n",
            "Interval 27149 (271480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.243 - mean_q: 12.577 - prob: 1.000\n",
            "\n",
            "Interval 27150 (271490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27151 (271500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.002 - mae: 7.301 - mean_q: 12.530 - prob: 1.000\n",
            "\n",
            "Interval 27152 (271510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 27153 (271520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.002 - mae: 7.114 - mean_q: 12.393 - prob: 1.000\n",
            "\n",
            "Interval 27154 (271530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.098 - mae: 7.301 - mean_q: 12.641 - prob: 1.000\n",
            "\n",
            "Interval 27155 (271540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.007 - mae: 7.402 - mean_q: 12.775 - prob: 1.000\n",
            "\n",
            "Interval 27156 (271550 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.072 - mae: 7.248 - mean_q: 12.657 - prob: 1.000\n",
            "\n",
            "Interval 27157 (271560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 27158 (271570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.006 - mae: 7.380 - mean_q: 12.769 - prob: 1.000\n",
            "\n",
            "Interval 27159 (271580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.005 - mae: 7.289 - mean_q: 12.544 - prob: 1.000\n",
            "\n",
            "Interval 27160 (271590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27161 (271600 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.008 - mae: 7.120 - mean_q: 12.341 - prob: 1.000\n",
            "\n",
            "Interval 27162 (271610 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.008 - mae: 7.237 - mean_q: 12.491 - prob: 1.000\n",
            "\n",
            "Interval 27163 (271620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27164 (271630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.006 - mae: 6.970 - mean_q: 12.116 - prob: 1.000\n",
            "\n",
            "Interval 27165 (271640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.012 - mae: 7.158 - mean_q: 12.399 - prob: 1.000\n",
            "\n",
            "Interval 27166 (271650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.007 - mae: 7.071 - mean_q: 12.214 - prob: 1.000\n",
            "\n",
            "Interval 27167 (271660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27168 (271670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.010 - mae: 7.384 - mean_q: 12.658 - prob: 1.000\n",
            "\n",
            "Interval 27169 (271680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.005 - mae: 6.932 - mean_q: 11.978 - prob: 1.000\n",
            "\n",
            "Interval 27170 (271690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 27171 (271700 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.007 - mae: 7.381 - mean_q: 12.677 - prob: 1.000\n",
            "\n",
            "Interval 27172 (271710 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.005 - mae: 7.459 - mean_q: 12.798 - prob: 1.000\n",
            "\n",
            "Interval 27173 (271720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27174 (271730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.003 - mae: 7.265 - mean_q: 12.640 - prob: 1.000\n",
            "\n",
            "Interval 27175 (271740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.007 - mae: 7.157 - mean_q: 12.257 - prob: 1.000\n",
            "\n",
            "Interval 27176 (271750 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 27177 (271760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.003 - mae: 7.118 - mean_q: 12.398 - prob: 1.000\n",
            "\n",
            "Interval 27178 (271770 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.006 - mae: 7.667 - mean_q: 13.063 - prob: 1.000\n",
            "\n",
            "Interval 27179 (271780 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 27180 (271790 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 7.398 - mean_q: 12.723 - prob: 1.000\n",
            "\n",
            "Interval 27181 (271800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.004 - mae: 7.511 - mean_q: 12.919 - prob: 1.000\n",
            "\n",
            "Interval 27182 (271810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27183 (271820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.004 - mae: 7.232 - mean_q: 12.495 - prob: 1.000\n",
            "\n",
            "Interval 27184 (271830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.004 - mae: 7.174 - mean_q: 12.473 - prob: 1.000\n",
            "\n",
            "Interval 27185 (271840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27186 (271850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.006 - mae: 7.058 - mean_q: 12.240 - prob: 1.000\n",
            "\n",
            "Interval 27187 (271860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.005 - mae: 7.399 - mean_q: 12.612 - prob: 1.000\n",
            "\n",
            "Interval 27188 (271870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.005 - mae: 7.379 - mean_q: 12.747 - prob: 1.000\n",
            "\n",
            "Interval 27189 (271880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.350 - mean_q: 12.629 - prob: 1.000\n",
            "\n",
            "Interval 27190 (271890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.005 - mae: 7.752 - mean_q: 13.153 - prob: 1.000\n",
            "\n",
            "Interval 27191 (271900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 27192 (271910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.004 - mae: 7.337 - mean_q: 12.598 - prob: 1.000\n",
            "\n",
            "Interval 27193 (271920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 27194 (271930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.006 - mae: 7.643 - mean_q: 12.973 - prob: 1.000\n",
            "\n",
            "Interval 27195 (271940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.003 - mae: 7.519 - mean_q: 12.922 - prob: 1.000\n",
            "\n",
            "Interval 27196 (271950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27197 (271960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.551 - mean_q: 12.821 - prob: 1.000\n",
            "\n",
            "Interval 27198 (271970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 27199 (271980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.002 - mae: 7.133 - mean_q: 12.365 - prob: 1.000\n",
            "\n",
            "Interval 27200 (271990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.169 - mean_q: 12.479 - prob: 1.000\n",
            "\n",
            "Interval 27201 (272000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27202 (272010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.731 - mean_q: 13.184 - prob: 1.000\n",
            "\n",
            "Interval 27203 (272020 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 27204 (272030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27205 (272040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -24.000 [-24.000, -24.000] - loss: 0.003 - mae: 7.348 - mean_q: 12.645 - prob: 1.000\n",
            "\n",
            "Interval 27206 (272050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27207 (272060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.004 - mae: 7.160 - mean_q: 12.248 - prob: 1.000\n",
            "\n",
            "Interval 27208 (272070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.003 - mae: 7.203 - mean_q: 12.341 - prob: 1.000\n",
            "\n",
            "Interval 27209 (272080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27210 (272090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.541 - mean_q: 12.963 - prob: 1.000\n",
            "\n",
            "Interval 27211 (272100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.002 - mae: 7.272 - mean_q: 12.460 - prob: 1.000\n",
            "\n",
            "Interval 27212 (272110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27213 (272120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 6.956 - mean_q: 12.067 - prob: 1.000\n",
            "\n",
            "Interval 27214 (272130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27215 (272140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.002 - mae: 7.286 - mean_q: 12.562 - prob: 1.000\n",
            "\n",
            "Interval 27216 (272150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27217 (272160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.003 - mae: 7.493 - mean_q: 12.862 - prob: 1.000\n",
            "\n",
            "Interval 27218 (272170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.344 - mean_q: 12.524 - prob: 1.000\n",
            "\n",
            "Interval 27219 (272180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.250 - mean_q: 12.486 - prob: 1.000\n",
            "\n",
            "Interval 27220 (272190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27221 (272200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.004 - mae: 7.026 - mean_q: 12.190 - prob: 1.000\n",
            "\n",
            "Interval 27222 (272210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.224 - mean_q: 12.570 - prob: 1.000\n",
            "\n",
            "Interval 27223 (272220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27224 (272230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.436 - mean_q: 12.824 - prob: 1.000\n",
            "\n",
            "Interval 27225 (272240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.006 - mae: 7.567 - mean_q: 12.957 - prob: 1.000\n",
            "\n",
            "Interval 27226 (272250 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.003 - mae: 7.446 - mean_q: 12.860 - prob: 1.000\n",
            "\n",
            "Interval 27227 (272260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.003 - mae: 7.490 - mean_q: 12.880 - prob: 1.000\n",
            "\n",
            "Interval 27228 (272270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.003 - mae: 7.335 - mean_q: 12.762 - prob: 1.000\n",
            "\n",
            "Interval 27229 (272280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 27230 (272290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.003 - mae: 7.542 - mean_q: 12.898 - prob: 1.000\n",
            "\n",
            "Interval 27231 (272300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27232 (272310 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.006 - mae: 7.623 - mean_q: 13.097 - prob: 1.000\n",
            "\n",
            "Interval 27233 (272320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.002 - mae: 7.464 - mean_q: 12.828 - prob: 1.000\n",
            "\n",
            "Interval 27234 (272330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.002 - mae: 7.344 - mean_q: 12.633 - prob: 1.000\n",
            "\n",
            "Interval 27235 (272340 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 27236 (272350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.002 - mae: 7.438 - mean_q: 12.773 - prob: 1.000\n",
            "\n",
            "Interval 27237 (272360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 27238 (272370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -17.000 [-17.000, -17.000] - loss: 0.003 - mae: 7.122 - mean_q: 12.212 - prob: 1.000\n",
            "\n",
            "Interval 27239 (272380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27240 (272390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.300 - mean_q: 12.564 - prob: 1.000\n",
            "\n",
            "Interval 27241 (272400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.003 - mae: 7.249 - mean_q: 12.576 - prob: 1.000\n",
            "\n",
            "Interval 27242 (272410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.002 - mae: 7.296 - mean_q: 12.624 - prob: 1.000\n",
            "\n",
            "Interval 27243 (272420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27244 (272430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.011 - mae: 7.555 - mean_q: 12.879 - prob: 1.000\n",
            "\n",
            "Interval 27245 (272440 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 27246 (272450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.005 - mae: 7.197 - mean_q: 12.414 - prob: 1.000\n",
            "\n",
            "Interval 27247 (272460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.005 - mae: 7.135 - mean_q: 12.309 - prob: 1.000\n",
            "\n",
            "Interval 27248 (272470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27249 (272480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.005 - mae: 7.286 - mean_q: 12.596 - prob: 1.000\n",
            "\n",
            "Interval 27250 (272490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.004 - mae: 7.422 - mean_q: 12.730 - prob: 1.000\n",
            "\n",
            "Interval 27251 (272500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 27252 (272510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.003 - mae: 7.388 - mean_q: 12.746 - prob: 1.000\n",
            "\n",
            "Interval 27253 (272520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.364 - mean_q: 12.707 - prob: 1.000\n",
            "\n",
            "Interval 27254 (272530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.372 - mean_q: 12.784 - prob: 1.000\n",
            "\n",
            "Interval 27255 (272540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27256 (272550 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.176 - mean_q: 12.307 - prob: 1.000\n",
            "\n",
            "Interval 27257 (272560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.002 - mae: 7.577 - mean_q: 12.952 - prob: 1.000\n",
            "\n",
            "Interval 27258 (272570 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -2.8000\n",
            "Interval 27259 (272580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.002 - mae: 7.066 - mean_q: 12.280 - prob: 1.000\n",
            "\n",
            "Interval 27260 (272590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.003 - mae: 7.635 - mean_q: 13.135 - prob: 1.000\n",
            "\n",
            "Interval 27261 (272600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27262 (272610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.434 - mean_q: 12.815 - prob: 1.000\n",
            "\n",
            "Interval 27263 (272620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.002 - mae: 7.273 - mean_q: 12.579 - prob: 1.000\n",
            "\n",
            "Interval 27264 (272630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.002 - mae: 7.268 - mean_q: 12.549 - prob: 1.000\n",
            "\n",
            "Interval 27265 (272640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27266 (272650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.003 - mae: 7.339 - mean_q: 12.671 - prob: 1.000\n",
            "\n",
            "Interval 27267 (272660 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.002 - mae: 7.407 - mean_q: 12.618 - prob: 1.000\n",
            "\n",
            "Interval 27268 (272670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 27269 (272680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.005 - mae: 7.196 - mean_q: 12.430 - prob: 1.000\n",
            "\n",
            "Interval 27270 (272690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.003 - mae: 7.125 - mean_q: 12.228 - prob: 1.000\n",
            "\n",
            "Interval 27271 (272700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.003 - mae: 7.428 - mean_q: 12.759 - prob: 1.000\n",
            "\n",
            "Interval 27272 (272710 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 27273 (272720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.003 - mae: 7.532 - mean_q: 12.913 - prob: 1.000\n",
            "\n",
            "Interval 27274 (272730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27275 (272740 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.003 - mae: 7.483 - mean_q: 12.821 - prob: 1.000\n",
            "\n",
            "Interval 27276 (272750 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.003 - mae: 7.335 - mean_q: 12.599 - prob: 1.000\n",
            "\n",
            "Interval 27277 (272760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 27278 (272770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.004 - mae: 7.270 - mean_q: 12.473 - prob: 1.000\n",
            "\n",
            "Interval 27279 (272780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27280 (272790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.555 - mean_q: 12.993 - prob: 1.000\n",
            "\n",
            "Interval 27281 (272800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.003 - mae: 7.164 - mean_q: 12.375 - prob: 1.000\n",
            "\n",
            "Interval 27282 (272810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.003 - mae: 7.308 - mean_q: 12.615 - prob: 1.000\n",
            "\n",
            "Interval 27283 (272820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27284 (272830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.512 - mean_q: 12.899 - prob: 1.000\n",
            "\n",
            "Interval 27285 (272840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27286 (272850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.002 - mae: 7.031 - mean_q: 12.257 - prob: 1.000\n",
            "\n",
            "Interval 27287 (272860 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 27288 (272870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.002 - mae: 7.467 - mean_q: 12.791 - prob: 1.000\n",
            "\n",
            "Interval 27289 (272880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.004 - mae: 7.377 - mean_q: 12.768 - prob: 1.000\n",
            "\n",
            "Interval 27290 (272890 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.484 - mean_q: 12.721 - prob: 1.000\n",
            "\n",
            "Interval 27291 (272900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.003 - mae: 7.324 - mean_q: 12.574 - prob: 1.000\n",
            "\n",
            "Interval 27292 (272910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.003 - mae: 7.392 - mean_q: 12.736 - prob: 1.000\n",
            "\n",
            "Interval 27293 (272920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27294 (272930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.004 - mae: 7.321 - mean_q: 12.661 - prob: 1.000\n",
            "\n",
            "Interval 27295 (272940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27296 (272950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.004 - mae: 7.524 - mean_q: 12.913 - prob: 1.000\n",
            "\n",
            "Interval 27297 (272960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.005 - mae: 7.460 - mean_q: 12.857 - prob: 1.000\n",
            "\n",
            "Interval 27298 (272970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27299 (272980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.004 - mae: 7.211 - mean_q: 12.527 - prob: 1.000\n",
            "\n",
            "Interval 27300 (272990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27301 (273000 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.003 - mae: 7.066 - mean_q: 12.327 - prob: 1.000\n",
            "\n",
            "Interval 27302 (273010 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.004 - mae: 7.360 - mean_q: 12.758 - prob: 1.000\n",
            "\n",
            "Interval 27303 (273020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.079 - mean_q: 12.340 - prob: 1.000\n",
            "\n",
            "Interval 27304 (273030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.003 - mae: 7.403 - mean_q: 12.668 - prob: 1.000\n",
            "\n",
            "Interval 27305 (273040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.004 - mae: 7.466 - mean_q: 12.842 - prob: 1.000\n",
            "\n",
            "Interval 27306 (273050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.004 - mae: 7.306 - mean_q: 12.535 - prob: 1.000\n",
            "\n",
            "Interval 27307 (273060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.003 - mae: 7.469 - mean_q: 12.982 - prob: 1.000\n",
            "\n",
            "Interval 27308 (273070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27309 (273080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 9.500 [5.000, 14.000] - loss: 0.002 - mae: 7.512 - mean_q: 12.984 - prob: 1.000\n",
            "\n",
            "Interval 27310 (273090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 27311 (273100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.002 - mae: 7.222 - mean_q: 12.491 - prob: 1.000\n",
            "\n",
            "Interval 27312 (273110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27313 (273120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.508 - mean_q: 12.790 - prob: 1.000\n",
            "\n",
            "Interval 27314 (273130 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.004 - mae: 7.391 - mean_q: 12.757 - prob: 1.000\n",
            "\n",
            "Interval 27315 (273140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.004 - mae: 7.187 - mean_q: 12.603 - prob: 1.000\n",
            "\n",
            "Interval 27316 (273150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27317 (273160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 9.500 [7.000, 12.000] - loss: 0.004 - mae: 7.131 - mean_q: 12.276 - prob: 1.000\n",
            "\n",
            "Interval 27318 (273170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27319 (273180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.004 - mean_q: 12.270 - prob: 1.000\n",
            "\n",
            "Interval 27320 (273190 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.005 - mae: 7.232 - mean_q: 12.540 - prob: 1.000\n",
            "\n",
            "Interval 27321 (273200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27322 (273210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.006 - mae: 7.312 - mean_q: 12.554 - prob: 1.000\n",
            "\n",
            "Interval 27323 (273220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27324 (273230 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.005 - mae: 7.435 - mean_q: 12.622 - prob: 1.000\n",
            "\n",
            "Interval 27325 (273240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27326 (273250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.004 - mae: 7.204 - mean_q: 12.500 - prob: 1.000\n",
            "\n",
            "Interval 27327 (273260 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.008 - mae: 6.949 - mean_q: 12.211 - prob: 1.000\n",
            "\n",
            "Interval 27328 (273270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27329 (273280 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 9.000 [5.000, 13.000] - loss: 0.005 - mae: 7.191 - mean_q: 12.459 - prob: 1.000\n",
            "\n",
            "Interval 27330 (273290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 27331 (273300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.005 - mae: 7.323 - mean_q: 12.612 - prob: 1.000\n",
            "\n",
            "Interval 27332 (273310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27333 (273320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.006 - mae: 7.504 - mean_q: 12.827 - prob: 1.000\n",
            "\n",
            "Interval 27334 (273330 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.004 - mae: 7.434 - mean_q: 12.860 - prob: 1.000\n",
            "\n",
            "Interval 27335 (273340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27336 (273350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.242 - mean_q: 12.695 - prob: 1.000\n",
            "\n",
            "Interval 27337 (273360 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.367 - mean_q: 12.799 - prob: 1.000\n",
            "\n",
            "Interval 27338 (273370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27339 (273380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.004 - mae: 7.293 - mean_q: 12.595 - prob: 1.000\n",
            "\n",
            "Interval 27340 (273390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.004 - mae: 7.091 - mean_q: 12.311 - prob: 1.000\n",
            "\n",
            "Interval 27341 (273400 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 27342 (273410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.003 - mae: 7.597 - mean_q: 12.963 - prob: 1.000\n",
            "\n",
            "Interval 27343 (273420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.552 - mean_q: 12.940 - prob: 1.000\n",
            "\n",
            "Interval 27344 (273430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.314 - mean_q: 12.534 - prob: 1.000\n",
            "\n",
            "Interval 27345 (273440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 27346 (273450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.003 - mae: 7.429 - mean_q: 12.881 - prob: 1.000\n",
            "\n",
            "Interval 27347 (273460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27348 (273470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -24.000 [-24.000, -24.000] - loss: 0.004 - mae: 7.434 - mean_q: 12.818 - prob: 1.000\n",
            "\n",
            "Interval 27349 (273480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27350 (273490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.003 - mae: 7.079 - mean_q: 12.081 - prob: 1.000\n",
            "\n",
            "Interval 27351 (273500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.004 - mae: 7.251 - mean_q: 12.488 - prob: 1.000\n",
            "\n",
            "Interval 27352 (273510 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 27353 (273520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.004 - mae: 7.224 - mean_q: 12.514 - prob: 1.000\n",
            "\n",
            "Interval 27354 (273530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.536 - mean_q: 12.947 - prob: 1.000\n",
            "\n",
            "Interval 27355 (273540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27356 (273550 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.003 - mae: 7.369 - mean_q: 12.715 - prob: 1.000\n",
            "\n",
            "Interval 27357 (273560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 27358 (273570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.003 - mae: 7.426 - mean_q: 12.873 - prob: 1.000\n",
            "\n",
            "Interval 27359 (273580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.345 - mean_q: 12.632 - prob: 1.000\n",
            "\n",
            "Interval 27360 (273590 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.230 - mean_q: 12.539 - prob: 1.000\n",
            "\n",
            "Interval 27361 (273600 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.003 - mae: 7.366 - mean_q: 12.690 - prob: 1.000\n",
            "\n",
            "Interval 27362 (273610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27363 (273620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.004 - mae: 7.098 - mean_q: 12.118 - prob: 1.000\n",
            "\n",
            "Interval 27364 (273630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 27365 (273640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.003 - mae: 7.701 - mean_q: 13.306 - prob: 1.000\n",
            "\n",
            "Interval 27366 (273650 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.003 - mae: 7.563 - mean_q: 13.037 - prob: 1.000\n",
            "\n",
            "Interval 27367 (273660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -20.000 [-20.000, -20.000] - loss: 0.003 - mae: 7.280 - mean_q: 12.432 - prob: 1.000\n",
            "\n",
            "Interval 27368 (273670 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 27369 (273680 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.548 - mean_q: 12.979 - prob: 1.000\n",
            "\n",
            "Interval 27370 (273690 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.002 - mae: 7.151 - mean_q: 12.281 - prob: 1.000\n",
            "\n",
            "Interval 27371 (273700 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.102 - mean_q: 12.253 - prob: 1.000\n",
            "\n",
            "Interval 27372 (273710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27373 (273720 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.579 - mean_q: 12.994 - prob: 1.000\n",
            "\n",
            "Interval 27374 (273730 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.003 - mae: 7.445 - mean_q: 12.787 - prob: 1.000\n",
            "\n",
            "Interval 27375 (273740 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.206 - mean_q: 12.459 - prob: 1.000\n",
            "\n",
            "Interval 27376 (273750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27377 (273760 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.281 - mean_q: 12.657 - prob: 1.000\n",
            "\n",
            "Interval 27378 (273770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.004 - mae: 7.007 - mean_q: 12.161 - prob: 1.000\n",
            "\n",
            "Interval 27379 (273780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.004 - mae: 7.264 - mean_q: 12.536 - prob: 1.000\n",
            "\n",
            "Interval 27380 (273790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27381 (273800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.402 - mean_q: 12.950 - prob: 1.000\n",
            "\n",
            "Interval 27382 (273810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.002 - mae: 7.425 - mean_q: 12.800 - prob: 1.000\n",
            "\n",
            "Interval 27383 (273820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 27384 (273830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.002 - mae: 7.102 - mean_q: 12.449 - prob: 1.000\n",
            "\n",
            "Interval 27385 (273840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27386 (273850 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.211 - mean_q: 12.522 - prob: 1.000\n",
            "\n",
            "Interval 27387 (273860 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 7.281 - mean_q: 12.607 - prob: 1.000\n",
            "\n",
            "Interval 27388 (273870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 27389 (273880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.002 - mae: 7.290 - mean_q: 12.694 - prob: 1.000\n",
            "\n",
            "Interval 27390 (273890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.002 - mae: 7.088 - mean_q: 12.288 - prob: 1.000\n",
            "\n",
            "Interval 27391 (273900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.004 - mae: 7.309 - mean_q: 12.433 - prob: 1.000\n",
            "\n",
            "Interval 27392 (273910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27393 (273920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.004 - mae: 7.304 - mean_q: 12.589 - prob: 1.000\n",
            "\n",
            "Interval 27394 (273930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.005 - mae: 7.416 - mean_q: 12.559 - prob: 1.000\n",
            "\n",
            "Interval 27395 (273940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27396 (273950 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.478 - mean_q: 12.780 - prob: 1.000\n",
            "\n",
            "Interval 27397 (273960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.004 - mae: 7.946 - mean_q: 13.434 - prob: 1.000\n",
            "\n",
            "Interval 27398 (273970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.006 - mae: 7.175 - mean_q: 12.372 - prob: 1.000\n",
            "\n",
            "Interval 27399 (273980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.007 - mae: 7.106 - mean_q: 12.277 - prob: 1.000\n",
            "\n",
            "Interval 27400 (273990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.008 - mae: 7.515 - mean_q: 12.826 - prob: 1.000\n",
            "\n",
            "Interval 27401 (274000 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 27402 (274010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.007 - mae: 7.384 - mean_q: 12.715 - prob: 1.000\n",
            "\n",
            "Interval 27403 (274020 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.006 - mae: 7.359 - mean_q: 12.705 - prob: 1.000\n",
            "\n",
            "Interval 27404 (274030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.007 - mae: 7.222 - mean_q: 12.424 - prob: 1.000\n",
            "\n",
            "Interval 27405 (274040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27406 (274050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.006 - mae: 7.507 - mean_q: 12.826 - prob: 1.000\n",
            "\n",
            "Interval 27407 (274060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27408 (274070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.005 - mae: 7.269 - mean_q: 12.417 - prob: 1.000\n",
            "\n",
            "Interval 27409 (274080 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 27410 (274090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -26.000 [-26.000, -26.000] - loss: 0.004 - mae: 7.393 - mean_q: 12.649 - prob: 1.000\n",
            "\n",
            "Interval 27411 (274100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27412 (274110 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.004 - mae: 7.359 - mean_q: 12.755 - prob: 1.000\n",
            "\n",
            "Interval 27413 (274120 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.004 - mae: 7.234 - mean_q: 12.576 - prob: 1.000\n",
            "\n",
            "Interval 27414 (274130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.005 - mae: 7.038 - mean_q: 12.113 - prob: 1.000\n",
            "\n",
            "Interval 27415 (274140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.004 - mae: 7.314 - mean_q: 12.626 - prob: 1.000\n",
            "\n",
            "Interval 27416 (274150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.003 - mae: 7.229 - mean_q: 12.648 - prob: 1.000\n",
            "\n",
            "Interval 27417 (274160 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.004 - mae: 7.202 - mean_q: 12.492 - prob: 1.000\n",
            "\n",
            "Interval 27418 (274170 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.003 - mae: 7.544 - mean_q: 12.826 - prob: 1.000\n",
            "\n",
            "Interval 27419 (274180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 27420 (274190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.002 - mae: 7.363 - mean_q: 12.779 - prob: 1.000\n",
            "\n",
            "Interval 27421 (274200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.294 - mean_q: 12.429 - prob: 1.000\n",
            "\n",
            "Interval 27422 (274210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 27423 (274220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.002 - mae: 7.098 - mean_q: 12.451 - prob: 1.000\n",
            "\n",
            "Interval 27424 (274230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.470 - mean_q: 12.872 - prob: 1.000\n",
            "\n",
            "Interval 27425 (274240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 27426 (274250 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.002 - mae: 7.320 - mean_q: 12.450 - prob: 1.000\n",
            "\n",
            "Interval 27427 (274260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.003 - mae: 7.282 - mean_q: 12.441 - prob: 1.000\n",
            "\n",
            "Interval 27428 (274270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27429 (274280 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.003 - mae: 7.236 - mean_q: 12.684 - prob: 1.000\n",
            "\n",
            "Interval 27430 (274290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.004 - mae: 7.433 - mean_q: 12.727 - prob: 1.000\n",
            "\n",
            "Interval 27431 (274300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 27432 (274310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.004 - mae: 7.486 - mean_q: 12.709 - prob: 1.000\n",
            "\n",
            "Interval 27433 (274320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.003 - mae: 7.618 - mean_q: 13.149 - prob: 1.000\n",
            "\n",
            "Interval 27434 (274330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.003 - mae: 7.476 - mean_q: 12.844 - prob: 1.000\n",
            "\n",
            "Interval 27435 (274340 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 27436 (274350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -18.000 [-18.000, -18.000] - loss: 0.002 - mae: 7.197 - mean_q: 12.414 - prob: 1.000\n",
            "\n",
            "Interval 27437 (274360 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 27438 (274370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.002 - mae: 7.077 - mean_q: 12.323 - prob: 1.000\n",
            "\n",
            "Interval 27439 (274380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -21.000 [-21.000, -21.000] - loss: 0.002 - mae: 7.161 - mean_q: 12.494 - prob: 1.000\n",
            "\n",
            "Interval 27440 (274390 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 27441 (274400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.448 - mean_q: 12.779 - prob: 1.000\n",
            "\n",
            "Interval 27442 (274410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.003 - mae: 7.435 - mean_q: 12.909 - prob: 1.000\n",
            "\n",
            "Interval 27443 (274420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27444 (274430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.211 - mean_q: 12.432 - prob: 1.000\n",
            "\n",
            "Interval 27445 (274440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 27446 (274450 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.002 - mae: 7.378 - mean_q: 12.733 - prob: 1.000\n",
            "\n",
            "Interval 27447 (274460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.559 - mean_q: 13.109 - prob: 1.000\n",
            "\n",
            "Interval 27448 (274470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27449 (274480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.005 - mae: 7.305 - mean_q: 12.614 - prob: 1.000\n",
            "\n",
            "Interval 27450 (274490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 27451 (274500 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -2.5000\n",
            "1 episodes - episode_reward: -25.000 [-25.000, -25.000] - loss: 0.004 - mae: 7.593 - mean_q: 13.082 - prob: 1.000\n",
            "\n",
            "Interval 27452 (274510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 27453 (274520 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -36.000 [-36.000, -36.000] - loss: 0.003 - mae: 7.404 - mean_q: 12.860 - prob: 1.000\n",
            "\n",
            "Interval 27454 (274530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.058 - mean_q: 12.148 - prob: 1.000\n",
            "\n",
            "Interval 27455 (274540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 27456 (274550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.003 - mae: 7.506 - mean_q: 12.919 - prob: 1.000\n",
            "\n",
            "Interval 27457 (274560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.004 - mae: 7.196 - mean_q: 12.453 - prob: 1.000\n",
            "\n",
            "Interval 27458 (274570 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 27459 (274580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.515 - mean_q: 12.860 - prob: 1.000\n",
            "\n",
            "Interval 27460 (274590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.626 - mean_q: 13.011 - prob: 1.000\n",
            "\n",
            "Interval 27461 (274600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.002 - mae: 7.582 - mean_q: 12.928 - prob: 1.000\n",
            "\n",
            "Interval 27462 (274610 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.358 - mean_q: 12.789 - prob: 1.000\n",
            "\n",
            "Interval 27463 (274620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.003 - mae: 7.543 - mean_q: 13.001 - prob: 1.000\n",
            "\n",
            "Interval 27464 (274630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.003 - mae: 7.352 - mean_q: 12.576 - prob: 1.000\n",
            "\n",
            "Interval 27465 (274640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27466 (274650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.376 - mean_q: 12.819 - prob: 1.000\n",
            "\n",
            "Interval 27467 (274660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 6.930 - mean_q: 11.984 - prob: 1.000\n",
            "\n",
            "Interval 27468 (274670 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 27469 (274680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 7.813 - mean_q: 13.222 - prob: 1.000\n",
            "\n",
            "Interval 27470 (274690 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 27471 (274700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.004 - mae: 7.446 - mean_q: 12.698 - prob: 1.000\n",
            "\n",
            "Interval 27472 (274710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.004 - mae: 7.259 - mean_q: 12.704 - prob: 1.000\n",
            "\n",
            "Interval 27473 (274720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27474 (274730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.003 - mae: 7.719 - mean_q: 13.151 - prob: 1.000\n",
            "\n",
            "Interval 27475 (274740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.522 - mean_q: 12.783 - prob: 1.000\n",
            "\n",
            "Interval 27476 (274750 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 27477 (274760 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.752 - mean_q: 13.126 - prob: 1.000\n",
            "\n",
            "Interval 27478 (274770 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.391 - mean_q: 12.645 - prob: 1.000\n",
            "\n",
            "Interval 27479 (274780 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.003 - mae: 7.138 - mean_q: 12.351 - prob: 1.000\n",
            "\n",
            "Interval 27480 (274790 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 27481 (274800 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.004 - mae: 7.367 - mean_q: 12.705 - prob: 1.000\n",
            "\n",
            "Interval 27482 (274810 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 27483 (274820 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.002 - mae: 7.368 - mean_q: 12.782 - prob: 1.000\n",
            "\n",
            "Interval 27484 (274830 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.002 - mae: 7.331 - mean_q: 12.602 - prob: 1.000\n",
            "\n",
            "Interval 27485 (274840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.567 - mean_q: 12.846 - prob: 1.000\n",
            "\n",
            "Interval 27486 (274850 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.677 - mean_q: 13.139 - prob: 1.000\n",
            "\n",
            "Interval 27487 (274860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27488 (274870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.004 - mae: 7.230 - mean_q: 12.478 - prob: 1.000\n",
            "\n",
            "Interval 27489 (274880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.005 - mae: 7.451 - mean_q: 12.816 - prob: 1.000\n",
            "\n",
            "Interval 27490 (274890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.005 - mae: 7.593 - mean_q: 13.142 - prob: 1.000\n",
            "\n",
            "Interval 27491 (274900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27492 (274910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.485 - mean_q: 12.974 - prob: 1.000\n",
            "\n",
            "Interval 27493 (274920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.002 - mae: 7.648 - mean_q: 13.201 - prob: 1.000\n",
            "\n",
            "Interval 27494 (274930 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 27495 (274940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.182 - mean_q: 12.415 - prob: 1.000\n",
            "\n",
            "Interval 27496 (274950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.002 - mae: 7.183 - mean_q: 12.380 - prob: 1.000\n",
            "\n",
            "Interval 27497 (274960 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.003 - mae: 7.267 - mean_q: 12.401 - prob: 1.000\n",
            "\n",
            "Interval 27498 (274970 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 27499 (274980 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 10.000 [8.000, 12.000] - loss: 0.003 - mae: 7.199 - mean_q: 12.422 - prob: 1.000\n",
            "\n",
            "Interval 27500 (274990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27501 (275000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.003 - mae: 7.537 - mean_q: 12.899 - prob: 1.000\n",
            "\n",
            "Interval 27502 (275010 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.003 - mae: 7.341 - mean_q: 12.725 - prob: 1.000\n",
            "\n",
            "Interval 27503 (275020 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.003 - mae: 7.771 - mean_q: 13.202 - prob: 1.000\n",
            "\n",
            "Interval 27504 (275030 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.476 - mean_q: 12.884 - prob: 1.000\n",
            "\n",
            "Interval 27505 (275040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 27506 (275050 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.004 - mae: 7.132 - mean_q: 12.336 - prob: 1.000\n",
            "\n",
            "Interval 27507 (275060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 27508 (275070 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.004 - mae: 7.252 - mean_q: 12.521 - prob: 1.000\n",
            "\n",
            "Interval 27509 (275080 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 27510 (275090 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 7.476 - mean_q: 12.860 - prob: 1.000\n",
            "\n",
            "Interval 27511 (275100 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.004 - mae: 7.234 - mean_q: 12.494 - prob: 1.000\n",
            "\n",
            "Interval 27512 (275110 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.005 - mae: 7.300 - mean_q: 12.667 - prob: 1.000\n",
            "\n",
            "Interval 27513 (275120 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.004 - mae: 7.176 - mean_q: 12.377 - prob: 1.000\n",
            "\n",
            "Interval 27514 (275130 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 27515 (275140 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.004 - mae: 6.990 - mean_q: 12.236 - prob: 1.000\n",
            "\n",
            "Interval 27516 (275150 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.004 - mae: 7.703 - mean_q: 13.333 - prob: 1.000\n",
            "\n",
            "Interval 27517 (275160 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.007 - mae: 7.549 - mean_q: 13.021 - prob: 1.000\n",
            "\n",
            "Interval 27518 (275170 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 27519 (275180 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.008 - mae: 7.744 - mean_q: 13.118 - prob: 1.000\n",
            "\n",
            "Interval 27520 (275190 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.007 - mae: 7.199 - mean_q: 12.417 - prob: 1.000\n",
            "\n",
            "Interval 27521 (275200 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 27522 (275210 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.006 - mae: 6.951 - mean_q: 12.016 - prob: 1.000\n",
            "\n",
            "Interval 27523 (275220 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.004 - mae: 7.366 - mean_q: 12.678 - prob: 1.000\n",
            "\n",
            "Interval 27524 (275230 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.003 - mae: 7.242 - mean_q: 12.535 - prob: 1.000\n",
            "\n",
            "Interval 27525 (275240 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 27526 (275250 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.440 - mean_q: 12.851 - prob: 1.000\n",
            "\n",
            "Interval 27527 (275260 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.464 - mean_q: 13.004 - prob: 1.000\n",
            "\n",
            "Interval 27528 (275270 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.003 - mae: 7.156 - mean_q: 12.269 - prob: 1.000\n",
            "\n",
            "Interval 27529 (275280 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.496 - mean_q: 12.863 - prob: 1.000\n",
            "\n",
            "Interval 27530 (275290 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.203 - mean_q: 12.547 - prob: 1.000\n",
            "\n",
            "Interval 27531 (275300 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 27532 (275310 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.002 - mae: 7.204 - mean_q: 12.532 - prob: 1.000\n",
            "\n",
            "Interval 27533 (275320 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 27534 (275330 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.357 - mean_q: 12.865 - prob: 1.000\n",
            "\n",
            "Interval 27535 (275340 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.558 - mean_q: 13.008 - prob: 1.000\n",
            "\n",
            "Interval 27536 (275350 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 27537 (275360 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.002 - mae: 7.473 - mean_q: 12.910 - prob: 1.000\n",
            "\n",
            "Interval 27538 (275370 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 27539 (275380 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.002 - mae: 7.297 - mean_q: 12.806 - prob: 1.000\n",
            "\n",
            "Interval 27540 (275390 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.283 - mean_q: 12.642 - prob: 1.000\n",
            "\n",
            "Interval 27541 (275400 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 27542 (275410 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.002 - mae: 7.599 - mean_q: 13.029 - prob: 1.000\n",
            "\n",
            "Interval 27543 (275420 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.002 - mae: 7.351 - mean_q: 12.685 - prob: 1.000\n",
            "\n",
            "Interval 27544 (275430 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.582 - mean_q: 13.024 - prob: 1.000\n",
            "\n",
            "Interval 27545 (275440 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 27546 (275450 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.539 - mean_q: 13.068 - prob: 1.000\n",
            "\n",
            "Interval 27547 (275460 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.329 - mean_q: 12.621 - prob: 1.000\n",
            "\n",
            "Interval 27548 (275470 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.003 - mae: 7.437 - mean_q: 12.957 - prob: 1.000\n",
            "\n",
            "Interval 27549 (275480 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.002 - mae: 7.425 - mean_q: 12.865 - prob: 1.000\n",
            "\n",
            "Interval 27550 (275490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.496 - mean_q: 12.882 - prob: 1.000\n",
            "\n",
            "Interval 27551 (275500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27552 (275510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.117 - mean_q: 12.193 - prob: 1.000\n",
            "\n",
            "Interval 27553 (275520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.002 - mae: 7.383 - mean_q: 12.619 - prob: 1.000\n",
            "\n",
            "Interval 27554 (275530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.003 - mae: 7.132 - mean_q: 12.416 - prob: 1.000\n",
            "\n",
            "Interval 27555 (275540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27556 (275550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.259 - mean_q: 12.648 - prob: 1.000\n",
            "\n",
            "Interval 27557 (275560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.218 - mean_q: 12.404 - prob: 1.000\n",
            "\n",
            "Interval 27558 (275570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27559 (275580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27560 (275590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.003 - mae: 7.554 - mean_q: 12.814 - prob: 1.000\n",
            "\n",
            "Interval 27561 (275600 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 27562 (275610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.003 - mae: 7.094 - mean_q: 12.340 - prob: 1.000\n",
            "\n",
            "Interval 27563 (275620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.003 - mae: 7.347 - mean_q: 12.677 - prob: 1.000\n",
            "\n",
            "Interval 27564 (275630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27565 (275640 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.441 - mean_q: 12.776 - prob: 1.000\n",
            "\n",
            "Interval 27566 (275650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.641 - mean_q: 13.071 - prob: 1.000\n",
            "\n",
            "Interval 27567 (275660 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 27568 (275670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.005 - mae: 7.687 - mean_q: 13.148 - prob: 1.000\n",
            "\n",
            "Interval 27569 (275680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.005 - mae: 7.549 - mean_q: 12.919 - prob: 1.000\n",
            "\n",
            "Interval 27570 (275690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 27571 (275700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.004 - mae: 7.384 - mean_q: 12.583 - prob: 1.000\n",
            "\n",
            "Interval 27572 (275710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27573 (275720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.003 - mae: 7.484 - mean_q: 12.655 - prob: 1.000\n",
            "\n",
            "Interval 27574 (275730 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 27575 (275740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.002 - mae: 7.091 - mean_q: 12.358 - prob: 1.000\n",
            "\n",
            "Interval 27576 (275750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.004 - mae: 7.518 - mean_q: 12.938 - prob: 1.000\n",
            "\n",
            "Interval 27577 (275760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27578 (275770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.003 - mae: 7.440 - mean_q: 12.805 - prob: 1.000\n",
            "\n",
            "Interval 27579 (275780 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.004 - mae: 7.302 - mean_q: 12.662 - prob: 1.000\n",
            "\n",
            "Interval 27580 (275790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27581 (275800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 7.271 - mean_q: 12.640 - prob: 1.000\n",
            "\n",
            "Interval 27582 (275810 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 27583 (275820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.004 - mae: 7.352 - mean_q: 12.796 - prob: 1.000\n",
            "\n",
            "Interval 27584 (275830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.003 - mae: 7.232 - mean_q: 12.547 - prob: 1.000\n",
            "\n",
            "Interval 27585 (275840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27586 (275850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.003 - mae: 7.544 - mean_q: 12.872 - prob: 1.000\n",
            "\n",
            "Interval 27587 (275860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.296 - mean_q: 12.576 - prob: 1.000\n",
            "\n",
            "Interval 27588 (275870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.003 - mae: 7.322 - mean_q: 12.513 - prob: 1.000\n",
            "\n",
            "Interval 27589 (275880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27590 (275890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.003 - mae: 7.283 - mean_q: 12.714 - prob: 1.000\n",
            "\n",
            "Interval 27591 (275900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.004 - mae: 7.521 - mean_q: 12.872 - prob: 1.000\n",
            "\n",
            "Interval 27592 (275910 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 27593 (275920 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.006 - mae: 7.162 - mean_q: 12.359 - prob: 1.000\n",
            "\n",
            "Interval 27594 (275930 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 27595 (275940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.003 - mae: 7.349 - mean_q: 12.598 - prob: 1.000\n",
            "\n",
            "Interval 27596 (275950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.004 - mae: 7.214 - mean_q: 12.477 - prob: 1.000\n",
            "\n",
            "Interval 27597 (275960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.004 - mae: 7.622 - mean_q: 13.133 - prob: 1.000\n",
            "\n",
            "Interval 27598 (275970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 27599 (275980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.003 - mae: 7.455 - mean_q: 12.871 - prob: 1.000\n",
            "\n",
            "Interval 27600 (275990 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 27601 (276000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.002 - mae: 7.176 - mean_q: 12.487 - prob: 1.000\n",
            "\n",
            "Interval 27602 (276010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.490 - mean_q: 12.971 - prob: 1.000\n",
            "\n",
            "Interval 27603 (276020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.003 - mae: 7.454 - mean_q: 12.603 - prob: 1.000\n",
            "\n",
            "Interval 27604 (276030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.004 - mae: 7.341 - mean_q: 12.594 - prob: 1.000\n",
            "\n",
            "Interval 27605 (276040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27606 (276050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.005 - mae: 7.535 - mean_q: 13.016 - prob: 1.000\n",
            "\n",
            "Interval 27607 (276060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.004 - mae: 7.172 - mean_q: 12.331 - prob: 1.000\n",
            "\n",
            "Interval 27608 (276070 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 27609 (276080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.103 - mean_q: 12.430 - prob: 1.000\n",
            "\n",
            "Interval 27610 (276090 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.005 - mae: 7.356 - mean_q: 12.729 - prob: 1.000\n",
            "\n",
            "Interval 27611 (276100 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 27612 (276110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.004 - mae: 7.378 - mean_q: 12.624 - prob: 1.000\n",
            "\n",
            "Interval 27613 (276120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.004 - mae: 7.038 - mean_q: 12.281 - prob: 1.000\n",
            "\n",
            "Interval 27614 (276130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 27615 (276140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.004 - mae: 7.277 - mean_q: 12.490 - prob: 1.000\n",
            "\n",
            "Interval 27616 (276150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.200 - mean_q: 12.529 - prob: 1.000\n",
            "\n",
            "Interval 27617 (276160 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.004 - mae: 7.102 - mean_q: 12.363 - prob: 1.000\n",
            "\n",
            "Interval 27618 (276170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27619 (276180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.006 - mae: 7.410 - mean_q: 12.633 - prob: 1.000\n",
            "\n",
            "Interval 27620 (276190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.003 - mae: 7.529 - mean_q: 12.919 - prob: 1.000\n",
            "\n",
            "Interval 27621 (276200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27622 (276210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.003 - mae: 7.172 - mean_q: 12.402 - prob: 1.000\n",
            "\n",
            "Interval 27623 (276220 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 27624 (276230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.6000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.002 - mae: 7.575 - mean_q: 13.027 - prob: 1.000\n",
            "\n",
            "Interval 27625 (276240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.003 - mae: 7.305 - mean_q: 12.358 - prob: 1.000\n",
            "\n",
            "Interval 27626 (276250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 6.970 - mean_q: 11.943 - prob: 1.000\n",
            "\n",
            "Interval 27627 (276260 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 27628 (276270 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 11.500 [9.000, 14.000] - loss: 0.003 - mae: 7.313 - mean_q: 12.575 - prob: 1.000\n",
            "\n",
            "Interval 27629 (276280 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.003 - mae: 7.746 - mean_q: 13.267 - prob: 1.000\n",
            "\n",
            "Interval 27630 (276290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 7.398 - mean_q: 12.621 - prob: 1.000\n",
            "\n",
            "Interval 27631 (276300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.004 - mae: 7.338 - mean_q: 12.639 - prob: 1.000\n",
            "\n",
            "Interval 27632 (276310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27633 (276320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.320 - mean_q: 12.514 - prob: 1.000\n",
            "\n",
            "Interval 27634 (276330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.005 - mae: 7.278 - mean_q: 12.555 - prob: 1.000\n",
            "\n",
            "Interval 27635 (276340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.005 - mae: 6.966 - mean_q: 12.129 - prob: 1.000\n",
            "\n",
            "Interval 27636 (276350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27637 (276360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.519 - mean_q: 12.957 - prob: 1.000\n",
            "\n",
            "Interval 27638 (276370 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.004 - mae: 7.181 - mean_q: 12.378 - prob: 1.000\n",
            "\n",
            "Interval 27639 (276380 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 27640 (276390 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 2.3000\n",
            "2 episodes - episode_reward: 5.000 [4.000, 6.000] - loss: 0.005 - mae: 7.087 - mean_q: 12.211 - prob: 1.000\n",
            "\n",
            "Interval 27641 (276400 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 27642 (276410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.004 - mae: 7.427 - mean_q: 12.776 - prob: 1.000\n",
            "\n",
            "Interval 27643 (276420 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 27644 (276430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.005 - mae: 7.428 - mean_q: 12.780 - prob: 1.000\n",
            "\n",
            "Interval 27645 (276440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.006 - mae: 7.177 - mean_q: 12.380 - prob: 1.000\n",
            "\n",
            "Interval 27646 (276450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 27647 (276460 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.010 - mae: 7.116 - mean_q: 12.366 - prob: 1.000\n",
            "\n",
            "Interval 27648 (276470 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 27649 (276480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.008 - mae: 7.196 - mean_q: 12.434 - prob: 1.000\n",
            "\n",
            "Interval 27650 (276490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.008 - mae: 7.273 - mean_q: 12.547 - prob: 1.000\n",
            "\n",
            "Interval 27651 (276500 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 27652 (276510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.009 - mae: 7.488 - mean_q: 12.854 - prob: 1.000\n",
            "\n",
            "Interval 27653 (276520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.007 - mae: 7.538 - mean_q: 12.898 - prob: 1.000\n",
            "\n",
            "Interval 27654 (276530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.005 - mae: 7.421 - mean_q: 12.683 - prob: 1.000\n",
            "\n",
            "Interval 27655 (276540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27656 (276550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.004 - mae: 7.222 - mean_q: 12.558 - prob: 1.000\n",
            "\n",
            "Interval 27657 (276560 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.005 - mae: 7.154 - mean_q: 12.318 - prob: 1.000\n",
            "\n",
            "Interval 27658 (276570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27659 (276580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.531 - mean_q: 13.005 - prob: 1.000\n",
            "\n",
            "Interval 27660 (276590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.003 - mae: 7.557 - mean_q: 13.137 - prob: 1.000\n",
            "\n",
            "Interval 27661 (276600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27662 (276610 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.543 - mean_q: 12.951 - prob: 1.000\n",
            "\n",
            "Interval 27663 (276620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.004 - mae: 7.237 - mean_q: 12.465 - prob: 1.000\n",
            "\n",
            "Interval 27664 (276630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 27665 (276640 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.004 - mae: 7.298 - mean_q: 12.657 - prob: 1.000\n",
            "\n",
            "Interval 27666 (276650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27667 (276660 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.277 - mean_q: 12.653 - prob: 1.000\n",
            "\n",
            "Interval 27668 (276670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.004 - mae: 7.356 - mean_q: 12.749 - prob: 1.000\n",
            "\n",
            "Interval 27669 (276680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27670 (276690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.005 - mae: 7.309 - mean_q: 12.520 - prob: 1.000\n",
            "\n",
            "Interval 27671 (276700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.462 - mean_q: 12.727 - prob: 1.000\n",
            "\n",
            "Interval 27672 (276710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 27673 (276720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.003 - mae: 7.607 - mean_q: 13.102 - prob: 1.000\n",
            "\n",
            "Interval 27674 (276730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.003 - mae: 7.340 - mean_q: 12.645 - prob: 1.000\n",
            "\n",
            "Interval 27675 (276740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27676 (276750 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.338 - mean_q: 12.679 - prob: 1.000\n",
            "\n",
            "Interval 27677 (276760 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.004 - mae: 7.202 - mean_q: 12.465 - prob: 1.000\n",
            "\n",
            "Interval 27678 (276770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 27679 (276780 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.002 - mae: 7.437 - mean_q: 12.742 - prob: 1.000\n",
            "\n",
            "Interval 27680 (276790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.003 - mae: 7.210 - mean_q: 12.495 - prob: 1.000\n",
            "\n",
            "Interval 27681 (276800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27682 (276810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.002 - mae: 7.410 - mean_q: 12.792 - prob: 1.000\n",
            "\n",
            "Interval 27683 (276820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.003 - mae: 7.569 - mean_q: 12.947 - prob: 1.000\n",
            "\n",
            "Interval 27684 (276830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.002 - mae: 7.001 - mean_q: 12.267 - prob: 1.000\n",
            "\n",
            "Interval 27685 (276840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27686 (276850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.190 - mean_q: 12.427 - prob: 1.000\n",
            "\n",
            "Interval 27687 (276860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.003 - mae: 7.619 - mean_q: 13.159 - prob: 1.000\n",
            "\n",
            "Interval 27688 (276870 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 27689 (276880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.003 - mae: 7.019 - mean_q: 12.188 - prob: 1.000\n",
            "\n",
            "Interval 27690 (276890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.394 - mean_q: 12.743 - prob: 1.000\n",
            "\n",
            "Interval 27691 (276900 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.002 - mae: 7.308 - mean_q: 12.671 - prob: 1.000\n",
            "\n",
            "Interval 27692 (276910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 27693 (276920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.003 - mae: 7.181 - mean_q: 12.412 - prob: 1.000\n",
            "\n",
            "Interval 27694 (276930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.002 - mae: 7.527 - mean_q: 13.014 - prob: 1.000\n",
            "\n",
            "Interval 27695 (276940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27696 (276950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.003 - mae: 7.501 - mean_q: 12.951 - prob: 1.000\n",
            "\n",
            "Interval 27697 (276960 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.452 - mean_q: 12.896 - prob: 1.000\n",
            "\n",
            "Interval 27698 (276970 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.002 - mae: 7.371 - mean_q: 12.874 - prob: 1.000\n",
            "\n",
            "Interval 27699 (276980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27700 (276990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.003 - mae: 7.303 - mean_q: 12.667 - prob: 1.000\n",
            "\n",
            "Interval 27701 (277000 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 27702 (277010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.004 - mae: 7.453 - mean_q: 12.824 - prob: 1.000\n",
            "\n",
            "Interval 27703 (277020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.003 - mae: 7.369 - mean_q: 12.701 - prob: 1.000\n",
            "\n",
            "Interval 27704 (277030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.003 - mae: 7.159 - mean_q: 12.404 - prob: 1.000\n",
            "\n",
            "Interval 27705 (277040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27706 (277050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 4.500 [-4.000, 13.000] - loss: 0.003 - mae: 7.394 - mean_q: 12.643 - prob: 1.000\n",
            "\n",
            "Interval 27707 (277060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 27708 (277070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.003 - mae: 7.210 - mean_q: 12.592 - prob: 1.000\n",
            "\n",
            "Interval 27709 (277080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.003 - mae: 7.323 - mean_q: 12.767 - prob: 1.000\n",
            "\n",
            "Interval 27710 (277090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 27711 (277100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.003 - mae: 7.642 - mean_q: 13.125 - prob: 1.000\n",
            "\n",
            "Interval 27712 (277110 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.454 - mean_q: 12.781 - prob: 1.000\n",
            "\n",
            "Interval 27713 (277120 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 27714 (277130 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.007 - mae: 7.186 - mean_q: 12.484 - prob: 1.000\n",
            "\n",
            "Interval 27715 (277140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27716 (277150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.004 - mae: 7.094 - mean_q: 12.325 - prob: 1.000\n",
            "\n",
            "Interval 27717 (277160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.147 - mean_q: 12.407 - prob: 1.000\n",
            "\n",
            "Interval 27718 (277170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.349 - mean_q: 12.704 - prob: 1.000\n",
            "\n",
            "Interval 27719 (277180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 27720 (277190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.002 - mae: 7.425 - mean_q: 12.803 - prob: 1.000\n",
            "\n",
            "Interval 27721 (277200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 27722 (277210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.002 - mae: 7.385 - mean_q: 12.782 - prob: 1.000\n",
            "\n",
            "Interval 27723 (277220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27724 (277230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.359 - mean_q: 12.626 - prob: 1.000\n",
            "\n",
            "Interval 27725 (277240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.003 - mae: 7.207 - mean_q: 12.349 - prob: 1.000\n",
            "\n",
            "Interval 27726 (277250 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.002 - mae: 7.305 - mean_q: 12.675 - prob: 1.000\n",
            "\n",
            "Interval 27727 (277260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27728 (277270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.541 - mean_q: 12.854 - prob: 1.000\n",
            "\n",
            "Interval 27729 (277280 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 27730 (277290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.003 - mae: 7.281 - mean_q: 12.580 - prob: 1.000\n",
            "\n",
            "Interval 27731 (277300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.074 - mean_q: 12.405 - prob: 1.000\n",
            "\n",
            "Interval 27732 (277310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.003 - mae: 7.252 - mean_q: 12.502 - prob: 1.000\n",
            "\n",
            "Interval 27733 (277320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.712 - mean_q: 13.051 - prob: 1.000\n",
            "\n",
            "Interval 27734 (277330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 27735 (277340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.003 - mae: 7.011 - mean_q: 12.163 - prob: 1.000\n",
            "\n",
            "Interval 27736 (277350 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 27737 (277360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.002 - mae: 7.235 - mean_q: 12.529 - prob: 1.000\n",
            "\n",
            "Interval 27738 (277370 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 27739 (277380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.002 - mae: 7.235 - mean_q: 12.580 - prob: 1.000\n",
            "\n",
            "Interval 27740 (277390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.454 - mean_q: 12.927 - prob: 1.000\n",
            "\n",
            "Interval 27741 (277400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27742 (277410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27743 (277420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.002 - mae: 7.343 - mean_q: 12.663 - prob: 1.000\n",
            "\n",
            "Interval 27744 (277430 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.003 - mae: 7.092 - mean_q: 12.351 - prob: 1.000\n",
            "\n",
            "Interval 27745 (277440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.003 - mae: 7.547 - mean_q: 12.877 - prob: 1.000\n",
            "\n",
            "Interval 27746 (277450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.003 - mae: 7.336 - mean_q: 12.393 - prob: 1.000\n",
            "\n",
            "Interval 27747 (277460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.002 - mae: 7.346 - mean_q: 12.668 - prob: 1.000\n",
            "\n",
            "Interval 27748 (277470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27749 (277480 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.003 - mae: 7.624 - mean_q: 13.065 - prob: 1.000\n",
            "\n",
            "Interval 27750 (277490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.201 - mean_q: 12.531 - prob: 1.000\n",
            "\n",
            "Interval 27751 (277500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27752 (277510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.002 - mae: 7.180 - mean_q: 12.504 - prob: 1.000\n",
            "\n",
            "Interval 27753 (277520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.002 - mae: 7.729 - mean_q: 13.249 - prob: 1.000\n",
            "\n",
            "Interval 27754 (277530 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.207 - mean_q: 12.420 - prob: 1.000\n",
            "\n",
            "Interval 27755 (277540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27756 (277550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.002 - mae: 7.421 - mean_q: 12.770 - prob: 1.000\n",
            "\n",
            "Interval 27757 (277560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 27758 (277570 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.001 - mae: 7.414 - mean_q: 12.916 - prob: 1.000\n",
            "\n",
            "Interval 27759 (277580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27760 (277590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 2.000 [-8.000, 12.000] - loss: 0.002 - mae: 7.385 - mean_q: 12.619 - prob: 1.000\n",
            "\n",
            "Interval 27761 (277600 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 27762 (277610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.414 - mean_q: 12.806 - prob: 1.000\n",
            "\n",
            "Interval 27763 (277620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27764 (277630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 7.351 - mean_q: 12.764 - prob: 1.000\n",
            "\n",
            "Interval 27765 (277640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.408 - mean_q: 12.775 - prob: 1.000\n",
            "\n",
            "Interval 27766 (277650 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.208 - mean_q: 12.441 - prob: 1.000\n",
            "\n",
            "Interval 27767 (277660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.004 - mae: 7.218 - mean_q: 12.534 - prob: 1.000\n",
            "\n",
            "Interval 27768 (277670 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 27769 (277680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.004 - mae: 7.421 - mean_q: 12.752 - prob: 1.000\n",
            "\n",
            "Interval 27770 (277690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 7.546 - mean_q: 12.957 - prob: 1.000\n",
            "\n",
            "Interval 27771 (277700 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 27772 (277710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.002 - mae: 7.325 - mean_q: 12.625 - prob: 1.000\n",
            "\n",
            "Interval 27773 (277720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.439 - mean_q: 12.938 - prob: 1.000\n",
            "\n",
            "Interval 27774 (277730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.306 - mean_q: 12.552 - prob: 1.000\n",
            "\n",
            "Interval 27775 (277740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27776 (277750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 6.946 - mean_q: 12.028 - prob: 1.000\n",
            "\n",
            "Interval 27777 (277760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.003 - mae: 7.019 - mean_q: 12.272 - prob: 1.000\n",
            "\n",
            "Interval 27778 (277770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 27779 (277780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.003 - mae: 7.171 - mean_q: 12.492 - prob: 1.000\n",
            "\n",
            "Interval 27780 (277790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.002 - mae: 7.576 - mean_q: 13.034 - prob: 1.000\n",
            "\n",
            "Interval 27781 (277800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.004 - mae: 7.277 - mean_q: 12.560 - prob: 1.000\n",
            "\n",
            "Interval 27782 (277810 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 27783 (277820 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.500 - mean_q: 12.950 - prob: 1.000\n",
            "\n",
            "Interval 27784 (277830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.729 - mean_q: 13.141 - prob: 1.000\n",
            "\n",
            "Interval 27785 (277840 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 27786 (277850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -25.000 [-25.000, -25.000] - loss: 0.003 - mae: 7.483 - mean_q: 12.847 - prob: 1.000\n",
            "\n",
            "Interval 27787 (277860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.004 - mae: 7.537 - mean_q: 12.892 - prob: 1.000\n",
            "\n",
            "Interval 27788 (277870 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -25.000 [-25.000, -25.000] - loss: 0.002 - mae: 7.294 - mean_q: 12.469 - prob: 1.000\n",
            "\n",
            "Interval 27789 (277880 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 27790 (277890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.003 - mae: 7.551 - mean_q: 13.022 - prob: 1.000\n",
            "\n",
            "Interval 27791 (277900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.004 - mae: 7.317 - mean_q: 12.490 - prob: 1.000\n",
            "\n",
            "Interval 27792 (277910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.003 - mae: 7.399 - mean_q: 12.591 - prob: 1.000\n",
            "\n",
            "Interval 27793 (277920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.003 - mae: 7.191 - mean_q: 12.441 - prob: 1.000\n",
            "\n",
            "Interval 27794 (277930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.003 - mae: 7.273 - mean_q: 12.627 - prob: 1.000\n",
            "\n",
            "Interval 27795 (277940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 7.316 - mean_q: 12.591 - prob: 1.000\n",
            "\n",
            "Interval 27796 (277950 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 27797 (277960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.422 - mean_q: 12.721 - prob: 1.000\n",
            "\n",
            "Interval 27798 (277970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.435 - mean_q: 12.761 - prob: 1.000\n",
            "\n",
            "Interval 27799 (277980 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.518 - mean_q: 12.816 - prob: 1.000\n",
            "\n",
            "Interval 27800 (277990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27801 (278000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.181 - mean_q: 12.339 - prob: 1.000\n",
            "\n",
            "Interval 27802 (278010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.003 - mae: 7.341 - mean_q: 12.757 - prob: 1.000\n",
            "\n",
            "Interval 27803 (278020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.003 - mae: 7.479 - mean_q: 12.691 - prob: 1.000\n",
            "\n",
            "Interval 27804 (278030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27805 (278040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 10.500 [8.000, 13.000] - loss: 0.003 - mae: 7.232 - mean_q: 12.488 - prob: 1.000\n",
            "\n",
            "Interval 27806 (278050 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 27807 (278060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.010 - mae: 7.619 - mean_q: 12.941 - prob: 1.000\n",
            "\n",
            "Interval 27808 (278070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27809 (278080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.005 - mae: 7.394 - mean_q: 12.798 - prob: 1.000\n",
            "\n",
            "Interval 27810 (278090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.005 - mae: 7.511 - mean_q: 12.910 - prob: 1.000\n",
            "\n",
            "Interval 27811 (278100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.006 - mae: 7.361 - mean_q: 12.697 - prob: 1.000\n",
            "\n",
            "Interval 27812 (278110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.004 - mae: 7.607 - mean_q: 13.037 - prob: 1.000\n",
            "\n",
            "Interval 27813 (278120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 27814 (278130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.005 - mae: 7.323 - mean_q: 12.654 - prob: 1.000\n",
            "\n",
            "Interval 27815 (278140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.004 - mae: 7.134 - mean_q: 12.378 - prob: 1.000\n",
            "\n",
            "Interval 27816 (278150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.003 - mae: 7.421 - mean_q: 12.690 - prob: 1.000\n",
            "\n",
            "Interval 27817 (278160 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.003 - mae: 7.315 - mean_q: 12.722 - prob: 1.000\n",
            "\n",
            "Interval 27818 (278170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.003 - mae: 7.240 - mean_q: 12.547 - prob: 1.000\n",
            "\n",
            "Interval 27819 (278180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27820 (278190 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 27821 (278200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.004 - mae: 7.133 - mean_q: 12.331 - prob: 1.000\n",
            "\n",
            "Interval 27822 (278210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.003 - mae: 7.473 - mean_q: 12.877 - prob: 1.000\n",
            "\n",
            "Interval 27823 (278220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.312 - mean_q: 12.614 - prob: 1.000\n",
            "\n",
            "Interval 27824 (278230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27825 (278240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.004 - mae: 7.254 - mean_q: 12.771 - prob: 1.000\n",
            "\n",
            "Interval 27826 (278250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27827 (278260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.004 - mae: 7.493 - mean_q: 12.939 - prob: 1.000\n",
            "\n",
            "Interval 27828 (278270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.003 - mae: 7.476 - mean_q: 12.558 - prob: 1.000\n",
            "\n",
            "Interval 27829 (278280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27830 (278290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.003 - mae: 7.486 - mean_q: 12.770 - prob: 1.000\n",
            "\n",
            "Interval 27831 (278300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.003 - mae: 7.274 - mean_q: 12.480 - prob: 1.000\n",
            "\n",
            "Interval 27832 (278310 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.004 - mae: 7.627 - mean_q: 13.105 - prob: 1.000\n",
            "\n",
            "Interval 27833 (278320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27834 (278330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.003 - mae: 7.497 - mean_q: 12.933 - prob: 1.000\n",
            "\n",
            "Interval 27835 (278340 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 27836 (278350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 7.321 - mean_q: 12.649 - prob: 1.000\n",
            "\n",
            "Interval 27837 (278360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.004 - mae: 7.583 - mean_q: 13.158 - prob: 1.000\n",
            "\n",
            "Interval 27838 (278370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27839 (278380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 27840 (278390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.002 - mae: 7.317 - mean_q: 12.534 - prob: 1.000\n",
            "\n",
            "Interval 27841 (278400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.002 - mae: 7.726 - mean_q: 13.197 - prob: 1.000\n",
            "\n",
            "Interval 27842 (278410 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.003 - mae: 7.212 - mean_q: 12.405 - prob: 1.000\n",
            "\n",
            "Interval 27843 (278420 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.003 - mae: 7.242 - mean_q: 12.353 - prob: 1.000\n",
            "\n",
            "Interval 27844 (278430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.004 - mae: 7.264 - mean_q: 12.554 - prob: 1.000\n",
            "\n",
            "Interval 27845 (278440 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 27846 (278450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.002 - mae: 7.298 - mean_q: 12.681 - prob: 1.000\n",
            "\n",
            "Interval 27847 (278460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27848 (278470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.191 - mean_q: 12.295 - prob: 1.000\n",
            "\n",
            "Interval 27849 (278480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.002 - mae: 7.191 - mean_q: 12.403 - prob: 1.000\n",
            "\n",
            "Interval 27850 (278490 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 27851 (278500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.003 - mae: 7.382 - mean_q: 12.827 - prob: 1.000\n",
            "\n",
            "Interval 27852 (278510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.003 - mae: 7.404 - mean_q: 12.735 - prob: 1.000\n",
            "\n",
            "Interval 27853 (278520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.004 - mae: 7.802 - mean_q: 13.205 - prob: 1.000\n",
            "\n",
            "Interval 27854 (278530 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 27855 (278540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.004 - mae: 7.136 - mean_q: 12.355 - prob: 1.000\n",
            "\n",
            "Interval 27856 (278550 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 27857 (278560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.005 - mae: 7.388 - mean_q: 12.778 - prob: 1.000\n",
            "\n",
            "Interval 27858 (278570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.007 - mae: 7.351 - mean_q: 12.644 - prob: 1.000\n",
            "\n",
            "Interval 27859 (278580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 27860 (278590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.006 - mae: 7.680 - mean_q: 13.084 - prob: 1.000\n",
            "\n",
            "Interval 27861 (278600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.005 - mae: 7.378 - mean_q: 12.781 - prob: 1.000\n",
            "\n",
            "Interval 27862 (278610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27863 (278620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.004 - mae: 7.442 - mean_q: 12.763 - prob: 1.000\n",
            "\n",
            "Interval 27864 (278630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.005 - mae: 7.106 - mean_q: 12.279 - prob: 1.000\n",
            "\n",
            "Interval 27865 (278640 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.005 - mae: 7.484 - mean_q: 12.784 - prob: 1.000\n",
            "\n",
            "Interval 27866 (278650 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.005 - mae: 7.249 - mean_q: 12.599 - prob: 1.000\n",
            "\n",
            "Interval 27867 (278660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27868 (278670 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 27869 (278680 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -23.000 [-23.000, -23.000] - loss: 0.004 - mae: 7.554 - mean_q: 13.120 - prob: 1.000\n",
            "\n",
            "Interval 27870 (278690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.454 - mean_q: 12.898 - prob: 1.000\n",
            "\n",
            "Interval 27871 (278700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27872 (278710 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.003 - mae: 7.610 - mean_q: 12.935 - prob: 1.000\n",
            "\n",
            "Interval 27873 (278720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27874 (278730 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.003 - mae: 7.245 - mean_q: 12.519 - prob: 1.000\n",
            "\n",
            "Interval 27875 (278740 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.003 - mae: 7.366 - mean_q: 12.771 - prob: 1.000\n",
            "\n",
            "Interval 27876 (278750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 27877 (278760 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.005 - mae: 7.199 - mean_q: 12.560 - prob: 1.000\n",
            "\n",
            "Interval 27878 (278770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.007 - mae: 7.607 - mean_q: 12.977 - prob: 1.000\n",
            "\n",
            "Interval 27879 (278780 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 27880 (278790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.004 - mae: 7.555 - mean_q: 13.040 - prob: 1.000\n",
            "\n",
            "Interval 27881 (278800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.005 - mae: 7.695 - mean_q: 13.159 - prob: 1.000\n",
            "\n",
            "Interval 27882 (278810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.569 - mean_q: 13.000 - prob: 1.000\n",
            "\n",
            "Interval 27883 (278820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 27884 (278830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.003 - mae: 7.618 - mean_q: 12.893 - prob: 1.000\n",
            "\n",
            "Interval 27885 (278840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.003 - mae: 7.401 - mean_q: 12.767 - prob: 1.000\n",
            "\n",
            "Interval 27886 (278850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27887 (278860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.756 - mean_q: 13.113 - prob: 1.000\n",
            "\n",
            "Interval 27888 (278870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.002 - mae: 6.974 - mean_q: 12.155 - prob: 1.000\n",
            "\n",
            "Interval 27889 (278880 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 27890 (278890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.004 - mae: 7.171 - mean_q: 12.356 - prob: 1.000\n",
            "\n",
            "Interval 27891 (278900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.003 - mae: 7.256 - mean_q: 12.605 - prob: 1.000\n",
            "\n",
            "Interval 27892 (278910 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.003 - mae: 7.208 - mean_q: 12.492 - prob: 1.000\n",
            "\n",
            "Interval 27893 (278920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27894 (278930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.612 - mean_q: 13.090 - prob: 1.000\n",
            "\n",
            "Interval 27895 (278940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.003 - mae: 7.266 - mean_q: 12.636 - prob: 1.000\n",
            "\n",
            "Interval 27896 (278950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27897 (278960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 7.541 - mean_q: 12.878 - prob: 1.000\n",
            "\n",
            "Interval 27898 (278970 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 27899 (278980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.003 - mae: 7.368 - mean_q: 12.677 - prob: 1.000\n",
            "\n",
            "Interval 27900 (278990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.004 - mae: 7.214 - mean_q: 12.687 - prob: 1.000\n",
            "\n",
            "Interval 27901 (279000 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 27902 (279010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.005 - mae: 7.397 - mean_q: 12.793 - prob: 1.000\n",
            "\n",
            "Interval 27903 (279020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27904 (279030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.005 - mae: 7.010 - mean_q: 12.315 - prob: 1.000\n",
            "\n",
            "Interval 27905 (279040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.005 - mae: 7.418 - mean_q: 12.881 - prob: 1.000\n",
            "\n",
            "Interval 27906 (279050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.004 - mae: 7.735 - mean_q: 13.209 - prob: 1.000\n",
            "\n",
            "Interval 27907 (279060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.004 - mae: 7.408 - mean_q: 12.846 - prob: 1.000\n",
            "\n",
            "Interval 27908 (279070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27909 (279080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.003 - mae: 6.913 - mean_q: 11.977 - prob: 1.000\n",
            "\n",
            "Interval 27910 (279090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.003 - mae: 7.627 - mean_q: 13.108 - prob: 1.000\n",
            "\n",
            "Interval 27911 (279100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.002 - mae: 7.455 - mean_q: 12.926 - prob: 1.000\n",
            "\n",
            "Interval 27912 (279110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.002 - mae: 7.227 - mean_q: 12.427 - prob: 1.000\n",
            "\n",
            "Interval 27913 (279120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 27914 (279130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.002 - mae: 6.911 - mean_q: 12.047 - prob: 1.000\n",
            "\n",
            "Interval 27915 (279140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.342 - mean_q: 12.648 - prob: 1.000\n",
            "\n",
            "Interval 27916 (279150 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.003 - mae: 7.439 - mean_q: 12.963 - prob: 1.000\n",
            "\n",
            "Interval 27917 (279160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 27918 (279170 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.003 - mae: 7.739 - mean_q: 13.323 - prob: 1.000\n",
            "\n",
            "Interval 27919 (279180 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.002 - mae: 7.413 - mean_q: 12.671 - prob: 1.000\n",
            "\n",
            "Interval 27920 (279190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27921 (279200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.002 - mae: 7.383 - mean_q: 12.795 - prob: 1.000\n",
            "\n",
            "Interval 27922 (279210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.222 - mean_q: 12.483 - prob: 1.000\n",
            "\n",
            "Interval 27923 (279220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.668 - mean_q: 13.127 - prob: 1.000\n",
            "\n",
            "Interval 27924 (279230 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 27925 (279240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.003 - mae: 7.484 - mean_q: 12.880 - prob: 1.000\n",
            "\n",
            "Interval 27926 (279250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27927 (279260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.328 - mean_q: 12.695 - prob: 1.000\n",
            "\n",
            "Interval 27928 (279270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.005 - mae: 7.230 - mean_q: 12.648 - prob: 1.000\n",
            "\n",
            "Interval 27929 (279280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.335 - mean_q: 12.673 - prob: 1.000\n",
            "\n",
            "Interval 27930 (279290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27931 (279300 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.003 - mae: 7.379 - mean_q: 12.831 - prob: 1.000\n",
            "\n",
            "Interval 27932 (279310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.005 - mae: 7.365 - mean_q: 12.756 - prob: 1.000\n",
            "\n",
            "Interval 27933 (279320 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 27934 (279330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.004 - mae: 7.403 - mean_q: 12.599 - prob: 1.000\n",
            "\n",
            "Interval 27935 (279340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.003 - mae: 7.176 - mean_q: 12.460 - prob: 1.000\n",
            "\n",
            "Interval 27936 (279350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27937 (279360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.004 - mae: 7.649 - mean_q: 13.006 - prob: 1.000\n",
            "\n",
            "Interval 27938 (279370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.203 - mean_q: 12.446 - prob: 1.000\n",
            "\n",
            "Interval 27939 (279380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 27940 (279390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.003 - mae: 7.476 - mean_q: 12.899 - prob: 1.000\n",
            "\n",
            "Interval 27941 (279400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27942 (279410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.004 - mae: 7.792 - mean_q: 13.149 - prob: 1.000\n",
            "\n",
            "Interval 27943 (279420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27944 (279430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.529 - mean_q: 12.949 - prob: 1.000\n",
            "\n",
            "Interval 27945 (279440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.003 - mae: 7.322 - mean_q: 12.657 - prob: 1.000\n",
            "\n",
            "Interval 27946 (279450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.003 - mae: 7.325 - mean_q: 12.585 - prob: 1.000\n",
            "\n",
            "Interval 27947 (279460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.003 - mae: 7.272 - mean_q: 12.478 - prob: 1.000\n",
            "\n",
            "Interval 27948 (279470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27949 (279480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.433 - mean_q: 12.826 - prob: 1.000\n",
            "\n",
            "Interval 27950 (279490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.463 - mean_q: 12.819 - prob: 1.000\n",
            "\n",
            "Interval 27951 (279500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.625 - mean_q: 13.080 - prob: 1.000\n",
            "\n",
            "Interval 27952 (279510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 27953 (279520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -17.000 [-17.000, -17.000] - loss: 0.002 - mae: 7.134 - mean_q: 12.353 - prob: 1.000\n",
            "\n",
            "Interval 27954 (279530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 27955 (279540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.003 - mae: 7.302 - mean_q: 12.862 - prob: 1.000\n",
            "\n",
            "Interval 27956 (279550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.205 - mean_q: 12.467 - prob: 1.000\n",
            "\n",
            "Interval 27957 (279560 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 27958 (279570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.003 - mae: 7.247 - mean_q: 12.521 - prob: 1.000\n",
            "\n",
            "Interval 27959 (279580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27960 (279590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.003 - mae: 7.529 - mean_q: 12.952 - prob: 1.000\n",
            "\n",
            "Interval 27961 (279600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.015 - mae: 7.241 - mean_q: 12.547 - prob: 1.000\n",
            "\n",
            "Interval 27962 (279610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27963 (279620 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.004 - mae: 7.519 - mean_q: 12.858 - prob: 1.000\n",
            "\n",
            "Interval 27964 (279630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27965 (279640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.003 - mae: 7.675 - mean_q: 13.152 - prob: 1.000\n",
            "\n",
            "Interval 27966 (279650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.222 - mean_q: 12.641 - prob: 1.000\n",
            "\n",
            "Interval 27967 (279660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27968 (279670 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.004 - mae: 7.622 - mean_q: 12.989 - prob: 1.000\n",
            "\n",
            "Interval 27969 (279680 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.022 - mae: 7.308 - mean_q: 12.509 - prob: 1.000\n",
            "\n",
            "Interval 27970 (279690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27971 (279700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.005 - mae: 7.368 - mean_q: 12.723 - prob: 1.000\n",
            "\n",
            "Interval 27972 (279710 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.008 - mae: 7.402 - mean_q: 12.805 - prob: 1.000\n",
            "\n",
            "Interval 27973 (279720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27974 (279730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.005 - mae: 7.176 - mean_q: 12.355 - prob: 1.000\n",
            "\n",
            "Interval 27975 (279740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.005 - mae: 7.742 - mean_q: 13.237 - prob: 1.000\n",
            "\n",
            "Interval 27976 (279750 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 27977 (279760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.004 - mae: 7.648 - mean_q: 13.230 - prob: 1.000\n",
            "\n",
            "Interval 27978 (279770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.005 - mae: 7.340 - mean_q: 12.633 - prob: 1.000\n",
            "\n",
            "Interval 27979 (279780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.004 - mae: 7.641 - mean_q: 13.164 - prob: 1.000\n",
            "\n",
            "Interval 27980 (279790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27981 (279800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.004 - mae: 7.575 - mean_q: 12.925 - prob: 1.000\n",
            "\n",
            "Interval 27982 (279810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.004 - mae: 7.109 - mean_q: 12.354 - prob: 1.000\n",
            "\n",
            "Interval 27983 (279820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27984 (279830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.430 - mean_q: 12.822 - prob: 1.000\n",
            "\n",
            "Interval 27985 (279840 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.003 - mae: 7.376 - mean_q: 12.724 - prob: 1.000\n",
            "\n",
            "Interval 27986 (279850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27987 (279860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.419 - mean_q: 12.841 - prob: 1.000\n",
            "\n",
            "Interval 27988 (279870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.284 - mean_q: 12.556 - prob: 1.000\n",
            "\n",
            "Interval 27989 (279880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 27990 (279890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.003 - mae: 7.063 - mean_q: 12.207 - prob: 1.000\n",
            "\n",
            "Interval 27991 (279900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.004 - mae: 7.156 - mean_q: 12.427 - prob: 1.000\n",
            "\n",
            "Interval 27992 (279910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.003 - mae: 7.496 - mean_q: 12.916 - prob: 1.000\n",
            "\n",
            "Interval 27993 (279920 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 27994 (279930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.003 - mae: 7.430 - mean_q: 12.777 - prob: 1.000\n",
            "\n",
            "Interval 27995 (279940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.003 - mae: 7.452 - mean_q: 12.711 - prob: 1.000\n",
            "\n",
            "Interval 27996 (279950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.004 - mae: 7.498 - mean_q: 12.910 - prob: 1.000\n",
            "\n",
            "Interval 27997 (279960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.562 - mean_q: 13.083 - prob: 1.000\n",
            "\n",
            "Interval 27998 (279970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.004 - mae: 7.620 - mean_q: 12.927 - prob: 1.000\n",
            "\n",
            "Interval 27999 (279980 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 28000 (279990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 6.907 - mean_q: 12.170 - prob: 1.000\n",
            "\n",
            "Interval 28001 (280000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.007 - mae: 7.365 - mean_q: 12.822 - prob: 1.000\n",
            "\n",
            "Interval 28002 (280010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28003 (280020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.006 - mae: 7.507 - mean_q: 12.849 - prob: 1.000\n",
            "\n",
            "Interval 28004 (280030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.006 - mae: 7.172 - mean_q: 12.574 - prob: 1.000\n",
            "\n",
            "Interval 28005 (280040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.004 - mae: 7.360 - mean_q: 12.793 - prob: 1.000\n",
            "\n",
            "Interval 28006 (280050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28007 (280060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 7.167 - mean_q: 12.498 - prob: 1.000\n",
            "\n",
            "Interval 28008 (280070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.005 - mae: 7.465 - mean_q: 12.820 - prob: 1.000\n",
            "\n",
            "Interval 28009 (280080 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 28010 (280090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.004 - mae: 7.241 - mean_q: 12.387 - prob: 1.000\n",
            "\n",
            "Interval 28011 (280100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.479 - mean_q: 12.875 - prob: 1.000\n",
            "\n",
            "Interval 28012 (280110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28013 (280120 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 6.922 - mean_q: 12.121 - prob: 1.000\n",
            "\n",
            "Interval 28014 (280130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 28015 (280140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.002 - mae: 7.213 - mean_q: 12.424 - prob: 1.000\n",
            "\n",
            "Interval 28016 (280150 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.003 - mae: 7.366 - mean_q: 12.714 - prob: 1.000\n",
            "\n",
            "Interval 28017 (280160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28018 (280170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.003 - mae: 7.409 - mean_q: 12.834 - prob: 1.000\n",
            "\n",
            "Interval 28019 (280180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.361 - mean_q: 12.690 - prob: 1.000\n",
            "\n",
            "Interval 28020 (280190 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 28021 (280200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.580 - mean_q: 13.036 - prob: 1.000\n",
            "\n",
            "Interval 28022 (280210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.899 - mean_q: 13.601 - prob: 1.000\n",
            "\n",
            "Interval 28023 (280220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.002 - mae: 7.741 - mean_q: 13.158 - prob: 1.000\n",
            "\n",
            "Interval 28024 (280230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.002 - mae: 7.251 - mean_q: 12.522 - prob: 1.000\n",
            "\n",
            "Interval 28025 (280240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 28026 (280250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.002 - mae: 7.413 - mean_q: 12.743 - prob: 1.000\n",
            "\n",
            "Interval 28027 (280260 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.578 - mean_q: 13.072 - prob: 1.000\n",
            "\n",
            "Interval 28028 (280270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28029 (280280 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.002 - mae: 7.153 - mean_q: 12.457 - prob: 1.000\n",
            "\n",
            "Interval 28030 (280290 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.561 - mean_q: 12.891 - prob: 1.000\n",
            "\n",
            "Interval 28031 (280300 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 7.105 - mean_q: 12.371 - prob: 1.000\n",
            "\n",
            "Interval 28032 (280310 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 28033 (280320 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.002 - mae: 7.001 - mean_q: 12.199 - prob: 1.000\n",
            "\n",
            "Interval 28034 (280330 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.098 - mean_q: 12.362 - prob: 1.000\n",
            "\n",
            "Interval 28035 (280340 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 28036 (280350 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.002 - mae: 7.326 - mean_q: 12.687 - prob: 1.000\n",
            "\n",
            "Interval 28037 (280360 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.002 - mae: 7.562 - mean_q: 12.938 - prob: 1.000\n",
            "\n",
            "Interval 28038 (280370 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.0000\n",
            "Interval 28039 (280380 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.002 - mae: 7.251 - mean_q: 12.445 - prob: 1.000\n",
            "\n",
            "Interval 28040 (280390 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.002 - mae: 7.328 - mean_q: 12.752 - prob: 1.000\n",
            "\n",
            "Interval 28041 (280400 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 28042 (280410 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.002 - mae: 6.988 - mean_q: 12.119 - prob: 1.000\n",
            "\n",
            "Interval 28043 (280420 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 28044 (280430 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -24.000 [-24.000, -24.000] - loss: 0.003 - mae: 7.429 - mean_q: 12.797 - prob: 1.000\n",
            "\n",
            "Interval 28045 (280440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 28046 (280450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.003 - mae: 7.459 - mean_q: 12.876 - prob: 1.000\n",
            "\n",
            "Interval 28047 (280460 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.004 - mae: 7.372 - mean_q: 12.741 - prob: 1.000\n",
            "\n",
            "Interval 28048 (280470 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.003 - mae: 7.455 - mean_q: 12.743 - prob: 1.000\n",
            "\n",
            "Interval 28049 (280480 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.003 - mae: 7.368 - mean_q: 12.677 - prob: 1.000\n",
            "\n",
            "Interval 28050 (280490 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 28051 (280500 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.013 - mae: 7.607 - mean_q: 13.062 - prob: 1.000\n",
            "\n",
            "Interval 28052 (280510 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.004 - mae: 7.484 - mean_q: 12.863 - prob: 1.000\n",
            "\n",
            "Interval 28053 (280520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.003 - mae: 7.507 - mean_q: 12.852 - prob: 1.000\n",
            "\n",
            "Interval 28054 (280530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.007 - mae: 7.531 - mean_q: 12.952 - prob: 1.000\n",
            "\n",
            "Interval 28055 (280540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.005 - mae: 7.408 - mean_q: 12.795 - prob: 1.000\n",
            "\n",
            "Interval 28056 (280550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.003 - mae: 7.668 - mean_q: 13.047 - prob: 1.000\n",
            "\n",
            "Interval 28057 (280560 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.004 - mae: 7.416 - mean_q: 12.602 - prob: 1.000\n",
            "\n",
            "Interval 28058 (280570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.264 - mean_q: 12.383 - prob: 1.000\n",
            "\n",
            "Interval 28059 (280580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28060 (280590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.003 - mae: 7.369 - mean_q: 12.766 - prob: 1.000\n",
            "\n",
            "Interval 28061 (280600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28062 (280610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 9.500 [7.000, 12.000] - loss: 0.003 - mae: 7.748 - mean_q: 13.297 - prob: 1.000\n",
            "\n",
            "Interval 28063 (280620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 28064 (280630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.005 - mae: 7.438 - mean_q: 12.806 - prob: 1.000\n",
            "\n",
            "Interval 28065 (280640 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 28066 (280650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.006 - mae: 7.004 - mean_q: 12.225 - prob: 1.000\n",
            "\n",
            "Interval 28067 (280660 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 28068 (280670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.005 - mae: 7.290 - mean_q: 12.740 - prob: 1.000\n",
            "\n",
            "Interval 28069 (280680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.004 - mae: 7.450 - mean_q: 12.883 - prob: 1.000\n",
            "\n",
            "Interval 28070 (280690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 28071 (280700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.006 - mae: 7.513 - mean_q: 12.918 - prob: 1.000\n",
            "\n",
            "Interval 28072 (280710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.005 - mae: 7.026 - mean_q: 12.294 - prob: 1.000\n",
            "\n",
            "Interval 28073 (280720 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 28074 (280730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.005 - mae: 7.324 - mean_q: 12.559 - prob: 1.000\n",
            "\n",
            "Interval 28075 (280740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.006 - mae: 7.836 - mean_q: 13.400 - prob: 1.000\n",
            "\n",
            "Interval 28076 (280750 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 28077 (280760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.005 - mae: 7.522 - mean_q: 12.924 - prob: 1.000\n",
            "\n",
            "Interval 28078 (280770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.005 - mae: 7.730 - mean_q: 13.111 - prob: 1.000\n",
            "\n",
            "Interval 28079 (280780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 28080 (280790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.006 - mae: 7.226 - mean_q: 12.631 - prob: 1.000\n",
            "\n",
            "Interval 28081 (280800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.007 - mae: 7.341 - mean_q: 12.510 - prob: 1.000\n",
            "\n",
            "Interval 28082 (280810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28083 (280820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 5.000 [-3.000, 13.000] - loss: 0.004 - mae: 7.093 - mean_q: 12.204 - prob: 1.000\n",
            "\n",
            "Interval 28084 (280830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.004 - mae: 7.336 - mean_q: 12.594 - prob: 1.000\n",
            "\n",
            "Interval 28085 (280840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 28086 (280850 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.003 - mae: 7.422 - mean_q: 12.683 - prob: 1.000\n",
            "\n",
            "Interval 28087 (280860 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.003 - mae: 7.367 - mean_q: 12.755 - prob: 1.000\n",
            "\n",
            "Interval 28088 (280870 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 28089 (280880 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.446 - mean_q: 12.774 - prob: 1.000\n",
            "\n",
            "Interval 28090 (280890 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.003 - mae: 7.548 - mean_q: 13.049 - prob: 1.000\n",
            "\n",
            "Interval 28091 (280900 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.003 - mae: 7.094 - mean_q: 12.161 - prob: 1.000\n",
            "\n",
            "Interval 28092 (280910 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 28093 (280920 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.341 - mean_q: 12.547 - prob: 1.000\n",
            "\n",
            "Interval 28094 (280930 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.005 - mae: 7.609 - mean_q: 13.076 - prob: 1.000\n",
            "\n",
            "Interval 28095 (280940 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.241 - mean_q: 12.447 - prob: 1.000\n",
            "\n",
            "Interval 28096 (280950 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 28097 (280960 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.002 - mae: 7.376 - mean_q: 12.809 - prob: 1.000\n",
            "\n",
            "Interval 28098 (280970 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 28099 (280980 steps performed)\n",
            "10/10 [==============================] - 0s 14ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.324 - mean_q: 12.683 - prob: 1.000\n",
            "\n",
            "Interval 28100 (280990 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.005 - mae: 7.059 - mean_q: 12.261 - prob: 1.000\n",
            "\n",
            "Interval 28101 (281000 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.004 - mae: 7.134 - mean_q: 12.379 - prob: 1.000\n",
            "\n",
            "Interval 28102 (281010 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 28103 (281020 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.9000\n",
            "Interval 28104 (281030 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.002 - mae: 7.235 - mean_q: 12.518 - prob: 1.000\n",
            "\n",
            "Interval 28105 (281040 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.004 - mae: 7.269 - mean_q: 12.556 - prob: 1.000\n",
            "\n",
            "Interval 28106 (281050 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 28107 (281060 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.004 - mae: 7.665 - mean_q: 13.103 - prob: 1.000\n",
            "\n",
            "Interval 28108 (281070 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.005 - mae: 7.123 - mean_q: 12.393 - prob: 1.000\n",
            "\n",
            "Interval 28109 (281080 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.005 - mae: 7.492 - mean_q: 12.836 - prob: 1.000\n",
            "\n",
            "Interval 28110 (281090 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 28111 (281100 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.518 - mean_q: 12.928 - prob: 1.000\n",
            "\n",
            "Interval 28112 (281110 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 28113 (281120 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.003 - mae: 7.566 - mean_q: 13.096 - prob: 1.000\n",
            "\n",
            "Interval 28114 (281130 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 28115 (281140 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.002 - mae: 7.059 - mean_q: 12.402 - prob: 1.000\n",
            "\n",
            "Interval 28116 (281150 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.464 - mean_q: 12.852 - prob: 1.000\n",
            "\n",
            "Interval 28117 (281160 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 28118 (281170 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.439 - mean_q: 12.828 - prob: 1.000\n",
            "\n",
            "Interval 28119 (281180 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.002 - mae: 7.605 - mean_q: 13.151 - prob: 1.000\n",
            "\n",
            "Interval 28120 (281190 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.330 - mean_q: 12.834 - prob: 1.000\n",
            "\n",
            "Interval 28121 (281200 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -2.8000\n",
            "Interval 28122 (281210 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -18.000 [-18.000, -18.000] - loss: 0.004 - mae: 7.824 - mean_q: 13.288 - prob: 1.000\n",
            "\n",
            "Interval 28123 (281220 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.9000\n",
            "Interval 28124 (281230 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.003 - mae: 7.406 - mean_q: 12.824 - prob: 1.000\n",
            "\n",
            "Interval 28125 (281240 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.222 - mean_q: 12.387 - prob: 1.000\n",
            "\n",
            "Interval 28126 (281250 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 28127 (281260 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.600 - mean_q: 13.084 - prob: 1.000\n",
            "\n",
            "Interval 28128 (281270 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.0000\n",
            "Interval 28129 (281280 steps performed)\n",
            "10/10 [==============================] - 0s 14ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.003 - mae: 7.084 - mean_q: 12.275 - prob: 1.000\n",
            "\n",
            "Interval 28130 (281290 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.003 - mae: 7.530 - mean_q: 12.879 - prob: 1.000\n",
            "\n",
            "Interval 28131 (281300 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 28132 (281310 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.003 - mae: 7.340 - mean_q: 12.598 - prob: 1.000\n",
            "\n",
            "Interval 28133 (281320 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 28134 (281330 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.441 - mean_q: 12.762 - prob: 1.000\n",
            "\n",
            "Interval 28135 (281340 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.003 - mae: 7.185 - mean_q: 12.419 - prob: 1.000\n",
            "\n",
            "Interval 28136 (281350 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -2.8000\n",
            "Interval 28137 (281360 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -18.000 [-18.000, -18.000] - loss: 0.004 - mae: 7.485 - mean_q: 12.799 - prob: 1.000\n",
            "\n",
            "Interval 28138 (281370 steps performed)\n",
            "10/10 [==============================] - 0s 14ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 7.273 - mean_q: 12.553 - prob: 1.000\n",
            "\n",
            "Interval 28139 (281380 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: -1.0000\n",
            "Interval 28140 (281390 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.004 - mae: 7.456 - mean_q: 12.842 - prob: 1.000\n",
            "\n",
            "Interval 28141 (281400 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.201 - mean_q: 12.593 - prob: 1.000\n",
            "\n",
            "Interval 28142 (281410 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.451 - mean_q: 12.860 - prob: 1.000\n",
            "\n",
            "Interval 28143 (281420 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.003 - mae: 7.333 - mean_q: 12.736 - prob: 1.000\n",
            "\n",
            "Interval 28144 (281430 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 28145 (281440 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.002 - mae: 7.453 - mean_q: 12.702 - prob: 1.000\n",
            "\n",
            "Interval 28146 (281450 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.001 - mean_q: 12.310 - prob: 1.000\n",
            "\n",
            "Interval 28147 (281460 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 28148 (281470 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 7.026 - mean_q: 12.148 - prob: 1.000\n",
            "\n",
            "Interval 28149 (281480 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.003 - mae: 7.231 - mean_q: 12.607 - prob: 1.000\n",
            "\n",
            "Interval 28150 (281490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.004 - mae: 7.379 - mean_q: 12.717 - prob: 1.000\n",
            "\n",
            "Interval 28151 (281500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 28152 (281510 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.003 - mae: 7.454 - mean_q: 12.903 - prob: 1.000\n",
            "\n",
            "Interval 28153 (281520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.003 - mae: 7.655 - mean_q: 13.013 - prob: 1.000\n",
            "\n",
            "Interval 28154 (281530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28155 (281540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.003 - mae: 7.207 - mean_q: 12.448 - prob: 1.000\n",
            "\n",
            "Interval 28156 (281550 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.003 - mae: 7.399 - mean_q: 12.808 - prob: 1.000\n",
            "\n",
            "Interval 28157 (281560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28158 (281570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.003 - mae: 7.452 - mean_q: 12.732 - prob: 1.000\n",
            "\n",
            "Interval 28159 (281580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.002 - mae: 7.480 - mean_q: 12.814 - prob: 1.000\n",
            "\n",
            "Interval 28160 (281590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.002 - mae: 7.191 - mean_q: 12.406 - prob: 1.000\n",
            "\n",
            "Interval 28161 (281600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.221 - mean_q: 12.473 - prob: 1.000\n",
            "\n",
            "Interval 28162 (281610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.003 - mae: 7.447 - mean_q: 12.831 - prob: 1.000\n",
            "\n",
            "Interval 28163 (281620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 28164 (281630 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.002 - mae: 7.526 - mean_q: 12.861 - prob: 1.000\n",
            "\n",
            "Interval 28165 (281640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 6.955 - mean_q: 12.218 - prob: 1.000\n",
            "\n",
            "Interval 28166 (281650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28167 (281660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.281 - mean_q: 12.623 - prob: 1.000\n",
            "\n",
            "Interval 28168 (281670 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.003 - mae: 7.554 - mean_q: 13.024 - prob: 1.000\n",
            "\n",
            "Interval 28169 (281680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.353 - mean_q: 12.535 - prob: 1.000\n",
            "\n",
            "Interval 28170 (281690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28171 (281700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.002 - mae: 7.474 - mean_q: 12.799 - prob: 1.000\n",
            "\n",
            "Interval 28172 (281710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 28173 (281720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -22.000 [-22.000, -22.000] - loss: 0.003 - mae: 7.097 - mean_q: 12.370 - prob: 1.000\n",
            "\n",
            "Interval 28174 (281730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 28175 (281740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.002 - mae: 7.310 - mean_q: 12.647 - prob: 1.000\n",
            "\n",
            "Interval 28176 (281750 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.003 - mae: 7.510 - mean_q: 12.890 - prob: 1.000\n",
            "\n",
            "Interval 28177 (281760 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.007 - mae: 7.350 - mean_q: 12.659 - prob: 1.000\n",
            "\n",
            "Interval 28178 (281770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28179 (281780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.003 - mae: 7.689 - mean_q: 13.072 - prob: 1.000\n",
            "\n",
            "Interval 28180 (281790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.004 - mae: 7.180 - mean_q: 12.626 - prob: 1.000\n",
            "\n",
            "Interval 28181 (281800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.483 - mean_q: 12.924 - prob: 1.000\n",
            "\n",
            "Interval 28182 (281810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.003 - mae: 7.391 - mean_q: 12.671 - prob: 1.000\n",
            "\n",
            "Interval 28183 (281820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28184 (281830 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.003 - mae: 7.522 - mean_q: 12.945 - prob: 1.000\n",
            "\n",
            "Interval 28185 (281840 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.003 - mae: 7.637 - mean_q: 13.138 - prob: 1.000\n",
            "\n",
            "Interval 28186 (281850 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.425 - mean_q: 12.754 - prob: 1.000\n",
            "\n",
            "Interval 28187 (281860 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.612 - mean_q: 13.003 - prob: 1.000\n",
            "\n",
            "Interval 28188 (281870 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 28189 (281880 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.003 - mae: 7.290 - mean_q: 12.626 - prob: 1.000\n",
            "\n",
            "Interval 28190 (281890 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.211 - mean_q: 12.381 - prob: 1.000\n",
            "\n",
            "Interval 28191 (281900 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.093 - mean_q: 12.309 - prob: 1.000\n",
            "\n",
            "Interval 28192 (281910 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.002 - mae: 7.320 - mean_q: 12.544 - prob: 1.000\n",
            "\n",
            "Interval 28193 (281920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28194 (281930 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.002 - mae: 7.176 - mean_q: 12.448 - prob: 1.000\n",
            "\n",
            "Interval 28195 (281940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.746 - mean_q: 13.093 - prob: 1.000\n",
            "\n",
            "Interval 28196 (281950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 28197 (281960 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.002 - mae: 7.701 - mean_q: 13.200 - prob: 1.000\n",
            "\n",
            "Interval 28198 (281970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28199 (281980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 7.694 - mean_q: 13.115 - prob: 1.000\n",
            "\n",
            "Interval 28200 (281990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.045 - mean_q: 12.252 - prob: 1.000\n",
            "\n",
            "Interval 28201 (282000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28202 (282010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.003 - mae: 7.594 - mean_q: 13.100 - prob: 1.000\n",
            "\n",
            "Interval 28203 (282020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 28204 (282030 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.003 - mae: 7.505 - mean_q: 12.866 - prob: 1.000\n",
            "\n",
            "Interval 28205 (282040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28206 (282050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.005 - mae: 7.451 - mean_q: 12.896 - prob: 1.000\n",
            "\n",
            "Interval 28207 (282060 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.004 - mae: 7.298 - mean_q: 12.594 - prob: 1.000\n",
            "\n",
            "Interval 28208 (282070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 28209 (282080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.005 - mae: 7.257 - mean_q: 12.586 - prob: 1.000\n",
            "\n",
            "Interval 28210 (282090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.005 - mae: 7.263 - mean_q: 12.584 - prob: 1.000\n",
            "\n",
            "Interval 28211 (282100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28212 (282110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.006 - mae: 7.615 - mean_q: 13.062 - prob: 1.000\n",
            "\n",
            "Interval 28213 (282120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28214 (282130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.003 - mae: 7.572 - mean_q: 12.926 - prob: 1.000\n",
            "\n",
            "Interval 28215 (282140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28216 (282150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.004 - mae: 7.358 - mean_q: 12.822 - prob: 1.000\n",
            "\n",
            "Interval 28217 (282160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.004 - mae: 7.345 - mean_q: 12.639 - prob: 1.000\n",
            "\n",
            "Interval 28218 (282170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28219 (282180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.004 - mae: 7.387 - mean_q: 12.617 - prob: 1.000\n",
            "\n",
            "Interval 28220 (282190 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.003 - mae: 7.470 - mean_q: 12.889 - prob: 1.000\n",
            "\n",
            "Interval 28221 (282200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28222 (282210 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.004 - mae: 7.895 - mean_q: 13.487 - prob: 1.000\n",
            "\n",
            "Interval 28223 (282220 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.004 - mae: 7.091 - mean_q: 12.294 - prob: 1.000\n",
            "\n",
            "Interval 28224 (282230 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.003 - mae: 7.551 - mean_q: 13.071 - prob: 1.000\n",
            "\n",
            "Interval 28225 (282240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28226 (282250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.003 - mae: 7.444 - mean_q: 12.889 - prob: 1.000\n",
            "\n",
            "Interval 28227 (282260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28228 (282270 steps performed)\n",
            "10/10 [==============================] - 0s 14ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.004 - mae: 7.424 - mean_q: 12.815 - prob: 1.000\n",
            "\n",
            "Interval 28229 (282280 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.007 - mae: 7.401 - mean_q: 12.914 - prob: 1.000\n",
            "\n",
            "Interval 28230 (282290 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.007 - mae: 7.259 - mean_q: 12.625 - prob: 1.000\n",
            "\n",
            "Interval 28231 (282300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28232 (282310 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.004 - mae: 7.386 - mean_q: 12.769 - prob: 1.000\n",
            "\n",
            "Interval 28233 (282320 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.004 - mae: 7.421 - mean_q: 12.966 - prob: 1.000\n",
            "\n",
            "Interval 28234 (282330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.006 - mae: 7.335 - mean_q: 12.757 - prob: 1.000\n",
            "\n",
            "Interval 28235 (282340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 28236 (282350 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.004 - mae: 7.681 - mean_q: 13.128 - prob: 1.000\n",
            "\n",
            "Interval 28237 (282360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28238 (282370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.003 - mae: 7.427 - mean_q: 12.820 - prob: 1.000\n",
            "\n",
            "Interval 28239 (282380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28240 (282390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.003 - mae: 7.161 - mean_q: 12.504 - prob: 1.000\n",
            "\n",
            "Interval 28241 (282400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.275 - mean_q: 12.516 - prob: 1.000\n",
            "\n",
            "Interval 28242 (282410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 28243 (282420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.003 - mae: 7.343 - mean_q: 12.650 - prob: 1.000\n",
            "\n",
            "Interval 28244 (282430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28245 (282440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.003 - mae: 7.167 - mean_q: 12.313 - prob: 1.000\n",
            "\n",
            "Interval 28246 (282450 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.004 - mae: 7.375 - mean_q: 12.593 - prob: 1.000\n",
            "\n",
            "Interval 28247 (282460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.003 - mae: 7.129 - mean_q: 12.302 - prob: 1.000\n",
            "\n",
            "Interval 28248 (282470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 28249 (282480 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.005 - mae: 7.458 - mean_q: 12.794 - prob: 1.000\n",
            "\n",
            "Interval 28250 (282490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.006 - mae: 7.478 - mean_q: 12.751 - prob: 1.000\n",
            "\n",
            "Interval 28251 (282500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 28252 (282510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.003 - mae: 7.388 - mean_q: 12.768 - prob: 1.000\n",
            "\n",
            "Interval 28253 (282520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.005 - mae: 7.376 - mean_q: 12.694 - prob: 1.000\n",
            "\n",
            "Interval 28254 (282530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28255 (282540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.004 - mae: 7.047 - mean_q: 12.299 - prob: 1.000\n",
            "\n",
            "Interval 28256 (282550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.003 - mae: 7.414 - mean_q: 12.909 - prob: 1.000\n",
            "\n",
            "Interval 28257 (282560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 28258 (282570 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.005 - mae: 7.512 - mean_q: 12.959 - prob: 1.000\n",
            "\n",
            "Interval 28259 (282580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28260 (282590 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.003 - mae: 7.259 - mean_q: 12.520 - prob: 1.000\n",
            "\n",
            "Interval 28261 (282600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.003 - mae: 7.437 - mean_q: 12.916 - prob: 1.000\n",
            "\n",
            "Interval 28262 (282610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28263 (282620 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.506 - mean_q: 12.940 - prob: 1.000\n",
            "\n",
            "Interval 28264 (282630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.006 - mae: 7.217 - mean_q: 12.331 - prob: 1.000\n",
            "\n",
            "Interval 28265 (282640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.004 - mae: 7.301 - mean_q: 12.640 - prob: 1.000\n",
            "\n",
            "Interval 28266 (282650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 28267 (282660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.007 - mae: 7.180 - mean_q: 12.383 - prob: 1.000\n",
            "\n",
            "Interval 28268 (282670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.003 - mae: 7.015 - mean_q: 12.279 - prob: 1.000\n",
            "\n",
            "Interval 28269 (282680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28270 (282690 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.270 - mean_q: 12.740 - prob: 1.000\n",
            "\n",
            "Interval 28271 (282700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.003 - mae: 7.433 - mean_q: 12.855 - prob: 1.000\n",
            "\n",
            "Interval 28272 (282710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28273 (282720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.005 - mae: 7.474 - mean_q: 12.890 - prob: 1.000\n",
            "\n",
            "Interval 28274 (282730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 28275 (282740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -18.000 [-18.000, -18.000] - loss: 0.004 - mae: 7.611 - mean_q: 12.999 - prob: 1.000\n",
            "\n",
            "Interval 28276 (282750 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.004 - mae: 7.296 - mean_q: 12.671 - prob: 1.000\n",
            "\n",
            "Interval 28277 (282760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 28278 (282770 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -38.000 [-38.000, -38.000] - loss: 0.004 - mae: 7.274 - mean_q: 12.593 - prob: 1.000\n",
            "\n",
            "Interval 28279 (282780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.006 - mae: 7.399 - mean_q: 12.681 - prob: 1.000\n",
            "\n",
            "Interval 28280 (282790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28281 (282800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.004 - mae: 7.684 - mean_q: 13.140 - prob: 1.000\n",
            "\n",
            "Interval 28282 (282810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.006 - mae: 7.056 - mean_q: 12.239 - prob: 1.000\n",
            "\n",
            "Interval 28283 (282820 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.004 - mae: 7.461 - mean_q: 12.732 - prob: 1.000\n",
            "\n",
            "Interval 28284 (282830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28285 (282840 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.002 - mae: 7.461 - mean_q: 12.762 - prob: 1.000\n",
            "\n",
            "Interval 28286 (282850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.582 - mean_q: 12.971 - prob: 1.000\n",
            "\n",
            "Interval 28287 (282860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28288 (282870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.226 - mean_q: 12.544 - prob: 1.000\n",
            "\n",
            "Interval 28289 (282880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.003 - mae: 7.473 - mean_q: 12.819 - prob: 1.000\n",
            "\n",
            "Interval 28290 (282890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.004 - mae: 7.197 - mean_q: 12.440 - prob: 1.000\n",
            "\n",
            "Interval 28291 (282900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.200 - mean_q: 12.389 - prob: 1.000\n",
            "\n",
            "Interval 28292 (282910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28293 (282920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.091 - mean_q: 12.481 - prob: 1.000\n",
            "\n",
            "Interval 28294 (282930 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.003 - mae: 7.243 - mean_q: 12.493 - prob: 1.000\n",
            "\n",
            "Interval 28295 (282940 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.003 - mae: 7.329 - mean_q: 12.842 - prob: 1.000\n",
            "\n",
            "Interval 28296 (282950 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.206 - mean_q: 12.535 - prob: 1.000\n",
            "\n",
            "Interval 28297 (282960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.003 - mae: 7.533 - mean_q: 12.921 - prob: 1.000\n",
            "\n",
            "Interval 28298 (282970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28299 (282980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.003 - mae: 7.347 - mean_q: 12.766 - prob: 1.000\n",
            "\n",
            "Interval 28300 (282990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 28301 (283000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.004 - mae: 7.191 - mean_q: 12.400 - prob: 1.000\n",
            "\n",
            "Interval 28302 (283010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.004 - mae: 7.133 - mean_q: 12.278 - prob: 1.000\n",
            "\n",
            "Interval 28303 (283020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.003 - mae: 7.544 - mean_q: 12.883 - prob: 1.000\n",
            "\n",
            "Interval 28304 (283030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28305 (283040 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.002 - mae: 7.081 - mean_q: 12.333 - prob: 1.000\n",
            "\n",
            "Interval 28306 (283050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.565 - mean_q: 12.953 - prob: 1.000\n",
            "\n",
            "Interval 28307 (283060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.002 - mae: 7.207 - mean_q: 12.453 - prob: 1.000\n",
            "\n",
            "Interval 28308 (283070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28309 (283080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.004 - mae: 7.292 - mean_q: 12.638 - prob: 1.000\n",
            "\n",
            "Interval 28310 (283090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.003 - mae: 7.515 - mean_q: 12.912 - prob: 1.000\n",
            "\n",
            "Interval 28311 (283100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.003 - mae: 7.255 - mean_q: 12.418 - prob: 1.000\n",
            "\n",
            "Interval 28312 (283110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.004 - mae: 7.407 - mean_q: 12.912 - prob: 1.000\n",
            "\n",
            "Interval 28313 (283120 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.004 - mae: 7.280 - mean_q: 12.697 - prob: 1.000\n",
            "\n",
            "Interval 28314 (283130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28315 (283140 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.004 - mae: 7.439 - mean_q: 12.825 - prob: 1.000\n",
            "\n",
            "Interval 28316 (283150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28317 (283160 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.004 - mae: 7.556 - mean_q: 12.966 - prob: 1.000\n",
            "\n",
            "Interval 28318 (283170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.004 - mae: 7.633 - mean_q: 13.145 - prob: 1.000\n",
            "\n",
            "Interval 28319 (283180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28320 (283190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.211 - mean_q: 12.493 - prob: 1.000\n",
            "\n",
            "Interval 28321 (283200 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.002 - mae: 7.037 - mean_q: 12.294 - prob: 1.000\n",
            "\n",
            "Interval 28322 (283210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28323 (283220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.569 - mean_q: 12.930 - prob: 1.000\n",
            "\n",
            "Interval 28324 (283230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 7.436 - mean_q: 12.779 - prob: 1.000\n",
            "\n",
            "Interval 28325 (283240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28326 (283250 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.003 - mae: 7.381 - mean_q: 12.731 - prob: 1.000\n",
            "\n",
            "Interval 28327 (283260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28328 (283270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.568 - mean_q: 12.992 - prob: 1.000\n",
            "\n",
            "Interval 28329 (283280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28330 (283290 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.002 - mae: 7.179 - mean_q: 12.377 - prob: 1.000\n",
            "\n",
            "Interval 28331 (283300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28332 (283310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.339 - mean_q: 12.731 - prob: 1.000\n",
            "\n",
            "Interval 28333 (283320 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.512 - mean_q: 12.796 - prob: 1.000\n",
            "\n",
            "Interval 28334 (283330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28335 (283340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.424 - mean_q: 12.714 - prob: 1.000\n",
            "\n",
            "Interval 28336 (283350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 7.454 - mean_q: 12.852 - prob: 1.000\n",
            "\n",
            "Interval 28337 (283360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 28338 (283370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.002 - mae: 7.235 - mean_q: 12.678 - prob: 1.000\n",
            "\n",
            "Interval 28339 (283380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28340 (283390 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.003 - mae: 7.231 - mean_q: 12.448 - prob: 1.000\n",
            "\n",
            "Interval 28341 (283400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.162 - mean_q: 12.401 - prob: 1.000\n",
            "\n",
            "Interval 28342 (283410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 28343 (283420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.003 - mae: 7.300 - mean_q: 12.552 - prob: 1.000\n",
            "\n",
            "Interval 28344 (283430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28345 (283440 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.193 - mean_q: 12.645 - prob: 1.000\n",
            "\n",
            "Interval 28346 (283450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.416 - mean_q: 12.711 - prob: 1.000\n",
            "\n",
            "Interval 28347 (283460 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.002 - mae: 7.395 - mean_q: 12.793 - prob: 1.000\n",
            "\n",
            "Interval 28348 (283470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28349 (283480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.283 - mean_q: 12.626 - prob: 1.000\n",
            "\n",
            "Interval 28350 (283490 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.357 - mean_q: 12.748 - prob: 1.000\n",
            "\n",
            "Interval 28351 (283500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.003 - mae: 7.328 - mean_q: 12.564 - prob: 1.000\n",
            "\n",
            "Interval 28352 (283510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28353 (283520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.003 - mae: 7.125 - mean_q: 12.366 - prob: 1.000\n",
            "\n",
            "Interval 28354 (283530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.293 - mean_q: 12.528 - prob: 1.000\n",
            "\n",
            "Interval 28355 (283540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28356 (283550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.291 - mean_q: 12.517 - prob: 1.000\n",
            "\n",
            "Interval 28357 (283560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.317 - mean_q: 12.576 - prob: 1.000\n",
            "\n",
            "Interval 28358 (283570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.156 - mean_q: 12.155 - prob: 1.000\n",
            "\n",
            "Interval 28359 (283580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28360 (283590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.421 - mean_q: 12.806 - prob: 1.000\n",
            "\n",
            "Interval 28361 (283600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.297 - mean_q: 12.663 - prob: 1.000\n",
            "\n",
            "Interval 28362 (283610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28363 (283620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.003 - mae: 7.593 - mean_q: 12.933 - prob: 1.000\n",
            "\n",
            "Interval 28364 (283630 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.003 - mae: 7.321 - mean_q: 12.637 - prob: 1.000\n",
            "\n",
            "Interval 28365 (283640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28366 (283650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 7.215 - mean_q: 12.569 - prob: 1.000\n",
            "\n",
            "Interval 28367 (283660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 28368 (283670 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.003 - mae: 7.235 - mean_q: 12.443 - prob: 1.000\n",
            "\n",
            "Interval 28369 (283680 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.532 - mean_q: 13.067 - prob: 1.000\n",
            "\n",
            "Interval 28370 (283690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 28371 (283700 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.002 - mae: 7.168 - mean_q: 12.443 - prob: 1.000\n",
            "\n",
            "Interval 28372 (283710 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.003 - mae: 7.393 - mean_q: 12.730 - prob: 1.000\n",
            "\n",
            "Interval 28373 (283720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28374 (283730 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.002 - mae: 7.484 - mean_q: 12.996 - prob: 1.000\n",
            "\n",
            "Interval 28375 (283740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.002 - mae: 7.107 - mean_q: 12.417 - prob: 1.000\n",
            "\n",
            "Interval 28376 (283750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.424 - mean_q: 12.794 - prob: 1.000\n",
            "\n",
            "Interval 28377 (283760 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 28378 (283770 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 12.000 [9.000, 15.000] - loss: 0.003 - mae: 7.758 - mean_q: 13.257 - prob: 1.000\n",
            "\n",
            "Interval 28379 (283780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28380 (283790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.461 - mean_q: 12.898 - prob: 1.000\n",
            "\n",
            "Interval 28381 (283800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.163 - mean_q: 12.370 - prob: 1.000\n",
            "\n",
            "Interval 28382 (283810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.003 - mae: 7.416 - mean_q: 12.761 - prob: 1.000\n",
            "\n",
            "Interval 28383 (283820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.215 - mean_q: 12.447 - prob: 1.000\n",
            "\n",
            "Interval 28384 (283830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28385 (283840 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.003 - mae: 7.477 - mean_q: 12.779 - prob: 1.000\n",
            "\n",
            "Interval 28386 (283850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.183 - mean_q: 12.404 - prob: 1.000\n",
            "\n",
            "Interval 28387 (283860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28388 (283870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 28389 (283880 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.003 - mae: 7.237 - mean_q: 12.574 - prob: 1.000\n",
            "\n",
            "Interval 28390 (283890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28391 (283900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.003 - mae: 6.943 - mean_q: 12.167 - prob: 1.000\n",
            "\n",
            "Interval 28392 (283910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.201 - mean_q: 12.386 - prob: 1.000\n",
            "\n",
            "Interval 28393 (283920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28394 (283930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.004 - mae: 7.258 - mean_q: 12.601 - prob: 1.000\n",
            "\n",
            "Interval 28395 (283940 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 7.150 - mean_q: 12.280 - prob: 1.000\n",
            "\n",
            "Interval 28396 (283950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28397 (283960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.091 - mean_q: 12.338 - prob: 1.000\n",
            "\n",
            "Interval 28398 (283970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.527 - mean_q: 12.966 - prob: 1.000\n",
            "\n",
            "Interval 28399 (283980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28400 (283990 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.507 - mean_q: 12.872 - prob: 1.000\n",
            "\n",
            "Interval 28401 (284000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.002 - mae: 7.460 - mean_q: 12.998 - prob: 1.000\n",
            "\n",
            "Interval 28402 (284010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28403 (284020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.240 - mean_q: 12.567 - prob: 1.000\n",
            "\n",
            "Interval 28404 (284030 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 28405 (284040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.002 - mae: 7.544 - mean_q: 12.984 - prob: 1.000\n",
            "\n",
            "Interval 28406 (284050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.780 - mean_q: 13.328 - prob: 1.000\n",
            "\n",
            "Interval 28407 (284060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.517 - mean_q: 12.921 - prob: 1.000\n",
            "\n",
            "Interval 28408 (284070 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 28409 (284080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.002 - mae: 7.229 - mean_q: 12.483 - prob: 1.000\n",
            "\n",
            "Interval 28410 (284090 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.232 - mean_q: 12.626 - prob: 1.000\n",
            "\n",
            "Interval 28411 (284100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28412 (284110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.002 - mae: 7.474 - mean_q: 12.713 - prob: 1.000\n",
            "\n",
            "Interval 28413 (284120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28414 (284130 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.227 - mean_q: 12.563 - prob: 1.000\n",
            "\n",
            "Interval 28415 (284140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.003 - mae: 7.490 - mean_q: 12.733 - prob: 1.000\n",
            "\n",
            "Interval 28416 (284150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28417 (284160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.005 - mae: 7.105 - mean_q: 12.283 - prob: 1.000\n",
            "\n",
            "Interval 28418 (284170 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.003 - mae: 6.995 - mean_q: 12.300 - prob: 1.000\n",
            "\n",
            "Interval 28419 (284180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28420 (284190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 7.322 - mean_q: 12.606 - prob: 1.000\n",
            "\n",
            "Interval 28421 (284200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.003 - mae: 7.536 - mean_q: 12.945 - prob: 1.000\n",
            "\n",
            "Interval 28422 (284210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28423 (284220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.005 - mae: 6.998 - mean_q: 12.132 - prob: 1.000\n",
            "\n",
            "Interval 28424 (284230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.004 - mae: 7.413 - mean_q: 12.695 - prob: 1.000\n",
            "\n",
            "Interval 28425 (284240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28426 (284250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.005 - mae: 7.306 - mean_q: 12.601 - prob: 1.000\n",
            "\n",
            "Interval 28427 (284260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.003 - mae: 7.108 - mean_q: 12.330 - prob: 1.000\n",
            "\n",
            "Interval 28428 (284270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28429 (284280 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.005 - mae: 7.356 - mean_q: 12.715 - prob: 1.000\n",
            "\n",
            "Interval 28430 (284290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28431 (284300 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.003 - mae: 7.376 - mean_q: 12.714 - prob: 1.000\n",
            "\n",
            "Interval 28432 (284310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 28433 (284320 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -25.000 [-25.000, -25.000] - loss: 0.003 - mae: 7.637 - mean_q: 13.063 - prob: 1.000\n",
            "\n",
            "Interval 28434 (284330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.002 - mae: 7.653 - mean_q: 13.010 - prob: 1.000\n",
            "\n",
            "Interval 28435 (284340 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 28436 (284350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.002 - mae: 7.688 - mean_q: 13.225 - prob: 1.000\n",
            "\n",
            "Interval 28437 (284360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28438 (284370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.002 - mae: 7.557 - mean_q: 12.927 - prob: 1.000\n",
            "\n",
            "Interval 28439 (284380 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.003 - mae: 7.329 - mean_q: 12.753 - prob: 1.000\n",
            "\n",
            "Interval 28440 (284390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 28441 (284400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.003 - mae: 7.284 - mean_q: 12.688 - prob: 1.000\n",
            "\n",
            "Interval 28442 (284410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 28443 (284420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -30.000 [-30.000, -30.000] - loss: 0.004 - mae: 7.576 - mean_q: 12.956 - prob: 1.000\n",
            "\n",
            "Interval 28444 (284430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28445 (284440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.6000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.003 - mae: 7.527 - mean_q: 12.888 - prob: 1.000\n",
            "\n",
            "Interval 28446 (284450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28447 (284460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.003 - mae: 7.277 - mean_q: 12.554 - prob: 1.000\n",
            "\n",
            "Interval 28448 (284470 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.003 - mae: 7.563 - mean_q: 13.037 - prob: 1.000\n",
            "\n",
            "Interval 28449 (284480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28450 (284490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.005 - mae: 7.327 - mean_q: 12.692 - prob: 1.000\n",
            "\n",
            "Interval 28451 (284500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.004 - mae: 7.358 - mean_q: 12.744 - prob: 1.000\n",
            "\n",
            "Interval 28452 (284510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28453 (284520 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.005 - mae: 7.293 - mean_q: 12.593 - prob: 1.000\n",
            "\n",
            "Interval 28454 (284530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.003 - mae: 7.461 - mean_q: 13.039 - prob: 1.000\n",
            "\n",
            "Interval 28455 (284540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.004 - mae: 7.345 - mean_q: 12.648 - prob: 1.000\n",
            "\n",
            "Interval 28456 (284550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 28457 (284560 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.004 - mae: 7.003 - mean_q: 12.186 - prob: 1.000\n",
            "\n",
            "Interval 28458 (284570 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.004 - mae: 7.337 - mean_q: 12.725 - prob: 1.000\n",
            "\n",
            "Interval 28459 (284580 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.004 - mae: 7.459 - mean_q: 12.918 - prob: 1.000\n",
            "\n",
            "Interval 28460 (284590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28461 (284600 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.004 - mae: 7.407 - mean_q: 12.793 - prob: 1.000\n",
            "\n",
            "Interval 28462 (284610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28463 (284620 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.002 - mae: 7.037 - mean_q: 12.193 - prob: 1.000\n",
            "\n",
            "Interval 28464 (284630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 28465 (284640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.003 - mae: 7.335 - mean_q: 12.511 - prob: 1.000\n",
            "\n",
            "Interval 28466 (284650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 28467 (284660 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.003 - mae: 7.478 - mean_q: 12.811 - prob: 1.000\n",
            "\n",
            "Interval 28468 (284670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.005 - mae: 7.094 - mean_q: 12.261 - prob: 1.000\n",
            "\n",
            "Interval 28469 (284680 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.004 - mae: 7.649 - mean_q: 13.131 - prob: 1.000\n",
            "\n",
            "Interval 28470 (284690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28471 (284700 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.003 - mae: 7.254 - mean_q: 12.571 - prob: 1.000\n",
            "\n",
            "Interval 28472 (284710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.003 - mae: 7.250 - mean_q: 12.665 - prob: 1.000\n",
            "\n",
            "Interval 28473 (284720 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.003 - mae: 7.405 - mean_q: 12.827 - prob: 1.000\n",
            "\n",
            "Interval 28474 (284730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28475 (284740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.216 - mean_q: 12.553 - prob: 1.000\n",
            "\n",
            "Interval 28476 (284750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.497 - mean_q: 12.867 - prob: 1.000\n",
            "\n",
            "Interval 28477 (284760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.002 - mae: 7.204 - mean_q: 12.544 - prob: 1.000\n",
            "\n",
            "Interval 28478 (284770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28479 (284780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 7.122 - mean_q: 12.292 - prob: 1.000\n",
            "\n",
            "Interval 28480 (284790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.002 - mae: 7.367 - mean_q: 12.816 - prob: 1.000\n",
            "\n",
            "Interval 28481 (284800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.737 - mean_q: 13.220 - prob: 1.000\n",
            "\n",
            "Interval 28482 (284810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 28483 (284820 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.003 - mae: 7.131 - mean_q: 12.485 - prob: 1.000\n",
            "\n",
            "Interval 28484 (284830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.003 - mae: 7.583 - mean_q: 12.983 - prob: 1.000\n",
            "\n",
            "Interval 28485 (284840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28486 (284850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.005 - mae: 7.218 - mean_q: 12.466 - prob: 1.000\n",
            "\n",
            "Interval 28487 (284860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28488 (284870 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.003 - mae: 7.363 - mean_q: 12.642 - prob: 1.000\n",
            "\n",
            "Interval 28489 (284880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.005 - mae: 7.506 - mean_q: 12.986 - prob: 1.000\n",
            "\n",
            "Interval 28490 (284890 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.004 - mae: 7.566 - mean_q: 12.997 - prob: 1.000\n",
            "\n",
            "Interval 28491 (284900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28492 (284910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.212 - mean_q: 12.629 - prob: 1.000\n",
            "\n",
            "Interval 28493 (284920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.003 - mae: 7.578 - mean_q: 13.045 - prob: 1.000\n",
            "\n",
            "Interval 28494 (284930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.002 - mae: 7.469 - mean_q: 12.786 - prob: 1.000\n",
            "\n",
            "Interval 28495 (284940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.003 - mae: 7.564 - mean_q: 13.016 - prob: 1.000\n",
            "\n",
            "Interval 28496 (284950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.002 - mae: 7.691 - mean_q: 13.049 - prob: 1.000\n",
            "\n",
            "Interval 28497 (284960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.002 - mae: 7.549 - mean_q: 12.814 - prob: 1.000\n",
            "\n",
            "Interval 28498 (284970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.698 - mean_q: 13.179 - prob: 1.000\n",
            "\n",
            "Interval 28499 (284980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28500 (284990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.004 - mae: 7.378 - mean_q: 12.589 - prob: 1.000\n",
            "\n",
            "Interval 28501 (285000 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 7.328 - mean_q: 12.756 - prob: 1.000\n",
            "\n",
            "Interval 28502 (285010 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 28503 (285020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.003 - mae: 7.147 - mean_q: 12.287 - prob: 1.000\n",
            "\n",
            "Interval 28504 (285030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28505 (285040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.004 - mae: 7.193 - mean_q: 12.383 - prob: 1.000\n",
            "\n",
            "Interval 28506 (285050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 28507 (285060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.003 - mae: 7.474 - mean_q: 12.893 - prob: 1.000\n",
            "\n",
            "Interval 28508 (285070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.004 - mae: 7.427 - mean_q: 12.717 - prob: 1.000\n",
            "\n",
            "Interval 28509 (285080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28510 (285090 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.342 - mean_q: 12.612 - prob: 1.000\n",
            "\n",
            "Interval 28511 (285100 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.002 - mae: 7.459 - mean_q: 12.910 - prob: 1.000\n",
            "\n",
            "Interval 28512 (285110 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.231 - mean_q: 12.506 - prob: 1.000\n",
            "\n",
            "Interval 28513 (285120 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.003 - mae: 7.399 - mean_q: 12.657 - prob: 1.000\n",
            "\n",
            "Interval 28514 (285130 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.003 - mae: 7.310 - mean_q: 12.667 - prob: 1.000\n",
            "\n",
            "Interval 28515 (285140 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 28516 (285150 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.003 - mae: 7.373 - mean_q: 12.688 - prob: 1.000\n",
            "\n",
            "Interval 28517 (285160 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 28518 (285170 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.002 - mae: 7.075 - mean_q: 12.294 - prob: 1.000\n",
            "\n",
            "Interval 28519 (285180 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -2.8000\n",
            "Interval 28520 (285190 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.002 - mae: 7.551 - mean_q: 12.992 - prob: 1.000\n",
            "\n",
            "Interval 28521 (285200 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.004 - mae: 7.156 - mean_q: 12.520 - prob: 1.000\n",
            "\n",
            "Interval 28522 (285210 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.004 - mae: 7.571 - mean_q: 13.068 - prob: 1.000\n",
            "\n",
            "Interval 28523 (285220 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.004 - mae: 7.564 - mean_q: 12.974 - prob: 1.000\n",
            "\n",
            "Interval 28524 (285230 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.006 - mae: 7.459 - mean_q: 12.824 - prob: 1.000\n",
            "\n",
            "Interval 28525 (285240 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 28526 (285250 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.005 - mae: 7.277 - mean_q: 12.381 - prob: 1.000\n",
            "\n",
            "Interval 28527 (285260 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.006 - mae: 6.990 - mean_q: 12.208 - prob: 1.000\n",
            "\n",
            "Interval 28528 (285270 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 28529 (285280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.006 - mae: 7.255 - mean_q: 12.346 - prob: 1.000\n",
            "\n",
            "Interval 28530 (285290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 28531 (285300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.003 - mae: 7.220 - mean_q: 12.438 - prob: 1.000\n",
            "\n",
            "Interval 28532 (285310 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.003 - mae: 7.512 - mean_q: 12.965 - prob: 1.000\n",
            "\n",
            "Interval 28533 (285320 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 28534 (285330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.003 - mae: 7.289 - mean_q: 12.593 - prob: 1.000\n",
            "\n",
            "Interval 28535 (285340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.004 - mae: 7.180 - mean_q: 12.358 - prob: 1.000\n",
            "\n",
            "Interval 28536 (285350 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.003 - mae: 7.099 - mean_q: 12.341 - prob: 1.000\n",
            "\n",
            "Interval 28537 (285360 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 28538 (285370 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.004 - mae: 7.482 - mean_q: 12.934 - prob: 1.000\n",
            "\n",
            "Interval 28539 (285380 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -2.8000\n",
            "Interval 28540 (285390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.004 - mae: 7.067 - mean_q: 12.303 - prob: 1.000\n",
            "\n",
            "Interval 28541 (285400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.226 - mean_q: 12.499 - prob: 1.000\n",
            "\n",
            "Interval 28542 (285410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28543 (285420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.287 - mean_q: 12.643 - prob: 1.000\n",
            "\n",
            "Interval 28544 (285430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.003 - mae: 7.424 - mean_q: 12.664 - prob: 1.000\n",
            "\n",
            "Interval 28545 (285440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.376 - mean_q: 12.616 - prob: 1.000\n",
            "\n",
            "Interval 28546 (285450 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 28547 (285460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.002 - mae: 7.301 - mean_q: 12.600 - prob: 1.000\n",
            "\n",
            "Interval 28548 (285470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28549 (285480 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.003 - mae: 7.243 - mean_q: 12.569 - prob: 1.000\n",
            "\n",
            "Interval 28550 (285490 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.002 - mae: 7.330 - mean_q: 12.749 - prob: 1.000\n",
            "\n",
            "Interval 28551 (285500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28552 (285510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.413 - mean_q: 12.880 - prob: 1.000\n",
            "\n",
            "Interval 28553 (285520 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 7.148 - mean_q: 12.453 - prob: 1.000\n",
            "\n",
            "Interval 28554 (285530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28555 (285540 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.003 - mae: 7.535 - mean_q: 13.031 - prob: 1.000\n",
            "\n",
            "Interval 28556 (285550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.374 - mean_q: 12.684 - prob: 1.000\n",
            "\n",
            "Interval 28557 (285560 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 28558 (285570 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.004 - mae: 7.285 - mean_q: 12.636 - prob: 1.000\n",
            "\n",
            "Interval 28559 (285580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.005 - mae: 7.512 - mean_q: 12.913 - prob: 1.000\n",
            "\n",
            "Interval 28560 (285590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28561 (285600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.580 - mean_q: 13.027 - prob: 1.000\n",
            "\n",
            "Interval 28562 (285610 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.180 - mean_q: 12.484 - prob: 1.000\n",
            "\n",
            "Interval 28563 (285620 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 28564 (285630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.003 - mae: 7.006 - mean_q: 12.155 - prob: 1.000\n",
            "\n",
            "Interval 28565 (285640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.002 - mae: 7.303 - mean_q: 12.587 - prob: 1.000\n",
            "\n",
            "Interval 28566 (285650 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.003 - mae: 7.443 - mean_q: 12.754 - prob: 1.000\n",
            "\n",
            "Interval 28567 (285660 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.003 - mae: 7.623 - mean_q: 13.185 - prob: 1.000\n",
            "\n",
            "Interval 28568 (285670 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 28569 (285680 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 7.344 - mean_q: 12.584 - prob: 1.000\n",
            "\n",
            "Interval 28570 (285690 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 28571 (285700 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.002 - mae: 7.488 - mean_q: 12.861 - prob: 1.000\n",
            "\n",
            "Interval 28572 (285710 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.002 - mae: 7.242 - mean_q: 12.544 - prob: 1.000\n",
            "\n",
            "Interval 28573 (285720 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 28574 (285730 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.003 - mae: 7.638 - mean_q: 13.011 - prob: 1.000\n",
            "\n",
            "Interval 28575 (285740 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.004 - mae: 7.208 - mean_q: 12.413 - prob: 1.000\n",
            "\n",
            "Interval 28576 (285750 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 28577 (285760 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.004 - mae: 7.536 - mean_q: 12.980 - prob: 1.000\n",
            "\n",
            "Interval 28578 (285770 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.005 - mae: 7.401 - mean_q: 12.753 - prob: 1.000\n",
            "\n",
            "Interval 28579 (285780 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 28580 (285790 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -19.000 [-19.000, -19.000] - loss: 0.004 - mae: 7.377 - mean_q: 12.695 - prob: 1.000\n",
            "\n",
            "Interval 28581 (285800 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 28582 (285810 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.004 - mae: 7.580 - mean_q: 12.747 - prob: 1.000\n",
            "\n",
            "Interval 28583 (285820 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 28584 (285830 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.003 - mae: 7.297 - mean_q: 12.651 - prob: 1.000\n",
            "\n",
            "Interval 28585 (285840 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.003 - mae: 7.514 - mean_q: 12.879 - prob: 1.000\n",
            "\n",
            "Interval 28586 (285850 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 28587 (285860 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.004 - mae: 7.260 - mean_q: 12.538 - prob: 1.000\n",
            "\n",
            "Interval 28588 (285870 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.360 - mean_q: 12.795 - prob: 1.000\n",
            "\n",
            "Interval 28589 (285880 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 28590 (285890 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 7.528 - mean_q: 13.040 - prob: 1.000\n",
            "\n",
            "Interval 28591 (285900 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.003 - mae: 7.584 - mean_q: 13.020 - prob: 1.000\n",
            "\n",
            "Interval 28592 (285910 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 28593 (285920 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.004 - mae: 7.389 - mean_q: 12.605 - prob: 1.000\n",
            "\n",
            "Interval 28594 (285930 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.003 - mae: 6.991 - mean_q: 12.208 - prob: 1.000\n",
            "\n",
            "Interval 28595 (285940 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 28596 (285950 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.003 - mae: 7.390 - mean_q: 12.705 - prob: 1.000\n",
            "\n",
            "Interval 28597 (285960 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.004 - mae: 7.386 - mean_q: 12.648 - prob: 1.000\n",
            "\n",
            "Interval 28598 (285970 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -2.8000\n",
            "Interval 28599 (285980 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -26.000 [-26.000, -26.000] - loss: 0.002 - mae: 7.316 - mean_q: 12.514 - prob: 1.000\n",
            "\n",
            "Interval 28600 (285990 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 28601 (286000 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.790 - mean_q: 13.250 - prob: 1.000\n",
            "\n",
            "Interval 28602 (286010 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.002 - mae: 7.354 - mean_q: 12.640 - prob: 1.000\n",
            "\n",
            "Interval 28603 (286020 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 28604 (286030 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.002 - mae: 7.173 - mean_q: 12.455 - prob: 1.000\n",
            "\n",
            "Interval 28605 (286040 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.004 - mae: 7.544 - mean_q: 12.966 - prob: 1.000\n",
            "\n",
            "Interval 28606 (286050 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 28607 (286060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.003 - mae: 7.097 - mean_q: 12.253 - prob: 1.000\n",
            "\n",
            "Interval 28608 (286070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.003 - mae: 7.250 - mean_q: 12.760 - prob: 1.000\n",
            "\n",
            "Interval 28609 (286080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28610 (286090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.178 - mean_q: 12.357 - prob: 1.000\n",
            "\n",
            "Interval 28611 (286100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.004 - mae: 7.038 - mean_q: 12.309 - prob: 1.000\n",
            "\n",
            "Interval 28612 (286110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.005 - mae: 7.306 - mean_q: 12.773 - prob: 1.000\n",
            "\n",
            "Interval 28613 (286120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.004 - mae: 7.352 - mean_q: 12.746 - prob: 1.000\n",
            "\n",
            "Interval 28614 (286130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28615 (286140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.003 - mae: 7.408 - mean_q: 12.808 - prob: 1.000\n",
            "\n",
            "Interval 28616 (286150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.005 - mae: 7.508 - mean_q: 12.782 - prob: 1.000\n",
            "\n",
            "Interval 28617 (286160 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 28618 (286170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.003 - mae: 7.362 - mean_q: 12.655 - prob: 1.000\n",
            "\n",
            "Interval 28619 (286180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28620 (286190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.198 - mean_q: 12.465 - prob: 1.000\n",
            "\n",
            "Interval 28621 (286200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.220 - mean_q: 12.441 - prob: 1.000\n",
            "\n",
            "Interval 28622 (286210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28623 (286220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.004 - mae: 7.323 - mean_q: 12.536 - prob: 1.000\n",
            "\n",
            "Interval 28624 (286230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.003 - mae: 7.631 - mean_q: 13.092 - prob: 1.000\n",
            "\n",
            "Interval 28625 (286240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.368 - mean_q: 12.773 - prob: 1.000\n",
            "\n",
            "Interval 28626 (286250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 28627 (286260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.003 - mae: 7.533 - mean_q: 13.029 - prob: 1.000\n",
            "\n",
            "Interval 28628 (286270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.003 - mae: 7.462 - mean_q: 12.813 - prob: 1.000\n",
            "\n",
            "Interval 28629 (286280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.004 - mae: 7.144 - mean_q: 12.216 - prob: 1.000\n",
            "\n",
            "Interval 28630 (286290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28631 (286300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.003 - mae: 7.307 - mean_q: 12.630 - prob: 1.000\n",
            "\n",
            "Interval 28632 (286310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.003 - mae: 7.171 - mean_q: 12.402 - prob: 1.000\n",
            "\n",
            "Interval 28633 (286320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.003 - mae: 7.564 - mean_q: 12.971 - prob: 1.000\n",
            "\n",
            "Interval 28634 (286330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28635 (286340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.003 - mae: 7.605 - mean_q: 13.151 - prob: 1.000\n",
            "\n",
            "Interval 28636 (286350 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 28637 (286360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.710 - mean_q: 13.034 - prob: 1.000\n",
            "\n",
            "Interval 28638 (286370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.463 - mean_q: 12.790 - prob: 1.000\n",
            "\n",
            "Interval 28639 (286380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.003 - mae: 7.041 - mean_q: 12.126 - prob: 1.000\n",
            "\n",
            "Interval 28640 (286390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28641 (286400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.003 - mae: 7.440 - mean_q: 12.755 - prob: 1.000\n",
            "\n",
            "Interval 28642 (286410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28643 (286420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.002 - mae: 7.655 - mean_q: 13.124 - prob: 1.000\n",
            "\n",
            "Interval 28644 (286430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 7.521 - mean_q: 12.877 - prob: 1.000\n",
            "\n",
            "Interval 28645 (286440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 28646 (286450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.003 - mae: 7.527 - mean_q: 12.936 - prob: 1.000\n",
            "\n",
            "Interval 28647 (286460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.004 - mae: 7.177 - mean_q: 12.474 - prob: 1.000\n",
            "\n",
            "Interval 28648 (286470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.003 - mae: 7.528 - mean_q: 12.886 - prob: 1.000\n",
            "\n",
            "Interval 28649 (286480 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 28650 (286490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.005 - mae: 7.532 - mean_q: 12.977 - prob: 1.000\n",
            "\n",
            "Interval 28651 (286500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.004 - mae: 7.273 - mean_q: 12.589 - prob: 1.000\n",
            "\n",
            "Interval 28652 (286510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28653 (286520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.006 - mae: 7.507 - mean_q: 12.887 - prob: 1.000\n",
            "\n",
            "Interval 28654 (286530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 28655 (286540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -17.000 [-17.000, -17.000] - loss: 0.004 - mae: 7.363 - mean_q: 12.668 - prob: 1.000\n",
            "\n",
            "Interval 28656 (286550 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 28657 (286560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.004 - mae: 7.321 - mean_q: 12.694 - prob: 1.000\n",
            "\n",
            "Interval 28658 (286570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.004 - mae: 7.525 - mean_q: 12.901 - prob: 1.000\n",
            "\n",
            "Interval 28659 (286580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.003 - mae: 7.516 - mean_q: 12.773 - prob: 1.000\n",
            "\n",
            "Interval 28660 (286590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.339 - mean_q: 12.695 - prob: 1.000\n",
            "\n",
            "Interval 28661 (286600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28662 (286610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.012 - mean_q: 12.219 - prob: 1.000\n",
            "\n",
            "Interval 28663 (286620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28664 (286630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.066 - mean_q: 12.189 - prob: 1.000\n",
            "\n",
            "Interval 28665 (286640 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.002 - mae: 7.702 - mean_q: 13.107 - prob: 1.000\n",
            "\n",
            "Interval 28666 (286650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 7.372 - mean_q: 12.728 - prob: 1.000\n",
            "\n",
            "Interval 28667 (286660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.369 - mean_q: 12.746 - prob: 1.000\n",
            "\n",
            "Interval 28668 (286670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.003 - mae: 7.127 - mean_q: 12.370 - prob: 1.000\n",
            "\n",
            "Interval 28669 (286680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 28670 (286690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28671 (286700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.002 - mae: 7.020 - mean_q: 12.269 - prob: 1.000\n",
            "\n",
            "Interval 28672 (286710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.002 - mae: 7.662 - mean_q: 13.104 - prob: 1.000\n",
            "\n",
            "Interval 28673 (286720 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 28674 (286730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.006 - mae: 7.118 - mean_q: 12.323 - prob: 1.000\n",
            "\n",
            "Interval 28675 (286740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.002 - mae: 7.439 - mean_q: 12.874 - prob: 1.000\n",
            "\n",
            "Interval 28676 (286750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.003 - mae: 7.492 - mean_q: 12.842 - prob: 1.000\n",
            "\n",
            "Interval 28677 (286760 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 28678 (286770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.233 - mean_q: 12.563 - prob: 1.000\n",
            "\n",
            "Interval 28679 (286780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28680 (286790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.003 - mae: 6.935 - mean_q: 12.145 - prob: 1.000\n",
            "\n",
            "Interval 28681 (286800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.003 - mae: 7.696 - mean_q: 13.256 - prob: 1.000\n",
            "\n",
            "Interval 28682 (286810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.003 - mae: 7.526 - mean_q: 12.926 - prob: 1.000\n",
            "\n",
            "Interval 28683 (286820 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 28684 (286830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.6000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.004 - mae: 7.548 - mean_q: 13.063 - prob: 1.000\n",
            "\n",
            "Interval 28685 (286840 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 28686 (286850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -24.000 [-24.000, -24.000] - loss: 0.003 - mae: 7.323 - mean_q: 12.715 - prob: 1.000\n",
            "\n",
            "Interval 28687 (286860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.510 - mean_q: 12.886 - prob: 1.000\n",
            "\n",
            "Interval 28688 (286870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28689 (286880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.002 - mae: 7.308 - mean_q: 12.526 - prob: 1.000\n",
            "\n",
            "Interval 28690 (286890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.704 - mean_q: 13.116 - prob: 1.000\n",
            "\n",
            "Interval 28691 (286900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28692 (286910 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.003 - mae: 7.697 - mean_q: 13.103 - prob: 1.000\n",
            "\n",
            "Interval 28693 (286920 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.419 - mean_q: 12.806 - prob: 1.000\n",
            "\n",
            "Interval 28694 (286930 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 28695 (286940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.002 - mae: 7.651 - mean_q: 13.092 - prob: 1.000\n",
            "\n",
            "Interval 28696 (286950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.398 - mean_q: 12.771 - prob: 1.000\n",
            "\n",
            "Interval 28697 (286960 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 28698 (286970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 10.000 [7.000, 13.000] - loss: 0.002 - mae: 7.446 - mean_q: 12.761 - prob: 1.000\n",
            "\n",
            "Interval 28699 (286980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28700 (286990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.002 - mae: 7.398 - mean_q: 12.853 - prob: 1.000\n",
            "\n",
            "Interval 28701 (287000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28702 (287010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.440 - mean_q: 12.919 - prob: 1.000\n",
            "\n",
            "Interval 28703 (287020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.002 - mae: 7.440 - mean_q: 12.592 - prob: 1.000\n",
            "\n",
            "Interval 28704 (287030 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 28705 (287040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.626 - mean_q: 13.146 - prob: 1.000\n",
            "\n",
            "Interval 28706 (287050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.334 - mean_q: 12.589 - prob: 1.000\n",
            "\n",
            "Interval 28707 (287060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 28708 (287070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.002 - mae: 7.122 - mean_q: 12.488 - prob: 1.000\n",
            "\n",
            "Interval 28709 (287080 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 28710 (287090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -18.000 [-18.000, -18.000] - loss: 0.002 - mae: 7.784 - mean_q: 13.327 - prob: 1.000\n",
            "\n",
            "Interval 28711 (287100 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.449 - mean_q: 12.902 - prob: 1.000\n",
            "\n",
            "Interval 28712 (287110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28713 (287120 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.002 - mae: 7.438 - mean_q: 12.803 - prob: 1.000\n",
            "\n",
            "Interval 28714 (287130 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.002 - mae: 7.275 - mean_q: 12.494 - prob: 1.000\n",
            "\n",
            "Interval 28715 (287140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28716 (287150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.003 - mae: 7.509 - mean_q: 12.812 - prob: 1.000\n",
            "\n",
            "Interval 28717 (287160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.003 - mae: 7.464 - mean_q: 12.802 - prob: 1.000\n",
            "\n",
            "Interval 28718 (287170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28719 (287180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 10.000 [8.000, 12.000] - loss: 0.003 - mae: 7.212 - mean_q: 12.443 - prob: 1.000\n",
            "\n",
            "Interval 28720 (287190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 28721 (287200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.005 - mae: 7.307 - mean_q: 12.568 - prob: 1.000\n",
            "\n",
            "Interval 28722 (287210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.004 - mae: 7.376 - mean_q: 12.681 - prob: 1.000\n",
            "\n",
            "Interval 28723 (287220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28724 (287230 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.004 - mae: 7.563 - mean_q: 12.919 - prob: 1.000\n",
            "\n",
            "Interval 28725 (287240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.005 - mae: 7.437 - mean_q: 12.878 - prob: 1.000\n",
            "\n",
            "Interval 28726 (287250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28727 (287260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.004 - mae: 6.978 - mean_q: 12.213 - prob: 1.000\n",
            "\n",
            "Interval 28728 (287270 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.006 - mae: 7.649 - mean_q: 13.018 - prob: 1.000\n",
            "\n",
            "Interval 28729 (287280 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 28730 (287290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.006 - mae: 7.328 - mean_q: 12.631 - prob: 1.000\n",
            "\n",
            "Interval 28731 (287300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.005 - mae: 7.176 - mean_q: 12.423 - prob: 1.000\n",
            "\n",
            "Interval 28732 (287310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 28733 (287320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.005 - mae: 7.317 - mean_q: 12.673 - prob: 1.000\n",
            "\n",
            "Interval 28734 (287330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.004 - mae: 7.446 - mean_q: 12.859 - prob: 1.000\n",
            "\n",
            "Interval 28735 (287340 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 28736 (287350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 6.978 - mean_q: 12.244 - prob: 1.000\n",
            "\n",
            "Interval 28737 (287360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.268 - mean_q: 12.662 - prob: 1.000\n",
            "\n",
            "Interval 28738 (287370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.278 - mean_q: 12.623 - prob: 1.000\n",
            "\n",
            "Interval 28739 (287380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.003 - mae: 7.076 - mean_q: 12.360 - prob: 1.000\n",
            "\n",
            "Interval 28740 (287390 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 28741 (287400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.003 - mae: 7.429 - mean_q: 12.804 - prob: 1.000\n",
            "\n",
            "Interval 28742 (287410 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 28743 (287420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.002 - mae: 7.450 - mean_q: 12.886 - prob: 1.000\n",
            "\n",
            "Interval 28744 (287430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 28745 (287440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.003 - mae: 7.570 - mean_q: 12.986 - prob: 1.000\n",
            "\n",
            "Interval 28746 (287450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28747 (287460 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.004 - mae: 7.318 - mean_q: 12.700 - prob: 1.000\n",
            "\n",
            "Interval 28748 (287470 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.002 - mae: 7.571 - mean_q: 13.031 - prob: 1.000\n",
            "\n",
            "Interval 28749 (287480 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.004 - mae: 7.466 - mean_q: 12.802 - prob: 1.000\n",
            "\n",
            "Interval 28750 (287490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28751 (287500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.003 - mae: 7.463 - mean_q: 12.884 - prob: 1.000\n",
            "\n",
            "Interval 28752 (287510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.003 - mae: 7.237 - mean_q: 12.533 - prob: 1.000\n",
            "\n",
            "Interval 28753 (287520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.002 - mae: 7.356 - mean_q: 12.613 - prob: 1.000\n",
            "\n",
            "Interval 28754 (287530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28755 (287540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.002 - mae: 7.364 - mean_q: 12.743 - prob: 1.000\n",
            "\n",
            "Interval 28756 (287550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.228 - mean_q: 12.528 - prob: 1.000\n",
            "\n",
            "Interval 28757 (287560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28758 (287570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.334 - mean_q: 12.585 - prob: 1.000\n",
            "\n",
            "Interval 28759 (287580 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.340 - mean_q: 12.774 - prob: 1.000\n",
            "\n",
            "Interval 28760 (287590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.275 - mean_q: 12.642 - prob: 1.000\n",
            "\n",
            "Interval 28761 (287600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.002 - mae: 7.155 - mean_q: 12.415 - prob: 1.000\n",
            "\n",
            "Interval 28762 (287610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28763 (287620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.429 - mean_q: 12.882 - prob: 1.000\n",
            "\n",
            "Interval 28764 (287630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.002 - mae: 7.031 - mean_q: 12.224 - prob: 1.000\n",
            "\n",
            "Interval 28765 (287640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.225 - mean_q: 12.541 - prob: 1.000\n",
            "\n",
            "Interval 28766 (287650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.460 - mean_q: 12.623 - prob: 1.000\n",
            "\n",
            "Interval 28767 (287660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 6.920 - mean_q: 12.017 - prob: 1.000\n",
            "\n",
            "Interval 28768 (287670 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.002 - mae: 7.043 - mean_q: 12.383 - prob: 1.000\n",
            "\n",
            "Interval 28769 (287680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28770 (287690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.273 - mean_q: 12.477 - prob: 1.000\n",
            "\n",
            "Interval 28771 (287700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.272 - mean_q: 12.541 - prob: 1.000\n",
            "\n",
            "Interval 28772 (287710 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 28773 (287720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.367 - mean_q: 12.547 - prob: 1.000\n",
            "\n",
            "Interval 28774 (287730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.002 - mae: 7.508 - mean_q: 12.687 - prob: 1.000\n",
            "\n",
            "Interval 28775 (287740 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 28776 (287750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -23.000 [-23.000, -23.000] - loss: 0.001 - mae: 7.711 - mean_q: 13.305 - prob: 1.000\n",
            "\n",
            "Interval 28777 (287760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.436 - mean_q: 12.877 - prob: 1.000\n",
            "\n",
            "Interval 28778 (287770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28779 (287780 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 28780 (287790 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.004 - mae: 7.278 - mean_q: 12.603 - prob: 1.000\n",
            "\n",
            "Interval 28781 (287800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.261 - mean_q: 12.513 - prob: 1.000\n",
            "\n",
            "Interval 28782 (287810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.629 - mean_q: 13.026 - prob: 1.000\n",
            "\n",
            "Interval 28783 (287820 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 28784 (287830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.003 - mae: 7.244 - mean_q: 12.612 - prob: 1.000\n",
            "\n",
            "Interval 28785 (287840 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 28786 (287850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.003 - mae: 7.369 - mean_q: 12.760 - prob: 1.000\n",
            "\n",
            "Interval 28787 (287860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.005 - mae: 7.386 - mean_q: 12.717 - prob: 1.000\n",
            "\n",
            "Interval 28788 (287870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 28789 (287880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -26.000 [-26.000, -26.000] - loss: 0.003 - mae: 7.454 - mean_q: 12.792 - prob: 1.000\n",
            "\n",
            "Interval 28790 (287890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.004 - mae: 7.553 - mean_q: 13.043 - prob: 1.000\n",
            "\n",
            "Interval 28791 (287900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28792 (287910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 6.940 - mean_q: 12.036 - prob: 1.000\n",
            "\n",
            "Interval 28793 (287920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.003 - mae: 7.212 - mean_q: 12.472 - prob: 1.000\n",
            "\n",
            "Interval 28794 (287930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.003 - mae: 7.191 - mean_q: 12.504 - prob: 1.000\n",
            "\n",
            "Interval 28795 (287940 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.004 - mae: 7.709 - mean_q: 13.191 - prob: 1.000\n",
            "\n",
            "Interval 28796 (287950 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 28797 (287960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.003 - mae: 7.516 - mean_q: 12.913 - prob: 1.000\n",
            "\n",
            "Interval 28798 (287970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.003 - mae: 7.463 - mean_q: 12.732 - prob: 1.000\n",
            "\n",
            "Interval 28799 (287980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28800 (287990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.177 - mean_q: 12.361 - prob: 1.000\n",
            "\n",
            "Interval 28801 (288000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.779 - mean_q: 13.205 - prob: 1.000\n",
            "\n",
            "Interval 28802 (288010 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 28803 (288020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.002 - mae: 7.598 - mean_q: 13.169 - prob: 1.000\n",
            "\n",
            "Interval 28804 (288030 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 28805 (288040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.003 - mae: 7.690 - mean_q: 13.266 - prob: 1.000\n",
            "\n",
            "Interval 28806 (288050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28807 (288060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.057 - mean_q: 12.227 - prob: 1.000\n",
            "\n",
            "Interval 28808 (288070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28809 (288080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.359 - mean_q: 12.689 - prob: 1.000\n",
            "\n",
            "Interval 28810 (288090 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.002 - mae: 7.448 - mean_q: 12.899 - prob: 1.000\n",
            "\n",
            "Interval 28811 (288100 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 28812 (288110 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.647 - mean_q: 13.192 - prob: 1.000\n",
            "\n",
            "Interval 28813 (288120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.015 - mean_q: 12.304 - prob: 1.000\n",
            "\n",
            "Interval 28814 (288130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28815 (288140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.306 - mean_q: 12.477 - prob: 1.000\n",
            "\n",
            "Interval 28816 (288150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 28817 (288160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.002 - mae: 7.169 - mean_q: 12.492 - prob: 1.000\n",
            "\n",
            "Interval 28818 (288170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.002 - mae: 7.335 - mean_q: 12.668 - prob: 1.000\n",
            "\n",
            "Interval 28819 (288180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 28820 (288190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -17.000 [-17.000, -17.000] - loss: 0.002 - mae: 7.242 - mean_q: 12.538 - prob: 1.000\n",
            "\n",
            "Interval 28821 (288200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28822 (288210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.002 - mae: 7.431 - mean_q: 12.887 - prob: 1.000\n",
            "\n",
            "Interval 28823 (288220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.417 - mean_q: 12.609 - prob: 1.000\n",
            "\n",
            "Interval 28824 (288230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28825 (288240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.484 - mean_q: 13.011 - prob: 1.000\n",
            "\n",
            "Interval 28826 (288250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.565 - mean_q: 12.972 - prob: 1.000\n",
            "\n",
            "Interval 28827 (288260 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 28828 (288270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.003 - mae: 7.525 - mean_q: 12.919 - prob: 1.000\n",
            "\n",
            "Interval 28829 (288280 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 28830 (288290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.003 - mae: 7.213 - mean_q: 12.506 - prob: 1.000\n",
            "\n",
            "Interval 28831 (288300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.003 - mae: 7.282 - mean_q: 12.690 - prob: 1.000\n",
            "\n",
            "Interval 28832 (288310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.003 - mae: 7.120 - mean_q: 12.351 - prob: 1.000\n",
            "\n",
            "Interval 28833 (288320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 28834 (288330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -23.000 [-23.000, -23.000] - loss: 0.004 - mae: 7.213 - mean_q: 12.392 - prob: 1.000\n",
            "\n",
            "Interval 28835 (288340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.402 - mean_q: 12.863 - prob: 1.000\n",
            "\n",
            "Interval 28836 (288350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 28837 (288360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.004 - mae: 7.643 - mean_q: 13.078 - prob: 1.000\n",
            "\n",
            "Interval 28838 (288370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28839 (288380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 7.511 - mean_q: 12.966 - prob: 1.000\n",
            "\n",
            "Interval 28840 (288390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 6.978 - mean_q: 12.150 - prob: 1.000\n",
            "\n",
            "Interval 28841 (288400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.003 - mae: 7.154 - mean_q: 12.377 - prob: 1.000\n",
            "\n",
            "Interval 28842 (288410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28843 (288420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.373 - mean_q: 12.857 - prob: 1.000\n",
            "\n",
            "Interval 28844 (288430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.003 - mae: 7.195 - mean_q: 12.537 - prob: 1.000\n",
            "\n",
            "Interval 28845 (288440 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 28846 (288450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.005 - mae: 7.463 - mean_q: 12.738 - prob: 1.000\n",
            "\n",
            "Interval 28847 (288460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.005 - mae: 7.495 - mean_q: 12.881 - prob: 1.000\n",
            "\n",
            "Interval 28848 (288470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28849 (288480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.004 - mae: 7.524 - mean_q: 12.910 - prob: 1.000\n",
            "\n",
            "Interval 28850 (288490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 28851 (288500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -17.000 [-17.000, -17.000] - loss: 0.003 - mae: 7.333 - mean_q: 12.705 - prob: 1.000\n",
            "\n",
            "Interval 28852 (288510 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 28853 (288520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.6000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.004 - mae: 7.232 - mean_q: 12.484 - prob: 1.000\n",
            "\n",
            "Interval 28854 (288530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.003 - mae: 7.500 - mean_q: 12.801 - prob: 1.000\n",
            "\n",
            "Interval 28855 (288540 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 28856 (288550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 7.288 - mean_q: 12.499 - prob: 1.000\n",
            "\n",
            "Interval 28857 (288560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.496 - mean_q: 12.860 - prob: 1.000\n",
            "\n",
            "Interval 28858 (288570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.610 - mean_q: 13.000 - prob: 1.000\n",
            "\n",
            "Interval 28859 (288580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.003 - mae: 7.334 - mean_q: 12.572 - prob: 1.000\n",
            "\n",
            "Interval 28860 (288590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28861 (288600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.002 - mae: 7.423 - mean_q: 12.695 - prob: 1.000\n",
            "\n",
            "Interval 28862 (288610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28863 (288620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.451 - mean_q: 12.812 - prob: 1.000\n",
            "\n",
            "Interval 28864 (288630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.627 - mean_q: 13.073 - prob: 1.000\n",
            "\n",
            "Interval 28865 (288640 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 28866 (288650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.002 - mae: 7.573 - mean_q: 13.029 - prob: 1.000\n",
            "\n",
            "Interval 28867 (288660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.002 - mae: 7.506 - mean_q: 12.913 - prob: 1.000\n",
            "\n",
            "Interval 28868 (288670 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 28869 (288680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.002 - mae: 7.732 - mean_q: 13.111 - prob: 1.000\n",
            "\n",
            "Interval 28870 (288690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.002 - mae: 7.164 - mean_q: 12.433 - prob: 1.000\n",
            "\n",
            "Interval 28871 (288700 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.380 - mean_q: 12.678 - prob: 1.000\n",
            "\n",
            "Interval 28872 (288710 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 28873 (288720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.641 - mean_q: 13.029 - prob: 1.000\n",
            "\n",
            "Interval 28874 (288730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.003 - mae: 7.165 - mean_q: 12.439 - prob: 1.000\n",
            "\n",
            "Interval 28875 (288740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28876 (288750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.067 - mean_q: 12.284 - prob: 1.000\n",
            "\n",
            "Interval 28877 (288760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.002 - mae: 7.684 - mean_q: 13.283 - prob: 1.000\n",
            "\n",
            "Interval 28878 (288770 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 28879 (288780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.003 - mae: 7.301 - mean_q: 12.601 - prob: 1.000\n",
            "\n",
            "Interval 28880 (288790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.002 - mae: 7.115 - mean_q: 12.309 - prob: 1.000\n",
            "\n",
            "Interval 28881 (288800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 28882 (288810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.004 - mae: 7.206 - mean_q: 12.501 - prob: 1.000\n",
            "\n",
            "Interval 28883 (288820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28884 (288830 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.005 - mae: 7.518 - mean_q: 12.919 - prob: 1.000\n",
            "\n",
            "Interval 28885 (288840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.004 - mae: 7.449 - mean_q: 12.912 - prob: 1.000\n",
            "\n",
            "Interval 28886 (288850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.004 - mae: 7.152 - mean_q: 12.409 - prob: 1.000\n",
            "\n",
            "Interval 28887 (288860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28888 (288870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.488 - mean_q: 12.802 - prob: 1.000\n",
            "\n",
            "Interval 28889 (288880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 6.869 - mean_q: 11.944 - prob: 1.000\n",
            "\n",
            "Interval 28890 (288890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28891 (288900 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.003 - mae: 7.138 - mean_q: 12.250 - prob: 1.000\n",
            "\n",
            "Interval 28892 (288910 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 28893 (288920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.003 - mae: 7.234 - mean_q: 12.473 - prob: 1.000\n",
            "\n",
            "Interval 28894 (288930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28895 (288940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 7.626 - mean_q: 12.992 - prob: 1.000\n",
            "\n",
            "Interval 28896 (288950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.401 - mean_q: 12.791 - prob: 1.000\n",
            "\n",
            "Interval 28897 (288960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28898 (288970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.346 - mean_q: 12.748 - prob: 1.000\n",
            "\n",
            "Interval 28899 (288980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.404 - mean_q: 12.801 - prob: 1.000\n",
            "\n",
            "Interval 28900 (288990 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 28901 (289000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -25.000 [-25.000, -25.000] - loss: 0.002 - mae: 7.050 - mean_q: 12.350 - prob: 1.000\n",
            "\n",
            "Interval 28902 (289010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.277 - mean_q: 12.510 - prob: 1.000\n",
            "\n",
            "Interval 28903 (289020 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 28904 (289030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 7.152 - mean_q: 12.461 - prob: 1.000\n",
            "\n",
            "Interval 28905 (289040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.143 - mean_q: 12.462 - prob: 1.000\n",
            "\n",
            "Interval 28906 (289050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 28907 (289060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.002 - mae: 7.370 - mean_q: 12.731 - prob: 1.000\n",
            "\n",
            "Interval 28908 (289070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.521 - mean_q: 12.957 - prob: 1.000\n",
            "\n",
            "Interval 28909 (289080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.005 - mae: 7.208 - mean_q: 12.360 - prob: 1.000\n",
            "\n",
            "Interval 28910 (289090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28911 (289100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.378 - mean_q: 12.699 - prob: 1.000\n",
            "\n",
            "Interval 28912 (289110 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.003 - mae: 7.314 - mean_q: 12.839 - prob: 1.000\n",
            "\n",
            "Interval 28913 (289120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 28914 (289130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.002 - mae: 7.292 - mean_q: 12.617 - prob: 1.000\n",
            "\n",
            "Interval 28915 (289140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.414 - mean_q: 12.860 - prob: 1.000\n",
            "\n",
            "Interval 28916 (289150 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.004 - mae: 7.155 - mean_q: 12.482 - prob: 1.000\n",
            "\n",
            "Interval 28917 (289160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28918 (289170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28919 (289180 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.003 - mae: 7.628 - mean_q: 13.099 - prob: 1.000\n",
            "\n",
            "Interval 28920 (289190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28921 (289200 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.267 - mean_q: 12.544 - prob: 1.000\n",
            "\n",
            "Interval 28922 (289210 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.003 - mae: 7.663 - mean_q: 13.010 - prob: 1.000\n",
            "\n",
            "Interval 28923 (289220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.002 - mae: 7.519 - mean_q: 13.016 - prob: 1.000\n",
            "\n",
            "Interval 28924 (289230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28925 (289240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.6000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.704 - mean_q: 13.149 - prob: 1.000\n",
            "\n",
            "Interval 28926 (289250 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 28927 (289260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -28.000 [-28.000, -28.000] - loss: 0.002 - mae: 7.364 - mean_q: 12.765 - prob: 1.000\n",
            "\n",
            "Interval 28928 (289270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.483 - mean_q: 12.961 - prob: 1.000\n",
            "\n",
            "Interval 28929 (289280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.407 - mean_q: 12.779 - prob: 1.000\n",
            "\n",
            "Interval 28930 (289290 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 28931 (289300 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -19.000 [-19.000, -19.000] - loss: 0.003 - mae: 7.541 - mean_q: 12.891 - prob: 1.000\n",
            "\n",
            "Interval 28932 (289310 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 28933 (289320 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.003 - mae: 7.493 - mean_q: 12.761 - prob: 1.000\n",
            "\n",
            "Interval 28934 (289330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.003 - mae: 7.325 - mean_q: 12.591 - prob: 1.000\n",
            "\n",
            "Interval 28935 (289340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28936 (289350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.003 - mae: 7.510 - mean_q: 12.747 - prob: 1.000\n",
            "\n",
            "Interval 28937 (289360 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 28938 (289370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: -2.000 [-16.000, 12.000] - loss: 0.002 - mae: 7.156 - mean_q: 12.460 - prob: 1.000\n",
            "\n",
            "Interval 28939 (289380 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 28940 (289390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.002 - mae: 7.133 - mean_q: 12.299 - prob: 1.000\n",
            "\n",
            "Interval 28941 (289400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28942 (289410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.003 - mae: 7.208 - mean_q: 12.519 - prob: 1.000\n",
            "\n",
            "Interval 28943 (289420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.003 - mae: 7.358 - mean_q: 12.748 - prob: 1.000\n",
            "\n",
            "Interval 28944 (289430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 28945 (289440 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.003 - mae: 7.614 - mean_q: 13.059 - prob: 1.000\n",
            "\n",
            "Interval 28946 (289450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.004 - mae: 7.610 - mean_q: 12.987 - prob: 1.000\n",
            "\n",
            "Interval 28947 (289460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.003 - mae: 7.618 - mean_q: 13.067 - prob: 1.000\n",
            "\n",
            "Interval 28948 (289470 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 28949 (289480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.004 - mae: 7.464 - mean_q: 12.910 - prob: 1.000\n",
            "\n",
            "Interval 28950 (289490 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.003 - mae: 7.118 - mean_q: 12.223 - prob: 1.000\n",
            "\n",
            "Interval 28951 (289500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28952 (289510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.005 - mae: 7.465 - mean_q: 12.753 - prob: 1.000\n",
            "\n",
            "Interval 28953 (289520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.006 - mae: 7.396 - mean_q: 12.725 - prob: 1.000\n",
            "\n",
            "Interval 28954 (289530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28955 (289540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.012 - mae: 7.646 - mean_q: 12.948 - prob: 1.000\n",
            "\n",
            "Interval 28956 (289550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.011 - mae: 7.696 - mean_q: 13.146 - prob: 1.000\n",
            "\n",
            "Interval 28957 (289560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.008 - mae: 7.688 - mean_q: 13.072 - prob: 1.000\n",
            "\n",
            "Interval 28958 (289570 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 28959 (289580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.006 - mae: 7.490 - mean_q: 12.737 - prob: 1.000\n",
            "\n",
            "Interval 28960 (289590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.003 - mae: 7.340 - mean_q: 12.746 - prob: 1.000\n",
            "\n",
            "Interval 28961 (289600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28962 (289610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 2.3000\n",
            "2 episodes - episode_reward: 5.000 [-3.000, 13.000] - loss: 0.004 - mae: 7.234 - mean_q: 12.550 - prob: 1.000\n",
            "\n",
            "Interval 28963 (289620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 28964 (289630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.003 - mae: 7.590 - mean_q: 13.082 - prob: 1.000\n",
            "\n",
            "Interval 28965 (289640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.6000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.003 - mae: 7.307 - mean_q: 12.727 - prob: 1.000\n",
            "\n",
            "Interval 28966 (289650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28967 (289660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.003 - mae: 7.541 - mean_q: 12.797 - prob: 1.000\n",
            "\n",
            "Interval 28968 (289670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.004 - mae: 7.490 - mean_q: 12.818 - prob: 1.000\n",
            "\n",
            "Interval 28969 (289680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.489 - mean_q: 12.875 - prob: 1.000\n",
            "\n",
            "Interval 28970 (289690 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.299 - mean_q: 12.636 - prob: 1.000\n",
            "\n",
            "Interval 28971 (289700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28972 (289710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.004 - mae: 7.384 - mean_q: 12.646 - prob: 1.000\n",
            "\n",
            "Interval 28973 (289720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.003 - mae: 7.514 - mean_q: 12.912 - prob: 1.000\n",
            "\n",
            "Interval 28974 (289730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28975 (289740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 7.531 - mean_q: 12.760 - prob: 1.000\n",
            "\n",
            "Interval 28976 (289750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.003 - mae: 7.222 - mean_q: 12.305 - prob: 1.000\n",
            "\n",
            "Interval 28977 (289760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.003 - mae: 7.370 - mean_q: 12.650 - prob: 1.000\n",
            "\n",
            "Interval 28978 (289770 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 28979 (289780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.004 - mae: 7.475 - mean_q: 12.858 - prob: 1.000\n",
            "\n",
            "Interval 28980 (289790 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.004 - mae: 7.213 - mean_q: 12.429 - prob: 1.000\n",
            "\n",
            "Interval 28981 (289800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.002 - mae: 7.244 - mean_q: 12.450 - prob: 1.000\n",
            "\n",
            "Interval 28982 (289810 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 28983 (289820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -19.000 [-19.000, -19.000] - loss: 0.002 - mae: 7.638 - mean_q: 13.122 - prob: 1.000\n",
            "\n",
            "Interval 28984 (289830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28985 (289840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.350 - mean_q: 12.555 - prob: 1.000\n",
            "\n",
            "Interval 28986 (289850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.003 - mae: 7.310 - mean_q: 12.563 - prob: 1.000\n",
            "\n",
            "Interval 28987 (289860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.497 - mean_q: 12.866 - prob: 1.000\n",
            "\n",
            "Interval 28988 (289870 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 28989 (289880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.002 - mae: 7.677 - mean_q: 13.191 - prob: 1.000\n",
            "\n",
            "Interval 28990 (289890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.002 - mae: 7.263 - mean_q: 12.506 - prob: 1.000\n",
            "\n",
            "Interval 28991 (289900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 28992 (289910 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.003 - mae: 7.427 - mean_q: 12.728 - prob: 1.000\n",
            "\n",
            "Interval 28993 (289920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.002 - mae: 7.619 - mean_q: 13.141 - prob: 1.000\n",
            "\n",
            "Interval 28994 (289930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.003 - mae: 7.102 - mean_q: 12.402 - prob: 1.000\n",
            "\n",
            "Interval 28995 (289940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.002 - mae: 7.405 - mean_q: 12.763 - prob: 1.000\n",
            "\n",
            "Interval 28996 (289950 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 28997 (289960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.588 - mean_q: 13.025 - prob: 1.000\n",
            "\n",
            "Interval 28998 (289970 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 28999 (289980 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.299 - mean_q: 12.586 - prob: 1.000\n",
            "\n",
            "Interval 29000 (289990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 29001 (290000 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.003 - mae: 7.486 - mean_q: 12.855 - prob: 1.000\n",
            "\n",
            "Interval 29002 (290010 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.538 - mean_q: 12.841 - prob: 1.000\n",
            "\n",
            "Interval 29003 (290020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 29004 (290030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.327 - mean_q: 12.645 - prob: 1.000\n",
            "\n",
            "Interval 29005 (290040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 29006 (290050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.003 - mae: 7.245 - mean_q: 12.655 - prob: 1.000\n",
            "\n",
            "Interval 29007 (290060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.344 - mean_q: 12.673 - prob: 1.000\n",
            "\n",
            "Interval 29008 (290070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.003 - mae: 7.301 - mean_q: 12.634 - prob: 1.000\n",
            "\n",
            "Interval 29009 (290080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 29010 (290090 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.003 - mae: 7.492 - mean_q: 12.977 - prob: 1.000\n",
            "\n",
            "Interval 29011 (290100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 29012 (290110 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.003 - mae: 7.120 - mean_q: 12.410 - prob: 1.000\n",
            "\n",
            "Interval 29013 (290120 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 7.566 - mean_q: 13.003 - prob: 1.000\n",
            "\n",
            "Interval 29014 (290130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 29015 (290140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.002 - mae: 7.269 - mean_q: 12.630 - prob: 1.000\n",
            "\n",
            "Interval 29016 (290150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.003 - mae: 7.594 - mean_q: 12.952 - prob: 1.000\n",
            "\n",
            "Interval 29017 (290160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 29018 (290170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -27.000 [-27.000, -27.000] - loss: 0.003 - mae: 7.354 - mean_q: 12.672 - prob: 1.000\n",
            "\n",
            "Interval 29019 (290180 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.002 - mae: 7.203 - mean_q: 12.614 - prob: 1.000\n",
            "\n",
            "Interval 29020 (290190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 29021 (290200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.481 - mean_q: 12.879 - prob: 1.000\n",
            "\n",
            "Interval 29022 (290210 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 29023 (290220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.002 - mae: 7.389 - mean_q: 12.799 - prob: 1.000\n",
            "\n",
            "Interval 29024 (290230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.437 - mean_q: 12.829 - prob: 1.000\n",
            "\n",
            "Interval 29025 (290240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 29026 (290250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.004 - mae: 7.392 - mean_q: 12.704 - prob: 1.000\n",
            "\n",
            "Interval 29027 (290260 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 29028 (290270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.003 - mae: 7.318 - mean_q: 12.635 - prob: 1.000\n",
            "\n",
            "Interval 29029 (290280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 29030 (290290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.004 - mae: 7.301 - mean_q: 12.517 - prob: 1.000\n",
            "\n",
            "Interval 29031 (290300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.005 - mae: 7.364 - mean_q: 12.717 - prob: 1.000\n",
            "\n",
            "Interval 29032 (290310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.005 - mae: 7.340 - mean_q: 12.694 - prob: 1.000\n",
            "\n",
            "Interval 29033 (290320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 29034 (290330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.005 - mae: 7.251 - mean_q: 12.483 - prob: 1.000\n",
            "\n",
            "Interval 29035 (290340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 29036 (290350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 7.193 - mean_q: 12.460 - prob: 1.000\n",
            "\n",
            "Interval 29037 (290360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.004 - mae: 7.480 - mean_q: 12.845 - prob: 1.000\n",
            "\n",
            "Interval 29038 (290370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 29039 (290380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.002 - mae: 7.451 - mean_q: 12.981 - prob: 1.000\n",
            "\n",
            "Interval 29040 (290390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 29041 (290400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.002 - mae: 7.365 - mean_q: 12.783 - prob: 1.000\n",
            "\n",
            "Interval 29042 (290410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.002 - mae: 7.208 - mean_q: 12.543 - prob: 1.000\n",
            "\n",
            "Interval 29043 (290420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 29044 (290430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.003 - mae: 7.458 - mean_q: 12.858 - prob: 1.000\n",
            "\n",
            "Interval 29045 (290440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.040 - mean_q: 12.203 - prob: 1.000\n",
            "\n",
            "Interval 29046 (290450 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 29047 (290460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.003 - mae: 7.371 - mean_q: 12.585 - prob: 1.000\n",
            "\n",
            "Interval 29048 (290470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.581 - mean_q: 12.997 - prob: 1.000\n",
            "\n",
            "Interval 29049 (290480 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 29050 (290490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 7.551 - mean_q: 12.986 - prob: 1.000\n",
            "\n",
            "Interval 29051 (290500 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 29052 (290510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.002 - mae: 7.283 - mean_q: 12.654 - prob: 1.000\n",
            "\n",
            "Interval 29053 (290520 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 29054 (290530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.003 - mae: 6.965 - mean_q: 12.170 - prob: 1.000\n",
            "\n",
            "Interval 29055 (290540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 29056 (290550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 8.000 [1.000, 15.000] - loss: 0.004 - mae: 7.340 - mean_q: 12.756 - prob: 1.000\n",
            "\n",
            "Interval 29057 (290560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 29058 (290570 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 29059 (290580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -19.000 [-19.000, -19.000] - loss: 0.002 - mae: 7.069 - mean_q: 12.384 - prob: 1.000\n",
            "\n",
            "Interval 29060 (290590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.283 - mean_q: 12.634 - prob: 1.000\n",
            "\n",
            "Interval 29061 (290600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 29062 (290610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.004 - mae: 7.510 - mean_q: 12.803 - prob: 1.000\n",
            "\n",
            "Interval 29063 (290620 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 29064 (290630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.002 - mae: 7.636 - mean_q: 13.079 - prob: 1.000\n",
            "\n",
            "Interval 29065 (290640 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 29066 (290650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.467 - mean_q: 12.666 - prob: 1.000\n",
            "\n",
            "Interval 29067 (290660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.252 - mean_q: 12.478 - prob: 1.000\n",
            "\n",
            "Interval 29068 (290670 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.180 - mean_q: 12.468 - prob: 1.000\n",
            "\n",
            "Interval 29069 (290680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 29070 (290690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.352 - mean_q: 12.588 - prob: 1.000\n",
            "\n",
            "Interval 29071 (290700 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 29072 (290710 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.002 - mae: 7.217 - mean_q: 12.575 - prob: 1.000\n",
            "\n",
            "Interval 29073 (290720 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.003 - mae: 7.455 - mean_q: 12.799 - prob: 1.000\n",
            "\n",
            "Interval 29074 (290730 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.003 - mae: 7.234 - mean_q: 12.366 - prob: 1.000\n",
            "\n",
            "Interval 29075 (290740 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.9000\n",
            "Interval 29076 (290750 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.003 - mae: 7.271 - mean_q: 12.598 - prob: 1.000\n",
            "\n",
            "Interval 29077 (290760 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.197 - mean_q: 12.445 - prob: 1.000\n",
            "\n",
            "Interval 29078 (290770 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 29079 (290780 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.002 - mae: 7.723 - mean_q: 13.207 - prob: 1.000\n",
            "\n",
            "Interval 29080 (290790 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.004 - mae: 7.180 - mean_q: 12.404 - prob: 1.000\n",
            "\n",
            "Interval 29081 (290800 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.9000\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36043 (360420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.483 - mean_q: 12.864 - prob: 1.000\n",
            "\n",
            "Interval 36044 (360430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.003 - mae: 7.497 - mean_q: 12.782 - prob: 1.000\n",
            "\n",
            "Interval 36045 (360440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.003 - mae: 7.337 - mean_q: 12.570 - prob: 1.000\n",
            "\n",
            "Interval 36046 (360450 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.004 - mae: 7.711 - mean_q: 13.335 - prob: 1.000\n",
            "\n",
            "Interval 36047 (360460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.002 - mae: 6.992 - mean_q: 12.157 - prob: 1.000\n",
            "\n",
            "Interval 36048 (360470 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 36049 (360480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.002 - mae: 7.429 - mean_q: 12.867 - prob: 1.000\n",
            "\n",
            "Interval 36050 (360490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 36051 (360500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.004 - mae: 7.392 - mean_q: 12.674 - prob: 1.000\n",
            "\n",
            "Interval 36052 (360510 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 7.611 - mean_q: 12.992 - prob: 1.000\n",
            "\n",
            "Interval 36053 (360520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.005 - mae: 7.155 - mean_q: 12.421 - prob: 1.000\n",
            "\n",
            "Interval 36054 (360530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.004 - mae: 7.280 - mean_q: 12.356 - prob: 1.000\n",
            "\n",
            "Interval 36055 (360540 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 36056 (360550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.002 - mae: 7.209 - mean_q: 12.551 - prob: 1.000\n",
            "\n",
            "Interval 36057 (360560 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 36058 (360570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.003 - mae: 7.301 - mean_q: 12.438 - prob: 1.000\n",
            "\n",
            "Interval 36059 (360580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.428 - mean_q: 12.814 - prob: 1.000\n",
            "\n",
            "Interval 36060 (360590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.002 - mae: 7.600 - mean_q: 13.020 - prob: 1.000\n",
            "\n",
            "Interval 36061 (360600 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36062 (360610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.304 - mean_q: 12.670 - prob: 1.000\n",
            "\n",
            "Interval 36063 (360620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.002 - mae: 7.339 - mean_q: 12.674 - prob: 1.000\n",
            "\n",
            "Interval 36064 (360630 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.002 - mae: 7.346 - mean_q: 12.693 - prob: 1.000\n",
            "\n",
            "Interval 36065 (360640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 36066 (360650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.399 - mean_q: 12.732 - prob: 1.000\n",
            "\n",
            "Interval 36067 (360660 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -3.7000\n",
            "Interval 36068 (360670 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -24.000 [-24.000, -24.000] - loss: 0.002 - mae: 7.335 - mean_q: 12.595 - prob: 1.000\n",
            "\n",
            "Interval 36069 (360680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.003 - mae: 7.233 - mean_q: 12.570 - prob: 1.000\n",
            "\n",
            "Interval 36070 (360690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 36071 (360700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.352 - mean_q: 12.632 - prob: 1.000\n",
            "\n",
            "Interval 36072 (360710 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36073 (360720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.578 - mean_q: 13.046 - prob: 1.000\n",
            "\n",
            "Interval 36074 (360730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.234 - mean_q: 12.539 - prob: 1.000\n",
            "\n",
            "Interval 36075 (360740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.308 - mean_q: 12.594 - prob: 1.000\n",
            "\n",
            "Interval 36076 (360750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 36077 (360760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 9.000 [5.000, 13.000] - loss: 0.001 - mae: 7.250 - mean_q: 12.499 - prob: 1.000\n",
            "\n",
            "Interval 36078 (360770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.001 - mae: 7.608 - mean_q: 12.888 - prob: 1.000\n",
            "\n",
            "Interval 36079 (360780 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36080 (360790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.303 - mean_q: 12.610 - prob: 1.000\n",
            "\n",
            "Interval 36081 (360800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.410 - mean_q: 12.738 - prob: 1.000\n",
            "\n",
            "Interval 36082 (360810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.384 - mean_q: 12.761 - prob: 1.000\n",
            "\n",
            "Interval 36083 (360820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 36084 (360830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.626 - mean_q: 13.130 - prob: 1.000\n",
            "\n",
            "Interval 36085 (360840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.331 - mean_q: 12.562 - prob: 1.000\n",
            "\n",
            "Interval 36086 (360850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.248 - mean_q: 12.500 - prob: 1.000\n",
            "\n",
            "Interval 36087 (360860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 36088 (360870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.456 - mean_q: 12.966 - prob: 1.000\n",
            "\n",
            "Interval 36089 (360880 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.002 - mae: 7.676 - mean_q: 13.192 - prob: 1.000\n",
            "\n",
            "Interval 36090 (360890 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36091 (360900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.002 - mae: 7.327 - mean_q: 12.588 - prob: 1.000\n",
            "\n",
            "Interval 36092 (360910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.002 - mae: 7.431 - mean_q: 12.687 - prob: 1.000\n",
            "\n",
            "Interval 36093 (360920 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.002 - mae: 7.382 - mean_q: 12.725 - prob: 1.000\n",
            "\n",
            "Interval 36094 (360930 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.001 - mae: 7.522 - mean_q: 12.792 - prob: 1.000\n",
            "\n",
            "Interval 36095 (360940 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36096 (360950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.428 - mean_q: 12.710 - prob: 1.000\n",
            "\n",
            "Interval 36097 (360960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 36098 (360970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.002 - mae: 7.394 - mean_q: 12.522 - prob: 1.000\n",
            "\n",
            "Interval 36099 (360980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.002 - mae: 7.120 - mean_q: 12.347 - prob: 1.000\n",
            "\n",
            "Interval 36100 (360990 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.101 - mean_q: 12.398 - prob: 1.000\n",
            "\n",
            "Interval 36101 (361000 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36102 (361010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.243 - mean_q: 12.505 - prob: 1.000\n",
            "\n",
            "Interval 36103 (361020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.002 - mae: 7.330 - mean_q: 12.695 - prob: 1.000\n",
            "\n",
            "Interval 36104 (361030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.583 - mean_q: 12.998 - prob: 1.000\n",
            "\n",
            "Interval 36105 (361040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.003 - mae: 7.158 - mean_q: 12.435 - prob: 1.000\n",
            "\n",
            "Interval 36106 (361050 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36107 (361060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.005 - mae: 7.486 - mean_q: 12.847 - prob: 1.000\n",
            "\n",
            "Interval 36108 (361070 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36109 (361080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.005 - mae: 7.484 - mean_q: 12.913 - prob: 1.000\n",
            "\n",
            "Interval 36110 (361090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.004 - mae: 7.398 - mean_q: 12.680 - prob: 1.000\n",
            "\n",
            "Interval 36111 (361100 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36112 (361110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.548 - mean_q: 12.889 - prob: 1.000\n",
            "\n",
            "Interval 36113 (361120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.177 - mean_q: 12.354 - prob: 1.000\n",
            "\n",
            "Interval 36114 (361130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.002 - mae: 7.305 - mean_q: 12.516 - prob: 1.000\n",
            "\n",
            "Interval 36115 (361140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 36116 (361150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.003 - mae: 7.138 - mean_q: 12.339 - prob: 1.000\n",
            "\n",
            "Interval 36117 (361160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 36118 (361170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.003 - mae: 7.145 - mean_q: 12.289 - prob: 1.000\n",
            "\n",
            "Interval 36119 (361180 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36120 (361190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.431 - mean_q: 12.676 - prob: 1.000\n",
            "\n",
            "Interval 36121 (361200 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.003 - mae: 7.587 - mean_q: 12.979 - prob: 1.000\n",
            "\n",
            "Interval 36122 (361210 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.002 - mae: 7.033 - mean_q: 12.263 - prob: 1.000\n",
            "\n",
            "Interval 36123 (361220 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36124 (361230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.002 - mae: 7.212 - mean_q: 12.329 - prob: 1.000\n",
            "\n",
            "Interval 36125 (361240 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36126 (361250 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.293 - mean_q: 12.537 - prob: 1.000\n",
            "\n",
            "Interval 36127 (361260 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.409 - mean_q: 12.816 - prob: 1.000\n",
            "\n",
            "Interval 36128 (361270 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 36129 (361280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.002 - mae: 7.649 - mean_q: 13.029 - prob: 1.000\n",
            "\n",
            "Interval 36130 (361290 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.004 - mae: 7.184 - mean_q: 12.593 - prob: 1.000\n",
            "\n",
            "Interval 36131 (361300 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 36132 (361310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -17.000 [-17.000, -17.000] - loss: 0.002 - mae: 7.430 - mean_q: 12.732 - prob: 1.000\n",
            "\n",
            "Interval 36133 (361320 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36134 (361330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.002 - mae: 7.385 - mean_q: 12.660 - prob: 1.000\n",
            "\n",
            "Interval 36135 (361340 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36136 (361350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.340 - mean_q: 12.662 - prob: 1.000\n",
            "\n",
            "Interval 36137 (361360 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.497 - mean_q: 12.879 - prob: 1.000\n",
            "\n",
            "Interval 36138 (361370 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 7.478 - mean_q: 12.818 - prob: 1.000\n",
            "\n",
            "Interval 36139 (361380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 36140 (361390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.004 - mae: 7.552 - mean_q: 13.062 - prob: 1.000\n",
            "\n",
            "Interval 36141 (361400 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36142 (361410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.005 - mae: 7.480 - mean_q: 12.945 - prob: 1.000\n",
            "\n",
            "Interval 36143 (361420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 36144 (361430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.003 - mae: 7.581 - mean_q: 12.948 - prob: 1.000\n",
            "\n",
            "Interval 36145 (361440 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 36146 (361450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.002 - mae: 7.597 - mean_q: 12.944 - prob: 1.000\n",
            "\n",
            "Interval 36147 (361460 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.792 - mean_q: 13.333 - prob: 1.000\n",
            "\n",
            "Interval 36148 (361470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 36149 (361480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.002 - mae: 6.762 - mean_q: 11.739 - prob: 1.000\n",
            "\n",
            "Interval 36150 (361490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 36151 (361500 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.002 - mae: 7.498 - mean_q: 12.927 - prob: 1.000\n",
            "\n",
            "Interval 36152 (361510 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36153 (361520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.465 - mean_q: 12.735 - prob: 1.000\n",
            "\n",
            "Interval 36154 (361530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.358 - mean_q: 12.822 - prob: 1.000\n",
            "\n",
            "Interval 36155 (361540 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36156 (361550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.176 - mean_q: 12.239 - prob: 1.000\n",
            "\n",
            "Interval 36157 (361560 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.002 - mae: 7.362 - mean_q: 12.657 - prob: 1.000\n",
            "\n",
            "Interval 36158 (361570 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 36159 (361580 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.419 - mean_q: 12.721 - prob: 1.000\n",
            "\n",
            "Interval 36160 (361590 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.331 - mean_q: 12.641 - prob: 1.000\n",
            "\n",
            "Interval 36161 (361600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.002 - mae: 7.564 - mean_q: 12.824 - prob: 1.000\n",
            "\n",
            "Interval 36162 (361610 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 36163 (361620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.002 - mae: 7.147 - mean_q: 12.367 - prob: 1.000\n",
            "\n",
            "Interval 36164 (361630 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 36165 (361640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.002 - mae: 7.425 - mean_q: 12.791 - prob: 1.000\n",
            "\n",
            "Interval 36166 (361650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.002 - mae: 7.551 - mean_q: 12.946 - prob: 1.000\n",
            "\n",
            "Interval 36167 (361660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.002 - mae: 7.453 - mean_q: 12.905 - prob: 1.000\n",
            "\n",
            "Interval 36168 (361670 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36169 (361680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.260 - mean_q: 12.668 - prob: 1.000\n",
            "\n",
            "Interval 36170 (361690 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 36171 (361700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.457 - mean_q: 12.775 - prob: 1.000\n",
            "\n",
            "Interval 36172 (361710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.407 - mean_q: 12.801 - prob: 1.000\n",
            "\n",
            "Interval 36173 (361720 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.464 - mean_q: 12.837 - prob: 1.000\n",
            "\n",
            "Interval 36174 (361730 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36175 (361740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.002 - mae: 7.428 - mean_q: 12.933 - prob: 1.000\n",
            "\n",
            "Interval 36176 (361750 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36177 (361760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.170 - mean_q: 12.565 - prob: 1.000\n",
            "\n",
            "Interval 36178 (361770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.001 - mae: 7.181 - mean_q: 12.537 - prob: 1.000\n",
            "\n",
            "Interval 36179 (361780 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36180 (361790 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.003 - mae: 7.569 - mean_q: 12.943 - prob: 1.000\n",
            "\n",
            "Interval 36181 (361800 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.348 - mean_q: 12.707 - prob: 1.000\n",
            "\n",
            "Interval 36182 (361810 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.180 - mean_q: 12.496 - prob: 1.000\n",
            "\n",
            "Interval 36183 (361820 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 36184 (361830 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.138 - mean_q: 12.429 - prob: 1.000\n",
            "\n",
            "Interval 36185 (361840 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 6.991 - mean_q: 12.134 - prob: 1.000\n",
            "\n",
            "Interval 36186 (361850 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 36187 (361860 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.002 - mae: 7.605 - mean_q: 12.998 - prob: 1.000\n",
            "\n",
            "Interval 36188 (361870 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -2.8000\n",
            "Interval 36189 (361880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.002 - mae: 7.394 - mean_q: 12.765 - prob: 1.000\n",
            "\n",
            "Interval 36190 (361890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 36191 (361900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.002 - mae: 7.403 - mean_q: 12.678 - prob: 1.000\n",
            "\n",
            "Interval 36192 (361910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.002 - mae: 7.175 - mean_q: 12.300 - prob: 1.000\n",
            "\n",
            "Interval 36193 (361920 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36194 (361930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.332 - mean_q: 12.626 - prob: 1.000\n",
            "\n",
            "Interval 36195 (361940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.002 - mae: 7.485 - mean_q: 12.886 - prob: 1.000\n",
            "\n",
            "Interval 36196 (361950 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.002 - mae: 7.532 - mean_q: 12.956 - prob: 1.000\n",
            "\n",
            "Interval 36197 (361960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 36198 (361970 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.002 - mae: 7.348 - mean_q: 12.640 - prob: 1.000\n",
            "\n",
            "Interval 36199 (361980 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 36200 (361990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.002 - mae: 7.320 - mean_q: 12.534 - prob: 1.000\n",
            "\n",
            "Interval 36201 (362000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.002 - mae: 7.052 - mean_q: 12.342 - prob: 1.000\n",
            "\n",
            "Interval 36202 (362010 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.751 - mean_q: 13.182 - prob: 1.000\n",
            "\n",
            "Interval 36203 (362020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 36204 (362030 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.002 - mae: 7.510 - mean_q: 12.904 - prob: 1.000\n",
            "\n",
            "Interval 36205 (362040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.315 - mean_q: 12.538 - prob: 1.000\n",
            "\n",
            "Interval 36206 (362050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.530 - mean_q: 12.997 - prob: 1.000\n",
            "\n",
            "Interval 36207 (362060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 36208 (362070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.002 - mae: 7.521 - mean_q: 12.881 - prob: 1.000\n",
            "\n",
            "Interval 36209 (362080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.194 - mean_q: 12.410 - prob: 1.000\n",
            "\n",
            "Interval 36210 (362090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.297 - mean_q: 12.729 - prob: 1.000\n",
            "\n",
            "Interval 36211 (362100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 36212 (362110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.001 - mae: 7.416 - mean_q: 12.775 - prob: 1.000\n",
            "\n",
            "Interval 36213 (362120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 36214 (362130 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.860 - mean_q: 13.333 - prob: 1.000\n",
            "\n",
            "Interval 36215 (362140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 36216 (362150 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.003 - mae: 7.416 - mean_q: 12.634 - prob: 1.000\n",
            "\n",
            "Interval 36217 (362160 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.002 - mae: 7.701 - mean_q: 13.186 - prob: 1.000\n",
            "\n",
            "Interval 36218 (362170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.002 - mae: 7.402 - mean_q: 12.864 - prob: 1.000\n",
            "\n",
            "Interval 36219 (362180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.763 - mean_q: 13.112 - prob: 1.000\n",
            "\n",
            "Interval 36220 (362190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.647 - mean_q: 13.107 - prob: 1.000\n",
            "\n",
            "Interval 36221 (362200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 36222 (362210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -27.000 [-27.000, -27.000] - loss: 0.002 - mae: 7.516 - mean_q: 12.777 - prob: 1.000\n",
            "\n",
            "Interval 36223 (362220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 36224 (362230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.618 - mean_q: 12.992 - prob: 1.000\n",
            "\n",
            "Interval 36225 (362240 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 36226 (362250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -26.000 [-26.000, -26.000] - loss: 0.009 - mae: 7.550 - mean_q: 12.855 - prob: 1.000\n",
            "\n",
            "Interval 36227 (362260 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 36228 (362270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.003 - mae: 7.304 - mean_q: 12.520 - prob: 1.000\n",
            "\n",
            "Interval 36229 (362280 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36230 (362290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.005 - mae: 7.650 - mean_q: 13.106 - prob: 1.000\n",
            "\n",
            "Interval 36231 (362300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.041 - mean_q: 12.301 - prob: 1.000\n",
            "\n",
            "Interval 36232 (362310 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36233 (362320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.002 - mae: 7.523 - mean_q: 12.956 - prob: 1.000\n",
            "\n",
            "Interval 36234 (362330 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.420 - mean_q: 12.758 - prob: 1.000\n",
            "\n",
            "Interval 36235 (362340 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 36236 (362350 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.759 - mean_q: 13.266 - prob: 1.000\n",
            "\n",
            "Interval 36237 (362360 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.193 - mean_q: 12.324 - prob: 1.000\n",
            "\n",
            "Interval 36238 (362370 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 36239 (362380 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.002 - mae: 7.038 - mean_q: 12.456 - prob: 1.000\n",
            "\n",
            "Interval 36240 (362390 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 36241 (362400 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.002 - mae: 7.508 - mean_q: 12.820 - prob: 1.000\n",
            "\n",
            "Interval 36242 (362410 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.002 - mae: 6.931 - mean_q: 12.097 - prob: 1.000\n",
            "\n",
            "Interval 36243 (362420 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 36244 (362430 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.002 - mae: 7.283 - mean_q: 12.611 - prob: 1.000\n",
            "\n",
            "Interval 36245 (362440 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.003 - mae: 7.364 - mean_q: 12.678 - prob: 1.000\n",
            "\n",
            "Interval 36246 (362450 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 36247 (362460 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.225 - mean_q: 12.522 - prob: 1.000\n",
            "\n",
            "Interval 36248 (362470 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 36249 (362480 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.003 - mae: 7.376 - mean_q: 12.811 - prob: 1.000\n",
            "\n",
            "Interval 36250 (362490 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.093 - mean_q: 12.387 - prob: 1.000\n",
            "\n",
            "Interval 36251 (362500 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 36252 (362510 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.003 - mae: 7.521 - mean_q: 12.924 - prob: 1.000\n",
            "\n",
            "Interval 36253 (362520 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.002 - mae: 7.344 - mean_q: 12.645 - prob: 1.000\n",
            "\n",
            "Interval 36254 (362530 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 36255 (362540 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.294 - mean_q: 12.571 - prob: 1.000\n",
            "\n",
            "Interval 36256 (362550 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.198 - mean_q: 12.348 - prob: 1.000\n",
            "\n",
            "Interval 36257 (362560 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 36258 (362570 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.246 - mean_q: 12.580 - prob: 1.000\n",
            "\n",
            "Interval 36259 (362580 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.136 - mean_q: 12.398 - prob: 1.000\n",
            "\n",
            "Interval 36260 (362590 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 36261 (362600 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -27.000 [-27.000, -27.000] - loss: 0.002 - mae: 7.366 - mean_q: 12.720 - prob: 1.000\n",
            "\n",
            "Interval 36262 (362610 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.002 - mae: 7.628 - mean_q: 12.989 - prob: 1.000\n",
            "\n",
            "Interval 36263 (362620 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 36264 (362630 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.294 - mean_q: 12.687 - prob: 1.000\n",
            "\n",
            "Interval 36265 (362640 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.616 - mean_q: 13.143 - prob: 1.000\n",
            "\n",
            "Interval 36266 (362650 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.612 - mean_q: 13.012 - prob: 1.000\n",
            "\n",
            "Interval 36267 (362660 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.003 - mae: 7.485 - mean_q: 12.750 - prob: 1.000\n",
            "\n",
            "Interval 36268 (362670 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 36269 (362680 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.208 - mean_q: 12.410 - prob: 1.000\n",
            "\n",
            "Interval 36270 (362690 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.670 - mean_q: 13.113 - prob: 1.000\n",
            "\n",
            "Interval 36271 (362700 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 36272 (362710 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.304 - mean_q: 12.749 - prob: 1.000\n",
            "\n",
            "Interval 36273 (362720 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 36274 (362730 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.422 - mean_q: 12.826 - prob: 1.000\n",
            "\n",
            "Interval 36275 (362740 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.511 - mean_q: 12.920 - prob: 1.000\n",
            "\n",
            "Interval 36276 (362750 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.129 - mean_q: 12.520 - prob: 1.000\n",
            "\n",
            "Interval 36277 (362760 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 36278 (362770 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.208 - mean_q: 12.467 - prob: 1.000\n",
            "\n",
            "Interval 36279 (362780 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36280 (362790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.002 - mae: 7.648 - mean_q: 13.085 - prob: 1.000\n",
            "\n",
            "Interval 36281 (362800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.417 - mean_q: 12.817 - prob: 1.000\n",
            "\n",
            "Interval 36282 (362810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.189 - mean_q: 12.351 - prob: 1.000\n",
            "\n",
            "Interval 36283 (362820 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 36284 (362830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.008 - mean_q: 12.297 - prob: 1.000\n",
            "\n",
            "Interval 36285 (362840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.448 - mean_q: 12.775 - prob: 1.000\n",
            "\n",
            "Interval 36286 (362850 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36287 (362860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.057 - mean_q: 12.275 - prob: 1.000\n",
            "\n",
            "Interval 36288 (362870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.002 - mae: 7.452 - mean_q: 12.881 - prob: 1.000\n",
            "\n",
            "Interval 36289 (362880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.326 - mean_q: 12.709 - prob: 1.000\n",
            "\n",
            "Interval 36290 (362890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 36291 (362900 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.002 - mae: 7.402 - mean_q: 12.618 - prob: 1.000\n",
            "\n",
            "Interval 36292 (362910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 7.524 - mean_q: 12.935 - prob: 1.000\n",
            "\n",
            "Interval 36293 (362920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 36294 (362930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.003 - mae: 7.192 - mean_q: 12.652 - prob: 1.000\n",
            "\n",
            "Interval 36295 (362940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.003 - mae: 7.221 - mean_q: 12.620 - prob: 1.000\n",
            "\n",
            "Interval 36296 (362950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.004 - mae: 7.360 - mean_q: 12.837 - prob: 1.000\n",
            "\n",
            "Interval 36297 (362960 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36298 (362970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.002 - mae: 7.310 - mean_q: 12.617 - prob: 1.000\n",
            "\n",
            "Interval 36299 (362980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.002 - mae: 7.488 - mean_q: 12.849 - prob: 1.000\n",
            "\n",
            "Interval 36300 (362990 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36301 (363000 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.395 - mean_q: 12.665 - prob: 1.000\n",
            "\n",
            "Interval 36302 (363010 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.002 - mae: 7.557 - mean_q: 12.912 - prob: 1.000\n",
            "\n",
            "Interval 36303 (363020 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36304 (363030 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.003 - mae: 7.202 - mean_q: 12.509 - prob: 1.000\n",
            "\n",
            "Interval 36305 (363040 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36306 (363050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.003 - mae: 7.390 - mean_q: 12.671 - prob: 1.000\n",
            "\n",
            "Interval 36307 (363060 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.002 - mae: 7.191 - mean_q: 12.449 - prob: 1.000\n",
            "\n",
            "Interval 36308 (363070 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36309 (363080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.517 - mean_q: 12.987 - prob: 1.000\n",
            "\n",
            "Interval 36310 (363090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.002 - mae: 7.468 - mean_q: 12.677 - prob: 1.000\n",
            "\n",
            "Interval 36311 (363100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.283 - mean_q: 12.642 - prob: 1.000\n",
            "\n",
            "Interval 36312 (363110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.499 - mean_q: 12.797 - prob: 1.000\n",
            "\n",
            "Interval 36313 (363120 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36314 (363130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.638 - mean_q: 13.086 - prob: 1.000\n",
            "\n",
            "Interval 36315 (363140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.002 - mae: 7.475 - mean_q: 12.855 - prob: 1.000\n",
            "\n",
            "Interval 36316 (363150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.002 - mae: 7.152 - mean_q: 12.550 - prob: 1.000\n",
            "\n",
            "Interval 36317 (363160 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.228 - mean_q: 12.474 - prob: 1.000\n",
            "\n",
            "Interval 36318 (363170 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36319 (363180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.205 - mean_q: 12.282 - prob: 1.000\n",
            "\n",
            "Interval 36320 (363190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.221 - mean_q: 12.473 - prob: 1.000\n",
            "\n",
            "Interval 36321 (363200 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 36322 (363210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.002 - mae: 7.414 - mean_q: 12.736 - prob: 1.000\n",
            "\n",
            "Interval 36323 (363220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.002 - mae: 7.057 - mean_q: 12.250 - prob: 1.000\n",
            "\n",
            "Interval 36324 (363230 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36325 (363240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.399 - mean_q: 12.781 - prob: 1.000\n",
            "\n",
            "Interval 36326 (363250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 36327 (363260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.003 - mae: 7.524 - mean_q: 13.053 - prob: 1.000\n",
            "\n",
            "Interval 36328 (363270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.004 - mae: 6.978 - mean_q: 11.933 - prob: 1.000\n",
            "\n",
            "Interval 36329 (363280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.004 - mae: 7.431 - mean_q: 12.706 - prob: 1.000\n",
            "\n",
            "Interval 36330 (363290 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.004 - mae: 7.545 - mean_q: 12.901 - prob: 1.000\n",
            "\n",
            "Interval 36331 (363300 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36332 (363310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.003 - mae: 7.202 - mean_q: 12.369 - prob: 1.000\n",
            "\n",
            "Interval 36333 (363320 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 36334 (363330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.002 - mae: 7.348 - mean_q: 12.720 - prob: 1.000\n",
            "\n",
            "Interval 36335 (363340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.613 - mean_q: 13.003 - prob: 1.000\n",
            "\n",
            "Interval 36336 (363350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 36337 (363360 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -19.000 [-19.000, -19.000] - loss: 0.003 - mae: 7.554 - mean_q: 13.043 - prob: 1.000\n",
            "\n",
            "Interval 36338 (363370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 36339 (363380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.006 - mae: 7.737 - mean_q: 13.160 - prob: 1.000\n",
            "\n",
            "Interval 36340 (363390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 36341 (363400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.005 - mae: 7.663 - mean_q: 13.125 - prob: 1.000\n",
            "\n",
            "Interval 36342 (363410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.004 - mae: 7.426 - mean_q: 12.859 - prob: 1.000\n",
            "\n",
            "Interval 36343 (363420 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 36344 (363430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.003 - mae: 7.183 - mean_q: 12.374 - prob: 1.000\n",
            "\n",
            "Interval 36345 (363440 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.317 - mean_q: 12.609 - prob: 1.000\n",
            "\n",
            "Interval 36346 (363450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.003 - mae: 7.350 - mean_q: 12.686 - prob: 1.000\n",
            "\n",
            "Interval 36347 (363460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.004 - mae: 7.374 - mean_q: 12.745 - prob: 1.000\n",
            "\n",
            "Interval 36348 (363470 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.213 - mean_q: 12.506 - prob: 1.000\n",
            "\n",
            "Interval 36349 (363480 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36350 (363490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.004 - mae: 7.261 - mean_q: 12.535 - prob: 1.000\n",
            "\n",
            "Interval 36351 (363500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 36352 (363510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.002 - mae: 7.353 - mean_q: 12.714 - prob: 1.000\n",
            "\n",
            "Interval 36353 (363520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.456 - mean_q: 12.875 - prob: 1.000\n",
            "\n",
            "Interval 36354 (363530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.002 - mae: 7.519 - mean_q: 12.833 - prob: 1.000\n",
            "\n",
            "Interval 36355 (363540 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 36356 (363550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.515 - mean_q: 13.049 - prob: 1.000\n",
            "\n",
            "Interval 36357 (363560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.748 - mean_q: 13.168 - prob: 1.000\n",
            "\n",
            "Interval 36358 (363570 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36359 (363580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.382 - mean_q: 12.819 - prob: 1.000\n",
            "\n",
            "Interval 36360 (363590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.002 - mae: 7.339 - mean_q: 12.532 - prob: 1.000\n",
            "\n",
            "Interval 36361 (363600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.584 - mean_q: 13.021 - prob: 1.000\n",
            "\n",
            "Interval 36362 (363610 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36363 (363620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 7.488 - mean_q: 12.766 - prob: 1.000\n",
            "\n",
            "Interval 36364 (363630 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.208 - mean_q: 12.532 - prob: 1.000\n",
            "\n",
            "Interval 36365 (363640 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36366 (363650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.002 - mae: 7.338 - mean_q: 12.687 - prob: 1.000\n",
            "\n",
            "Interval 36367 (363660 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36368 (363670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.431 - mean_q: 12.710 - prob: 1.000\n",
            "\n",
            "Interval 36369 (363680 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.372 - mean_q: 12.679 - prob: 1.000\n",
            "\n",
            "Interval 36370 (363690 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 36371 (363700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.532 - mean_q: 13.069 - prob: 1.000\n",
            "\n",
            "Interval 36372 (363710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.348 - mean_q: 12.715 - prob: 1.000\n",
            "\n",
            "Interval 36373 (363720 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36374 (363730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.002 - mae: 7.220 - mean_q: 12.476 - prob: 1.000\n",
            "\n",
            "Interval 36375 (363740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.080 - mean_q: 12.447 - prob: 1.000\n",
            "\n",
            "Interval 36376 (363750 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 36377 (363760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.002 - mae: 7.324 - mean_q: 12.808 - prob: 1.000\n",
            "\n",
            "Interval 36378 (363770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.349 - mean_q: 12.843 - prob: 1.000\n",
            "\n",
            "Interval 36379 (363780 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36380 (363790 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.581 - mean_q: 13.096 - prob: 1.000\n",
            "\n",
            "Interval 36381 (363800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.600 - mean_q: 13.071 - prob: 1.000\n",
            "\n",
            "Interval 36382 (363810 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36383 (363820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.375 - mean_q: 12.807 - prob: 1.000\n",
            "\n",
            "Interval 36384 (363830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.261 - mean_q: 12.593 - prob: 1.000\n",
            "\n",
            "Interval 36385 (363840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 36386 (363850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.002 - mae: 7.538 - mean_q: 12.879 - prob: 1.000\n",
            "\n",
            "Interval 36387 (363860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.649 - mean_q: 13.152 - prob: 1.000\n",
            "\n",
            "Interval 36388 (363870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.393 - mean_q: 12.653 - prob: 1.000\n",
            "\n",
            "Interval 36389 (363880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 36390 (363890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.155 - mean_q: 12.373 - prob: 1.000\n",
            "\n",
            "Interval 36391 (363900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.330 - mean_q: 12.486 - prob: 1.000\n",
            "\n",
            "Interval 36392 (363910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.668 - mean_q: 13.211 - prob: 1.000\n",
            "\n",
            "Interval 36393 (363920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.237 - mean_q: 12.561 - prob: 1.000\n",
            "\n",
            "Interval 36394 (363930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.002 - mae: 7.531 - mean_q: 12.709 - prob: 1.000\n",
            "\n",
            "Interval 36395 (363940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 36396 (363950 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.479 - mean_q: 12.949 - prob: 1.000\n",
            "\n",
            "Interval 36397 (363960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.359 - mean_q: 12.836 - prob: 1.000\n",
            "\n",
            "Interval 36398 (363970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.406 - mean_q: 12.771 - prob: 1.000\n",
            "\n",
            "Interval 36399 (363980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.629 - mean_q: 13.100 - prob: 1.000\n",
            "\n",
            "Interval 36400 (363990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 36401 (364000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.359 - mean_q: 12.687 - prob: 1.000\n",
            "\n",
            "Interval 36402 (364010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.002 - mae: 7.663 - mean_q: 13.143 - prob: 1.000\n",
            "\n",
            "Interval 36403 (364020 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36404 (364030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.270 - mean_q: 12.646 - prob: 1.000\n",
            "\n",
            "Interval 36405 (364040 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36406 (364050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.002 - mae: 7.343 - mean_q: 12.386 - prob: 1.000\n",
            "\n",
            "Interval 36407 (364060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.365 - mean_q: 12.678 - prob: 1.000\n",
            "\n",
            "Interval 36408 (364070 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 36409 (364080 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.002 - mae: 7.665 - mean_q: 13.249 - prob: 1.000\n",
            "\n",
            "Interval 36410 (364090 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.474 - mean_q: 12.816 - prob: 1.000\n",
            "\n",
            "Interval 36411 (364100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.307 - mean_q: 12.585 - prob: 1.000\n",
            "\n",
            "Interval 36412 (364110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.003 - mae: 7.312 - mean_q: 12.441 - prob: 1.000\n",
            "\n",
            "Interval 36413 (364120 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 36414 (364130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.002 - mae: 7.176 - mean_q: 12.453 - prob: 1.000\n",
            "\n",
            "Interval 36415 (364140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.344 - mean_q: 12.685 - prob: 1.000\n",
            "\n",
            "Interval 36416 (364150 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36417 (364160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.003 - mae: 7.260 - mean_q: 12.563 - prob: 1.000\n",
            "\n",
            "Interval 36418 (364170 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 36419 (364180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.002 - mae: 7.340 - mean_q: 12.596 - prob: 1.000\n",
            "\n",
            "Interval 36420 (364190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.578 - mean_q: 12.923 - prob: 1.000\n",
            "\n",
            "Interval 36421 (364200 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36422 (364210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.054 - mean_q: 12.220 - prob: 1.000\n",
            "\n",
            "Interval 36423 (364220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.599 - mean_q: 13.096 - prob: 1.000\n",
            "\n",
            "Interval 36424 (364230 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36425 (364240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.444 - mean_q: 12.832 - prob: 1.000\n",
            "\n",
            "Interval 36426 (364250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.004 - mae: 7.493 - mean_q: 12.996 - prob: 1.000\n",
            "\n",
            "Interval 36427 (364260 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 36428 (364270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.003 - mae: 7.025 - mean_q: 12.182 - prob: 1.000\n",
            "\n",
            "Interval 36429 (364280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.003 - mae: 7.317 - mean_q: 12.498 - prob: 1.000\n",
            "\n",
            "Interval 36430 (364290 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36431 (364300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.002 - mae: 7.523 - mean_q: 12.901 - prob: 1.000\n",
            "\n",
            "Interval 36432 (364310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.002 - mae: 7.326 - mean_q: 12.631 - prob: 1.000\n",
            "\n",
            "Interval 36433 (364320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 36434 (364330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.002 - mae: 7.065 - mean_q: 12.254 - prob: 1.000\n",
            "\n",
            "Interval 36435 (364340 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.222 - mean_q: 12.599 - prob: 1.000\n",
            "\n",
            "Interval 36436 (364350 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.002 - mae: 7.439 - mean_q: 12.780 - prob: 1.000\n",
            "\n",
            "Interval 36437 (364360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 36438 (364370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.002 - mae: 7.293 - mean_q: 12.578 - prob: 1.000\n",
            "\n",
            "Interval 36439 (364380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.004 - mae: 7.405 - mean_q: 12.750 - prob: 1.000\n",
            "\n",
            "Interval 36440 (364390 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36441 (364400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.180 - mean_q: 12.465 - prob: 1.000\n",
            "\n",
            "Interval 36442 (364410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 36443 (364420 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.004 - mae: 7.465 - mean_q: 12.867 - prob: 1.000\n",
            "\n",
            "Interval 36444 (364430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.004 - mae: 7.896 - mean_q: 13.473 - prob: 1.000\n",
            "\n",
            "Interval 36445 (364440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.368 - mean_q: 12.567 - prob: 1.000\n",
            "\n",
            "Interval 36446 (364450 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36447 (364460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.002 - mae: 7.344 - mean_q: 12.677 - prob: 1.000\n",
            "\n",
            "Interval 36448 (364470 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36449 (364480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.486 - mean_q: 12.838 - prob: 1.000\n",
            "\n",
            "Interval 36450 (364490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.302 - mean_q: 12.556 - prob: 1.000\n",
            "\n",
            "Interval 36451 (364500 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36452 (364510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.508 - mean_q: 12.747 - prob: 1.000\n",
            "\n",
            "Interval 36453 (364520 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.002 - mae: 7.449 - mean_q: 12.696 - prob: 1.000\n",
            "\n",
            "Interval 36454 (364530 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 36455 (364540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -40.000 [-40.000, -40.000] - loss: 0.001 - mae: 7.292 - mean_q: 12.331 - prob: 1.000\n",
            "\n",
            "Interval 36456 (364550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.577 - mean_q: 13.058 - prob: 1.000\n",
            "\n",
            "Interval 36457 (364560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.003 - mae: 7.302 - mean_q: 12.732 - prob: 1.000\n",
            "\n",
            "Interval 36458 (364570 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36459 (364580 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.651 - mean_q: 13.244 - prob: 1.000\n",
            "\n",
            "Interval 36460 (364590 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36461 (364600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.002 - mae: 7.542 - mean_q: 13.002 - prob: 1.000\n",
            "\n",
            "Interval 36462 (364610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.003 - mae: 7.223 - mean_q: 12.550 - prob: 1.000\n",
            "\n",
            "Interval 36463 (364620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.003 - mae: 7.502 - mean_q: 12.936 - prob: 1.000\n",
            "\n",
            "Interval 36464 (364630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 36465 (364640 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.003 - mae: 7.376 - mean_q: 12.692 - prob: 1.000\n",
            "\n",
            "Interval 36466 (364650 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36467 (364660 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.437 - mean_q: 12.834 - prob: 1.000\n",
            "\n",
            "Interval 36468 (364670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.257 - mean_q: 12.572 - prob: 1.000\n",
            "\n",
            "Interval 36469 (364680 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.463 - mean_q: 12.735 - prob: 1.000\n",
            "\n",
            "Interval 36470 (364690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.276 - mean_q: 12.419 - prob: 1.000\n",
            "\n",
            "Interval 36471 (364700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.002 - mae: 7.393 - mean_q: 12.735 - prob: 1.000\n",
            "\n",
            "Interval 36472 (364710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.345 - mean_q: 12.721 - prob: 1.000\n",
            "\n",
            "Interval 36473 (364720 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 36474 (364730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.002 - mae: 7.337 - mean_q: 12.628 - prob: 1.000\n",
            "\n",
            "Interval 36475 (364740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.302 - mean_q: 12.460 - prob: 1.000\n",
            "\n",
            "Interval 36476 (364750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 36477 (364760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.283 - mean_q: 12.544 - prob: 1.000\n",
            "\n",
            "Interval 36478 (364770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 36479 (364780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.002 - mae: 7.486 - mean_q: 12.880 - prob: 1.000\n",
            "\n",
            "Interval 36480 (364790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.586 - mean_q: 12.954 - prob: 1.000\n",
            "\n",
            "Interval 36481 (364800 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 36482 (364810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.002 - mae: 7.580 - mean_q: 12.944 - prob: 1.000\n",
            "\n",
            "Interval 36483 (364820 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 36484 (364830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.003 - mae: 7.396 - mean_q: 12.716 - prob: 1.000\n",
            "\n",
            "Interval 36485 (364840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 6.861 - mean_q: 12.013 - prob: 1.000\n",
            "\n",
            "Interval 36486 (364850 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36487 (364860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.111 - mean_q: 12.369 - prob: 1.000\n",
            "\n",
            "Interval 36488 (364870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.002 - mae: 7.494 - mean_q: 12.762 - prob: 1.000\n",
            "\n",
            "Interval 36489 (364880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 36490 (364890 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.157 - mean_q: 12.295 - prob: 1.000\n",
            "\n",
            "Interval 36491 (364900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.002 - mae: 7.175 - mean_q: 12.422 - prob: 1.000\n",
            "\n",
            "Interval 36492 (364910 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 36493 (364920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.447 - mean_q: 12.903 - prob: 1.000\n",
            "\n",
            "Interval 36494 (364930 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36495 (364940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.002 - mae: 7.346 - mean_q: 12.672 - prob: 1.000\n",
            "\n",
            "Interval 36496 (364950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.592 - mean_q: 13.108 - prob: 1.000\n",
            "\n",
            "Interval 36497 (364960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.494 - mean_q: 12.895 - prob: 1.000\n",
            "\n",
            "Interval 36498 (364970 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36499 (364980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.177 - mean_q: 12.492 - prob: 1.000\n",
            "\n",
            "Interval 36500 (364990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.003 - mae: 7.152 - mean_q: 12.377 - prob: 1.000\n",
            "\n",
            "Interval 36501 (365000 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 36502 (365010 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.003 - mae: 7.061 - mean_q: 12.040 - prob: 1.000\n",
            "\n",
            "Interval 36503 (365020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.002 - mae: 7.080 - mean_q: 12.279 - prob: 1.000\n",
            "\n",
            "Interval 36504 (365030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 36505 (365040 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.002 - mae: 6.858 - mean_q: 12.044 - prob: 1.000\n",
            "\n",
            "Interval 36506 (365050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.029 - mean_q: 12.118 - prob: 1.000\n",
            "\n",
            "Interval 36507 (365060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.053 - mean_q: 12.295 - prob: 1.000\n",
            "\n",
            "Interval 36508 (365070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 36509 (365080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.315 - mean_q: 12.620 - prob: 1.000\n",
            "\n",
            "Interval 36510 (365090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.451 - mean_q: 12.903 - prob: 1.000\n",
            "\n",
            "Interval 36511 (365100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.002 - mae: 7.170 - mean_q: 12.371 - prob: 1.000\n",
            "\n",
            "Interval 36512 (365110 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.493 - mean_q: 12.736 - prob: 1.000\n",
            "\n",
            "Interval 36513 (365120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 36514 (365130 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.144 - mean_q: 12.428 - prob: 1.000\n",
            "\n",
            "Interval 36515 (365140 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36516 (365150 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.198 - mean_q: 12.330 - prob: 1.000\n",
            "\n",
            "Interval 36517 (365160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.528 - mean_q: 12.964 - prob: 1.000\n",
            "\n",
            "Interval 36518 (365170 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36519 (365180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.223 - mean_q: 12.530 - prob: 1.000\n",
            "\n",
            "Interval 36520 (365190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.221 - mean_q: 12.331 - prob: 1.000\n",
            "\n",
            "Interval 36521 (365200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.240 - mean_q: 12.370 - prob: 1.000\n",
            "\n",
            "Interval 36522 (365210 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36523 (365220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.414 - mean_q: 12.776 - prob: 1.000\n",
            "\n",
            "Interval 36524 (365230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.260 - mean_q: 12.602 - prob: 1.000\n",
            "\n",
            "Interval 36525 (365240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.228 - mean_q: 12.512 - prob: 1.000\n",
            "\n",
            "Interval 36526 (365250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 36527 (365260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.173 - mean_q: 12.652 - prob: 1.000\n",
            "\n",
            "Interval 36528 (365270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.211 - mean_q: 12.508 - prob: 1.000\n",
            "\n",
            "Interval 36529 (365280 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36530 (365290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.170 - mean_q: 12.448 - prob: 1.000\n",
            "\n",
            "Interval 36531 (365300 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.002 - mae: 7.191 - mean_q: 12.357 - prob: 1.000\n",
            "\n",
            "Interval 36532 (365310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.003 - mae: 7.278 - mean_q: 12.598 - prob: 1.000\n",
            "\n",
            "Interval 36533 (365320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.002 - mae: 7.624 - mean_q: 13.151 - prob: 1.000\n",
            "\n",
            "Interval 36534 (365330 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36535 (365340 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.507 - mean_q: 12.769 - prob: 1.000\n",
            "\n",
            "Interval 36536 (365350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.004 - mae: 7.722 - mean_q: 13.135 - prob: 1.000\n",
            "\n",
            "Interval 36537 (365360 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36538 (365370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.004 - mae: 7.403 - mean_q: 12.715 - prob: 1.000\n",
            "\n",
            "Interval 36539 (365380 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36540 (365390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.003 - mae: 7.410 - mean_q: 12.689 - prob: 1.000\n",
            "\n",
            "Interval 36541 (365400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.382 - mean_q: 12.707 - prob: 1.000\n",
            "\n",
            "Interval 36542 (365410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 36543 (365420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.002 - mae: 7.411 - mean_q: 12.739 - prob: 1.000\n",
            "\n",
            "Interval 36544 (365430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.003 - mae: 7.357 - mean_q: 12.556 - prob: 1.000\n",
            "\n",
            "Interval 36545 (365440 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36546 (365450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.002 - mae: 7.552 - mean_q: 12.930 - prob: 1.000\n",
            "\n",
            "Interval 36547 (365460 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 36548 (365470 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.002 - mae: 7.208 - mean_q: 12.541 - prob: 1.000\n",
            "\n",
            "Interval 36549 (365480 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36550 (365490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.509 - mean_q: 12.912 - prob: 1.000\n",
            "\n",
            "Interval 36551 (365500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.002 - mae: 7.147 - mean_q: 12.310 - prob: 1.000\n",
            "\n",
            "Interval 36552 (365510 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 36553 (365520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.002 - mae: 7.316 - mean_q: 12.628 - prob: 1.000\n",
            "\n",
            "Interval 36554 (365530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.314 - mean_q: 12.602 - prob: 1.000\n",
            "\n",
            "Interval 36555 (365540 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36556 (365550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.317 - mean_q: 12.547 - prob: 1.000\n",
            "\n",
            "Interval 36557 (365560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.432 - mean_q: 12.780 - prob: 1.000\n",
            "\n",
            "Interval 36558 (365570 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36559 (365580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.002 - mae: 7.217 - mean_q: 12.487 - prob: 1.000\n",
            "\n",
            "Interval 36560 (365590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.003 - mae: 7.397 - mean_q: 12.593 - prob: 1.000\n",
            "\n",
            "Interval 36561 (365600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.003 - mae: 7.264 - mean_q: 12.613 - prob: 1.000\n",
            "\n",
            "Interval 36562 (365610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 36563 (365620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.361 - mean_q: 12.730 - prob: 1.000\n",
            "\n",
            "Interval 36564 (365630 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 36565 (365640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -26.000 [-26.000, -26.000] - loss: 0.003 - mae: 7.483 - mean_q: 12.910 - prob: 1.000\n",
            "\n",
            "Interval 36566 (365650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.269 - mean_q: 12.533 - prob: 1.000\n",
            "\n",
            "Interval 36567 (365660 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36568 (365670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.004 - mae: 7.458 - mean_q: 12.776 - prob: 1.000\n",
            "\n",
            "Interval 36569 (365680 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.003 - mae: 7.310 - mean_q: 12.613 - prob: 1.000\n",
            "\n",
            "Interval 36570 (365690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.004 - mae: 7.430 - mean_q: 12.831 - prob: 1.000\n",
            "\n",
            "Interval 36571 (365700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.005 - mae: 7.254 - mean_q: 12.663 - prob: 1.000\n",
            "\n",
            "Interval 36572 (365710 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.005 - mae: 7.414 - mean_q: 12.941 - prob: 1.000\n",
            "\n",
            "Interval 36573 (365720 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36574 (365730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.495 - mean_q: 12.894 - prob: 1.000\n",
            "\n",
            "Interval 36575 (365740 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36576 (365750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.003 - mae: 7.530 - mean_q: 13.082 - prob: 1.000\n",
            "\n",
            "Interval 36577 (365760 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36578 (365770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.002 - mae: 7.141 - mean_q: 12.340 - prob: 1.000\n",
            "\n",
            "Interval 36579 (365780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.129 - mean_q: 12.324 - prob: 1.000\n",
            "\n",
            "Interval 36580 (365790 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.003 - mae: 6.986 - mean_q: 12.009 - prob: 1.000\n",
            "\n",
            "Interval 36581 (365800 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36582 (365810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.106 - mean_q: 12.339 - prob: 1.000\n",
            "\n",
            "Interval 36583 (365820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.417 - mean_q: 12.751 - prob: 1.000\n",
            "\n",
            "Interval 36584 (365830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.302 - mean_q: 12.480 - prob: 1.000\n",
            "\n",
            "Interval 36585 (365840 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36586 (365850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.174 - mean_q: 12.419 - prob: 1.000\n",
            "\n",
            "Interval 36587 (365860 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36588 (365870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.002 - mae: 7.553 - mean_q: 12.946 - prob: 1.000\n",
            "\n",
            "Interval 36589 (365880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.595 - mean_q: 12.875 - prob: 1.000\n",
            "\n",
            "Interval 36590 (365890 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36591 (365900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.004 - mae: 7.537 - mean_q: 12.953 - prob: 1.000\n",
            "\n",
            "Interval 36592 (365910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.003 - mae: 7.309 - mean_q: 12.641 - prob: 1.000\n",
            "\n",
            "Interval 36593 (365920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.003 - mae: 7.524 - mean_q: 12.842 - prob: 1.000\n",
            "\n",
            "Interval 36594 (365930 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36595 (365940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.594 - mean_q: 13.014 - prob: 1.000\n",
            "\n",
            "Interval 36596 (365950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.492 - mean_q: 12.956 - prob: 1.000\n",
            "\n",
            "Interval 36597 (365960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 36598 (365970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.279 - mean_q: 12.530 - prob: 1.000\n",
            "\n",
            "Interval 36599 (365980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 36600 (365990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.002 - mae: 7.451 - mean_q: 12.728 - prob: 1.000\n",
            "\n",
            "Interval 36601 (366000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 6.987 - mean_q: 12.170 - prob: 1.000\n",
            "\n",
            "Interval 36602 (366010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 36603 (366020 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.002 - mae: 7.586 - mean_q: 12.950 - prob: 1.000\n",
            "\n",
            "Interval 36604 (366030 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.002 - mae: 7.373 - mean_q: 12.687 - prob: 1.000\n",
            "\n",
            "Interval 36605 (366040 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.002 - mae: 7.579 - mean_q: 13.032 - prob: 1.000\n",
            "\n",
            "Interval 36606 (366050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 36607 (366060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.002 - mae: 7.657 - mean_q: 13.169 - prob: 1.000\n",
            "\n",
            "Interval 36608 (366070 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.002 - mae: 7.498 - mean_q: 12.822 - prob: 1.000\n",
            "\n",
            "Interval 36609 (366080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 36610 (366090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.351 - mean_q: 12.695 - prob: 1.000\n",
            "\n",
            "Interval 36611 (366100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.125 - mean_q: 12.276 - prob: 1.000\n",
            "\n",
            "Interval 36612 (366110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.415 - mean_q: 12.841 - prob: 1.000\n",
            "\n",
            "Interval 36613 (366120 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 36614 (366130 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.002 - mae: 7.473 - mean_q: 12.823 - prob: 1.000\n",
            "\n",
            "Interval 36615 (366140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.002 - mae: 7.474 - mean_q: 12.811 - prob: 1.000\n",
            "\n",
            "Interval 36616 (366150 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36617 (366160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.706 - mean_q: 13.149 - prob: 1.000\n",
            "\n",
            "Interval 36618 (366170 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.235 - mean_q: 12.384 - prob: 1.000\n",
            "\n",
            "Interval 36619 (366180 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36620 (366190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.350 - mean_q: 12.649 - prob: 1.000\n",
            "\n",
            "Interval 36621 (366200 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36622 (366210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.384 - mean_q: 12.654 - prob: 1.000\n",
            "\n",
            "Interval 36623 (366220 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 36624 (366230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.350 - mean_q: 12.625 - prob: 1.000\n",
            "\n",
            "Interval 36625 (366240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.002 - mae: 7.249 - mean_q: 12.507 - prob: 1.000\n",
            "\n",
            "Interval 36626 (366250 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 36627 (366260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.002 - mae: 7.382 - mean_q: 12.661 - prob: 1.000\n",
            "\n",
            "Interval 36628 (366270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.210 - mean_q: 12.468 - prob: 1.000\n",
            "\n",
            "Interval 36629 (366280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.002 - mae: 7.274 - mean_q: 12.556 - prob: 1.000\n",
            "\n",
            "Interval 36630 (366290 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36631 (366300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.003 - mae: 7.746 - mean_q: 13.161 - prob: 1.000\n",
            "\n",
            "Interval 36632 (366310 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.342 - mean_q: 12.581 - prob: 1.000\n",
            "\n",
            "Interval 36633 (366320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 36634 (366330 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.002 - mae: 7.472 - mean_q: 12.761 - prob: 1.000\n",
            "\n",
            "Interval 36635 (366340 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 36636 (366350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.002 - mae: 7.578 - mean_q: 12.942 - prob: 1.000\n",
            "\n",
            "Interval 36637 (366360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.636 - mean_q: 13.242 - prob: 1.000\n",
            "\n",
            "Interval 36638 (366370 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.263 - mean_q: 12.737 - prob: 1.000\n",
            "\n",
            "Interval 36639 (366380 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36640 (366390 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.002 - mae: 7.475 - mean_q: 12.877 - prob: 1.000\n",
            "\n",
            "Interval 36641 (366400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.233 - mean_q: 12.472 - prob: 1.000\n",
            "\n",
            "Interval 36642 (366410 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.003 - mae: 7.603 - mean_q: 13.061 - prob: 1.000\n",
            "\n",
            "Interval 36643 (366420 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 36644 (366430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.007 - mae: 7.450 - mean_q: 12.944 - prob: 1.000\n",
            "\n",
            "Interval 36645 (366440 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36646 (366450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 7.291 - mean_q: 12.581 - prob: 1.000\n",
            "\n",
            "Interval 36647 (366460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.004 - mae: 7.340 - mean_q: 12.570 - prob: 1.000\n",
            "\n",
            "Interval 36648 (366470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.003 - mae: 7.423 - mean_q: 12.806 - prob: 1.000\n",
            "\n",
            "Interval 36649 (366480 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36650 (366490 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36651 (366500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.002 - mae: 7.355 - mean_q: 12.687 - prob: 1.000\n",
            "\n",
            "Interval 36652 (366510 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36653 (366520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.002 - mae: 7.394 - mean_q: 12.652 - prob: 1.000\n",
            "\n",
            "Interval 36654 (366530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.608 - mean_q: 12.990 - prob: 1.000\n",
            "\n",
            "Interval 36655 (366540 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36656 (366550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.002 - mae: 7.240 - mean_q: 12.435 - prob: 1.000\n",
            "\n",
            "Interval 36657 (366560 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36658 (366570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.398 - mean_q: 12.780 - prob: 1.000\n",
            "\n",
            "Interval 36659 (366580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.303 - mean_q: 12.775 - prob: 1.000\n",
            "\n",
            "Interval 36660 (366590 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.172 - mean_q: 12.541 - prob: 1.000\n",
            "\n",
            "Interval 36661 (366600 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36662 (366610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.335 - mean_q: 12.589 - prob: 1.000\n",
            "\n",
            "Interval 36663 (366620 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36664 (366630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.289 - mean_q: 12.622 - prob: 1.000\n",
            "\n",
            "Interval 36665 (366640 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 36666 (366650 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.002 - mae: 7.300 - mean_q: 12.525 - prob: 1.000\n",
            "\n",
            "Interval 36667 (366660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.321 - mean_q: 12.642 - prob: 1.000\n",
            "\n",
            "Interval 36668 (366670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 36669 (366680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.003 - mae: 7.386 - mean_q: 12.767 - prob: 1.000\n",
            "\n",
            "Interval 36670 (366690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.002 - mae: 7.150 - mean_q: 12.478 - prob: 1.000\n",
            "\n",
            "Interval 36671 (366700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.180 - mean_q: 12.331 - prob: 1.000\n",
            "\n",
            "Interval 36672 (366710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -3.7000\n",
            "Interval 36673 (366720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -24.000 [-24.000, -24.000] - loss: 0.001 - mae: 7.323 - mean_q: 12.828 - prob: 1.000\n",
            "\n",
            "Interval 36674 (366730 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36675 (366740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.590 - mean_q: 13.091 - prob: 1.000\n",
            "\n",
            "Interval 36676 (366750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.650 - mean_q: 13.117 - prob: 1.000\n",
            "\n",
            "Interval 36677 (366760 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.530 - mean_q: 12.964 - prob: 1.000\n",
            "\n",
            "Interval 36678 (366770 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.200 - mean_q: 12.442 - prob: 1.000\n",
            "\n",
            "Interval 36679 (366780 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36680 (366790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.299 - mean_q: 12.436 - prob: 1.000\n",
            "\n",
            "Interval 36681 (366800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.355 - mean_q: 12.611 - prob: 1.000\n",
            "\n",
            "Interval 36682 (366810 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36683 (366820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.154 - mean_q: 12.438 - prob: 1.000\n",
            "\n",
            "Interval 36684 (366830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.299 - mean_q: 12.519 - prob: 1.000\n",
            "\n",
            "Interval 36685 (366840 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36686 (366850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.349 - mean_q: 12.659 - prob: 1.000\n",
            "\n",
            "Interval 36687 (366860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.167 - mean_q: 12.455 - prob: 1.000\n",
            "\n",
            "Interval 36688 (366870 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36689 (366880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.503 - mean_q: 12.909 - prob: 1.000\n",
            "\n",
            "Interval 36690 (366890 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 36691 (366900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.002 - mae: 7.244 - mean_q: 12.434 - prob: 1.000\n",
            "\n",
            "Interval 36692 (366910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 7.127 - mean_q: 12.337 - prob: 1.000\n",
            "\n",
            "Interval 36693 (366920 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 36694 (366930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 6.500 [0.000, 13.000] - loss: 0.002 - mae: 7.426 - mean_q: 12.813 - prob: 1.000\n",
            "\n",
            "Interval 36695 (366940 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36696 (366950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.002 - mae: 7.328 - mean_q: 12.542 - prob: 1.000\n",
            "\n",
            "Interval 36697 (366960 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 36698 (366970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.002 - mae: 7.126 - mean_q: 12.429 - prob: 1.000\n",
            "\n",
            "Interval 36699 (366980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.752 - mean_q: 13.026 - prob: 1.000\n",
            "\n",
            "Interval 36700 (366990 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36701 (367000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.290 - mean_q: 12.555 - prob: 1.000\n",
            "\n",
            "Interval 36702 (367010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.392 - mean_q: 12.837 - prob: 1.000\n",
            "\n",
            "Interval 36703 (367020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 36704 (367030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.520 - mean_q: 12.988 - prob: 1.000\n",
            "\n",
            "Interval 36705 (367040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.002 - mae: 7.577 - mean_q: 13.009 - prob: 1.000\n",
            "\n",
            "Interval 36706 (367050 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36707 (367060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.002 - mae: 7.271 - mean_q: 12.647 - prob: 1.000\n",
            "\n",
            "Interval 36708 (367070 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36709 (367080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.002 - mae: 7.223 - mean_q: 12.497 - prob: 1.000\n",
            "\n",
            "Interval 36710 (367090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.002 - mae: 7.396 - mean_q: 12.679 - prob: 1.000\n",
            "\n",
            "Interval 36711 (367100 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 36712 (367110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.002 - mae: 7.610 - mean_q: 12.999 - prob: 1.000\n",
            "\n",
            "Interval 36713 (367120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.313 - mean_q: 12.517 - prob: 1.000\n",
            "\n",
            "Interval 36714 (367130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.005 - mae: 7.526 - mean_q: 12.833 - prob: 1.000\n",
            "\n",
            "Interval 36715 (367140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.002 - mae: 7.337 - mean_q: 12.631 - prob: 1.000\n",
            "\n",
            "Interval 36716 (367150 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.002 - mae: 7.432 - mean_q: 12.823 - prob: 1.000\n",
            "\n",
            "Interval 36717 (367160 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36718 (367170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 12.000 [9.000, 15.000] - loss: 0.002 - mae: 7.402 - mean_q: 12.826 - prob: 1.000\n",
            "\n",
            "Interval 36719 (367180 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.003 - mae: 7.287 - mean_q: 12.448 - prob: 1.000\n",
            "\n",
            "Interval 36720 (367190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 36721 (367200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.486 - mean_q: 12.910 - prob: 1.000\n",
            "\n",
            "Interval 36722 (367210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.002 - mae: 7.187 - mean_q: 12.297 - prob: 1.000\n",
            "\n",
            "Interval 36723 (367220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.002 - mae: 7.533 - mean_q: 12.885 - prob: 1.000\n",
            "\n",
            "Interval 36724 (367230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.251 - mean_q: 12.530 - prob: 1.000\n",
            "\n",
            "Interval 36725 (367240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.003 - mae: 7.198 - mean_q: 12.418 - prob: 1.000\n",
            "\n",
            "Interval 36726 (367250 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 36727 (367260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.003 - mae: 7.327 - mean_q: 12.750 - prob: 1.000\n",
            "\n",
            "Interval 36728 (367270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.004 - mae: 7.067 - mean_q: 12.335 - prob: 1.000\n",
            "\n",
            "Interval 36729 (367280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 36730 (367290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.003 - mae: 7.557 - mean_q: 12.848 - prob: 1.000\n",
            "\n",
            "Interval 36731 (367300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.571 - mean_q: 12.913 - prob: 1.000\n",
            "\n",
            "Interval 36732 (367310 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.002 - mae: 7.132 - mean_q: 12.348 - prob: 1.000\n",
            "\n",
            "Interval 36733 (367320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 36734 (367330 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.321 - mean_q: 12.631 - prob: 1.000\n",
            "\n",
            "Interval 36735 (367340 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.358 - mean_q: 12.682 - prob: 1.000\n",
            "\n",
            "Interval 36736 (367350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 36737 (367360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.002 - mae: 7.580 - mean_q: 12.971 - prob: 1.000\n",
            "\n",
            "Interval 36738 (367370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 36739 (367380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.002 - mae: 7.411 - mean_q: 12.822 - prob: 1.000\n",
            "\n",
            "Interval 36740 (367390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 36741 (367400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.002 - mae: 7.452 - mean_q: 12.639 - prob: 1.000\n",
            "\n",
            "Interval 36742 (367410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.405 - mean_q: 12.759 - prob: 1.000\n",
            "\n",
            "Interval 36743 (367420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 36744 (367430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.003 - mae: 7.314 - mean_q: 12.537 - prob: 1.000\n",
            "\n",
            "Interval 36745 (367440 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.003 - mae: 7.414 - mean_q: 12.720 - prob: 1.000\n",
            "\n",
            "Interval 36746 (367450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.002 - mae: 7.465 - mean_q: 12.874 - prob: 1.000\n",
            "\n",
            "Interval 36747 (367460 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 36748 (367470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -18.000 [-18.000, -18.000] - loss: 0.001 - mae: 7.423 - mean_q: 12.728 - prob: 1.000\n",
            "\n",
            "Interval 36749 (367480 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.505 - mean_q: 12.947 - prob: 1.000\n",
            "\n",
            "Interval 36750 (367490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 36751 (367500 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.002 - mae: 7.473 - mean_q: 12.736 - prob: 1.000\n",
            "\n",
            "Interval 36752 (367510 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36753 (367520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.002 - mae: 7.404 - mean_q: 12.756 - prob: 1.000\n",
            "\n",
            "Interval 36754 (367530 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36755 (367540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 7.527 - mean_q: 12.928 - prob: 1.000\n",
            "\n",
            "Interval 36756 (367550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.136 - mean_q: 12.451 - prob: 1.000\n",
            "\n",
            "Interval 36757 (367560 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36758 (367570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.003 - mae: 7.400 - mean_q: 12.780 - prob: 1.000\n",
            "\n",
            "Interval 36759 (367580 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.442 - mean_q: 12.628 - prob: 1.000\n",
            "\n",
            "Interval 36760 (367590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.187 - mean_q: 12.433 - prob: 1.000\n",
            "\n",
            "Interval 36761 (367600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 36762 (367610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.004 - mae: 7.918 - mean_q: 13.470 - prob: 1.000\n",
            "\n",
            "Interval 36763 (367620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.759 - mean_q: 13.281 - prob: 1.000\n",
            "\n",
            "Interval 36764 (367630 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36765 (367640 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.003 - mae: 7.615 - mean_q: 12.997 - prob: 1.000\n",
            "\n",
            "Interval 36766 (367650 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.006 - mae: 7.338 - mean_q: 12.618 - prob: 1.000\n",
            "\n",
            "Interval 36767 (367660 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.004 - mae: 7.578 - mean_q: 13.036 - prob: 1.000\n",
            "\n",
            "Interval 36768 (367670 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.003 - mae: 7.716 - mean_q: 13.171 - prob: 1.000\n",
            "\n",
            "Interval 36769 (367680 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 7.339 - mean_q: 12.789 - prob: 1.000\n",
            "\n",
            "Interval 36770 (367690 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 36771 (367700 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.003 - mae: 7.200 - mean_q: 12.427 - prob: 1.000\n",
            "\n",
            "Interval 36772 (367710 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.006 - mae: 7.492 - mean_q: 12.843 - prob: 1.000\n",
            "\n",
            "Interval 36773 (367720 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.9000\n",
            "Interval 36774 (367730 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.005 - mae: 7.209 - mean_q: 12.225 - prob: 1.000\n",
            "\n",
            "Interval 36775 (367740 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.005 - mae: 7.850 - mean_q: 13.371 - prob: 1.000\n",
            "\n",
            "Interval 36776 (367750 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 36777 (367760 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.005 - mae: 7.365 - mean_q: 12.619 - prob: 1.000\n",
            "\n",
            "Interval 36778 (367770 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.005 - mae: 7.314 - mean_q: 12.697 - prob: 1.000\n",
            "\n",
            "Interval 36779 (367780 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.005 - mae: 7.163 - mean_q: 12.406 - prob: 1.000\n",
            "\n",
            "Interval 36780 (367790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 36781 (367800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.004 - mae: 7.308 - mean_q: 12.557 - prob: 1.000\n",
            "\n",
            "Interval 36782 (367810 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36783 (367820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.396 - mean_q: 12.754 - prob: 1.000\n",
            "\n",
            "Interval 36784 (367830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -21.000 [-21.000, -21.000] - loss: 0.003 - mae: 7.283 - mean_q: 12.462 - prob: 1.000\n",
            "\n",
            "Interval 36785 (367840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.004 - mae: 7.358 - mean_q: 12.774 - prob: 1.000\n",
            "\n",
            "Interval 36786 (367850 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36787 (367860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.004 - mae: 7.374 - mean_q: 12.766 - prob: 1.000\n",
            "\n",
            "Interval 36788 (367870 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36789 (367880 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36790 (367890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 36791 (367900 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -21.000 [-21.000, -21.000] - loss: 0.003 - mae: 7.250 - mean_q: 12.575 - prob: 1.000\n",
            "\n",
            "Interval 36792 (367910 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.005 - mae: 7.509 - mean_q: 12.792 - prob: 1.000\n",
            "\n",
            "Interval 36793 (367920 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.003 - mae: 7.687 - mean_q: 13.013 - prob: 1.000\n",
            "\n",
            "Interval 36794 (367930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 36795 (367940 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -25.000 [-25.000, -25.000] - loss: 0.005 - mae: 7.444 - mean_q: 12.930 - prob: 1.000\n",
            "\n",
            "Interval 36796 (367950 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.004 - mae: 7.234 - mean_q: 12.529 - prob: 1.000\n",
            "\n",
            "Interval 36797 (367960 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 36798 (367970 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.007 - mae: 7.416 - mean_q: 12.710 - prob: 1.000\n",
            "\n",
            "Interval 36799 (367980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.6000\n",
            "1 episodes - episode_reward: -21.000 [-21.000, -21.000] - loss: 0.002 - mae: 7.258 - mean_q: 12.503 - prob: 1.000\n",
            "\n",
            "Interval 36800 (367990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 36801 (368000 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.002 - mae: 7.275 - mean_q: 12.548 - prob: 1.000\n",
            "\n",
            "Interval 36802 (368010 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.385 - mean_q: 12.671 - prob: 1.000\n",
            "\n",
            "Interval 36803 (368020 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.307 - mean_q: 12.462 - prob: 1.000\n",
            "\n",
            "Interval 36804 (368030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 36805 (368040 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.512 - mean_q: 12.868 - prob: 1.000\n",
            "\n",
            "Interval 36806 (368050 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.004 - mae: 7.326 - mean_q: 12.703 - prob: 1.000\n",
            "\n",
            "Interval 36807 (368060 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 36808 (368070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.003 - mae: 7.055 - mean_q: 12.129 - prob: 1.000\n",
            "\n",
            "Interval 36809 (368080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.003 - mae: 7.226 - mean_q: 12.391 - prob: 1.000\n",
            "\n",
            "Interval 36810 (368090 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36811 (368100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.452 - mean_q: 12.625 - prob: 1.000\n",
            "\n",
            "Interval 36812 (368110 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 36813 (368120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -17.000 [-17.000, -17.000] - loss: 0.002 - mae: 7.362 - mean_q: 12.721 - prob: 1.000\n",
            "\n",
            "Interval 36814 (368130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.312 - mean_q: 12.568 - prob: 1.000\n",
            "\n",
            "Interval 36815 (368140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 36816 (368150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.003 - mae: 7.227 - mean_q: 12.493 - prob: 1.000\n",
            "\n",
            "Interval 36817 (368160 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.002 - mae: 7.364 - mean_q: 12.650 - prob: 1.000\n",
            "\n",
            "Interval 36818 (368170 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.003 - mae: 7.473 - mean_q: 12.854 - prob: 1.000\n",
            "\n",
            "Interval 36819 (368180 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.002 - mae: 7.404 - mean_q: 12.903 - prob: 1.000\n",
            "\n",
            "Interval 36820 (368190 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.9000\n",
            "Interval 36821 (368200 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.361 - mean_q: 12.683 - prob: 1.000\n",
            "\n",
            "Interval 36822 (368210 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.337 - mean_q: 12.718 - prob: 1.000\n",
            "\n",
            "Interval 36823 (368220 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 36824 (368230 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.425 - mean_q: 12.736 - prob: 1.000\n",
            "\n",
            "Interval 36825 (368240 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.203 - mean_q: 12.506 - prob: 1.000\n",
            "\n",
            "Interval 36826 (368250 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 36827 (368260 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.524 - mean_q: 13.108 - prob: 1.000\n",
            "\n",
            "Interval 36828 (368270 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.002 - mae: 7.256 - mean_q: 12.494 - prob: 1.000\n",
            "\n",
            "Interval 36829 (368280 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.002 - mae: 7.262 - mean_q: 12.508 - prob: 1.000\n",
            "\n",
            "Interval 36830 (368290 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.002 - mae: 7.275 - mean_q: 12.454 - prob: 1.000\n",
            "\n",
            "Interval 36831 (368300 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 36832 (368310 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.002 - mae: 7.367 - mean_q: 12.618 - prob: 1.000\n",
            "\n",
            "Interval 36833 (368320 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.327 - mean_q: 12.494 - prob: 1.000\n",
            "\n",
            "Interval 36834 (368330 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.002 - mae: 7.290 - mean_q: 12.599 - prob: 1.000\n",
            "\n",
            "Interval 36835 (368340 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 36836 (368350 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.465 - mean_q: 12.809 - prob: 1.000\n",
            "\n",
            "Interval 36837 (368360 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.183 - mean_q: 12.325 - prob: 1.000\n",
            "\n",
            "Interval 36838 (368370 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.752 - mean_q: 13.190 - prob: 1.000\n",
            "\n",
            "Interval 36839 (368380 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.002 - mae: 7.383 - mean_q: 12.748 - prob: 1.000\n",
            "\n",
            "Interval 36840 (368390 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: -1.0000\n",
            "Interval 36841 (368400 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.335 - mean_q: 12.710 - prob: 1.000\n",
            "\n",
            "Interval 36842 (368410 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.002 - mae: 7.218 - mean_q: 12.412 - prob: 1.000\n",
            "\n",
            "Interval 36843 (368420 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.058 - mean_q: 12.170 - prob: 1.000\n",
            "\n",
            "Interval 36844 (368430 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.9000\n",
            "Interval 36845 (368440 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.289 - mean_q: 12.545 - prob: 1.000\n",
            "\n",
            "Interval 36846 (368450 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.523 - mean_q: 12.870 - prob: 1.000\n",
            "\n",
            "Interval 36847 (368460 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 36848 (368470 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.333 - mean_q: 12.793 - prob: 1.000\n",
            "\n",
            "Interval 36849 (368480 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 36850 (368490 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.147 - mean_q: 12.356 - prob: 1.000\n",
            "\n",
            "Interval 36851 (368500 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.541 - mean_q: 12.993 - prob: 1.000\n",
            "\n",
            "Interval 36852 (368510 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.369 - mean_q: 12.798 - prob: 1.000\n",
            "\n",
            "Interval 36853 (368520 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 36854 (368530 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 36855 (368540 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.001 - mae: 7.409 - mean_q: 12.751 - prob: 1.000\n",
            "\n",
            "Interval 36856 (368550 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.369 - mean_q: 12.668 - prob: 1.000\n",
            "\n",
            "Interval 36857 (368560 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 36858 (368570 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.361 - mean_q: 12.683 - prob: 1.000\n",
            "\n",
            "Interval 36859 (368580 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 36860 (368590 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -18.000 [-18.000, -18.000] - loss: 0.002 - mae: 7.547 - mean_q: 13.013 - prob: 1.000\n",
            "\n",
            "Interval 36861 (368600 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.005 - mae: 7.359 - mean_q: 12.588 - prob: 1.000\n",
            "\n",
            "Interval 36862 (368610 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 36863 (368620 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -21.000 [-21.000, -21.000] - loss: 0.002 - mae: 7.627 - mean_q: 13.054 - prob: 1.000\n",
            "\n",
            "Interval 36864 (368630 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.003 - mae: 7.378 - mean_q: 12.611 - prob: 1.000\n",
            "\n",
            "Interval 36865 (368640 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.0000\n",
            "Interval 36866 (368650 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.357 - mean_q: 12.727 - prob: 1.000\n",
            "\n",
            "Interval 36867 (368660 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.502 - mean_q: 12.998 - prob: 1.000\n",
            "\n",
            "Interval 36868 (368670 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.9000\n",
            "Interval 36869 (368680 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.002 - mae: 7.257 - mean_q: 12.550 - prob: 1.000\n",
            "\n",
            "Interval 36870 (368690 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.002 - mae: 7.540 - mean_q: 13.063 - prob: 1.000\n",
            "\n",
            "Interval 36871 (368700 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.002 - mae: 7.691 - mean_q: 13.160 - prob: 1.000\n",
            "\n",
            "Interval 36872 (368710 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 36873 (368720 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.002 - mae: 7.262 - mean_q: 12.559 - prob: 1.000\n",
            "\n",
            "Interval 36874 (368730 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 36875 (368740 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.256 - mean_q: 12.537 - prob: 1.000\n",
            "\n",
            "Interval 36876 (368750 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.476 - mean_q: 12.860 - prob: 1.000\n",
            "\n",
            "Interval 36877 (368760 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 36878 (368770 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.408 - mean_q: 12.804 - prob: 1.000\n",
            "\n",
            "Interval 36879 (368780 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.002 - mae: 7.637 - mean_q: 13.211 - prob: 1.000\n",
            "\n",
            "Interval 36880 (368790 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 36881 (368800 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.545 - mean_q: 12.951 - prob: 1.000\n",
            "\n",
            "Interval 36882 (368810 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.002 - mae: 7.319 - mean_q: 12.584 - prob: 1.000\n",
            "\n",
            "Interval 36883 (368820 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 36884 (368830 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.002 - mae: 7.179 - mean_q: 12.556 - prob: 1.000\n",
            "\n",
            "Interval 36885 (368840 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 36886 (368850 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.002 - mae: 7.570 - mean_q: 12.943 - prob: 1.000\n",
            "\n",
            "Interval 36887 (368860 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 36888 (368870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.002 - mae: 7.614 - mean_q: 13.079 - prob: 1.000\n",
            "\n",
            "Interval 36889 (368880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.002 - mae: 7.363 - mean_q: 12.791 - prob: 1.000\n",
            "\n",
            "Interval 36890 (368890 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 36891 (368900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.002 - mae: 7.251 - mean_q: 12.500 - prob: 1.000\n",
            "\n",
            "Interval 36892 (368910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.002 - mae: 7.192 - mean_q: 12.587 - prob: 1.000\n",
            "\n",
            "Interval 36893 (368920 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36894 (368930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.488 - mean_q: 12.994 - prob: 1.000\n",
            "\n",
            "Interval 36895 (368940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.003 - mae: 7.507 - mean_q: 13.045 - prob: 1.000\n",
            "\n",
            "Interval 36896 (368950 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36897 (368960 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 9.000 [4.000, 14.000] - loss: 0.003 - mae: 7.258 - mean_q: 12.563 - prob: 1.000\n",
            "\n",
            "Interval 36898 (368970 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36899 (368980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.704 - mean_q: 13.203 - prob: 1.000\n",
            "\n",
            "Interval 36900 (368990 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36901 (369000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 36902 (369010 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.002 - mae: 7.274 - mean_q: 12.413 - prob: 1.000\n",
            "\n",
            "Interval 36903 (369020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.002 - mae: 7.477 - mean_q: 12.956 - prob: 1.000\n",
            "\n",
            "Interval 36904 (369030 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.405 - mean_q: 12.742 - prob: 1.000\n",
            "\n",
            "Interval 36905 (369040 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36906 (369050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.401 - mean_q: 12.619 - prob: 1.000\n",
            "\n",
            "Interval 36907 (369060 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36908 (369070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.550 - mean_q: 13.002 - prob: 1.000\n",
            "\n",
            "Interval 36909 (369080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.435 - mean_q: 12.744 - prob: 1.000\n",
            "\n",
            "Interval 36910 (369090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.156 - mean_q: 12.375 - prob: 1.000\n",
            "\n",
            "Interval 36911 (369100 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 36912 (369110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.326 - mean_q: 12.646 - prob: 1.000\n",
            "\n",
            "Interval 36913 (369120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.333 - mean_q: 12.519 - prob: 1.000\n",
            "\n",
            "Interval 36914 (369130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 36915 (369140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.002 - mae: 7.406 - mean_q: 12.821 - prob: 1.000\n",
            "\n",
            "Interval 36916 (369150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.002 - mae: 7.720 - mean_q: 13.273 - prob: 1.000\n",
            "\n",
            "Interval 36917 (369160 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 36918 (369170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.338 - mean_q: 12.725 - prob: 1.000\n",
            "\n",
            "Interval 36919 (369180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 7.064 - mean_q: 12.221 - prob: 1.000\n",
            "\n",
            "Interval 36920 (369190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.003 - mae: 7.462 - mean_q: 12.865 - prob: 1.000\n",
            "\n",
            "Interval 36921 (369200 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36922 (369210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.360 - mean_q: 12.801 - prob: 1.000\n",
            "\n",
            "Interval 36923 (369220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 7.323 - mean_q: 12.627 - prob: 1.000\n",
            "\n",
            "Interval 36924 (369230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 36925 (369240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.002 - mae: 7.500 - mean_q: 12.870 - prob: 1.000\n",
            "\n",
            "Interval 36926 (369250 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36927 (369260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.002 - mae: 7.634 - mean_q: 13.154 - prob: 1.000\n",
            "\n",
            "Interval 36928 (369270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.501 - mean_q: 12.862 - prob: 1.000\n",
            "\n",
            "Interval 36929 (369280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 36930 (369290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.273 - mean_q: 12.622 - prob: 1.000\n",
            "\n",
            "Interval 36931 (369300 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.350 - mean_q: 12.688 - prob: 1.000\n",
            "\n",
            "Interval 36932 (369310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 36933 (369320 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.002 - mae: 7.324 - mean_q: 12.596 - prob: 1.000\n",
            "\n",
            "Interval 36934 (369330 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 11.500 [10.000, 13.000] - loss: 0.001 - mae: 7.352 - mean_q: 12.619 - prob: 1.000\n",
            "\n",
            "Interval 36935 (369340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 36936 (369350 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.002 - mae: 7.538 - mean_q: 12.936 - prob: 1.000\n",
            "\n",
            "Interval 36937 (369360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 36938 (369370 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.262 - mean_q: 12.522 - prob: 1.000\n",
            "\n",
            "Interval 36939 (369380 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 36940 (369390 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -27.000 [-27.000, -27.000] - loss: 0.008 - mae: 7.268 - mean_q: 12.518 - prob: 1.000\n",
            "\n",
            "Interval 36941 (369400 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.007 - mae: 7.321 - mean_q: 12.726 - prob: 1.000\n",
            "\n",
            "Interval 36942 (369410 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.008 - mae: 7.416 - mean_q: 12.803 - prob: 1.000\n",
            "\n",
            "Interval 36943 (369420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.007 - mae: 7.473 - mean_q: 12.727 - prob: 1.000\n",
            "\n",
            "Interval 36944 (369430 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 36945 (369440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.005 - mae: 7.452 - mean_q: 12.743 - prob: 1.000\n",
            "\n",
            "Interval 36946 (369450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 36947 (369460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.004 - mae: 7.533 - mean_q: 12.797 - prob: 1.000\n",
            "\n",
            "Interval 36948 (369470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 36949 (369480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.003 - mae: 7.629 - mean_q: 13.158 - prob: 1.000\n",
            "\n",
            "Interval 36950 (369490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.255 - mean_q: 12.717 - prob: 1.000\n",
            "\n",
            "Interval 36951 (369500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.004 - mae: 7.258 - mean_q: 12.603 - prob: 1.000\n",
            "\n",
            "Interval 36952 (369510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 36953 (369520 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 2.3000\n",
            "2 episodes - episode_reward: -5.500 [-15.000, 4.000] - loss: 0.002 - mae: 7.664 - mean_q: 13.278 - prob: 1.000\n",
            "\n",
            "Interval 36954 (369530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 36955 (369540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.003 - mae: 7.608 - mean_q: 12.907 - prob: 1.000\n",
            "\n",
            "Interval 36956 (369550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 36957 (369560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.002 - mae: 7.423 - mean_q: 12.691 - prob: 1.000\n",
            "\n",
            "Interval 36958 (369570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.258 - mean_q: 12.644 - prob: 1.000\n",
            "\n",
            "Interval 36959 (369580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.002 - mae: 6.953 - mean_q: 12.152 - prob: 1.000\n",
            "\n",
            "Interval 36960 (369590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 36961 (369600 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.002 - mae: 7.384 - mean_q: 12.758 - prob: 1.000\n",
            "\n",
            "Interval 36962 (369610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.542 - mean_q: 13.017 - prob: 1.000\n",
            "\n",
            "Interval 36963 (369620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 36964 (369630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.341 - mean_q: 12.678 - prob: 1.000\n",
            "\n",
            "Interval 36965 (369640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 36966 (369650 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -18.000 [-18.000, -18.000] - loss: 0.002 - mae: 7.650 - mean_q: 13.038 - prob: 1.000\n",
            "\n",
            "Interval 36967 (369660 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36968 (369670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.002 - mae: 7.225 - mean_q: 12.625 - prob: 1.000\n",
            "\n",
            "Interval 36969 (369680 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.261 - mean_q: 12.493 - prob: 1.000\n",
            "\n",
            "Interval 36970 (369690 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 36971 (369700 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.002 - mae: 7.544 - mean_q: 12.877 - prob: 1.000\n",
            "\n",
            "Interval 36972 (369710 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 36973 (369720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.612 - mean_q: 13.082 - prob: 1.000\n",
            "\n",
            "Interval 36974 (369730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.329 - mean_q: 12.652 - prob: 1.000\n",
            "\n",
            "Interval 36975 (369740 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36976 (369750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.227 - mean_q: 12.526 - prob: 1.000\n",
            "\n",
            "Interval 36977 (369760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.003 - mae: 7.468 - mean_q: 12.914 - prob: 1.000\n",
            "\n",
            "Interval 36978 (369770 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36979 (369780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 6.877 - mean_q: 12.089 - prob: 1.000\n",
            "\n",
            "Interval 36980 (369790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.392 - mean_q: 12.837 - prob: 1.000\n",
            "\n",
            "Interval 36981 (369800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.381 - mean_q: 12.835 - prob: 1.000\n",
            "\n",
            "Interval 36982 (369810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.234 - mean_q: 12.569 - prob: 1.000\n",
            "\n",
            "Interval 36983 (369820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 36984 (369830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.178 - mean_q: 12.399 - prob: 1.000\n",
            "\n",
            "Interval 36985 (369840 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36986 (369850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.628 - mean_q: 13.042 - prob: 1.000\n",
            "\n",
            "Interval 36987 (369860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.136 - mean_q: 12.393 - prob: 1.000\n",
            "\n",
            "Interval 36988 (369870 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 36989 (369880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -18.000 [-18.000, -18.000] - loss: 0.001 - mae: 7.436 - mean_q: 12.655 - prob: 1.000\n",
            "\n",
            "Interval 36990 (369890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.359 - mean_q: 12.699 - prob: 1.000\n",
            "\n",
            "Interval 36991 (369900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 36992 (369910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -25.000 [-25.000, -25.000] - loss: 0.001 - mae: 7.297 - mean_q: 12.475 - prob: 1.000\n",
            "\n",
            "Interval 36993 (369920 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36994 (369930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 11.000 [7.000, 15.000] - loss: 0.001 - mae: 7.297 - mean_q: 12.672 - prob: 1.000\n",
            "\n",
            "Interval 36995 (369940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.200 - mean_q: 12.478 - prob: 1.000\n",
            "\n",
            "Interval 36996 (369950 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 36997 (369960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.487 - mean_q: 12.827 - prob: 1.000\n",
            "\n",
            "Interval 36998 (369970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 36999 (369980 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.448 - mean_q: 12.783 - prob: 1.000\n",
            "\n",
            "Interval 37000 (369990 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 37001 (370000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.002 - mae: 7.375 - mean_q: 12.540 - prob: 1.000\n",
            "\n",
            "Interval 37002 (370010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.374 - mean_q: 12.639 - prob: 1.000\n",
            "\n",
            "Interval 37003 (370020 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 37004 (370030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.002 - mae: 7.549 - mean_q: 13.143 - prob: 1.000\n",
            "\n",
            "Interval 37005 (370040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.533 - mean_q: 12.993 - prob: 1.000\n",
            "\n",
            "Interval 37006 (370050 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37007 (370060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.684 - mean_q: 13.007 - prob: 1.000\n",
            "\n",
            "Interval 37008 (370070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.003 - mae: 7.206 - mean_q: 12.319 - prob: 1.000\n",
            "\n",
            "Interval 37009 (370080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 37010 (370090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.378 - mean_q: 12.752 - prob: 1.000\n",
            "\n",
            "Interval 37011 (370100 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.360 - mean_q: 12.684 - prob: 1.000\n",
            "\n",
            "Interval 37012 (370110 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37013 (370120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.177 - mean_q: 12.304 - prob: 1.000\n",
            "\n",
            "Interval 37014 (370130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.423 - mean_q: 12.866 - prob: 1.000\n",
            "\n",
            "Interval 37015 (370140 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 37016 (370150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.002 - mae: 7.588 - mean_q: 13.099 - prob: 1.000\n",
            "\n",
            "Interval 37017 (370160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 37018 (370170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -26.000 [-26.000, -26.000] - loss: 0.002 - mae: 7.576 - mean_q: 13.088 - prob: 1.000\n",
            "\n",
            "Interval 37019 (370180 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 37020 (370190 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.002 - mae: 7.298 - mean_q: 12.541 - prob: 1.000\n",
            "\n",
            "Interval 37021 (370200 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 37022 (370210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.003 - mae: 7.452 - mean_q: 12.791 - prob: 1.000\n",
            "\n",
            "Interval 37023 (370220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.003 - mae: 7.121 - mean_q: 12.304 - prob: 1.000\n",
            "\n",
            "Interval 37024 (370230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 37025 (370240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.002 - mae: 7.609 - mean_q: 13.063 - prob: 1.000\n",
            "\n",
            "Interval 37026 (370250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.002 - mae: 7.676 - mean_q: 13.057 - prob: 1.000\n",
            "\n",
            "Interval 37027 (370260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.002 - mae: 7.412 - mean_q: 12.682 - prob: 1.000\n",
            "\n",
            "Interval 37028 (370270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37029 (370280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.496 - mean_q: 12.981 - prob: 1.000\n",
            "\n",
            "Interval 37030 (370290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.002 - mae: 7.466 - mean_q: 12.673 - prob: 1.000\n",
            "\n",
            "Interval 37031 (370300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.366 - mean_q: 12.596 - prob: 1.000\n",
            "\n",
            "Interval 37032 (370310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.274 - mean_q: 12.597 - prob: 1.000\n",
            "\n",
            "Interval 37033 (370320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37034 (370330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 2.3000\n",
            "2 episodes - episode_reward: 12.000 [9.000, 15.000] - loss: 0.003 - mae: 7.304 - mean_q: 12.744 - prob: 1.000\n",
            "\n",
            "Interval 37035 (370340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.002 - mae: 7.515 - mean_q: 12.860 - prob: 1.000\n",
            "\n",
            "Interval 37036 (370350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.003 - mae: 7.350 - mean_q: 12.551 - prob: 1.000\n",
            "\n",
            "Interval 37037 (370360 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37038 (370370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.002 - mae: 7.299 - mean_q: 12.514 - prob: 1.000\n",
            "\n",
            "Interval 37039 (370380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 37040 (370390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.002 - mae: 7.265 - mean_q: 12.475 - prob: 1.000\n",
            "\n",
            "Interval 37041 (370400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.328 - mean_q: 12.611 - prob: 1.000\n",
            "\n",
            "Interval 37042 (370410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.298 - mean_q: 12.669 - prob: 1.000\n",
            "\n",
            "Interval 37043 (370420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37044 (370430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.849 - mean_q: 13.433 - prob: 1.000\n",
            "\n",
            "Interval 37045 (370440 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 37046 (370450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.003 - mae: 7.279 - mean_q: 12.463 - prob: 1.000\n",
            "\n",
            "Interval 37047 (370460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.003 - mae: 7.204 - mean_q: 12.452 - prob: 1.000\n",
            "\n",
            "Interval 37048 (370470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37049 (370480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.003 - mae: 7.463 - mean_q: 12.860 - prob: 1.000\n",
            "\n",
            "Interval 37050 (370490 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.003 - mae: 7.238 - mean_q: 12.497 - prob: 1.000\n",
            "\n",
            "Interval 37051 (370500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37052 (370510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.002 - mae: 7.729 - mean_q: 13.326 - prob: 1.000\n",
            "\n",
            "Interval 37053 (370520 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.002 - mae: 7.493 - mean_q: 12.930 - prob: 1.000\n",
            "\n",
            "Interval 37054 (370530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37055 (370540 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.002 - mae: 7.317 - mean_q: 12.675 - prob: 1.000\n",
            "\n",
            "Interval 37056 (370550 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.385 - mean_q: 12.708 - prob: 1.000\n",
            "\n",
            "Interval 37057 (370560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.435 - mean_q: 12.848 - prob: 1.000\n",
            "\n",
            "Interval 37058 (370570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37059 (370580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.395 - mean_q: 12.662 - prob: 1.000\n",
            "\n",
            "Interval 37060 (370590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.002 - mae: 7.207 - mean_q: 12.601 - prob: 1.000\n",
            "\n",
            "Interval 37061 (370600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37062 (370610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.675 - mean_q: 13.127 - prob: 1.000\n",
            "\n",
            "Interval 37063 (370620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37064 (370630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.531 - mean_q: 12.974 - prob: 1.000\n",
            "\n",
            "Interval 37065 (370640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.325 - mean_q: 12.600 - prob: 1.000\n",
            "\n",
            "Interval 37066 (370650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.239 - mean_q: 12.489 - prob: 1.000\n",
            "\n",
            "Interval 37067 (370660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.428 - mean_q: 12.827 - prob: 1.000\n",
            "\n",
            "Interval 37068 (370670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37069 (370680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.486 - mean_q: 12.877 - prob: 1.000\n",
            "\n",
            "Interval 37070 (370690 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 37071 (370700 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.002 - mae: 7.351 - mean_q: 12.749 - prob: 1.000\n",
            "\n",
            "Interval 37072 (370710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37073 (370720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.343 - mean_q: 12.716 - prob: 1.000\n",
            "\n",
            "Interval 37074 (370730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.452 - mean_q: 12.794 - prob: 1.000\n",
            "\n",
            "Interval 37075 (370740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.415 - mean_q: 12.659 - prob: 1.000\n",
            "\n",
            "Interval 37076 (370750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37077 (370760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.002 - mae: 7.420 - mean_q: 12.910 - prob: 1.000\n",
            "\n",
            "Interval 37078 (370770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 37079 (370780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.002 - mae: 7.651 - mean_q: 13.054 - prob: 1.000\n",
            "\n",
            "Interval 37080 (370790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37081 (370800 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.003 - mae: 7.497 - mean_q: 12.821 - prob: 1.000\n",
            "\n",
            "Interval 37082 (370810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37083 (370820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.004 - mae: 7.481 - mean_q: 12.913 - prob: 1.000\n",
            "\n",
            "Interval 37084 (370830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.003 - mae: 7.333 - mean_q: 12.708 - prob: 1.000\n",
            "\n",
            "Interval 37085 (370840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.004 - mae: 7.385 - mean_q: 12.611 - prob: 1.000\n",
            "\n",
            "Interval 37086 (370850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37087 (370860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.005 - mae: 7.188 - mean_q: 12.386 - prob: 1.000\n",
            "\n",
            "Interval 37088 (370870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.006 - mae: 7.347 - mean_q: 12.676 - prob: 1.000\n",
            "\n",
            "Interval 37089 (370880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37090 (370890 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.004 - mae: 7.467 - mean_q: 12.718 - prob: 1.000\n",
            "\n",
            "Interval 37091 (370900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37092 (370910 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.007 - mae: 7.229 - mean_q: 12.628 - prob: 1.000\n",
            "\n",
            "Interval 37093 (370920 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.004 - mae: 7.546 - mean_q: 12.829 - prob: 1.000\n",
            "\n",
            "Interval 37094 (370930 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.003 - mae: 7.216 - mean_q: 12.359 - prob: 1.000\n",
            "\n",
            "Interval 37095 (370940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37096 (370950 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.002 - mae: 7.428 - mean_q: 12.738 - prob: 1.000\n",
            "\n",
            "Interval 37097 (370960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 37098 (370970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.003 - mae: 7.037 - mean_q: 12.291 - prob: 1.000\n",
            "\n",
            "Interval 37099 (370980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37100 (370990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 7.256 - mean_q: 12.487 - prob: 1.000\n",
            "\n",
            "Interval 37101 (371000 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.004 - mae: 7.021 - mean_q: 12.099 - prob: 1.000\n",
            "\n",
            "Interval 37102 (371010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37103 (371020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 7.440 - mean_q: 12.928 - prob: 1.000\n",
            "\n",
            "Interval 37104 (371030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37105 (371040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.003 - mae: 6.979 - mean_q: 12.013 - prob: 1.000\n",
            "\n",
            "Interval 37106 (371050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.277 - mean_q: 12.717 - prob: 1.000\n",
            "\n",
            "Interval 37107 (371060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 37108 (371070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.003 - mae: 7.273 - mean_q: 12.645 - prob: 1.000\n",
            "\n",
            "Interval 37109 (371080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37110 (371090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 7.532 - mean_q: 13.055 - prob: 1.000\n",
            "\n",
            "Interval 37111 (371100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37112 (371110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.088 - mean_q: 12.277 - prob: 1.000\n",
            "\n",
            "Interval 37113 (371120 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 37114 (371130 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -25.000 [-25.000, -25.000] - loss: 0.002 - mae: 7.186 - mean_q: 12.437 - prob: 1.000\n",
            "\n",
            "Interval 37115 (371140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.633 - mean_q: 13.133 - prob: 1.000\n",
            "\n",
            "Interval 37116 (371150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37117 (371160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.003 - mae: 7.528 - mean_q: 12.915 - prob: 1.000\n",
            "\n",
            "Interval 37118 (371170 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.002 - mae: 7.625 - mean_q: 13.043 - prob: 1.000\n",
            "\n",
            "Interval 37119 (371180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37120 (371190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.264 - mean_q: 12.658 - prob: 1.000\n",
            "\n",
            "Interval 37121 (371200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37122 (371210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.458 - mean_q: 12.792 - prob: 1.000\n",
            "\n",
            "Interval 37123 (371220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37124 (371230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.002 - mae: 7.536 - mean_q: 12.955 - prob: 1.000\n",
            "\n",
            "Interval 37125 (371240 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.213 - mean_q: 12.403 - prob: 1.000\n",
            "\n",
            "Interval 37126 (371250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 37127 (371260 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.547 - mean_q: 12.995 - prob: 1.000\n",
            "\n",
            "Interval 37128 (371270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 37129 (371280 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.481 - mean_q: 12.784 - prob: 1.000\n",
            "\n",
            "Interval 37130 (371290 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.215 - mean_q: 12.647 - prob: 1.000\n",
            "\n",
            "Interval 37131 (371300 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.001 - mae: 7.204 - mean_q: 12.341 - prob: 1.000\n",
            "\n",
            "Interval 37132 (371310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.360 - mean_q: 12.793 - prob: 1.000\n",
            "\n",
            "Interval 37133 (371320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 37134 (371330 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.002 - mae: 7.284 - mean_q: 12.618 - prob: 1.000\n",
            "\n",
            "Interval 37135 (371340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 37136 (371350 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.002 - mae: 7.382 - mean_q: 12.698 - prob: 1.000\n",
            "\n",
            "Interval 37137 (371360 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 37138 (371370 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.002 - mae: 7.481 - mean_q: 12.948 - prob: 1.000\n",
            "\n",
            "Interval 37139 (371380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 37140 (371390 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.002 - mae: 7.327 - mean_q: 12.778 - prob: 1.000\n",
            "\n",
            "Interval 37141 (371400 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.571 - mean_q: 13.012 - prob: 1.000\n",
            "\n",
            "Interval 37142 (371410 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.576 - mean_q: 13.095 - prob: 1.000\n",
            "\n",
            "Interval 37143 (371420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37144 (371430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.345 - mean_q: 12.671 - prob: 1.000\n",
            "\n",
            "Interval 37145 (371440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37146 (371450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.539 - mean_q: 12.916 - prob: 1.000\n",
            "\n",
            "Interval 37147 (371460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.002 - mae: 7.680 - mean_q: 13.062 - prob: 1.000\n",
            "\n",
            "Interval 37148 (371470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37149 (371480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.350 - mean_q: 12.663 - prob: 1.000\n",
            "\n",
            "Interval 37150 (371490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 37151 (371500 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.002 - mae: 7.542 - mean_q: 13.002 - prob: 1.000\n",
            "\n",
            "Interval 37152 (371510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.150 - mean_q: 12.379 - prob: 1.000\n",
            "\n",
            "Interval 37153 (371520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37154 (371530 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.002 - mae: 7.375 - mean_q: 12.715 - prob: 1.000\n",
            "\n",
            "Interval 37155 (371540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.002 - mae: 6.989 - mean_q: 12.224 - prob: 1.000\n",
            "\n",
            "Interval 37156 (371550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.440 - mean_q: 12.791 - prob: 1.000\n",
            "\n",
            "Interval 37157 (371560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37158 (371570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.352 - mean_q: 12.797 - prob: 1.000\n",
            "\n",
            "Interval 37159 (371580 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.509 - mean_q: 12.800 - prob: 1.000\n",
            "\n",
            "Interval 37160 (371590 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.529 - mean_q: 12.876 - prob: 1.000\n",
            "\n",
            "Interval 37161 (371600 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.325 - mean_q: 12.628 - prob: 1.000\n",
            "\n",
            "Interval 37162 (371610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37163 (371620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.079 - mean_q: 12.402 - prob: 1.000\n",
            "\n",
            "Interval 37164 (371630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.287 - mean_q: 12.598 - prob: 1.000\n",
            "\n",
            "Interval 37165 (371640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.002 - mae: 7.842 - mean_q: 13.506 - prob: 1.000\n",
            "\n",
            "Interval 37166 (371650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37167 (371660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.002 - mae: 7.672 - mean_q: 13.216 - prob: 1.000\n",
            "\n",
            "Interval 37168 (371670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37169 (371680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.413 - mean_q: 12.775 - prob: 1.000\n",
            "\n",
            "Interval 37170 (371690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.297 - mean_q: 12.631 - prob: 1.000\n",
            "\n",
            "Interval 37171 (371700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.069 - mean_q: 12.326 - prob: 1.000\n",
            "\n",
            "Interval 37172 (371710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.002 - mae: 7.404 - mean_q: 12.754 - prob: 1.000\n",
            "\n",
            "Interval 37173 (371720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37174 (371730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.014 - mean_q: 12.156 - prob: 1.000\n",
            "\n",
            "Interval 37175 (371740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.295 - mean_q: 12.472 - prob: 1.000\n",
            "\n",
            "Interval 37176 (371750 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37177 (371760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.002 - mae: 7.256 - mean_q: 12.595 - prob: 1.000\n",
            "\n",
            "Interval 37178 (371770 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.130 - mean_q: 12.364 - prob: 1.000\n",
            "\n",
            "Interval 37179 (371780 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.003 - mae: 7.533 - mean_q: 12.781 - prob: 1.000\n",
            "\n",
            "Interval 37180 (371790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37181 (371800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.202 - mean_q: 12.593 - prob: 1.000\n",
            "\n",
            "Interval 37182 (371810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.647 - mean_q: 13.020 - prob: 1.000\n",
            "\n",
            "Interval 37183 (371820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37184 (371830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.390 - mean_q: 12.758 - prob: 1.000\n",
            "\n",
            "Interval 37185 (371840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 37186 (371850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -35.000 [-35.000, -35.000] - loss: 0.002 - mae: 7.610 - mean_q: 13.062 - prob: 1.000\n",
            "\n",
            "Interval 37187 (371860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.002 - mae: 7.260 - mean_q: 12.517 - prob: 1.000\n",
            "\n",
            "Interval 37188 (371870 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.260 - mean_q: 12.534 - prob: 1.000\n",
            "\n",
            "Interval 37189 (371880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37190 (371890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.002 - mae: 7.334 - mean_q: 12.645 - prob: 1.000\n",
            "\n",
            "Interval 37191 (371900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37192 (371910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.222 - mean_q: 12.578 - prob: 1.000\n",
            "\n",
            "Interval 37193 (371920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37194 (371930 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.309 - mean_q: 12.522 - prob: 1.000\n",
            "\n",
            "Interval 37195 (371940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.418 - mean_q: 12.884 - prob: 1.000\n",
            "\n",
            "Interval 37196 (371950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37197 (371960 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.175 - mean_q: 12.397 - prob: 1.000\n",
            "\n",
            "Interval 37198 (371970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.194 - mean_q: 12.368 - prob: 1.000\n",
            "\n",
            "Interval 37199 (371980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.003 - mae: 7.566 - mean_q: 12.976 - prob: 1.000\n",
            "\n",
            "Interval 37200 (371990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37201 (372000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.002 - mae: 7.587 - mean_q: 13.011 - prob: 1.000\n",
            "\n",
            "Interval 37202 (372010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.002 - mae: 7.095 - mean_q: 12.306 - prob: 1.000\n",
            "\n",
            "Interval 37203 (372020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.422 - mean_q: 12.830 - prob: 1.000\n",
            "\n",
            "Interval 37204 (372030 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37205 (372040 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.004 - mae: 7.427 - mean_q: 12.828 - prob: 1.000\n",
            "\n",
            "Interval 37206 (372050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37207 (372060 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.003 - mae: 7.250 - mean_q: 12.406 - prob: 1.000\n",
            "\n",
            "Interval 37208 (372070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.443 - mean_q: 12.785 - prob: 1.000\n",
            "\n",
            "Interval 37209 (372080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.003 - mae: 7.148 - mean_q: 12.332 - prob: 1.000\n",
            "\n",
            "Interval 37210 (372090 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37211 (372100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.002 - mae: 7.400 - mean_q: 12.641 - prob: 1.000\n",
            "\n",
            "Interval 37212 (372110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.003 - mae: 7.338 - mean_q: 12.563 - prob: 1.000\n",
            "\n",
            "Interval 37213 (372120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37214 (372130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.192 - mean_q: 12.495 - prob: 1.000\n",
            "\n",
            "Interval 37215 (372140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.002 - mae: 7.075 - mean_q: 12.268 - prob: 1.000\n",
            "\n",
            "Interval 37216 (372150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 37217 (372160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -17.000 [-17.000, -17.000] - loss: 0.002 - mae: 7.188 - mean_q: 12.517 - prob: 1.000\n",
            "\n",
            "Interval 37218 (372170 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37219 (372180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.404 - mean_q: 12.676 - prob: 1.000\n",
            "\n",
            "Interval 37220 (372190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.003 - mae: 7.522 - mean_q: 12.969 - prob: 1.000\n",
            "\n",
            "Interval 37221 (372200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 37222 (372210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.001 - mae: 7.550 - mean_q: 13.075 - prob: 1.000\n",
            "\n",
            "Interval 37223 (372220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37224 (372230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.002 - mae: 7.250 - mean_q: 12.518 - prob: 1.000\n",
            "\n",
            "Interval 37225 (372240 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.576 - mean_q: 12.958 - prob: 1.000\n",
            "\n",
            "Interval 37226 (372250 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37227 (372260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.002 - mae: 7.372 - mean_q: 12.829 - prob: 1.000\n",
            "\n",
            "Interval 37228 (372270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37229 (372280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.002 - mae: 7.699 - mean_q: 12.973 - prob: 1.000\n",
            "\n",
            "Interval 37230 (372290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.6000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.002 - mae: 7.433 - mean_q: 12.739 - prob: 1.000\n",
            "\n",
            "Interval 37231 (372300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.003 - mae: 7.488 - mean_q: 12.955 - prob: 1.000\n",
            "\n",
            "Interval 37232 (372310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 37233 (372320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.002 - mae: 7.438 - mean_q: 12.832 - prob: 1.000\n",
            "\n",
            "Interval 37234 (372330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.002 - mae: 7.309 - mean_q: 12.670 - prob: 1.000\n",
            "\n",
            "Interval 37235 (372340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37236 (372350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.220 - mean_q: 12.528 - prob: 1.000\n",
            "\n",
            "Interval 37237 (372360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.546 - mean_q: 13.032 - prob: 1.000\n",
            "\n",
            "Interval 37238 (372370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 37239 (372380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.002 - mae: 7.191 - mean_q: 12.431 - prob: 1.000\n",
            "\n",
            "Interval 37240 (372390 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.477 - mean_q: 12.667 - prob: 1.000\n",
            "\n",
            "Interval 37241 (372400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.441 - mean_q: 12.665 - prob: 1.000\n",
            "\n",
            "Interval 37242 (372410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37243 (372420 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.498 - mean_q: 12.936 - prob: 1.000\n",
            "\n",
            "Interval 37244 (372430 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.437 - mean_q: 12.829 - prob: 1.000\n",
            "\n",
            "Interval 37245 (372440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.241 - mean_q: 12.580 - prob: 1.000\n",
            "\n",
            "Interval 37246 (372450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37247 (372460 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.300 - mean_q: 12.669 - prob: 1.000\n",
            "\n",
            "Interval 37248 (372470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.356 - mean_q: 12.515 - prob: 1.000\n",
            "\n",
            "Interval 37249 (372480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 37250 (372490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.348 - mean_q: 12.650 - prob: 1.000\n",
            "\n",
            "Interval 37251 (372500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37252 (372510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 37253 (372520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.001 - mae: 7.529 - mean_q: 12.922 - prob: 1.000\n",
            "\n",
            "Interval 37254 (372530 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37255 (372540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -19.000 [-19.000, -19.000] - loss: 0.001 - mae: 6.964 - mean_q: 12.160 - prob: 1.000\n",
            "\n",
            "Interval 37256 (372550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.512 - mean_q: 12.784 - prob: 1.000\n",
            "\n",
            "Interval 37257 (372560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 37258 (372570 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.001 - mae: 7.313 - mean_q: 12.717 - prob: 1.000\n",
            "\n",
            "Interval 37259 (372580 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 37260 (372590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.002 - mae: 7.320 - mean_q: 12.643 - prob: 1.000\n",
            "\n",
            "Interval 37261 (372600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37262 (372610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.374 - mean_q: 12.671 - prob: 1.000\n",
            "\n",
            "Interval 37263 (372620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37264 (372630 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.001 - mae: 7.214 - mean_q: 12.486 - prob: 1.000\n",
            "\n",
            "Interval 37265 (372640 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 37266 (372650 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.363 - mean_q: 12.744 - prob: 1.000\n",
            "\n",
            "Interval 37267 (372660 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37268 (372670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.520 - mean_q: 12.870 - prob: 1.000\n",
            "\n",
            "Interval 37269 (372680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37270 (372690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.002 - mae: 7.609 - mean_q: 12.936 - prob: 1.000\n",
            "\n",
            "Interval 37271 (372700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.549 - mean_q: 12.895 - prob: 1.000\n",
            "\n",
            "Interval 37272 (372710 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.002 - mae: 7.375 - mean_q: 12.713 - prob: 1.000\n",
            "\n",
            "Interval 37273 (372720 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37274 (372730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.445 - mean_q: 12.759 - prob: 1.000\n",
            "\n",
            "Interval 37275 (372740 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37276 (372750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.401 - mean_q: 12.707 - prob: 1.000\n",
            "\n",
            "Interval 37277 (372760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.569 - mean_q: 12.994 - prob: 1.000\n",
            "\n",
            "Interval 37278 (372770 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37279 (372780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 10.000 [6.000, 14.000] - loss: 0.002 - mae: 7.435 - mean_q: 12.769 - prob: 1.000\n",
            "\n",
            "Interval 37280 (372790 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 37281 (372800 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.003 - mae: 7.268 - mean_q: 12.582 - prob: 1.000\n",
            "\n",
            "Interval 37282 (372810 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 37283 (372820 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.002 - mae: 7.071 - mean_q: 12.266 - prob: 1.000\n",
            "\n",
            "Interval 37284 (372830 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 6.927 - mean_q: 11.913 - prob: 1.000\n",
            "\n",
            "Interval 37285 (372840 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 37286 (372850 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.515 - mean_q: 12.757 - prob: 1.000\n",
            "\n",
            "Interval 37287 (372860 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.402 - mean_q: 12.671 - prob: 1.000\n",
            "\n",
            "Interval 37288 (372870 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -2.8000\n",
            "Interval 37289 (372880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.002 - mae: 7.503 - mean_q: 13.015 - prob: 1.000\n",
            "\n",
            "Interval 37290 (372890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.290 - mean_q: 12.697 - prob: 1.000\n",
            "\n",
            "Interval 37291 (372900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37292 (372910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.001 - mae: 7.539 - mean_q: 12.950 - prob: 1.000\n",
            "\n",
            "Interval 37293 (372920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.191 - mean_q: 12.507 - prob: 1.000\n",
            "\n",
            "Interval 37294 (372930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.002 - mae: 7.422 - mean_q: 12.672 - prob: 1.000\n",
            "\n",
            "Interval 37295 (372940 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37296 (372950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.658 - mean_q: 13.200 - prob: 1.000\n",
            "\n",
            "Interval 37297 (372960 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37298 (372970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.002 - mae: 7.466 - mean_q: 12.845 - prob: 1.000\n",
            "\n",
            "Interval 37299 (372980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.003 - mae: 7.484 - mean_q: 12.847 - prob: 1.000\n",
            "\n",
            "Interval 37300 (372990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37301 (373000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.457 - mean_q: 12.838 - prob: 1.000\n",
            "\n",
            "Interval 37302 (373010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.836 - mean_q: 13.437 - prob: 1.000\n",
            "\n",
            "Interval 37303 (373020 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.706 - mean_q: 12.957 - prob: 1.000\n",
            "\n",
            "Interval 37304 (373030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37305 (373040 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.003 - mae: 7.342 - mean_q: 12.635 - prob: 1.000\n",
            "\n",
            "Interval 37306 (373050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.302 - mean_q: 12.565 - prob: 1.000\n",
            "\n",
            "Interval 37307 (373060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37308 (373070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.291 - mean_q: 12.568 - prob: 1.000\n",
            "\n",
            "Interval 37309 (373080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.647 - mean_q: 12.996 - prob: 1.000\n",
            "\n",
            "Interval 37310 (373090 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 37311 (373100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.004 - mae: 7.338 - mean_q: 12.602 - prob: 1.000\n",
            "\n",
            "Interval 37312 (373110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37313 (373120 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.003 - mae: 7.057 - mean_q: 12.207 - prob: 1.000\n",
            "\n",
            "Interval 37314 (373130 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.325 - mean_q: 12.634 - prob: 1.000\n",
            "\n",
            "Interval 37315 (373140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37316 (373150 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.002 - mae: 7.593 - mean_q: 13.077 - prob: 1.000\n",
            "\n",
            "Interval 37317 (373160 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.002 - mae: 7.514 - mean_q: 12.857 - prob: 1.000\n",
            "\n",
            "Interval 37318 (373170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37319 (373180 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.002 - mae: 7.737 - mean_q: 13.203 - prob: 1.000\n",
            "\n",
            "Interval 37320 (373190 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 37321 (373200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.002 - mae: 7.361 - mean_q: 12.648 - prob: 1.000\n",
            "\n",
            "Interval 37322 (373210 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.378 - mean_q: 12.789 - prob: 1.000\n",
            "\n",
            "Interval 37323 (373220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37324 (373230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.288 - mean_q: 12.574 - prob: 1.000\n",
            "\n",
            "Interval 37325 (373240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.200 - mean_q: 12.494 - prob: 1.000\n",
            "\n",
            "Interval 37326 (373250 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37327 (373260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.213 - mean_q: 12.499 - prob: 1.000\n",
            "\n",
            "Interval 37328 (373270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.355 - mean_q: 12.669 - prob: 1.000\n",
            "\n",
            "Interval 37329 (373280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.228 - mean_q: 12.570 - prob: 1.000\n",
            "\n",
            "Interval 37330 (373290 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 37331 (373300 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.001 - mae: 7.275 - mean_q: 12.759 - prob: 1.000\n",
            "\n",
            "Interval 37332 (373310 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.001 - mae: 7.317 - mean_q: 12.574 - prob: 1.000\n",
            "\n",
            "Interval 37333 (373320 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 37334 (373330 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.002 - mae: 7.463 - mean_q: 12.871 - prob: 1.000\n",
            "\n",
            "Interval 37335 (373340 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.326 - mean_q: 12.568 - prob: 1.000\n",
            "\n",
            "Interval 37336 (373350 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.468 - mean_q: 12.834 - prob: 1.000\n",
            "\n",
            "Interval 37337 (373360 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 37338 (373370 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.577 - mean_q: 13.051 - prob: 1.000\n",
            "\n",
            "Interval 37339 (373380 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.291 - mean_q: 12.579 - prob: 1.000\n",
            "\n",
            "Interval 37340 (373390 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 37341 (373400 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.351 - mean_q: 12.794 - prob: 1.000\n",
            "\n",
            "Interval 37342 (373410 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 37343 (373420 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.006 - mae: 7.262 - mean_q: 12.439 - prob: 1.000\n",
            "\n",
            "Interval 37344 (373430 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.004 - mae: 7.491 - mean_q: 12.776 - prob: 1.000\n",
            "\n",
            "Interval 37345 (373440 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.166 - mean_q: 12.336 - prob: 1.000\n",
            "\n",
            "Interval 37346 (373450 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 37347 (373460 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.002 - mae: 7.197 - mean_q: 12.311 - prob: 1.000\n",
            "\n",
            "Interval 37348 (373470 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: -1.0000\n",
            "Interval 37349 (373480 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.002 - mae: 7.217 - mean_q: 12.522 - prob: 1.000\n",
            "\n",
            "Interval 37350 (373490 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.004 - mae: 7.107 - mean_q: 12.468 - prob: 1.000\n",
            "\n",
            "Interval 37351 (373500 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 37352 (373510 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.004 - mae: 7.448 - mean_q: 12.787 - prob: 1.000\n",
            "\n",
            "Interval 37353 (373520 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.003 - mae: 7.286 - mean_q: 12.609 - prob: 1.000\n",
            "\n",
            "Interval 37354 (373530 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 37355 (373540 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.003 - mae: 7.582 - mean_q: 12.954 - prob: 1.000\n",
            "\n",
            "Interval 37356 (373550 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.002 - mae: 7.252 - mean_q: 12.537 - prob: 1.000\n",
            "\n",
            "Interval 37357 (373560 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.102 - mean_q: 12.322 - prob: 1.000\n",
            "\n",
            "Interval 37358 (373570 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 37359 (373580 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.002 - mae: 7.473 - mean_q: 12.844 - prob: 1.000\n",
            "\n",
            "Interval 37360 (373590 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 37361 (373600 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.411 - mean_q: 12.771 - prob: 1.000\n",
            "\n",
            "Interval 37362 (373610 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.659 - mean_q: 13.042 - prob: 1.000\n",
            "\n",
            "Interval 37363 (373620 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.409 - mean_q: 12.855 - prob: 1.000\n",
            "\n",
            "Interval 37364 (373630 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 37365 (373640 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 7.204 - mean_q: 12.427 - prob: 1.000\n",
            "\n",
            "Interval 37366 (373650 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 37367 (373660 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.004 - mae: 7.182 - mean_q: 12.369 - prob: 1.000\n",
            "\n",
            "Interval 37368 (373670 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.235 - mean_q: 12.536 - prob: 1.000\n",
            "\n",
            "Interval 37369 (373680 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 37370 (373690 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.002 - mae: 7.096 - mean_q: 12.388 - prob: 1.000\n",
            "\n",
            "Interval 37371 (373700 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.003 - mae: 7.367 - mean_q: 12.575 - prob: 1.000\n",
            "\n",
            "Interval 37372 (373710 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 37373 (373720 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.242 - mean_q: 12.541 - prob: 1.000\n",
            "\n",
            "Interval 37374 (373730 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37375 (373740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.002 - mae: 7.460 - mean_q: 12.945 - prob: 1.000\n",
            "\n",
            "Interval 37376 (373750 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 7.347 - mean_q: 12.606 - prob: 1.000\n",
            "\n",
            "Interval 37377 (373760 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 37378 (373770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.002 - mae: 7.262 - mean_q: 12.576 - prob: 1.000\n",
            "\n",
            "Interval 37379 (373780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.002 - mae: 7.271 - mean_q: 12.585 - prob: 1.000\n",
            "\n",
            "Interval 37380 (373790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.664 - mean_q: 13.044 - prob: 1.000\n",
            "\n",
            "Interval 37381 (373800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.593 - mean_q: 13.010 - prob: 1.000\n",
            "\n",
            "Interval 37382 (373810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37383 (373820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.321 - mean_q: 12.563 - prob: 1.000\n",
            "\n",
            "Interval 37384 (373830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.301 - mean_q: 12.659 - prob: 1.000\n",
            "\n",
            "Interval 37385 (373840 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 37386 (373850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.002 - mae: 7.816 - mean_q: 13.148 - prob: 1.000\n",
            "\n",
            "Interval 37387 (373860 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.002 - mae: 7.511 - mean_q: 12.930 - prob: 1.000\n",
            "\n",
            "Interval 37388 (373870 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37389 (373880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.371 - mean_q: 12.724 - prob: 1.000\n",
            "\n",
            "Interval 37390 (373890 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37391 (373900 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.355 - mean_q: 12.715 - prob: 1.000\n",
            "\n",
            "Interval 37392 (373910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.002 - mae: 7.523 - mean_q: 12.864 - prob: 1.000\n",
            "\n",
            "Interval 37393 (373920 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.419 - mean_q: 12.793 - prob: 1.000\n",
            "\n",
            "Interval 37394 (373930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37395 (373940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.246 - mean_q: 12.385 - prob: 1.000\n",
            "\n",
            "Interval 37396 (373950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37397 (373960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.002 - mae: 7.175 - mean_q: 12.455 - prob: 1.000\n",
            "\n",
            "Interval 37398 (373970 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.588 - mean_q: 13.056 - prob: 1.000\n",
            "\n",
            "Interval 37399 (373980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 37400 (373990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 1.000 [-11.000, 13.000] - loss: 0.002 - mae: 7.022 - mean_q: 12.105 - prob: 1.000\n",
            "\n",
            "Interval 37401 (374000 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37402 (374010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.133 - mean_q: 12.349 - prob: 1.000\n",
            "\n",
            "Interval 37403 (374020 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 37404 (374030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -23.000 [-23.000, -23.000] - loss: 0.002 - mae: 7.360 - mean_q: 12.625 - prob: 1.000\n",
            "\n",
            "Interval 37405 (374040 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37406 (374050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.002 - mae: 7.283 - mean_q: 12.560 - prob: 1.000\n",
            "\n",
            "Interval 37407 (374060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.002 - mae: 7.504 - mean_q: 12.756 - prob: 1.000\n",
            "\n",
            "Interval 37408 (374070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.423 - mean_q: 12.821 - prob: 1.000\n",
            "\n",
            "Interval 37409 (374080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 37410 (374090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.003 - mae: 7.483 - mean_q: 12.763 - prob: 1.000\n",
            "\n",
            "Interval 37411 (374100 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37412 (374110 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.003 - mae: 7.533 - mean_q: 12.983 - prob: 1.000\n",
            "\n",
            "Interval 37413 (374120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.003 - mae: 7.459 - mean_q: 12.944 - prob: 1.000\n",
            "\n",
            "Interval 37414 (374130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.003 - mae: 7.481 - mean_q: 12.883 - prob: 1.000\n",
            "\n",
            "Interval 37415 (374140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37416 (374150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.327 - mean_q: 12.601 - prob: 1.000\n",
            "\n",
            "Interval 37417 (374160 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.002 - mae: 7.278 - mean_q: 12.670 - prob: 1.000\n",
            "\n",
            "Interval 37418 (374170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.135 - mean_q: 12.298 - prob: 1.000\n",
            "\n",
            "Interval 37419 (374180 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37420 (374190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.398 - mean_q: 12.775 - prob: 1.000\n",
            "\n",
            "Interval 37421 (374200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.477 - mean_q: 12.951 - prob: 1.000\n",
            "\n",
            "Interval 37422 (374210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37423 (374220 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.001 - mae: 7.606 - mean_q: 13.100 - prob: 1.000\n",
            "\n",
            "Interval 37424 (374230 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 37425 (374240 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.490 - mean_q: 12.762 - prob: 1.000\n",
            "\n",
            "Interval 37426 (374250 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.071 - mean_q: 12.273 - prob: 1.000\n",
            "\n",
            "Interval 37427 (374260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 37428 (374270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.003 - mae: 7.560 - mean_q: 12.949 - prob: 1.000\n",
            "\n",
            "Interval 37429 (374280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.005 - mae: 7.558 - mean_q: 13.088 - prob: 1.000\n",
            "\n",
            "Interval 37430 (374290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.613 - mean_q: 13.019 - prob: 1.000\n",
            "\n",
            "Interval 37431 (374300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.002 - mae: 7.580 - mean_q: 12.947 - prob: 1.000\n",
            "\n",
            "Interval 37432 (374310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.215 - mean_q: 12.470 - prob: 1.000\n",
            "\n",
            "Interval 37433 (374320 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37434 (374330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.002 - mae: 7.262 - mean_q: 12.443 - prob: 1.000\n",
            "\n",
            "Interval 37435 (374340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.006 - mean_q: 12.251 - prob: 1.000\n",
            "\n",
            "Interval 37436 (374350 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37437 (374360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.003 - mae: 7.525 - mean_q: 12.860 - prob: 1.000\n",
            "\n",
            "Interval 37438 (374370 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.139 - mean_q: 12.394 - prob: 1.000\n",
            "\n",
            "Interval 37439 (374380 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.002 - mae: 7.550 - mean_q: 13.009 - prob: 1.000\n",
            "\n",
            "Interval 37440 (374390 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37441 (374400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.290 - mean_q: 12.602 - prob: 1.000\n",
            "\n",
            "Interval 37442 (374410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37443 (374420 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.264 - mean_q: 12.465 - prob: 1.000\n",
            "\n",
            "Interval 37444 (374430 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.002 - mae: 7.367 - mean_q: 12.559 - prob: 1.000\n",
            "\n",
            "Interval 37445 (374440 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37446 (374450 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.420 - mean_q: 12.757 - prob: 1.000\n",
            "\n",
            "Interval 37447 (374460 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37448 (374470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.158 - mean_q: 12.349 - prob: 1.000\n",
            "\n",
            "Interval 37449 (374480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.474 - mean_q: 12.796 - prob: 1.000\n",
            "\n",
            "Interval 37450 (374490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.002 - mae: 6.994 - mean_q: 12.144 - prob: 1.000\n",
            "\n",
            "Interval 37451 (374500 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37452 (374510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.387 - mean_q: 12.769 - prob: 1.000\n",
            "\n",
            "Interval 37453 (374520 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.539 - mean_q: 12.953 - prob: 1.000\n",
            "\n",
            "Interval 37454 (374530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37455 (374540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.087 - mean_q: 12.303 - prob: 1.000\n",
            "\n",
            "Interval 37456 (374550 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 37457 (374560 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.002 - mae: 7.560 - mean_q: 12.943 - prob: 1.000\n",
            "\n",
            "Interval 37458 (374570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37459 (374580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.270 - mean_q: 12.531 - prob: 1.000\n",
            "\n",
            "Interval 37460 (374590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.232 - mean_q: 12.542 - prob: 1.000\n",
            "\n",
            "Interval 37461 (374600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.545 - mean_q: 12.991 - prob: 1.000\n",
            "\n",
            "Interval 37462 (374610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 37463 (374620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.002 - mae: 7.252 - mean_q: 12.515 - prob: 1.000\n",
            "\n",
            "Interval 37464 (374630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.002 - mae: 7.205 - mean_q: 12.493 - prob: 1.000\n",
            "\n",
            "Interval 37465 (374640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.715 - mean_q: 13.036 - prob: 1.000\n",
            "\n",
            "Interval 37466 (374650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37467 (374660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.210 - mean_q: 12.530 - prob: 1.000\n",
            "\n",
            "Interval 37468 (374670 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 37469 (374680 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -17.000 [-17.000, -17.000] - loss: 0.002 - mae: 7.275 - mean_q: 12.545 - prob: 1.000\n",
            "\n",
            "Interval 37470 (374690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37471 (374700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.176 - mean_q: 12.498 - prob: 1.000\n",
            "\n",
            "Interval 37472 (374710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.002 - mae: 7.049 - mean_q: 12.248 - prob: 1.000\n",
            "\n",
            "Interval 37473 (374720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37474 (374730 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.002 - mae: 7.557 - mean_q: 13.037 - prob: 1.000\n",
            "\n",
            "Interval 37475 (374740 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.773 - mean_q: 13.248 - prob: 1.000\n",
            "\n",
            "Interval 37476 (374750 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.399 - mean_q: 12.842 - prob: 1.000\n",
            "\n",
            "Interval 37477 (374760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37478 (374770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.002 - mae: 7.580 - mean_q: 12.902 - prob: 1.000\n",
            "\n",
            "Interval 37479 (374780 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37480 (374790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.455 - mean_q: 12.881 - prob: 1.000\n",
            "\n",
            "Interval 37481 (374800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.512 - mean_q: 12.687 - prob: 1.000\n",
            "\n",
            "Interval 37482 (374810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37483 (374820 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.418 - mean_q: 12.802 - prob: 1.000\n",
            "\n",
            "Interval 37484 (374830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.002 - mae: 7.498 - mean_q: 12.961 - prob: 1.000\n",
            "\n",
            "Interval 37485 (374840 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.776 - mean_q: 13.218 - prob: 1.000\n",
            "\n",
            "Interval 37486 (374850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.125 - mean_q: 12.384 - prob: 1.000\n",
            "\n",
            "Interval 37487 (374860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 37488 (374870 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.523 - mean_q: 12.966 - prob: 1.000\n",
            "\n",
            "Interval 37489 (374880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.465 - mean_q: 12.864 - prob: 1.000\n",
            "\n",
            "Interval 37490 (374890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37491 (374900 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.163 - mean_q: 12.584 - prob: 1.000\n",
            "\n",
            "Interval 37492 (374910 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 12.000 [11.000, 13.000] - loss: 0.002 - mae: 7.430 - mean_q: 12.772 - prob: 1.000\n",
            "\n",
            "Interval 37493 (374920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.002 - mae: 7.435 - mean_q: 12.815 - prob: 1.000\n",
            "\n",
            "Interval 37494 (374930 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37495 (374940 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37496 (374950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.003 - mae: 7.058 - mean_q: 12.375 - prob: 1.000\n",
            "\n",
            "Interval 37497 (374960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.002 - mae: 7.683 - mean_q: 13.161 - prob: 1.000\n",
            "\n",
            "Interval 37498 (374970 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 37499 (374980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.002 - mae: 7.477 - mean_q: 12.761 - prob: 1.000\n",
            "\n",
            "Interval 37500 (374990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.002 - mae: 7.273 - mean_q: 12.587 - prob: 1.000\n",
            "\n",
            "Interval 37501 (375000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 37502 (375010 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 37503 (375020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37504 (375030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -42.000 [-42.000, -42.000] - loss: 0.002 - mae: 7.548 - mean_q: 13.016 - prob: 1.000\n",
            "\n",
            "Interval 37505 (375040 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37506 (375050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.002 - mae: 7.512 - mean_q: 12.799 - prob: 1.000\n",
            "\n",
            "Interval 37507 (375060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.002 - mae: 7.211 - mean_q: 12.439 - prob: 1.000\n",
            "\n",
            "Interval 37508 (375070 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37509 (375080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.106 - mean_q: 12.459 - prob: 1.000\n",
            "\n",
            "Interval 37510 (375090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.275 - mean_q: 12.558 - prob: 1.000\n",
            "\n",
            "Interval 37511 (375100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.003 - mae: 7.647 - mean_q: 13.085 - prob: 1.000\n",
            "\n",
            "Interval 37512 (375110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.004 - mae: 7.270 - mean_q: 12.487 - prob: 1.000\n",
            "\n",
            "Interval 37513 (375120 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 37514 (375130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.188 - mean_q: 12.528 - prob: 1.000\n",
            "\n",
            "Interval 37515 (375140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.263 - mean_q: 12.666 - prob: 1.000\n",
            "\n",
            "Interval 37516 (375150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.002 - mae: 7.119 - mean_q: 12.439 - prob: 1.000\n",
            "\n",
            "Interval 37517 (375160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 37518 (375170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.003 - mae: 7.014 - mean_q: 12.162 - prob: 1.000\n",
            "\n",
            "Interval 37519 (375180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37520 (375190 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 9.500 [6.000, 13.000] - loss: 0.002 - mae: 7.573 - mean_q: 12.966 - prob: 1.000\n",
            "\n",
            "Interval 37521 (375200 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 37522 (375210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.003 - mae: 7.151 - mean_q: 12.360 - prob: 1.000\n",
            "\n",
            "Interval 37523 (375220 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.003 - mae: 7.451 - mean_q: 12.858 - prob: 1.000\n",
            "\n",
            "Interval 37524 (375230 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.003 - mae: 7.613 - mean_q: 13.112 - prob: 1.000\n",
            "\n",
            "Interval 37525 (375240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37526 (375250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.003 - mae: 7.409 - mean_q: 12.541 - prob: 1.000\n",
            "\n",
            "Interval 37527 (375260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.003 - mae: 7.583 - mean_q: 13.073 - prob: 1.000\n",
            "\n",
            "Interval 37528 (375270 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 37529 (375280 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.002 - mae: 7.433 - mean_q: 12.843 - prob: 1.000\n",
            "\n",
            "Interval 37530 (375290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.298 - mean_q: 12.595 - prob: 1.000\n",
            "\n",
            "Interval 37531 (375300 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37532 (375310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 9.000 [4.000, 14.000] - loss: 0.003 - mae: 7.120 - mean_q: 12.193 - prob: 1.000\n",
            "\n",
            "Interval 37533 (375320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37534 (375330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.010 - mean_q: 12.098 - prob: 1.000\n",
            "\n",
            "Interval 37535 (375340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.003 - mae: 7.483 - mean_q: 12.910 - prob: 1.000\n",
            "\n",
            "Interval 37536 (375350 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37537 (375360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.002 - mae: 7.302 - mean_q: 12.635 - prob: 1.000\n",
            "\n",
            "Interval 37538 (375370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.002 - mae: 7.381 - mean_q: 12.803 - prob: 1.000\n",
            "\n",
            "Interval 37539 (375380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.333 - mean_q: 12.542 - prob: 1.000\n",
            "\n",
            "Interval 37540 (375390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 37541 (375400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.002 - mae: 7.296 - mean_q: 12.472 - prob: 1.000\n",
            "\n",
            "Interval 37542 (375410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.474 - mean_q: 12.853 - prob: 1.000\n",
            "\n",
            "Interval 37543 (375420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37544 (375430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.464 - mean_q: 12.879 - prob: 1.000\n",
            "\n",
            "Interval 37545 (375440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.295 - mean_q: 12.589 - prob: 1.000\n",
            "\n",
            "Interval 37546 (375450 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.003 - mae: 7.463 - mean_q: 12.835 - prob: 1.000\n",
            "\n",
            "Interval 37547 (375460 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -3.7000\n",
            "Interval 37548 (375470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -23.000 [-23.000, -23.000] - loss: 0.001 - mae: 7.166 - mean_q: 12.403 - prob: 1.000\n",
            "\n",
            "Interval 37549 (375480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.002 - mae: 7.637 - mean_q: 12.905 - prob: 1.000\n",
            "\n",
            "Interval 37550 (375490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37551 (375500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.002 - mae: 7.197 - mean_q: 12.513 - prob: 1.000\n",
            "\n",
            "Interval 37552 (375510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 37553 (375520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.002 - mae: 7.337 - mean_q: 12.727 - prob: 1.000\n",
            "\n",
            "Interval 37554 (375530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.322 - mean_q: 12.527 - prob: 1.000\n",
            "\n",
            "Interval 37555 (375540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.447 - mean_q: 12.658 - prob: 1.000\n",
            "\n",
            "Interval 37556 (375550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.578 - mean_q: 12.864 - prob: 1.000\n",
            "\n",
            "Interval 37557 (375560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37558 (375570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.715 - mean_q: 13.162 - prob: 1.000\n",
            "\n",
            "Interval 37559 (375580 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.645 - mean_q: 13.139 - prob: 1.000\n",
            "\n",
            "Interval 37560 (375590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.002 - mae: 7.555 - mean_q: 12.897 - prob: 1.000\n",
            "\n",
            "Interval 37561 (375600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 37562 (375610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.002 - mae: 7.411 - mean_q: 12.843 - prob: 1.000\n",
            "\n",
            "Interval 37563 (375620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.228 - mean_q: 12.375 - prob: 1.000\n",
            "\n",
            "Interval 37564 (375630 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37565 (375640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.449 - mean_q: 12.802 - prob: 1.000\n",
            "\n",
            "Interval 37566 (375650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.216 - mean_q: 12.517 - prob: 1.000\n",
            "\n",
            "Interval 37567 (375660 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.003 - mae: 7.364 - mean_q: 12.729 - prob: 1.000\n",
            "\n",
            "Interval 37568 (375670 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37569 (375680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.374 - mean_q: 12.627 - prob: 1.000\n",
            "\n",
            "Interval 37570 (375690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.169 - mean_q: 12.339 - prob: 1.000\n",
            "\n",
            "Interval 37571 (375700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37572 (375710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.002 - mae: 7.715 - mean_q: 13.252 - prob: 1.000\n",
            "\n",
            "Interval 37573 (375720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.319 - mean_q: 12.664 - prob: 1.000\n",
            "\n",
            "Interval 37574 (375730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.002 - mae: 7.167 - mean_q: 12.376 - prob: 1.000\n",
            "\n",
            "Interval 37575 (375740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37576 (375750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.485 - mean_q: 13.024 - prob: 1.000\n",
            "\n",
            "Interval 37577 (375760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.177 - mean_q: 12.402 - prob: 1.000\n",
            "\n",
            "Interval 37578 (375770 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 37579 (375780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 6.986 - mean_q: 12.317 - prob: 1.000\n",
            "\n",
            "Interval 37580 (375790 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 37581 (375800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.002 - mae: 7.503 - mean_q: 12.956 - prob: 1.000\n",
            "\n",
            "Interval 37582 (375810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 37583 (375820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.400 - mean_q: 12.893 - prob: 1.000\n",
            "\n",
            "Interval 37584 (375830 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.707 - mean_q: 13.263 - prob: 1.000\n",
            "\n",
            "Interval 37585 (375840 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 37586 (375850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -18.000 [-18.000, -18.000] - loss: 0.001 - mae: 7.188 - mean_q: 12.566 - prob: 1.000\n",
            "\n",
            "Interval 37587 (375860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.465 - mean_q: 12.907 - prob: 1.000\n",
            "\n",
            "Interval 37588 (375870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37589 (375880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.398 - mean_q: 12.718 - prob: 1.000\n",
            "\n",
            "Interval 37590 (375890 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.599 - mean_q: 12.952 - prob: 1.000\n",
            "\n",
            "Interval 37591 (375900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 37592 (375910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.002 - mae: 7.377 - mean_q: 12.711 - prob: 1.000\n",
            "\n",
            "Interval 37593 (375920 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 7.544 - mean_q: 12.955 - prob: 1.000\n",
            "\n",
            "Interval 37594 (375930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37595 (375940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.003 - mae: 7.556 - mean_q: 13.097 - prob: 1.000\n",
            "\n",
            "Interval 37596 (375950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.004 - mae: 7.284 - mean_q: 12.655 - prob: 1.000\n",
            "\n",
            "Interval 37597 (375960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.004 - mae: 7.114 - mean_q: 12.477 - prob: 1.000\n",
            "\n",
            "Interval 37598 (375970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37599 (375980 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.002 - mae: 7.328 - mean_q: 12.598 - prob: 1.000\n",
            "\n",
            "Interval 37600 (375990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37601 (376000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.530 - mean_q: 12.860 - prob: 1.000\n",
            "\n",
            "Interval 37602 (376010 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.002 - mae: 7.185 - mean_q: 12.432 - prob: 1.000\n",
            "\n",
            "Interval 37603 (376020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.002 - mae: 7.316 - mean_q: 12.679 - prob: 1.000\n",
            "\n",
            "Interval 37604 (376030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37605 (376040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.003 - mae: 7.452 - mean_q: 12.817 - prob: 1.000\n",
            "\n",
            "Interval 37606 (376050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.004 - mae: 7.459 - mean_q: 12.797 - prob: 1.000\n",
            "\n",
            "Interval 37607 (376060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.003 - mae: 7.125 - mean_q: 12.305 - prob: 1.000\n",
            "\n",
            "Interval 37608 (376070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.004 - mae: 7.502 - mean_q: 12.800 - prob: 1.000\n",
            "\n",
            "Interval 37609 (376080 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.266 - mean_q: 12.544 - prob: 1.000\n",
            "\n",
            "Interval 37610 (376090 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37611 (376100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.556 - mean_q: 12.849 - prob: 1.000\n",
            "\n",
            "Interval 37612 (376110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.003 - mae: 7.118 - mean_q: 12.336 - prob: 1.000\n",
            "\n",
            "Interval 37613 (376120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.383 - mean_q: 12.726 - prob: 1.000\n",
            "\n",
            "Interval 37614 (376130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37615 (376140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.002 - mae: 7.370 - mean_q: 12.702 - prob: 1.000\n",
            "\n",
            "Interval 37616 (376150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.002 - mae: 7.312 - mean_q: 12.635 - prob: 1.000\n",
            "\n",
            "Interval 37617 (376160 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 37618 (376170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.002 - mae: 7.435 - mean_q: 12.825 - prob: 1.000\n",
            "\n",
            "Interval 37619 (376180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.002 - mae: 7.376 - mean_q: 12.584 - prob: 1.000\n",
            "\n",
            "Interval 37620 (376190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37621 (376200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.330 - mean_q: 12.601 - prob: 1.000\n",
            "\n",
            "Interval 37622 (376210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.387 - mean_q: 12.696 - prob: 1.000\n",
            "\n",
            "Interval 37623 (376220 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.002 - mae: 7.227 - mean_q: 12.397 - prob: 1.000\n",
            "\n",
            "Interval 37624 (376230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.087 - mean_q: 12.178 - prob: 1.000\n",
            "\n",
            "Interval 37625 (376240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37626 (376250 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.424 - mean_q: 12.698 - prob: 1.000\n",
            "\n",
            "Interval 37627 (376260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 37628 (376270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.002 - mae: 7.225 - mean_q: 12.528 - prob: 1.000\n",
            "\n",
            "Interval 37629 (376280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.002 - mae: 7.391 - mean_q: 12.588 - prob: 1.000\n",
            "\n",
            "Interval 37630 (376290 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37631 (376300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.306 - mean_q: 12.563 - prob: 1.000\n",
            "\n",
            "Interval 37632 (376310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.621 - mean_q: 13.192 - prob: 1.000\n",
            "\n",
            "Interval 37633 (376320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37634 (376330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.487 - mean_q: 12.709 - prob: 1.000\n",
            "\n",
            "Interval 37635 (376340 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.336 - mean_q: 12.843 - prob: 1.000\n",
            "\n",
            "Interval 37636 (376350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 37637 (376360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.002 - mae: 7.208 - mean_q: 12.356 - prob: 1.000\n",
            "\n",
            "Interval 37638 (376370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.402 - mean_q: 12.876 - prob: 1.000\n",
            "\n",
            "Interval 37639 (376380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37640 (376390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.003 - mae: 7.344 - mean_q: 12.732 - prob: 1.000\n",
            "\n",
            "Interval 37641 (376400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.002 - mae: 7.421 - mean_q: 12.703 - prob: 1.000\n",
            "\n",
            "Interval 37642 (376410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 37643 (376420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.002 - mae: 7.171 - mean_q: 12.414 - prob: 1.000\n",
            "\n",
            "Interval 37644 (376430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37645 (376440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.308 - mean_q: 12.575 - prob: 1.000\n",
            "\n",
            "Interval 37646 (376450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.290 - mean_q: 12.566 - prob: 1.000\n",
            "\n",
            "Interval 37647 (376460 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 37648 (376470 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.002 - mae: 7.347 - mean_q: 12.538 - prob: 1.000\n",
            "\n",
            "Interval 37649 (376480 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 37650 (376490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.002 - mae: 7.437 - mean_q: 12.890 - prob: 1.000\n",
            "\n",
            "Interval 37651 (376500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.002 - mae: 7.115 - mean_q: 12.203 - prob: 1.000\n",
            "\n",
            "Interval 37652 (376510 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.331 - mean_q: 12.658 - prob: 1.000\n",
            "\n",
            "Interval 37653 (376520 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.005 - mae: 7.330 - mean_q: 12.520 - prob: 1.000\n",
            "\n",
            "Interval 37654 (376530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37655 (376540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.003 - mae: 7.068 - mean_q: 12.340 - prob: 1.000\n",
            "\n",
            "Interval 37656 (376550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37657 (376560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.536 - mean_q: 13.043 - prob: 1.000\n",
            "\n",
            "Interval 37658 (376570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.002 - mae: 7.101 - mean_q: 12.402 - prob: 1.000\n",
            "\n",
            "Interval 37659 (376580 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37660 (376590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.277 - mean_q: 12.547 - prob: 1.000\n",
            "\n",
            "Interval 37661 (376600 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.002 - mae: 7.347 - mean_q: 12.546 - prob: 1.000\n",
            "\n",
            "Interval 37662 (376610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 37663 (376620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.002 - mae: 7.270 - mean_q: 12.599 - prob: 1.000\n",
            "\n",
            "Interval 37664 (376630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.259 - mean_q: 12.649 - prob: 1.000\n",
            "\n",
            "Interval 37665 (376640 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.003 - mae: 7.313 - mean_q: 12.621 - prob: 1.000\n",
            "\n",
            "Interval 37666 (376650 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37667 (376660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37668 (376670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.002 - mae: 6.997 - mean_q: 12.091 - prob: 1.000\n",
            "\n",
            "Interval 37669 (376680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.202 - mean_q: 12.449 - prob: 1.000\n",
            "\n",
            "Interval 37670 (376690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.002 - mae: 7.205 - mean_q: 12.347 - prob: 1.000\n",
            "\n",
            "Interval 37671 (376700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.002 - mae: 7.587 - mean_q: 12.917 - prob: 1.000\n",
            "\n",
            "Interval 37672 (376710 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37673 (376720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.003 - mae: 7.473 - mean_q: 12.904 - prob: 1.000\n",
            "\n",
            "Interval 37674 (376730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.157 - mean_q: 12.154 - prob: 1.000\n",
            "\n",
            "Interval 37675 (376740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.002 - mae: 7.441 - mean_q: 12.616 - prob: 1.000\n",
            "\n",
            "Interval 37676 (376750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 37677 (376760 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.004 - mae: 7.348 - mean_q: 12.609 - prob: 1.000\n",
            "\n",
            "Interval 37678 (376770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.007 - mae: 7.542 - mean_q: 12.968 - prob: 1.000\n",
            "\n",
            "Interval 37679 (376780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37680 (376790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.004 - mae: 7.420 - mean_q: 12.666 - prob: 1.000\n",
            "\n",
            "Interval 37681 (376800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 37682 (376810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.003 - mae: 7.574 - mean_q: 12.956 - prob: 1.000\n",
            "\n",
            "Interval 37683 (376820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.002 - mae: 7.428 - mean_q: 12.860 - prob: 1.000\n",
            "\n",
            "Interval 37684 (376830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37685 (376840 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.003 - mae: 7.381 - mean_q: 12.725 - prob: 1.000\n",
            "\n",
            "Interval 37686 (376850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 37687 (376860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.003 - mae: 7.413 - mean_q: 12.889 - prob: 1.000\n",
            "\n",
            "Interval 37688 (376870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.071 - mean_q: 12.368 - prob: 1.000\n",
            "\n",
            "Interval 37689 (376880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37690 (376890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.699 - mean_q: 13.207 - prob: 1.000\n",
            "\n",
            "Interval 37691 (376900 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.789 - mean_q: 13.106 - prob: 1.000\n",
            "\n",
            "Interval 37692 (376910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 37693 (376920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 2.3000\n",
            "2 episodes - episode_reward: -0.500 [-4.000, 3.000] - loss: 0.001 - mae: 7.128 - mean_q: 12.311 - prob: 1.000\n",
            "\n",
            "Interval 37694 (376930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 37695 (376940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -23.000 [-23.000, -23.000] - loss: 0.001 - mae: 7.407 - mean_q: 12.816 - prob: 1.000\n",
            "\n",
            "Interval 37696 (376950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.704 - mean_q: 13.143 - prob: 1.000\n",
            "\n",
            "Interval 37697 (376960 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37698 (376970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.507 - mean_q: 12.910 - prob: 1.000\n",
            "\n",
            "Interval 37699 (376980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.318 - mean_q: 12.710 - prob: 1.000\n",
            "\n",
            "Interval 37700 (376990 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 37701 (377000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.363 - mean_q: 12.824 - prob: 1.000\n",
            "\n",
            "Interval 37702 (377010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.304 - mean_q: 12.467 - prob: 1.000\n",
            "\n",
            "Interval 37703 (377020 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37704 (377030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.231 - mean_q: 12.505 - prob: 1.000\n",
            "\n",
            "Interval 37705 (377040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.430 - mean_q: 12.759 - prob: 1.000\n",
            "\n",
            "Interval 37706 (377050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.002 - mae: 7.399 - mean_q: 12.705 - prob: 1.000\n",
            "\n",
            "Interval 37707 (377060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37708 (377070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.619 - mean_q: 13.000 - prob: 1.000\n",
            "\n",
            "Interval 37709 (377080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37710 (377090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.533 - mean_q: 12.870 - prob: 1.000\n",
            "\n",
            "Interval 37711 (377100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.429 - mean_q: 12.812 - prob: 1.000\n",
            "\n",
            "Interval 37712 (377110 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37713 (377120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.402 - mean_q: 12.731 - prob: 1.000\n",
            "\n",
            "Interval 37714 (377130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.593 - mean_q: 12.921 - prob: 1.000\n",
            "\n",
            "Interval 37715 (377140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.152 - mean_q: 12.384 - prob: 1.000\n",
            "\n",
            "Interval 37716 (377150 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 37717 (377160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.001 - mae: 7.057 - mean_q: 12.298 - prob: 1.000\n",
            "\n",
            "Interval 37718 (377170 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37719 (377180 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37720 (377190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.002 - mae: 7.466 - mean_q: 12.811 - prob: 1.000\n",
            "\n",
            "Interval 37721 (377200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.002 - mae: 7.665 - mean_q: 13.109 - prob: 1.000\n",
            "\n",
            "Interval 37722 (377210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.003 - mae: 7.348 - mean_q: 12.630 - prob: 1.000\n",
            "\n",
            "Interval 37723 (377220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37724 (377230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.002 - mae: 7.933 - mean_q: 13.662 - prob: 1.000\n",
            "\n",
            "Interval 37725 (377240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.343 - mean_q: 12.700 - prob: 1.000\n",
            "\n",
            "Interval 37726 (377250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.236 - mean_q: 12.539 - prob: 1.000\n",
            "\n",
            "Interval 37727 (377260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.002 - mae: 6.998 - mean_q: 12.244 - prob: 1.000\n",
            "\n",
            "Interval 37728 (377270 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.473 - mean_q: 12.742 - prob: 1.000\n",
            "\n",
            "Interval 37729 (377280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.470 - mean_q: 12.810 - prob: 1.000\n",
            "\n",
            "Interval 37730 (377290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.621 - mean_q: 13.058 - prob: 1.000\n",
            "\n",
            "Interval 37731 (377300 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37732 (377310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.190 - mean_q: 12.522 - prob: 1.000\n",
            "\n",
            "Interval 37733 (377320 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 37734 (377330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -20.000 [-20.000, -20.000] - loss: 0.001 - mae: 7.589 - mean_q: 13.020 - prob: 1.000\n",
            "\n",
            "Interval 37735 (377340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.277 - mean_q: 12.558 - prob: 1.000\n",
            "\n",
            "Interval 37736 (377350 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 37737 (377360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.532 - mean_q: 12.904 - prob: 1.000\n",
            "\n",
            "Interval 37738 (377370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37739 (377380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.463 - mean_q: 12.761 - prob: 1.000\n",
            "\n",
            "Interval 37740 (377390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37741 (377400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.001 - mae: 7.415 - mean_q: 12.836 - prob: 1.000\n",
            "\n",
            "Interval 37742 (377410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.427 - mean_q: 12.813 - prob: 1.000\n",
            "\n",
            "Interval 37743 (377420 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 37744 (377430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.130 - mean_q: 12.580 - prob: 1.000\n",
            "\n",
            "Interval 37745 (377440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.002 - mae: 7.405 - mean_q: 12.665 - prob: 1.000\n",
            "\n",
            "Interval 37746 (377450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37747 (377460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.491 - mean_q: 12.788 - prob: 1.000\n",
            "\n",
            "Interval 37748 (377470 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37749 (377480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.084 - mean_q: 12.287 - prob: 1.000\n",
            "\n",
            "Interval 37750 (377490 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.233 - mean_q: 12.434 - prob: 1.000\n",
            "\n",
            "Interval 37751 (377500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.461 - mean_q: 12.895 - prob: 1.000\n",
            "\n",
            "Interval 37752 (377510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37753 (377520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.002 - mae: 7.184 - mean_q: 12.482 - prob: 1.000\n",
            "\n",
            "Interval 37754 (377530 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37755 (377540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.136 - mean_q: 12.407 - prob: 1.000\n",
            "\n",
            "Interval 37756 (377550 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 37757 (377560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.002 - mae: 7.465 - mean_q: 12.768 - prob: 1.000\n",
            "\n",
            "Interval 37758 (377570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.002 - mae: 7.625 - mean_q: 12.894 - prob: 1.000\n",
            "\n",
            "Interval 37759 (377580 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37760 (377590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.241 - mean_q: 12.601 - prob: 1.000\n",
            "\n",
            "Interval 37761 (377600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37762 (377610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.002 - mae: 7.435 - mean_q: 12.790 - prob: 1.000\n",
            "\n",
            "Interval 37763 (377620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.714 - mean_q: 13.164 - prob: 1.000\n",
            "\n",
            "Interval 37764 (377630 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 37765 (377640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.001 - mae: 7.543 - mean_q: 12.942 - prob: 1.000\n",
            "\n",
            "Interval 37766 (377650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37767 (377660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.002 - mae: 7.245 - mean_q: 12.504 - prob: 1.000\n",
            "\n",
            "Interval 37768 (377670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.372 - mean_q: 12.765 - prob: 1.000\n",
            "\n",
            "Interval 37769 (377680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37770 (377690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 6.991 - mean_q: 12.243 - prob: 1.000\n",
            "\n",
            "Interval 37771 (377700 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37772 (377710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -18.000 [-18.000, -18.000] - loss: 0.002 - mae: 7.287 - mean_q: 12.542 - prob: 1.000\n",
            "\n",
            "Interval 37773 (377720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37774 (377730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.002 - mae: 7.269 - mean_q: 12.577 - prob: 1.000\n",
            "\n",
            "Interval 37775 (377740 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37776 (377750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.409 - mean_q: 12.708 - prob: 1.000\n",
            "\n",
            "Interval 37777 (377760 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.002 - mae: 7.331 - mean_q: 12.750 - prob: 1.000\n",
            "\n",
            "Interval 37778 (377770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37779 (377780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.210 - mean_q: 12.443 - prob: 1.000\n",
            "\n",
            "Interval 37780 (377790 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37781 (377800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.611 - mean_q: 13.013 - prob: 1.000\n",
            "\n",
            "Interval 37782 (377810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.024 - mean_q: 12.186 - prob: 1.000\n",
            "\n",
            "Interval 37783 (377820 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.229 - mean_q: 12.533 - prob: 1.000\n",
            "\n",
            "Interval 37784 (377830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.441 - mean_q: 12.913 - prob: 1.000\n",
            "\n",
            "Interval 37785 (377840 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37786 (377850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.304 - mean_q: 12.668 - prob: 1.000\n",
            "\n",
            "Interval 37787 (377860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.454 - mean_q: 12.787 - prob: 1.000\n",
            "\n",
            "Interval 37788 (377870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.143 - mean_q: 12.339 - prob: 1.000\n",
            "\n",
            "Interval 37789 (377880 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37790 (377890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.353 - mean_q: 12.650 - prob: 1.000\n",
            "\n",
            "Interval 37791 (377900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.471 - mean_q: 12.781 - prob: 1.000\n",
            "\n",
            "Interval 37792 (377910 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.002 - mae: 7.279 - mean_q: 12.482 - prob: 1.000\n",
            "\n",
            "Interval 37793 (377920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.379 - mean_q: 12.800 - prob: 1.000\n",
            "\n",
            "Interval 37794 (377930 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 37795 (377940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.002 - mae: 7.427 - mean_q: 12.897 - prob: 1.000\n",
            "\n",
            "Interval 37796 (377950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.492 - mean_q: 12.831 - prob: 1.000\n",
            "\n",
            "Interval 37797 (377960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37798 (377970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 10.000 [7.000, 13.000] - loss: 0.002 - mae: 7.151 - mean_q: 12.360 - prob: 1.000\n",
            "\n",
            "Interval 37799 (377980 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37800 (377990 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.002 - mae: 7.320 - mean_q: 12.591 - prob: 1.000\n",
            "\n",
            "Interval 37801 (378000 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.472 - mean_q: 12.817 - prob: 1.000\n",
            "\n",
            "Interval 37802 (378010 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 37803 (378020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.002 - mae: 7.045 - mean_q: 12.250 - prob: 1.000\n",
            "\n",
            "Interval 37804 (378030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.438 - mean_q: 12.926 - prob: 1.000\n",
            "\n",
            "Interval 37805 (378040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.002 - mae: 7.513 - mean_q: 13.002 - prob: 1.000\n",
            "\n",
            "Interval 37806 (378050 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 37807 (378060 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -22.000 [-22.000, -22.000] - loss: 0.002 - mae: 7.244 - mean_q: 12.454 - prob: 1.000\n",
            "\n",
            "Interval 37808 (378070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.310 - mean_q: 12.709 - prob: 1.000\n",
            "\n",
            "Interval 37809 (378080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.003 - mae: 7.234 - mean_q: 12.312 - prob: 1.000\n",
            "\n",
            "Interval 37810 (378090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37811 (378100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.368 - mean_q: 12.658 - prob: 1.000\n",
            "\n",
            "Interval 37812 (378110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.003 - mae: 7.387 - mean_q: 12.802 - prob: 1.000\n",
            "\n",
            "Interval 37813 (378120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 37814 (378130 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.004 - mae: 7.300 - mean_q: 12.633 - prob: 1.000\n",
            "\n",
            "Interval 37815 (378140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.473 - mean_q: 12.981 - prob: 1.000\n",
            "\n",
            "Interval 37816 (378150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.002 - mae: 7.506 - mean_q: 12.821 - prob: 1.000\n",
            "\n",
            "Interval 37817 (378160 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 37818 (378170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 6.000 [0.000, 12.000] - loss: 0.002 - mae: 7.446 - mean_q: 12.827 - prob: 1.000\n",
            "\n",
            "Interval 37819 (378180 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37820 (378190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.711 - mean_q: 13.081 - prob: 1.000\n",
            "\n",
            "Interval 37821 (378200 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37822 (378210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.002 - mae: 7.480 - mean_q: 12.794 - prob: 1.000\n",
            "\n",
            "Interval 37823 (378220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.511 - mean_q: 12.788 - prob: 1.000\n",
            "\n",
            "Interval 37824 (378230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37825 (378240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.440 - mean_q: 12.826 - prob: 1.000\n",
            "\n",
            "Interval 37826 (378250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.491 - mean_q: 12.960 - prob: 1.000\n",
            "\n",
            "Interval 37827 (378260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37828 (378270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.521 - mean_q: 12.880 - prob: 1.000\n",
            "\n",
            "Interval 37829 (378280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 37830 (378290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.002 - mae: 7.608 - mean_q: 13.128 - prob: 1.000\n",
            "\n",
            "Interval 37831 (378300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.417 - mean_q: 12.529 - prob: 1.000\n",
            "\n",
            "Interval 37832 (378310 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37833 (378320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.002 - mae: 7.244 - mean_q: 12.565 - prob: 1.000\n",
            "\n",
            "Interval 37834 (378330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.002 - mae: 7.488 - mean_q: 12.984 - prob: 1.000\n",
            "\n",
            "Interval 37835 (378340 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37836 (378350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.340 - mean_q: 12.636 - prob: 1.000\n",
            "\n",
            "Interval 37837 (378360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 37838 (378370 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.002 - mae: 7.242 - mean_q: 12.410 - prob: 1.000\n",
            "\n",
            "Interval 37839 (378380 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 37840 (378390 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.003 - mae: 7.479 - mean_q: 12.775 - prob: 1.000\n",
            "\n",
            "Interval 37841 (378400 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.002 - mae: 7.033 - mean_q: 12.262 - prob: 1.000\n",
            "\n",
            "Interval 37842 (378410 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 37843 (378420 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.312 - mean_q: 12.755 - prob: 1.000\n",
            "\n",
            "Interval 37844 (378430 steps performed)\n",
            "10/10 [==============================] - 0s 14ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.003 - mae: 7.370 - mean_q: 12.776 - prob: 1.000\n",
            "\n",
            "Interval 37845 (378440 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.0000\n",
            "Interval 37846 (378450 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.358 - mean_q: 12.728 - prob: 1.000\n",
            "\n",
            "Interval 37847 (378460 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.662 - mean_q: 13.065 - prob: 1.000\n",
            "\n",
            "Interval 37848 (378470 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.003 - mae: 7.523 - mean_q: 12.754 - prob: 1.000\n",
            "\n",
            "Interval 37849 (378480 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.113 - mean_q: 12.308 - prob: 1.000\n",
            "\n",
            "Interval 37850 (378490 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.002 - mae: 7.472 - mean_q: 12.818 - prob: 1.000\n",
            "\n",
            "Interval 37851 (378500 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 37852 (378510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.002 - mae: 7.439 - mean_q: 12.753 - prob: 1.000\n",
            "\n",
            "Interval 37853 (378520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.003 - mae: 7.487 - mean_q: 12.771 - prob: 1.000\n",
            "\n",
            "Interval 37854 (378530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.002 - mae: 7.592 - mean_q: 13.104 - prob: 1.000\n",
            "\n",
            "Interval 37855 (378540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.528 - mean_q: 12.950 - prob: 1.000\n",
            "\n",
            "Interval 37856 (378550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.002 - mae: 7.256 - mean_q: 12.541 - prob: 1.000\n",
            "\n",
            "Interval 37857 (378560 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37858 (378570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -26.000 [-26.000, -26.000] - loss: 0.003 - mae: 7.167 - mean_q: 12.246 - prob: 1.000\n",
            "\n",
            "Interval 37859 (378580 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 37860 (378590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37861 (378600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.003 - mae: 7.333 - mean_q: 12.727 - prob: 1.000\n",
            "\n",
            "Interval 37862 (378610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.003 - mae: 7.439 - mean_q: 12.872 - prob: 1.000\n",
            "\n",
            "Interval 37863 (378620 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 37864 (378630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.003 - mae: 7.603 - mean_q: 12.930 - prob: 1.000\n",
            "\n",
            "Interval 37865 (378640 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37866 (378650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.003 - mae: 7.410 - mean_q: 12.789 - prob: 1.000\n",
            "\n",
            "Interval 37867 (378660 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.003 - mae: 7.357 - mean_q: 12.678 - prob: 1.000\n",
            "\n",
            "Interval 37868 (378670 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.003 - mae: 7.254 - mean_q: 12.736 - prob: 1.000\n",
            "\n",
            "Interval 37869 (378680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 37870 (378690 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.002 - mae: 7.542 - mean_q: 12.911 - prob: 1.000\n",
            "\n",
            "Interval 37871 (378700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37872 (378710 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.002 - mae: 7.253 - mean_q: 12.593 - prob: 1.000\n",
            "\n",
            "Interval 37873 (378720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 37874 (378730 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.002 - mae: 7.773 - mean_q: 13.298 - prob: 1.000\n",
            "\n",
            "Interval 37875 (378740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37876 (378750 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.161 - mean_q: 12.372 - prob: 1.000\n",
            "\n",
            "Interval 37877 (378760 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.092 - mean_q: 12.296 - prob: 1.000\n",
            "\n",
            "Interval 37878 (378770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37879 (378780 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.003 - mae: 7.818 - mean_q: 13.423 - prob: 1.000\n",
            "\n",
            "Interval 37880 (378790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.002 - mae: 7.170 - mean_q: 12.350 - prob: 1.000\n",
            "\n",
            "Interval 37881 (378800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.002 - mae: 7.690 - mean_q: 12.986 - prob: 1.000\n",
            "\n",
            "Interval 37882 (378810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37883 (378820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 7.239 - mean_q: 12.632 - prob: 1.000\n",
            "\n",
            "Interval 37884 (378830 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 37885 (378840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -19.000 [-19.000, -19.000] - loss: 0.002 - mae: 7.364 - mean_q: 12.693 - prob: 1.000\n",
            "\n",
            "Interval 37886 (378850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37887 (378860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.589 - mean_q: 13.121 - prob: 1.000\n",
            "\n",
            "Interval 37888 (378870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.344 - mean_q: 12.778 - prob: 1.000\n",
            "\n",
            "Interval 37889 (378880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 37890 (378890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37891 (378900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.002 - mae: 7.022 - mean_q: 12.365 - prob: 1.000\n",
            "\n",
            "Interval 37892 (378910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.459 - mean_q: 12.882 - prob: 1.000\n",
            "\n",
            "Interval 37893 (378920 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.002 - mae: 7.460 - mean_q: 12.757 - prob: 1.000\n",
            "\n",
            "Interval 37894 (378930 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 37895 (378940 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.002 - mae: 7.106 - mean_q: 12.408 - prob: 1.000\n",
            "\n",
            "Interval 37896 (378950 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 37897 (378960 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 6.990 - mean_q: 12.223 - prob: 1.000\n",
            "\n",
            "Interval 37898 (378970 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.332 - mean_q: 12.512 - prob: 1.000\n",
            "\n",
            "Interval 37899 (378980 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.0000\n",
            "Interval 37900 (378990 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.086 - mean_q: 12.298 - prob: 1.000\n",
            "\n",
            "Interval 37901 (379000 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.369 - mean_q: 12.745 - prob: 1.000\n",
            "\n",
            "Interval 37902 (379010 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.412 - mean_q: 12.689 - prob: 1.000\n",
            "\n",
            "Interval 37903 (379020 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.447 - mean_q: 12.794 - prob: 1.000\n",
            "\n",
            "Interval 37904 (379030 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 37905 (379040 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.002 - mae: 7.478 - mean_q: 12.860 - prob: 1.000\n",
            "\n",
            "Interval 37906 (379050 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 37907 (379060 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.274 - mean_q: 12.576 - prob: 1.000\n",
            "\n",
            "Interval 37908 (379070 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -24.000 [-24.000, -24.000] - loss: 0.002 - mae: 7.166 - mean_q: 12.350 - prob: 1.000\n",
            "\n",
            "Interval 37909 (379080 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.0000\n",
            "Interval 37910 (379090 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.002 - mae: 7.220 - mean_q: 12.534 - prob: 1.000\n",
            "\n",
            "Interval 37911 (379100 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 37912 (379110 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 37913 (379120 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.002 - mae: 7.235 - mean_q: 12.504 - prob: 1.000\n",
            "\n",
            "Interval 37914 (379130 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.514 - mean_q: 12.847 - prob: 1.000\n",
            "\n",
            "Interval 37915 (379140 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 37916 (379150 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.428 - mean_q: 13.030 - prob: 1.000\n",
            "\n",
            "Interval 37917 (379160 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.001 - mae: 7.583 - mean_q: 13.024 - prob: 1.000\n",
            "\n",
            "Interval 37918 (379170 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.0000\n",
            "Interval 37919 (379180 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.377 - mean_q: 12.804 - prob: 1.000\n",
            "\n",
            "Interval 37920 (379190 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.171 - mean_q: 12.489 - prob: 1.000\n",
            "\n",
            "Interval 37921 (379200 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.351 - mean_q: 12.569 - prob: 1.000\n",
            "\n",
            "Interval 37922 (379210 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.455 - mean_q: 12.865 - prob: 1.000\n",
            "\n",
            "Interval 37923 (379220 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.385 - mean_q: 12.838 - prob: 1.000\n",
            "\n",
            "Interval 37924 (379230 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 37925 (379240 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.002 - mae: 7.391 - mean_q: 12.719 - prob: 1.000\n",
            "\n",
            "Interval 37926 (379250 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 37927 (379260 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.186 - mean_q: 12.431 - prob: 1.000\n",
            "\n",
            "Interval 37928 (379270 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.444 - mean_q: 12.700 - prob: 1.000\n",
            "\n",
            "Interval 37929 (379280 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.496 - mean_q: 13.034 - prob: 1.000\n",
            "\n",
            "Interval 37930 (379290 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.519 - mean_q: 12.941 - prob: 1.000\n",
            "\n",
            "Interval 37931 (379300 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.0000\n",
            "Interval 37932 (379310 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.309 - mean_q: 12.595 - prob: 1.000\n",
            "\n",
            "Interval 37933 (379320 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.003 - mae: 7.409 - mean_q: 12.736 - prob: 1.000\n",
            "\n",
            "Interval 37934 (379330 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.003 - mae: 7.073 - mean_q: 12.351 - prob: 1.000\n",
            "\n",
            "Interval 37935 (379340 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: -1.9000\n",
            "Interval 37936 (379350 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.003 - mae: 7.438 - mean_q: 12.761 - prob: 1.000\n",
            "\n",
            "Interval 37937 (379360 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 37938 (379370 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.002 - mae: 7.382 - mean_q: 12.616 - prob: 1.000\n",
            "\n",
            "Interval 37939 (379380 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.298 - mean_q: 12.596 - prob: 1.000\n",
            "\n",
            "Interval 37940 (379390 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 37941 (379400 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.002 - mae: 7.360 - mean_q: 12.742 - prob: 1.000\n",
            "\n",
            "Interval 37942 (379410 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 37943 (379420 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.002 - mae: 7.265 - mean_q: 12.535 - prob: 1.000\n",
            "\n",
            "Interval 37944 (379430 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.225 - mean_q: 12.429 - prob: 1.000\n",
            "\n",
            "Interval 37945 (379440 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.9000\n",
            "Interval 37946 (379450 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.002 - mae: 7.330 - mean_q: 12.712 - prob: 1.000\n",
            "\n",
            "Interval 37947 (379460 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -2.8000\n",
            "Interval 37948 (379470 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.002 - mae: 7.386 - mean_q: 12.751 - prob: 1.000\n",
            "\n",
            "Interval 37949 (379480 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 37950 (379490 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.002 - mae: 7.190 - mean_q: 12.345 - prob: 1.000\n",
            "\n",
            "Interval 37951 (379500 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.289 - mean_q: 12.628 - prob: 1.000\n",
            "\n",
            "Interval 37952 (379510 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 37953 (379520 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.002 - mae: 7.310 - mean_q: 12.660 - prob: 1.000\n",
            "\n",
            "Interval 37954 (379530 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.0000\n",
            "Interval 37955 (379540 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.315 - mean_q: 12.609 - prob: 1.000\n",
            "\n",
            "Interval 37956 (379550 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 7.194 - mean_q: 12.583 - prob: 1.000\n",
            "\n",
            "Interval 37957 (379560 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: -1.0000\n",
            "Interval 37958 (379570 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.004 - mae: 7.777 - mean_q: 13.322 - prob: 1.000\n",
            "\n",
            "Interval 37959 (379580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.004 - mae: 7.280 - mean_q: 12.584 - prob: 1.000\n",
            "\n",
            "Interval 37960 (379590 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37961 (379600 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.148 - mean_q: 12.298 - prob: 1.000\n",
            "\n",
            "Interval 37962 (379610 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.615 - mean_q: 12.929 - prob: 1.000\n",
            "\n",
            "Interval 37963 (379620 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37964 (379630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.001 - mae: 7.292 - mean_q: 12.583 - prob: 1.000\n",
            "\n",
            "Interval 37965 (379640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.002 - mae: 7.127 - mean_q: 12.235 - prob: 1.000\n",
            "\n",
            "Interval 37966 (379650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.304 - mean_q: 12.595 - prob: 1.000\n",
            "\n",
            "Interval 37967 (379660 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37968 (379670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.544 - mean_q: 12.861 - prob: 1.000\n",
            "\n",
            "Interval 37969 (379680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.418 - mean_q: 12.779 - prob: 1.000\n",
            "\n",
            "Interval 37970 (379690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.414 - mean_q: 12.800 - prob: 1.000\n",
            "\n",
            "Interval 37971 (379700 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.488 - mean_q: 12.775 - prob: 1.000\n",
            "\n",
            "Interval 37972 (379710 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 37973 (379720 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.003 - mae: 7.535 - mean_q: 12.955 - prob: 1.000\n",
            "\n",
            "Interval 37974 (379730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.551 - mean_q: 13.091 - prob: 1.000\n",
            "\n",
            "Interval 37975 (379740 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.003 - mae: 7.767 - mean_q: 13.192 - prob: 1.000\n",
            "\n",
            "Interval 37976 (379750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37977 (379760 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.128 - mean_q: 12.385 - prob: 1.000\n",
            "\n",
            "Interval 37978 (379770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.002 - mae: 7.289 - mean_q: 12.545 - prob: 1.000\n",
            "\n",
            "Interval 37979 (379780 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37980 (379790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.160 - mean_q: 12.392 - prob: 1.000\n",
            "\n",
            "Interval 37981 (379800 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 37982 (379810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.002 - mae: 7.591 - mean_q: 13.094 - prob: 1.000\n",
            "\n",
            "Interval 37983 (379820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.002 - mae: 7.353 - mean_q: 12.716 - prob: 1.000\n",
            "\n",
            "Interval 37984 (379830 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 37985 (379840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -17.000 [-17.000, -17.000] - loss: 0.003 - mae: 7.581 - mean_q: 13.158 - prob: 1.000\n",
            "\n",
            "Interval 37986 (379850 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 37987 (379860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.003 - mae: 7.491 - mean_q: 12.946 - prob: 1.000\n",
            "\n",
            "Interval 37988 (379870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.004 - mae: 7.236 - mean_q: 12.378 - prob: 1.000\n",
            "\n",
            "Interval 37989 (379880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.003 - mae: 7.384 - mean_q: 12.621 - prob: 1.000\n",
            "\n",
            "Interval 37990 (379890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37991 (379900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.002 - mae: 7.496 - mean_q: 12.889 - prob: 1.000\n",
            "\n",
            "Interval 37992 (379910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.206 - mean_q: 12.411 - prob: 1.000\n",
            "\n",
            "Interval 37993 (379920 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 37994 (379930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.003 - mae: 7.370 - mean_q: 12.705 - prob: 1.000\n",
            "\n",
            "Interval 37995 (379940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 37996 (379950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.447 - mean_q: 12.961 - prob: 1.000\n",
            "\n",
            "Interval 37997 (379960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.361 - mean_q: 12.714 - prob: 1.000\n",
            "\n",
            "Interval 37998 (379970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.352 - mean_q: 12.630 - prob: 1.000\n",
            "\n",
            "Interval 37999 (379980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.002 - mae: 7.310 - mean_q: 12.605 - prob: 1.000\n",
            "\n",
            "Interval 38000 (379990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.199 - mean_q: 12.385 - prob: 1.000\n",
            "\n",
            "Interval 38001 (380000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 38002 (380010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.002 - mae: 7.149 - mean_q: 12.328 - prob: 1.000\n",
            "\n",
            "Interval 38003 (380020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.406 - mean_q: 12.692 - prob: 1.000\n",
            "\n",
            "Interval 38004 (380030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38005 (380040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.002 - mae: 7.285 - mean_q: 12.640 - prob: 1.000\n",
            "\n",
            "Interval 38006 (380050 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.350 - mean_q: 12.695 - prob: 1.000\n",
            "\n",
            "Interval 38007 (380060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38008 (380070 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.435 - mean_q: 12.912 - prob: 1.000\n",
            "\n",
            "Interval 38009 (380080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38010 (380090 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.003 - mae: 7.136 - mean_q: 12.314 - prob: 1.000\n",
            "\n",
            "Interval 38011 (380100 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.352 - mean_q: 12.787 - prob: 1.000\n",
            "\n",
            "Interval 38012 (380110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.002 - mae: 7.276 - mean_q: 12.427 - prob: 1.000\n",
            "\n",
            "Interval 38013 (380120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38014 (380130 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.004 - mae: 7.643 - mean_q: 13.067 - prob: 1.000\n",
            "\n",
            "Interval 38015 (380140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.463 - mean_q: 12.975 - prob: 1.000\n",
            "\n",
            "Interval 38016 (380150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.589 - mean_q: 13.074 - prob: 1.000\n",
            "\n",
            "Interval 38017 (380160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38018 (380170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.423 - mean_q: 13.018 - prob: 1.000\n",
            "\n",
            "Interval 38019 (380180 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.551 - mean_q: 12.960 - prob: 1.000\n",
            "\n",
            "Interval 38020 (380190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38021 (380200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.003 - mae: 7.337 - mean_q: 12.644 - prob: 1.000\n",
            "\n",
            "Interval 38022 (380210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38023 (380220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.212 - mean_q: 12.450 - prob: 1.000\n",
            "\n",
            "Interval 38024 (380230 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.627 - mean_q: 13.020 - prob: 1.000\n",
            "\n",
            "Interval 38025 (380240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.423 - mean_q: 12.850 - prob: 1.000\n",
            "\n",
            "Interval 38026 (380250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38027 (380260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.213 - mean_q: 12.438 - prob: 1.000\n",
            "\n",
            "Interval 38028 (380270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.222 - mean_q: 12.418 - prob: 1.000\n",
            "\n",
            "Interval 38029 (380280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38030 (380290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.297 - mean_q: 12.592 - prob: 1.000\n",
            "\n",
            "Interval 38031 (380300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.002 - mae: 7.400 - mean_q: 12.839 - prob: 1.000\n",
            "\n",
            "Interval 38032 (380310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.003 - mae: 7.471 - mean_q: 12.906 - prob: 1.000\n",
            "\n",
            "Interval 38033 (380320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38034 (380330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 6.912 - mean_q: 11.928 - prob: 1.000\n",
            "\n",
            "Interval 38035 (380340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.532 - mean_q: 12.911 - prob: 1.000\n",
            "\n",
            "Interval 38036 (380350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.565 - mean_q: 13.078 - prob: 1.000\n",
            "\n",
            "Interval 38037 (380360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 38038 (380370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.001 - mae: 7.382 - mean_q: 12.608 - prob: 1.000\n",
            "\n",
            "Interval 38039 (380380 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.308 - mean_q: 12.737 - prob: 1.000\n",
            "\n",
            "Interval 38040 (380390 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.253 - mean_q: 12.516 - prob: 1.000\n",
            "\n",
            "Interval 38041 (380400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.497 - mean_q: 12.974 - prob: 1.000\n",
            "\n",
            "Interval 38042 (380410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.003 - mae: 7.357 - mean_q: 12.703 - prob: 1.000\n",
            "\n",
            "Interval 38043 (380420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.002 - mae: 7.399 - mean_q: 12.834 - prob: 1.000\n",
            "\n",
            "Interval 38044 (380430 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38045 (380440 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.492 - mean_q: 12.855 - prob: 1.000\n",
            "\n",
            "Interval 38046 (380450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.226 - mean_q: 12.579 - prob: 1.000\n",
            "\n",
            "Interval 38047 (380460 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 38048 (380470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.002 - mae: 7.535 - mean_q: 13.031 - prob: 1.000\n",
            "\n",
            "Interval 38049 (380480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 38050 (380490 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.002 - mae: 7.344 - mean_q: 12.581 - prob: 1.000\n",
            "\n",
            "Interval 38051 (380500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38052 (380510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.417 - mean_q: 12.686 - prob: 1.000\n",
            "\n",
            "Interval 38053 (380520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.259 - mean_q: 12.604 - prob: 1.000\n",
            "\n",
            "Interval 38054 (380530 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38055 (380540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.718 - mean_q: 13.088 - prob: 1.000\n",
            "\n",
            "Interval 38056 (380550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.503 - mean_q: 12.861 - prob: 1.000\n",
            "\n",
            "Interval 38057 (380560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.237 - mean_q: 12.516 - prob: 1.000\n",
            "\n",
            "Interval 38058 (380570 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 38059 (380580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.002 - mae: 7.511 - mean_q: 12.909 - prob: 1.000\n",
            "\n",
            "Interval 38060 (380590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.003 - mae: 7.420 - mean_q: 12.795 - prob: 1.000\n",
            "\n",
            "Interval 38061 (380600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.002 - mae: 7.374 - mean_q: 12.673 - prob: 1.000\n",
            "\n",
            "Interval 38062 (380610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.343 - mean_q: 12.571 - prob: 1.000\n",
            "\n",
            "Interval 38063 (380620 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38064 (380630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.406 - mean_q: 12.838 - prob: 1.000\n",
            "\n",
            "Interval 38065 (380640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.628 - mean_q: 13.090 - prob: 1.000\n",
            "\n",
            "Interval 38066 (380650 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.171 - mean_q: 12.407 - prob: 1.000\n",
            "\n",
            "Interval 38067 (380660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.002 - mae: 7.379 - mean_q: 12.597 - prob: 1.000\n",
            "\n",
            "Interval 38068 (380670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.325 - mean_q: 12.790 - prob: 1.000\n",
            "\n",
            "Interval 38069 (380680 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38070 (380690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.471 - mean_q: 13.014 - prob: 1.000\n",
            "\n",
            "Interval 38071 (380700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.250 - mean_q: 12.548 - prob: 1.000\n",
            "\n",
            "Interval 38072 (380710 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 38073 (380720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.001 - mae: 7.166 - mean_q: 12.410 - prob: 1.000\n",
            "\n",
            "Interval 38074 (380730 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38075 (380740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.446 - mean_q: 12.706 - prob: 1.000\n",
            "\n",
            "Interval 38076 (380750 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38077 (380760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.365 - mean_q: 12.770 - prob: 1.000\n",
            "\n",
            "Interval 38078 (380770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.226 - mean_q: 12.531 - prob: 1.000\n",
            "\n",
            "Interval 38079 (380780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38080 (380790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.214 - mean_q: 12.495 - prob: 1.000\n",
            "\n",
            "Interval 38081 (380800 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38082 (380810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.107 - mean_q: 12.488 - prob: 1.000\n",
            "\n",
            "Interval 38083 (380820 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 38084 (380830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.002 - mae: 7.388 - mean_q: 12.853 - prob: 1.000\n",
            "\n",
            "Interval 38085 (380840 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.002 - mae: 7.297 - mean_q: 12.634 - prob: 1.000\n",
            "\n",
            "Interval 38086 (380850 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38087 (380860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.002 - mae: 7.156 - mean_q: 12.370 - prob: 1.000\n",
            "\n",
            "Interval 38088 (380870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.186 - mean_q: 12.488 - prob: 1.000\n",
            "\n",
            "Interval 38089 (380880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38090 (380890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.004 - mae: 7.375 - mean_q: 12.773 - prob: 1.000\n",
            "\n",
            "Interval 38091 (380900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.003 - mae: 7.668 - mean_q: 13.264 - prob: 1.000\n",
            "\n",
            "Interval 38092 (380910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.559 - mean_q: 12.886 - prob: 1.000\n",
            "\n",
            "Interval 38093 (380920 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38094 (380930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.399 - mean_q: 12.836 - prob: 1.000\n",
            "\n",
            "Interval 38095 (380940 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.250 - mean_q: 12.561 - prob: 1.000\n",
            "\n",
            "Interval 38096 (380950 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38097 (380960 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.002 - mae: 7.231 - mean_q: 12.498 - prob: 1.000\n",
            "\n",
            "Interval 38098 (380970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 38099 (380980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 8.000 [1.000, 15.000] - loss: 0.002 - mae: 7.334 - mean_q: 12.580 - prob: 1.000\n",
            "\n",
            "Interval 38100 (380990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38101 (381000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 9.500 [6.000, 13.000] - loss: 0.002 - mae: 7.112 - mean_q: 12.228 - prob: 1.000\n",
            "\n",
            "Interval 38102 (381010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 38103 (381020 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.190 - mean_q: 12.486 - prob: 1.000\n",
            "\n",
            "Interval 38104 (381030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 38105 (381040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.002 - mae: 7.101 - mean_q: 12.348 - prob: 1.000\n",
            "\n",
            "Interval 38106 (381050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.467 - mean_q: 12.867 - prob: 1.000\n",
            "\n",
            "Interval 38107 (381060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.225 - mean_q: 12.383 - prob: 1.000\n",
            "\n",
            "Interval 38108 (381070 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38109 (381080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.309 - mean_q: 12.542 - prob: 1.000\n",
            "\n",
            "Interval 38110 (381090 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.497 - mean_q: 12.826 - prob: 1.000\n",
            "\n",
            "Interval 38111 (381100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38112 (381110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.339 - mean_q: 12.742 - prob: 1.000\n",
            "\n",
            "Interval 38113 (381120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.277 - mean_q: 12.397 - prob: 1.000\n",
            "\n",
            "Interval 38114 (381130 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.374 - mean_q: 12.805 - prob: 1.000\n",
            "\n",
            "Interval 38115 (381140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38116 (381150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.563 - mean_q: 12.892 - prob: 1.000\n",
            "\n",
            "Interval 38117 (381160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.002 - mae: 7.225 - mean_q: 12.509 - prob: 1.000\n",
            "\n",
            "Interval 38118 (381170 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.002 - mae: 7.136 - mean_q: 12.385 - prob: 1.000\n",
            "\n",
            "Interval 38119 (381180 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 38120 (381190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.002 - mae: 7.513 - mean_q: 12.918 - prob: 1.000\n",
            "\n",
            "Interval 38121 (381200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.508 - mean_q: 13.064 - prob: 1.000\n",
            "\n",
            "Interval 38122 (381210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.676 - mean_q: 13.131 - prob: 1.000\n",
            "\n",
            "Interval 38123 (381220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.002 - mae: 7.585 - mean_q: 12.904 - prob: 1.000\n",
            "\n",
            "Interval 38124 (381230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38125 (381240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.058 - mean_q: 12.185 - prob: 1.000\n",
            "\n",
            "Interval 38126 (381250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.002 - mae: 7.330 - mean_q: 12.718 - prob: 1.000\n",
            "\n",
            "Interval 38127 (381260 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38128 (381270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.444 - mean_q: 12.839 - prob: 1.000\n",
            "\n",
            "Interval 38129 (381280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.082 - mean_q: 12.216 - prob: 1.000\n",
            "\n",
            "Interval 38130 (381290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38131 (381300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.286 - mean_q: 12.655 - prob: 1.000\n",
            "\n",
            "Interval 38132 (381310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.004 - mae: 7.422 - mean_q: 12.802 - prob: 1.000\n",
            "\n",
            "Interval 38133 (381320 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.273 - mean_q: 12.660 - prob: 1.000\n",
            "\n",
            "Interval 38134 (381330 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 38135 (381340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.002 - mae: 7.163 - mean_q: 12.456 - prob: 1.000\n",
            "\n",
            "Interval 38136 (381350 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38137 (381360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.388 - mean_q: 12.732 - prob: 1.000\n",
            "\n",
            "Interval 38138 (381370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.340 - mean_q: 12.569 - prob: 1.000\n",
            "\n",
            "Interval 38139 (381380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.002 - mae: 7.400 - mean_q: 12.698 - prob: 1.000\n",
            "\n",
            "Interval 38140 (381390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 38141 (381400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.001 - mae: 7.688 - mean_q: 13.160 - prob: 1.000\n",
            "\n",
            "Interval 38142 (381410 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 38143 (381420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.365 - mean_q: 12.823 - prob: 1.000\n",
            "\n",
            "Interval 38144 (381430 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38145 (381440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.354 - mean_q: 12.618 - prob: 1.000\n",
            "\n",
            "Interval 38146 (381450 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 38147 (381460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.002 - mae: 7.261 - mean_q: 12.558 - prob: 1.000\n",
            "\n",
            "Interval 38148 (381470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.543 - mean_q: 12.953 - prob: 1.000\n",
            "\n",
            "Interval 38149 (381480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.561 - mean_q: 12.998 - prob: 1.000\n",
            "\n",
            "Interval 38150 (381490 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.002 - mae: 7.227 - mean_q: 12.469 - prob: 1.000\n",
            "\n",
            "Interval 38151 (381500 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38152 (381510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.330 - mean_q: 12.698 - prob: 1.000\n",
            "\n",
            "Interval 38153 (381520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.073 - mean_q: 12.312 - prob: 1.000\n",
            "\n",
            "Interval 38154 (381530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38155 (381540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.143 - mean_q: 12.336 - prob: 1.000\n",
            "\n",
            "Interval 38156 (381550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.362 - mean_q: 12.811 - prob: 1.000\n",
            "\n",
            "Interval 38157 (381560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38158 (381570 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.162 - mean_q: 12.240 - prob: 1.000\n",
            "\n",
            "Interval 38159 (381580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.476 - mean_q: 12.828 - prob: 1.000\n",
            "\n",
            "Interval 38160 (381590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.002 - mae: 7.661 - mean_q: 13.109 - prob: 1.000\n",
            "\n",
            "Interval 38161 (381600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.628 - mean_q: 12.937 - prob: 1.000\n",
            "\n",
            "Interval 38162 (381610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38163 (381620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.004 - mae: 7.492 - mean_q: 12.806 - prob: 1.000\n",
            "\n",
            "Interval 38164 (381630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38165 (381640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 7.340 - mean_q: 12.773 - prob: 1.000\n",
            "\n",
            "Interval 38166 (381650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.003 - mae: 7.370 - mean_q: 12.627 - prob: 1.000\n",
            "\n",
            "Interval 38167 (381660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.003 - mae: 7.690 - mean_q: 13.155 - prob: 1.000\n",
            "\n",
            "Interval 38168 (381670 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 38169 (381680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.003 - mae: 7.464 - mean_q: 12.658 - prob: 1.000\n",
            "\n",
            "Interval 38170 (381690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.003 - mae: 7.613 - mean_q: 13.012 - prob: 1.000\n",
            "\n",
            "Interval 38171 (381700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.002 - mae: 7.428 - mean_q: 12.853 - prob: 1.000\n",
            "\n",
            "Interval 38172 (381710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.002 - mae: 7.205 - mean_q: 12.483 - prob: 1.000\n",
            "\n",
            "Interval 38173 (381720 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38174 (381730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.002 - mae: 7.450 - mean_q: 12.754 - prob: 1.000\n",
            "\n",
            "Interval 38175 (381740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 38176 (381750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -18.000 [-18.000, -18.000] - loss: 0.002 - mae: 7.521 - mean_q: 12.937 - prob: 1.000\n",
            "\n",
            "Interval 38177 (381760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38178 (381770 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.610 - mean_q: 13.028 - prob: 1.000\n",
            "\n",
            "Interval 38179 (381780 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 38180 (381790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.002 - mae: 7.342 - mean_q: 12.519 - prob: 1.000\n",
            "\n",
            "Interval 38181 (381800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.002 - mae: 7.237 - mean_q: 12.542 - prob: 1.000\n",
            "\n",
            "Interval 38182 (381810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.283 - mean_q: 12.619 - prob: 1.000\n",
            "\n",
            "Interval 38183 (381820 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38184 (381830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -18.000 [-18.000, -18.000] - loss: 0.002 - mae: 7.009 - mean_q: 12.339 - prob: 1.000\n",
            "\n",
            "Interval 38185 (381840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.002 - mae: 7.338 - mean_q: 12.655 - prob: 1.000\n",
            "\n",
            "Interval 38186 (381850 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38187 (381860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.430 - mean_q: 12.781 - prob: 1.000\n",
            "\n",
            "Interval 38188 (381870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.263 - mean_q: 12.577 - prob: 1.000\n",
            "\n",
            "Interval 38189 (381880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.517 - mean_q: 12.917 - prob: 1.000\n",
            "\n",
            "Interval 38190 (381890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.351 - mean_q: 12.715 - prob: 1.000\n",
            "\n",
            "Interval 38191 (381900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38192 (381910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.817 - mean_q: 13.283 - prob: 1.000\n",
            "\n",
            "Interval 38193 (381920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.002 - mae: 7.785 - mean_q: 13.354 - prob: 1.000\n",
            "\n",
            "Interval 38194 (381930 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 38195 (381940 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.002 - mae: 7.388 - mean_q: 12.795 - prob: 1.000\n",
            "\n",
            "Interval 38196 (381950 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38197 (381960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.528 - mean_q: 12.882 - prob: 1.000\n",
            "\n",
            "Interval 38198 (381970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.264 - mean_q: 12.546 - prob: 1.000\n",
            "\n",
            "Interval 38199 (381980 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38200 (381990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.243 - mean_q: 12.432 - prob: 1.000\n",
            "\n",
            "Interval 38201 (382000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38202 (382010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.630 - mean_q: 13.033 - prob: 1.000\n",
            "\n",
            "Interval 38203 (382020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.121 - mean_q: 12.401 - prob: 1.000\n",
            "\n",
            "Interval 38204 (382030 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.001 - mae: 7.470 - mean_q: 12.668 - prob: 1.000\n",
            "\n",
            "Interval 38205 (382040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 38206 (382050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.002 - mae: 7.176 - mean_q: 12.451 - prob: 1.000\n",
            "\n",
            "Interval 38207 (382060 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38208 (382070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.002 - mae: 7.457 - mean_q: 12.873 - prob: 1.000\n",
            "\n",
            "Interval 38209 (382080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.003 - mae: 7.318 - mean_q: 12.675 - prob: 1.000\n",
            "\n",
            "Interval 38210 (382090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 38211 (382100 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.003 - mae: 7.434 - mean_q: 12.903 - prob: 1.000\n",
            "\n",
            "Interval 38212 (382110 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.003 - mae: 7.650 - mean_q: 13.132 - prob: 1.000\n",
            "\n",
            "Interval 38213 (382120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.003 - mae: 7.321 - mean_q: 12.637 - prob: 1.000\n",
            "\n",
            "Interval 38214 (382130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 38215 (382140 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.002 - mae: 7.545 - mean_q: 12.940 - prob: 1.000\n",
            "\n",
            "Interval 38216 (382150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 38217 (382160 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.002 - mae: 7.423 - mean_q: 12.740 - prob: 1.000\n",
            "\n",
            "Interval 38218 (382170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.219 - mean_q: 12.342 - prob: 1.000\n",
            "\n",
            "Interval 38219 (382180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.002 - mae: 7.345 - mean_q: 12.759 - prob: 1.000\n",
            "\n",
            "Interval 38220 (382190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38221 (382200 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.240 - mean_q: 12.518 - prob: 1.000\n",
            "\n",
            "Interval 38222 (382210 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.003 - mae: 7.789 - mean_q: 13.282 - prob: 1.000\n",
            "\n",
            "Interval 38223 (382220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.127 - mean_q: 12.385 - prob: 1.000\n",
            "\n",
            "Interval 38224 (382230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.362 - mean_q: 12.841 - prob: 1.000\n",
            "\n",
            "Interval 38225 (382240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38226 (382250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.002 - mae: 7.422 - mean_q: 12.758 - prob: 1.000\n",
            "\n",
            "Interval 38227 (382260 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38228 (382270 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38229 (382280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.003 - mae: 7.686 - mean_q: 13.071 - prob: 1.000\n",
            "\n",
            "Interval 38230 (382290 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.002 - mae: 7.455 - mean_q: 12.851 - prob: 1.000\n",
            "\n",
            "Interval 38231 (382300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38232 (382310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.597 - mean_q: 13.030 - prob: 1.000\n",
            "\n",
            "Interval 38233 (382320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.519 - mean_q: 12.816 - prob: 1.000\n",
            "\n",
            "Interval 38234 (382330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 38235 (382340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.002 - mae: 7.256 - mean_q: 12.434 - prob: 1.000\n",
            "\n",
            "Interval 38236 (382350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.583 - mean_q: 12.976 - prob: 1.000\n",
            "\n",
            "Interval 38237 (382360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.002 - mae: 7.096 - mean_q: 12.253 - prob: 1.000\n",
            "\n",
            "Interval 38238 (382370 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38239 (382380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38240 (382390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 38241 (382400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -39.000 [-39.000, -39.000] - loss: 0.004 - mae: 7.381 - mean_q: 12.855 - prob: 1.000\n",
            "\n",
            "Interval 38242 (382410 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38243 (382420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.004 - mae: 7.492 - mean_q: 12.915 - prob: 1.000\n",
            "\n",
            "Interval 38244 (382430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 38245 (382440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.006 - mae: 7.286 - mean_q: 12.533 - prob: 1.000\n",
            "\n",
            "Interval 38246 (382450 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.005 - mae: 7.601 - mean_q: 13.011 - prob: 1.000\n",
            "\n",
            "Interval 38247 (382460 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 38248 (382470 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38249 (382480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.002 - mae: 7.095 - mean_q: 12.432 - prob: 1.000\n",
            "\n",
            "Interval 38250 (382490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.486 - mean_q: 12.902 - prob: 1.000\n",
            "\n",
            "Interval 38251 (382500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.767 - mean_q: 13.296 - prob: 1.000\n",
            "\n",
            "Interval 38252 (382510 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38253 (382520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 11.000 [7.000, 15.000] - loss: 0.002 - mae: 7.428 - mean_q: 12.904 - prob: 1.000\n",
            "\n",
            "Interval 38254 (382530 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38255 (382540 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.002 - mae: 7.320 - mean_q: 12.443 - prob: 1.000\n",
            "\n",
            "Interval 38256 (382550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38257 (382560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.634 - mean_q: 13.044 - prob: 1.000\n",
            "\n",
            "Interval 38258 (382570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38259 (382580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.290 - mean_q: 12.519 - prob: 1.000\n",
            "\n",
            "Interval 38260 (382590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.356 - mean_q: 12.707 - prob: 1.000\n",
            "\n",
            "Interval 38261 (382600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38262 (382610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.124 - mean_q: 12.416 - prob: 1.000\n",
            "\n",
            "Interval 38263 (382620 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.002 - mae: 7.788 - mean_q: 13.275 - prob: 1.000\n",
            "\n",
            "Interval 38264 (382630 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 38265 (382640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.002 - mae: 7.328 - mean_q: 12.640 - prob: 1.000\n",
            "\n",
            "Interval 38266 (382650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.235 - mean_q: 12.478 - prob: 1.000\n",
            "\n",
            "Interval 38267 (382660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 38268 (382670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.501 - mean_q: 12.900 - prob: 1.000\n",
            "\n",
            "Interval 38269 (382680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.160 - mean_q: 12.422 - prob: 1.000\n",
            "\n",
            "Interval 38270 (382690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 38271 (382700 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.002 - mae: 7.019 - mean_q: 12.204 - prob: 1.000\n",
            "\n",
            "Interval 38272 (382710 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 38273 (382720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.002 - mae: 7.188 - mean_q: 12.516 - prob: 1.000\n",
            "\n",
            "Interval 38274 (382730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.228 - mean_q: 12.504 - prob: 1.000\n",
            "\n",
            "Interval 38275 (382740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.001 - mae: 7.592 - mean_q: 12.997 - prob: 1.000\n",
            "\n",
            "Interval 38276 (382750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.002 - mae: 7.218 - mean_q: 12.445 - prob: 1.000\n",
            "\n",
            "Interval 38277 (382760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38278 (382770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.249 - mean_q: 12.520 - prob: 1.000\n",
            "\n",
            "Interval 38279 (382780 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38280 (382790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.002 - mae: 7.399 - mean_q: 12.864 - prob: 1.000\n",
            "\n",
            "Interval 38281 (382800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.458 - mean_q: 12.887 - prob: 1.000\n",
            "\n",
            "Interval 38282 (382810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.309 - mean_q: 12.629 - prob: 1.000\n",
            "\n",
            "Interval 38283 (382820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38284 (382830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.389 - mean_q: 12.764 - prob: 1.000\n",
            "\n",
            "Interval 38285 (382840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.002 - mae: 7.238 - mean_q: 12.523 - prob: 1.000\n",
            "\n",
            "Interval 38286 (382850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38287 (382860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.354 - mean_q: 12.534 - prob: 1.000\n",
            "\n",
            "Interval 38288 (382870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38289 (382880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.002 - mae: 7.468 - mean_q: 12.783 - prob: 1.000\n",
            "\n",
            "Interval 38290 (382890 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38291 (382900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.291 - mean_q: 12.710 - prob: 1.000\n",
            "\n",
            "Interval 38292 (382910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.001 - mae: 7.341 - mean_q: 12.748 - prob: 1.000\n",
            "\n",
            "Interval 38293 (382920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38294 (382930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -19.000 [-19.000, -19.000] - loss: 0.002 - mae: 7.420 - mean_q: 12.894 - prob: 1.000\n",
            "\n",
            "Interval 38295 (382940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 38296 (382950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -17.000 [-17.000, -17.000] - loss: 0.001 - mae: 7.398 - mean_q: 12.733 - prob: 1.000\n",
            "\n",
            "Interval 38297 (382960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 38298 (382970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.533 - mean_q: 12.911 - prob: 1.000\n",
            "\n",
            "Interval 38299 (382980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.359 - mean_q: 12.701 - prob: 1.000\n",
            "\n",
            "Interval 38300 (382990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.301 - mean_q: 12.629 - prob: 1.000\n",
            "\n",
            "Interval 38301 (383000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.361 - mean_q: 12.777 - prob: 1.000\n",
            "\n",
            "Interval 38302 (383010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.707 - mean_q: 13.267 - prob: 1.000\n",
            "\n",
            "Interval 38303 (383020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 38304 (383030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.002 - mae: 7.383 - mean_q: 12.565 - prob: 1.000\n",
            "\n",
            "Interval 38305 (383040 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38306 (383050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.675 - mean_q: 13.037 - prob: 1.000\n",
            "\n",
            "Interval 38307 (383060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.002 - mae: 7.223 - mean_q: 12.574 - prob: 1.000\n",
            "\n",
            "Interval 38308 (383070 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.777 - mean_q: 13.368 - prob: 1.000\n",
            "\n",
            "Interval 38309 (383080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38310 (383090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.175 - mean_q: 12.516 - prob: 1.000\n",
            "\n",
            "Interval 38311 (383100 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38312 (383110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.003 - mae: 7.399 - mean_q: 12.766 - prob: 1.000\n",
            "\n",
            "Interval 38313 (383120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.002 - mae: 7.396 - mean_q: 12.735 - prob: 1.000\n",
            "\n",
            "Interval 38314 (383130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38315 (383140 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.002 - mae: 7.091 - mean_q: 12.275 - prob: 1.000\n",
            "\n",
            "Interval 38316 (383150 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38317 (383160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.003 - mae: 7.147 - mean_q: 12.415 - prob: 1.000\n",
            "\n",
            "Interval 38318 (383170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.002 - mae: 7.247 - mean_q: 12.478 - prob: 1.000\n",
            "\n",
            "Interval 38319 (383180 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38320 (383190 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.003 - mae: 7.121 - mean_q: 12.196 - prob: 1.000\n",
            "\n",
            "Interval 38321 (383200 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 38322 (383210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.003 - mae: 7.142 - mean_q: 12.450 - prob: 1.000\n",
            "\n",
            "Interval 38323 (383220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 38324 (383230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.003 - mae: 7.180 - mean_q: 12.472 - prob: 1.000\n",
            "\n",
            "Interval 38325 (383240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.452 - mean_q: 12.888 - prob: 1.000\n",
            "\n",
            "Interval 38326 (383250 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38327 (383260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.647 - mean_q: 13.008 - prob: 1.000\n",
            "\n",
            "Interval 38328 (383270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.441 - mean_q: 12.627 - prob: 1.000\n",
            "\n",
            "Interval 38329 (383280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38330 (383290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.002 - mae: 7.381 - mean_q: 12.714 - prob: 1.000\n",
            "\n",
            "Interval 38331 (383300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -32.000 [-32.000, -32.000] - loss: 0.002 - mae: 7.537 - mean_q: 13.028 - prob: 1.000\n",
            "\n",
            "Interval 38332 (383310 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 38333 (383320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.002 - mae: 7.290 - mean_q: 12.675 - prob: 1.000\n",
            "\n",
            "Interval 38334 (383330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.334 - mean_q: 12.636 - prob: 1.000\n",
            "\n",
            "Interval 38335 (383340 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.396 - mean_q: 12.764 - prob: 1.000\n",
            "\n",
            "Interval 38336 (383350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.002 - mae: 6.904 - mean_q: 12.005 - prob: 1.000\n",
            "\n",
            "Interval 38337 (383360 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38338 (383370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.002 - mae: 7.166 - mean_q: 12.569 - prob: 1.000\n",
            "\n",
            "Interval 38339 (383380 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 38340 (383390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.003 - mae: 7.311 - mean_q: 12.610 - prob: 1.000\n",
            "\n",
            "Interval 38341 (383400 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38342 (383410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.003 - mae: 7.349 - mean_q: 12.684 - prob: 1.000\n",
            "\n",
            "Interval 38343 (383420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.262 - mean_q: 12.489 - prob: 1.000\n",
            "\n",
            "Interval 38344 (383430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 38345 (383440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -17.000 [-17.000, -17.000] - loss: 0.002 - mae: 7.533 - mean_q: 13.038 - prob: 1.000\n",
            "\n",
            "Interval 38346 (383450 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38347 (383460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.474 - mean_q: 12.935 - prob: 1.000\n",
            "\n",
            "Interval 38348 (383470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.001 - mae: 7.168 - mean_q: 12.368 - prob: 1.000\n",
            "\n",
            "Interval 38349 (383480 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 38350 (383490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.001 - mae: 7.376 - mean_q: 12.814 - prob: 1.000\n",
            "\n",
            "Interval 38351 (383500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.191 - mean_q: 12.438 - prob: 1.000\n",
            "\n",
            "Interval 38352 (383510 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38353 (383520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.414 - mean_q: 12.858 - prob: 1.000\n",
            "\n",
            "Interval 38354 (383530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.333 - mean_q: 12.634 - prob: 1.000\n",
            "\n",
            "Interval 38355 (383540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.478 - mean_q: 12.909 - prob: 1.000\n",
            "\n",
            "Interval 38356 (383550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.003 - mae: 7.420 - mean_q: 12.808 - prob: 1.000\n",
            "\n",
            "Interval 38357 (383560 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.003 - mae: 7.454 - mean_q: 12.902 - prob: 1.000\n",
            "\n",
            "Interval 38358 (383570 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.002 - mae: 7.260 - mean_q: 12.564 - prob: 1.000\n",
            "\n",
            "Interval 38359 (383580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.526 - mean_q: 13.083 - prob: 1.000\n",
            "\n",
            "Interval 38360 (383590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38361 (383600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.370 - mean_q: 12.691 - prob: 1.000\n",
            "\n",
            "Interval 38362 (383610 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.250 - mean_q: 12.635 - prob: 1.000\n",
            "\n",
            "Interval 38363 (383620 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 38364 (383630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.248 - mean_q: 12.518 - prob: 1.000\n",
            "\n",
            "Interval 38365 (383640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.331 - mean_q: 12.597 - prob: 1.000\n",
            "\n",
            "Interval 38366 (383650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.403 - mean_q: 12.765 - prob: 1.000\n",
            "\n",
            "Interval 38367 (383660 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38368 (383670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.481 - mean_q: 12.762 - prob: 1.000\n",
            "\n",
            "Interval 38369 (383680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 38370 (383690 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.002 - mae: 7.395 - mean_q: 12.767 - prob: 1.000\n",
            "\n",
            "Interval 38371 (383700 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.002 - mae: 7.056 - mean_q: 12.223 - prob: 1.000\n",
            "\n",
            "Interval 38372 (383710 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 38373 (383720 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.528 - mean_q: 13.009 - prob: 1.000\n",
            "\n",
            "Interval 38374 (383730 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.093 - mean_q: 12.351 - prob: 1.000\n",
            "\n",
            "Interval 38375 (383740 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.299 - mean_q: 12.547 - prob: 1.000\n",
            "\n",
            "Interval 38376 (383750 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 38377 (383760 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.511 - mean_q: 12.816 - prob: 1.000\n",
            "\n",
            "Interval 38378 (383770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.002 - mae: 7.438 - mean_q: 12.829 - prob: 1.000\n",
            "\n",
            "Interval 38379 (383780 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 38380 (383790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.002 - mae: 7.223 - mean_q: 12.331 - prob: 1.000\n",
            "\n",
            "Interval 38381 (383800 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 38382 (383810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.003 - mae: 7.429 - mean_q: 12.824 - prob: 1.000\n",
            "\n",
            "Interval 38383 (383820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.002 - mae: 7.520 - mean_q: 12.827 - prob: 1.000\n",
            "\n",
            "Interval 38384 (383830 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 38385 (383840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.002 - mae: 7.188 - mean_q: 12.497 - prob: 1.000\n",
            "\n",
            "Interval 38386 (383850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 38387 (383860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.002 - mae: 7.351 - mean_q: 12.736 - prob: 1.000\n",
            "\n",
            "Interval 38388 (383870 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38389 (383880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.002 - mae: 7.391 - mean_q: 12.824 - prob: 1.000\n",
            "\n",
            "Interval 38390 (383890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38391 (383900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.003 - mae: 7.397 - mean_q: 12.946 - prob: 1.000\n",
            "\n",
            "Interval 38392 (383910 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38393 (383920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.005 - mae: 7.176 - mean_q: 12.496 - prob: 1.000\n",
            "\n",
            "Interval 38394 (383930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.004 - mae: 7.392 - mean_q: 12.731 - prob: 1.000\n",
            "\n",
            "Interval 38395 (383940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.003 - mae: 7.482 - mean_q: 12.847 - prob: 1.000\n",
            "\n",
            "Interval 38396 (383950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38397 (383960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.003 - mae: 7.463 - mean_q: 12.724 - prob: 1.000\n",
            "\n",
            "Interval 38398 (383970 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.003 - mae: 7.308 - mean_q: 12.679 - prob: 1.000\n",
            "\n",
            "Interval 38399 (383980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 38400 (383990 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.002 - mae: 7.303 - mean_q: 12.514 - prob: 1.000\n",
            "\n",
            "Interval 38401 (384000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 38402 (384010 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.002 - mae: 7.349 - mean_q: 12.614 - prob: 1.000\n",
            "\n",
            "Interval 38403 (384020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.003 - mae: 7.442 - mean_q: 13.049 - prob: 1.000\n",
            "\n",
            "Interval 38404 (384030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38405 (384040 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.005 - mae: 7.614 - mean_q: 13.209 - prob: 1.000\n",
            "\n",
            "Interval 38406 (384050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.004 - mae: 7.038 - mean_q: 12.325 - prob: 1.000\n",
            "\n",
            "Interval 38407 (384060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38408 (384070 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -2.8000\n",
            "Interval 38409 (384080 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -18.000 [-18.000, -18.000] - loss: 0.004 - mae: 7.203 - mean_q: 12.504 - prob: 1.000\n",
            "\n",
            "Interval 38410 (384090 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.008 - mae: 7.204 - mean_q: 12.563 - prob: 1.000\n",
            "\n",
            "Interval 38411 (384100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.005 - mae: 7.399 - mean_q: 12.629 - prob: 1.000\n",
            "\n",
            "Interval 38412 (384110 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.004 - mae: 7.480 - mean_q: 12.893 - prob: 1.000\n",
            "\n",
            "Interval 38413 (384120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 38414 (384130 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.003 - mae: 7.556 - mean_q: 12.991 - prob: 1.000\n",
            "\n",
            "Interval 38415 (384140 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.002 - mae: 7.374 - mean_q: 12.663 - prob: 1.000\n",
            "\n",
            "Interval 38416 (384150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38417 (384160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.002 - mae: 7.221 - mean_q: 12.688 - prob: 1.000\n",
            "\n",
            "Interval 38418 (384170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.138 - mean_q: 12.400 - prob: 1.000\n",
            "\n",
            "Interval 38419 (384180 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38420 (384190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.456 - mean_q: 12.791 - prob: 1.000\n",
            "\n",
            "Interval 38421 (384200 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.173 - mean_q: 12.368 - prob: 1.000\n",
            "\n",
            "Interval 38422 (384210 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 38423 (384220 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.242 - mean_q: 12.722 - prob: 1.000\n",
            "\n",
            "Interval 38424 (384230 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.572 - mean_q: 12.976 - prob: 1.000\n",
            "\n",
            "Interval 38425 (384240 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.539 - mean_q: 12.939 - prob: 1.000\n",
            "\n",
            "Interval 38426 (384250 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 38427 (384260 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.175 - mean_q: 12.498 - prob: 1.000\n",
            "\n",
            "Interval 38428 (384270 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.084 - mean_q: 12.203 - prob: 1.000\n",
            "\n",
            "Interval 38429 (384280 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 38430 (384290 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.002 - mae: 7.341 - mean_q: 12.683 - prob: 1.000\n",
            "\n",
            "Interval 38431 (384300 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.002 - mae: 7.492 - mean_q: 12.940 - prob: 1.000\n",
            "\n",
            "Interval 38432 (384310 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 38433 (384320 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.002 - mae: 7.229 - mean_q: 12.595 - prob: 1.000\n",
            "\n",
            "Interval 38434 (384330 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.003 - mae: 7.317 - mean_q: 12.616 - prob: 1.000\n",
            "\n",
            "Interval 38435 (384340 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.335 - mean_q: 12.715 - prob: 1.000\n",
            "\n",
            "Interval 38436 (384350 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.001 - mae: 7.332 - mean_q: 12.835 - prob: 1.000\n",
            "\n",
            "Interval 38437 (384360 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 38438 (384370 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.317 - mean_q: 12.593 - prob: 1.000\n",
            "\n",
            "Interval 38439 (384380 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.174 - mean_q: 12.485 - prob: 1.000\n",
            "\n",
            "Interval 38440 (384390 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.369 - mean_q: 12.641 - prob: 1.000\n",
            "\n",
            "Interval 38441 (384400 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -2.8000\n",
            "Interval 38442 (384410 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.004 - mae: 7.068 - mean_q: 12.224 - prob: 1.000\n",
            "\n",
            "Interval 38443 (384420 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.290 - mean_q: 12.714 - prob: 1.000\n",
            "\n",
            "Interval 38444 (384430 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.238 - mean_q: 12.527 - prob: 1.000\n",
            "\n",
            "Interval 38445 (384440 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 38446 (384450 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.002 - mae: 7.338 - mean_q: 12.622 - prob: 1.000\n",
            "\n",
            "Interval 38447 (384460 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.262 - mean_q: 12.661 - prob: 1.000\n",
            "\n",
            "Interval 38448 (384470 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 38449 (384480 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.003 - mae: 7.396 - mean_q: 12.707 - prob: 1.000\n",
            "\n",
            "Interval 38450 (384490 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 38451 (384500 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.003 - mae: 7.494 - mean_q: 12.861 - prob: 1.000\n",
            "\n",
            "Interval 38452 (384510 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.002 - mae: 7.595 - mean_q: 13.023 - prob: 1.000\n",
            "\n",
            "Interval 38453 (384520 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 38454 (384530 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 38455 (384540 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.661 - mean_q: 13.174 - prob: 1.000\n",
            "\n",
            "Interval 38456 (384550 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.161 - mean_q: 12.407 - prob: 1.000\n",
            "\n",
            "Interval 38457 (384560 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.252 - mean_q: 12.379 - prob: 1.000\n",
            "\n",
            "Interval 38458 (384570 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 38459 (384580 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.442 - mean_q: 12.960 - prob: 1.000\n",
            "\n",
            "Interval 38460 (384590 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.152 - mean_q: 12.380 - prob: 1.000\n",
            "\n",
            "Interval 38461 (384600 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 38462 (384610 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.004 - mae: 7.195 - mean_q: 12.376 - prob: 1.000\n",
            "\n",
            "Interval 38463 (384620 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 38464 (384630 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.003 - mae: 7.278 - mean_q: 12.588 - prob: 1.000\n",
            "\n",
            "Interval 38465 (384640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.003 - mae: 7.192 - mean_q: 12.416 - prob: 1.000\n",
            "\n",
            "Interval 38466 (384650 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38467 (384660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.032 - mean_q: 12.197 - prob: 1.000\n",
            "\n",
            "Interval 38468 (384670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.004 - mae: 7.350 - mean_q: 12.607 - prob: 1.000\n",
            "\n",
            "Interval 38469 (384680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.514 - mean_q: 12.911 - prob: 1.000\n",
            "\n",
            "Interval 38470 (384690 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 38471 (384700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.002 - mae: 7.106 - mean_q: 12.261 - prob: 1.000\n",
            "\n",
            "Interval 38472 (384710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 38473 (384720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.003 - mae: 7.464 - mean_q: 12.901 - prob: 1.000\n",
            "\n",
            "Interval 38474 (384730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.209 - mean_q: 12.559 - prob: 1.000\n",
            "\n",
            "Interval 38475 (384740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38476 (384750 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.002 - mae: 7.482 - mean_q: 13.013 - prob: 1.000\n",
            "\n",
            "Interval 38477 (384760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.691 - mean_q: 13.205 - prob: 1.000\n",
            "\n",
            "Interval 38478 (384770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.581 - mean_q: 13.011 - prob: 1.000\n",
            "\n",
            "Interval 38479 (384780 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38480 (384790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.349 - mean_q: 12.635 - prob: 1.000\n",
            "\n",
            "Interval 38481 (384800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.001 - mae: 7.706 - mean_q: 13.105 - prob: 1.000\n",
            "\n",
            "Interval 38482 (384810 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.037 - mean_q: 12.212 - prob: 1.000\n",
            "\n",
            "Interval 38483 (384820 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38484 (384830 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.002 - mae: 7.080 - mean_q: 12.399 - prob: 1.000\n",
            "\n",
            "Interval 38485 (384840 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38486 (384850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.564 - mean_q: 13.094 - prob: 1.000\n",
            "\n",
            "Interval 38487 (384860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.475 - mean_q: 12.851 - prob: 1.000\n",
            "\n",
            "Interval 38488 (384870 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38489 (384880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.204 - mean_q: 12.450 - prob: 1.000\n",
            "\n",
            "Interval 38490 (384890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.139 - mean_q: 12.370 - prob: 1.000\n",
            "\n",
            "Interval 38491 (384900 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38492 (384910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.118 - mean_q: 12.156 - prob: 1.000\n",
            "\n",
            "Interval 38493 (384920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.268 - mean_q: 12.433 - prob: 1.000\n",
            "\n",
            "Interval 38494 (384930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.092 - mean_q: 12.398 - prob: 1.000\n",
            "\n",
            "Interval 38495 (384940 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.612 - mean_q: 12.993 - prob: 1.000\n",
            "\n",
            "Interval 38496 (384950 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38497 (384960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.002 - mae: 7.346 - mean_q: 12.624 - prob: 1.000\n",
            "\n",
            "Interval 38498 (384970 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38499 (384980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.409 - mean_q: 12.844 - prob: 1.000\n",
            "\n",
            "Interval 38500 (384990 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38501 (385000 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 8.500 [2.000, 15.000] - loss: 0.002 - mae: 7.006 - mean_q: 12.075 - prob: 1.000\n",
            "\n",
            "Interval 38502 (385010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.185 - mean_q: 12.375 - prob: 1.000\n",
            "\n",
            "Interval 38503 (385020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38504 (385030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.286 - mean_q: 12.493 - prob: 1.000\n",
            "\n",
            "Interval 38505 (385040 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.016 - mean_q: 12.309 - prob: 1.000\n",
            "\n",
            "Interval 38506 (385050 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38507 (385060 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38508 (385070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.484 - mean_q: 12.920 - prob: 1.000\n",
            "\n",
            "Interval 38509 (385080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 38510 (385090 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -18.000 [-18.000, -18.000] - loss: 0.001 - mae: 7.372 - mean_q: 12.652 - prob: 1.000\n",
            "\n",
            "Interval 38511 (385100 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.301 - mean_q: 12.608 - prob: 1.000\n",
            "\n",
            "Interval 38512 (385110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.394 - mean_q: 12.767 - prob: 1.000\n",
            "\n",
            "Interval 38513 (385120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.006 - mae: 7.459 - mean_q: 12.759 - prob: 1.000\n",
            "\n",
            "Interval 38514 (385130 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.135 - mean_q: 12.381 - prob: 1.000\n",
            "\n",
            "Interval 38515 (385140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.003 - mae: 7.554 - mean_q: 12.900 - prob: 1.000\n",
            "\n",
            "Interval 38516 (385150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38517 (385160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.002 - mae: 7.511 - mean_q: 13.064 - prob: 1.000\n",
            "\n",
            "Interval 38518 (385170 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38519 (385180 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.293 - mean_q: 12.578 - prob: 1.000\n",
            "\n",
            "Interval 38520 (385190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.293 - mean_q: 12.581 - prob: 1.000\n",
            "\n",
            "Interval 38521 (385200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.003 - mae: 7.038 - mean_q: 12.348 - prob: 1.000\n",
            "\n",
            "Interval 38522 (385210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38523 (385220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.003 - mae: 7.622 - mean_q: 13.017 - prob: 1.000\n",
            "\n",
            "Interval 38524 (385230 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38525 (385240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.002 - mae: 7.373 - mean_q: 12.640 - prob: 1.000\n",
            "\n",
            "Interval 38526 (385250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.002 - mae: 7.563 - mean_q: 12.885 - prob: 1.000\n",
            "\n",
            "Interval 38527 (385260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.257 - mean_q: 12.578 - prob: 1.000\n",
            "\n",
            "Interval 38528 (385270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.003 - mae: 7.228 - mean_q: 12.584 - prob: 1.000\n",
            "\n",
            "Interval 38529 (385280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.003 - mae: 7.323 - mean_q: 12.618 - prob: 1.000\n",
            "\n",
            "Interval 38530 (385290 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 38531 (385300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.004 - mae: 7.266 - mean_q: 12.496 - prob: 1.000\n",
            "\n",
            "Interval 38532 (385310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38533 (385320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.724 - mean_q: 13.172 - prob: 1.000\n",
            "\n",
            "Interval 38534 (385330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.435 - mean_q: 12.714 - prob: 1.000\n",
            "\n",
            "Interval 38535 (385340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38536 (385350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.002 - mae: 7.436 - mean_q: 12.834 - prob: 1.000\n",
            "\n",
            "Interval 38537 (385360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.003 - mae: 7.137 - mean_q: 12.518 - prob: 1.000\n",
            "\n",
            "Interval 38538 (385370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38539 (385380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 38540 (385390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.002 - mae: 7.529 - mean_q: 12.989 - prob: 1.000\n",
            "\n",
            "Interval 38541 (385400 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.522 - mean_q: 12.982 - prob: 1.000\n",
            "\n",
            "Interval 38542 (385410 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.417 - mean_q: 12.799 - prob: 1.000\n",
            "\n",
            "Interval 38543 (385420 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 38544 (385430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.463 - mean_q: 12.728 - prob: 1.000\n",
            "\n",
            "Interval 38545 (385440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.319 - mean_q: 12.673 - prob: 1.000\n",
            "\n",
            "Interval 38546 (385450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38547 (385460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.002 - mae: 6.970 - mean_q: 12.179 - prob: 1.000\n",
            "\n",
            "Interval 38548 (385470 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.002 - mae: 7.555 - mean_q: 13.013 - prob: 1.000\n",
            "\n",
            "Interval 38549 (385480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.002 - mae: 7.500 - mean_q: 12.909 - prob: 1.000\n",
            "\n",
            "Interval 38550 (385490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38551 (385500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.388 - mean_q: 12.762 - prob: 1.000\n",
            "\n",
            "Interval 38552 (385510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.112 - mean_q: 12.195 - prob: 1.000\n",
            "\n",
            "Interval 38553 (385520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38554 (385530 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38555 (385540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.002 - mae: 7.391 - mean_q: 12.688 - prob: 1.000\n",
            "\n",
            "Interval 38556 (385550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.002 - mae: 7.151 - mean_q: 12.439 - prob: 1.000\n",
            "\n",
            "Interval 38557 (385560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.002 - mae: 7.104 - mean_q: 12.336 - prob: 1.000\n",
            "\n",
            "Interval 38558 (385570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.459 - mean_q: 12.746 - prob: 1.000\n",
            "\n",
            "Interval 38559 (385580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38560 (385590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.431 - mean_q: 12.807 - prob: 1.000\n",
            "\n",
            "Interval 38561 (385600 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 38562 (385610 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.081 - mean_q: 12.407 - prob: 1.000\n",
            "\n",
            "Interval 38563 (385620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38564 (385630 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 38565 (385640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -18.000 [-18.000, -18.000] - loss: 0.001 - mae: 7.440 - mean_q: 12.855 - prob: 1.000\n",
            "\n",
            "Interval 38566 (385650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.697 - mean_q: 13.044 - prob: 1.000\n",
            "\n",
            "Interval 38567 (385660 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.150 - mean_q: 12.453 - prob: 1.000\n",
            "\n",
            "Interval 38568 (385670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38569 (385680 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.296 - mean_q: 12.607 - prob: 1.000\n",
            "\n",
            "Interval 38570 (385690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.232 - mean_q: 12.508 - prob: 1.000\n",
            "\n",
            "Interval 38571 (385700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 38572 (385710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.002 - mae: 7.308 - mean_q: 12.670 - prob: 1.000\n",
            "\n",
            "Interval 38573 (385720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38574 (385730 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.521 - mean_q: 12.853 - prob: 1.000\n",
            "\n",
            "Interval 38575 (385740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 38576 (385750 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -17.000 [-17.000, -17.000] - loss: 0.002 - mae: 7.595 - mean_q: 13.142 - prob: 1.000\n",
            "\n",
            "Interval 38577 (385760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38578 (385770 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.052 - mean_q: 12.363 - prob: 1.000\n",
            "\n",
            "Interval 38579 (385780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 38580 (385790 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.002 - mae: 7.101 - mean_q: 12.290 - prob: 1.000\n",
            "\n",
            "Interval 38581 (385800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.005 - mae: 7.383 - mean_q: 12.835 - prob: 1.000\n",
            "\n",
            "Interval 38582 (385810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38583 (385820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38584 (385830 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.003 - mae: 7.540 - mean_q: 12.904 - prob: 1.000\n",
            "\n",
            "Interval 38585 (385840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38586 (385850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.002 - mae: 7.328 - mean_q: 12.675 - prob: 1.000\n",
            "\n",
            "Interval 38587 (385860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38588 (385870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.002 - mae: 7.353 - mean_q: 12.714 - prob: 1.000\n",
            "\n",
            "Interval 38589 (385880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.481 - mean_q: 12.829 - prob: 1.000\n",
            "\n",
            "Interval 38590 (385890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 38591 (385900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.002 - mae: 7.400 - mean_q: 12.726 - prob: 1.000\n",
            "\n",
            "Interval 38592 (385910 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38593 (385920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.720 - mean_q: 13.143 - prob: 1.000\n",
            "\n",
            "Interval 38594 (385930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.196 - mean_q: 12.585 - prob: 1.000\n",
            "\n",
            "Interval 38595 (385940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38596 (385950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.002 - mae: 7.163 - mean_q: 12.504 - prob: 1.000\n",
            "\n",
            "Interval 38597 (385960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.002 - mae: 7.429 - mean_q: 12.838 - prob: 1.000\n",
            "\n",
            "Interval 38598 (385970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38599 (385980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.329 - mean_q: 12.716 - prob: 1.000\n",
            "\n",
            "Interval 38600 (385990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38601 (386000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.614 - mean_q: 13.158 - prob: 1.000\n",
            "\n",
            "Interval 38602 (386010 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.639 - mean_q: 13.169 - prob: 1.000\n",
            "\n",
            "Interval 38603 (386020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.252 - mean_q: 12.500 - prob: 1.000\n",
            "\n",
            "Interval 38604 (386030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38605 (386040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.319 - mean_q: 12.538 - prob: 1.000\n",
            "\n",
            "Interval 38606 (386050 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.392 - mean_q: 12.561 - prob: 1.000\n",
            "\n",
            "Interval 38607 (386060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38608 (386070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.002 - mae: 7.260 - mean_q: 12.723 - prob: 1.000\n",
            "\n",
            "Interval 38609 (386080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38610 (386090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 8.000 [3.000, 13.000] - loss: 0.001 - mae: 7.158 - mean_q: 12.452 - prob: 1.000\n",
            "\n",
            "Interval 38611 (386100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 38612 (386110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.002 - mae: 7.166 - mean_q: 12.564 - prob: 1.000\n",
            "\n",
            "Interval 38613 (386120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.318 - mean_q: 12.663 - prob: 1.000\n",
            "\n",
            "Interval 38614 (386130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 38615 (386140 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.334 - mean_q: 12.515 - prob: 1.000\n",
            "\n",
            "Interval 38616 (386150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38617 (386160 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.532 - mean_q: 12.899 - prob: 1.000\n",
            "\n",
            "Interval 38618 (386170 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.258 - mean_q: 12.475 - prob: 1.000\n",
            "\n",
            "Interval 38619 (386180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 38620 (386190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.266 - mean_q: 12.602 - prob: 1.000\n",
            "\n",
            "Interval 38621 (386200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38622 (386210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.002 - mae: 7.078 - mean_q: 12.298 - prob: 1.000\n",
            "\n",
            "Interval 38623 (386220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38624 (386230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.002 - mae: 7.535 - mean_q: 12.954 - prob: 1.000\n",
            "\n",
            "Interval 38625 (386240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.252 - mean_q: 12.519 - prob: 1.000\n",
            "\n",
            "Interval 38626 (386250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.385 - mean_q: 12.634 - prob: 1.000\n",
            "\n",
            "Interval 38627 (386260 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.332 - mean_q: 12.546 - prob: 1.000\n",
            "\n",
            "Interval 38628 (386270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38629 (386280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.078 - mean_q: 12.324 - prob: 1.000\n",
            "\n",
            "Interval 38630 (386290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 38631 (386300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.002 - mae: 7.290 - mean_q: 12.705 - prob: 1.000\n",
            "\n",
            "Interval 38632 (386310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.435 - mean_q: 12.777 - prob: 1.000\n",
            "\n",
            "Interval 38633 (386320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 38634 (386330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.371 - mean_q: 12.646 - prob: 1.000\n",
            "\n",
            "Interval 38635 (386340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.352 - mean_q: 12.639 - prob: 1.000\n",
            "\n",
            "Interval 38636 (386350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.707 - mean_q: 13.133 - prob: 1.000\n",
            "\n",
            "Interval 38637 (386360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38638 (386370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.002 - mae: 7.452 - mean_q: 12.791 - prob: 1.000\n",
            "\n",
            "Interval 38639 (386380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.003 - mae: 7.677 - mean_q: 13.077 - prob: 1.000\n",
            "\n",
            "Interval 38640 (386390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.003 - mae: 6.955 - mean_q: 12.107 - prob: 1.000\n",
            "\n",
            "Interval 38641 (386400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38642 (386410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.003 - mae: 7.199 - mean_q: 12.367 - prob: 1.000\n",
            "\n",
            "Interval 38643 (386420 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.004 - mae: 7.381 - mean_q: 12.619 - prob: 1.000\n",
            "\n",
            "Interval 38644 (386430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38645 (386440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.473 - mean_q: 12.859 - prob: 1.000\n",
            "\n",
            "Interval 38646 (386450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38647 (386460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.244 - mean_q: 12.390 - prob: 1.000\n",
            "\n",
            "Interval 38648 (386470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.004 - mae: 7.316 - mean_q: 12.585 - prob: 1.000\n",
            "\n",
            "Interval 38649 (386480 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38650 (386490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.003 - mae: 6.973 - mean_q: 12.208 - prob: 1.000\n",
            "\n",
            "Interval 38651 (386500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 38652 (386510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.005 - mae: 7.270 - mean_q: 12.463 - prob: 1.000\n",
            "\n",
            "Interval 38653 (386520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38654 (386530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.005 - mae: 7.570 - mean_q: 13.035 - prob: 1.000\n",
            "\n",
            "Interval 38655 (386540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.005 - mae: 7.242 - mean_q: 12.322 - prob: 1.000\n",
            "\n",
            "Interval 38656 (386550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38657 (386560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.162 - mean_q: 12.504 - prob: 1.000\n",
            "\n",
            "Interval 38658 (386570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.003 - mae: 7.280 - mean_q: 12.608 - prob: 1.000\n",
            "\n",
            "Interval 38659 (386580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 38660 (386590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.003 - mae: 7.259 - mean_q: 12.523 - prob: 1.000\n",
            "\n",
            "Interval 38661 (386600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.244 - mean_q: 12.433 - prob: 1.000\n",
            "\n",
            "Interval 38662 (386610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 38663 (386620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.002 - mae: 7.122 - mean_q: 12.404 - prob: 1.000\n",
            "\n",
            "Interval 38664 (386630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.523 - mean_q: 13.027 - prob: 1.000\n",
            "\n",
            "Interval 38665 (386640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.002 - mae: 7.134 - mean_q: 12.423 - prob: 1.000\n",
            "\n",
            "Interval 38666 (386650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38667 (386660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.002 - mae: 7.327 - mean_q: 12.723 - prob: 1.000\n",
            "\n",
            "Interval 38668 (386670 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.792 - mean_q: 13.385 - prob: 1.000\n",
            "\n",
            "Interval 38669 (386680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.572 - mean_q: 13.105 - prob: 1.000\n",
            "\n",
            "Interval 38670 (386690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38671 (386700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38672 (386710 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.017 - mae: 7.408 - mean_q: 12.783 - prob: 1.000\n",
            "\n",
            "Interval 38673 (386720 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.009 - mae: 7.542 - mean_q: 12.977 - prob: 1.000\n",
            "\n",
            "Interval 38674 (386730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.008 - mae: 7.734 - mean_q: 13.329 - prob: 1.000\n",
            "\n",
            "Interval 38675 (386740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 38676 (386750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.004 - mae: 7.453 - mean_q: 12.874 - prob: 1.000\n",
            "\n",
            "Interval 38677 (386760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.004 - mae: 7.490 - mean_q: 12.878 - prob: 1.000\n",
            "\n",
            "Interval 38678 (386770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38679 (386780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.026 - mean_q: 12.316 - prob: 1.000\n",
            "\n",
            "Interval 38680 (386790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38681 (386800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.002 - mae: 7.477 - mean_q: 12.854 - prob: 1.000\n",
            "\n",
            "Interval 38682 (386810 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 6.974 - mean_q: 12.114 - prob: 1.000\n",
            "\n",
            "Interval 38683 (386820 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38684 (386830 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.450 - mean_q: 12.873 - prob: 1.000\n",
            "\n",
            "Interval 38685 (386840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.002 - mae: 7.705 - mean_q: 13.133 - prob: 1.000\n",
            "\n",
            "Interval 38686 (386850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38687 (386860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.204 - mean_q: 12.374 - prob: 1.000\n",
            "\n",
            "Interval 38688 (386870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.002 - mae: 7.568 - mean_q: 12.969 - prob: 1.000\n",
            "\n",
            "Interval 38689 (386880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 38690 (386890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.492 - mean_q: 12.792 - prob: 1.000\n",
            "\n",
            "Interval 38691 (386900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38692 (386910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.418 - mean_q: 12.808 - prob: 1.000\n",
            "\n",
            "Interval 38693 (386920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38694 (386930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.132 - mean_q: 12.251 - prob: 1.000\n",
            "\n",
            "Interval 38695 (386940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.687 - mean_q: 13.245 - prob: 1.000\n",
            "\n",
            "Interval 38696 (386950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.429 - mean_q: 12.871 - prob: 1.000\n",
            "\n",
            "Interval 38697 (386960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.683 - mean_q: 13.003 - prob: 1.000\n",
            "\n",
            "Interval 38698 (386970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 38699 (386980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.568 - mean_q: 12.978 - prob: 1.000\n",
            "\n",
            "Interval 38700 (386990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.160 - mean_q: 12.380 - prob: 1.000\n",
            "\n",
            "Interval 38701 (387000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.292 - mean_q: 12.603 - prob: 1.000\n",
            "\n",
            "Interval 38702 (387010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38703 (387020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.633 - mean_q: 13.082 - prob: 1.000\n",
            "\n",
            "Interval 38704 (387030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.460 - mean_q: 12.744 - prob: 1.000\n",
            "\n",
            "Interval 38705 (387040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.228 - mean_q: 12.464 - prob: 1.000\n",
            "\n",
            "Interval 38706 (387050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38707 (387060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.192 - mean_q: 12.509 - prob: 1.000\n",
            "\n",
            "Interval 38708 (387070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.242 - mean_q: 12.602 - prob: 1.000\n",
            "\n",
            "Interval 38709 (387080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.566 - mean_q: 12.986 - prob: 1.000\n",
            "\n",
            "Interval 38710 (387090 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 38711 (387100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.001 - mae: 7.645 - mean_q: 13.079 - prob: 1.000\n",
            "\n",
            "Interval 38712 (387110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.465 - mean_q: 12.906 - prob: 1.000\n",
            "\n",
            "Interval 38713 (387120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.406 - mean_q: 12.766 - prob: 1.000\n",
            "\n",
            "Interval 38714 (387130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.001 - mae: 7.125 - mean_q: 12.300 - prob: 1.000\n",
            "\n",
            "Interval 38715 (387140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.481 - mean_q: 13.001 - prob: 1.000\n",
            "\n",
            "Interval 38716 (387150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.251 - mean_q: 12.512 - prob: 1.000\n",
            "\n",
            "Interval 38717 (387160 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38718 (387170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.511 - mean_q: 12.818 - prob: 1.000\n",
            "\n",
            "Interval 38719 (387180 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.481 - mean_q: 12.955 - prob: 1.000\n",
            "\n",
            "Interval 38720 (387190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 38721 (387200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.001 - mae: 7.552 - mean_q: 12.977 - prob: 1.000\n",
            "\n",
            "Interval 38722 (387210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 38723 (387220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.028 - mean_q: 12.288 - prob: 1.000\n",
            "\n",
            "Interval 38724 (387230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38725 (387240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.356 - mean_q: 12.572 - prob: 1.000\n",
            "\n",
            "Interval 38726 (387250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.271 - mean_q: 12.596 - prob: 1.000\n",
            "\n",
            "Interval 38727 (387260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 38728 (387270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.002 - mae: 7.333 - mean_q: 12.559 - prob: 1.000\n",
            "\n",
            "Interval 38729 (387280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38730 (387290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.433 - mean_q: 12.721 - prob: 1.000\n",
            "\n",
            "Interval 38731 (387300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.178 - mean_q: 12.414 - prob: 1.000\n",
            "\n",
            "Interval 38732 (387310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.680 - mean_q: 13.122 - prob: 1.000\n",
            "\n",
            "Interval 38733 (387320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.741 - mean_q: 13.223 - prob: 1.000\n",
            "\n",
            "Interval 38734 (387330 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.407 - mean_q: 12.794 - prob: 1.000\n",
            "\n",
            "Interval 38735 (387340 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 38736 (387350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.024 - mean_q: 12.095 - prob: 1.000\n",
            "\n",
            "Interval 38737 (387360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.529 - mean_q: 13.090 - prob: 1.000\n",
            "\n",
            "Interval 38738 (387370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.797 - mean_q: 13.247 - prob: 1.000\n",
            "\n",
            "Interval 38739 (387380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.247 - mean_q: 12.512 - prob: 1.000\n",
            "\n",
            "Interval 38740 (387390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38741 (387400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.435 - mean_q: 12.983 - prob: 1.000\n",
            "\n",
            "Interval 38742 (387410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 38743 (387420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.002 - mae: 7.369 - mean_q: 12.651 - prob: 1.000\n",
            "\n",
            "Interval 38744 (387430 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.002 - mae: 7.426 - mean_q: 12.620 - prob: 1.000\n",
            "\n",
            "Interval 38745 (387440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.115 - mean_q: 12.210 - prob: 1.000\n",
            "\n",
            "Interval 38746 (387450 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38747 (387460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.101 - mean_q: 12.281 - prob: 1.000\n",
            "\n",
            "Interval 38748 (387470 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.488 - mean_q: 12.869 - prob: 1.000\n",
            "\n",
            "Interval 38749 (387480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.508 - mean_q: 12.947 - prob: 1.000\n",
            "\n",
            "Interval 38750 (387490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.281 - mean_q: 12.548 - prob: 1.000\n",
            "\n",
            "Interval 38751 (387500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38752 (387510 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.411 - mean_q: 12.761 - prob: 1.000\n",
            "\n",
            "Interval 38753 (387520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38754 (387530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.002 - mae: 7.205 - mean_q: 12.405 - prob: 1.000\n",
            "\n",
            "Interval 38755 (387540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.321 - mean_q: 12.757 - prob: 1.000\n",
            "\n",
            "Interval 38756 (387550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38757 (387560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.002 - mae: 7.471 - mean_q: 12.718 - prob: 1.000\n",
            "\n",
            "Interval 38758 (387570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38759 (387580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.619 - mean_q: 12.947 - prob: 1.000\n",
            "\n",
            "Interval 38760 (387590 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.053 - mean_q: 12.292 - prob: 1.000\n",
            "\n",
            "Interval 38761 (387600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38762 (387610 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.123 - mean_q: 12.302 - prob: 1.000\n",
            "\n",
            "Interval 38763 (387620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.002 - mae: 7.748 - mean_q: 13.289 - prob: 1.000\n",
            "\n",
            "Interval 38764 (387630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 38765 (387640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.002 - mae: 7.161 - mean_q: 12.408 - prob: 1.000\n",
            "\n",
            "Interval 38766 (387650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 38767 (387660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -25.000 [-25.000, -25.000] - loss: 0.003 - mae: 7.658 - mean_q: 13.154 - prob: 1.000\n",
            "\n",
            "Interval 38768 (387670 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.629 - mean_q: 13.156 - prob: 1.000\n",
            "\n",
            "Interval 38769 (387680 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.003 - mae: 7.391 - mean_q: 12.672 - prob: 1.000\n",
            "\n",
            "Interval 38770 (387690 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.446 - mean_q: 12.822 - prob: 1.000\n",
            "\n",
            "Interval 38771 (387700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38772 (387710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.002 - mae: 7.562 - mean_q: 12.996 - prob: 1.000\n",
            "\n",
            "Interval 38773 (387720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.002 - mae: 7.104 - mean_q: 12.330 - prob: 1.000\n",
            "\n",
            "Interval 38774 (387730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38775 (387740 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.377 - mean_q: 12.712 - prob: 1.000\n",
            "\n",
            "Interval 38776 (387750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.471 - mean_q: 12.943 - prob: 1.000\n",
            "\n",
            "Interval 38777 (387760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38778 (387770 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.003 - mae: 7.691 - mean_q: 13.222 - prob: 1.000\n",
            "\n",
            "Interval 38779 (387780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38780 (387790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.222 - mean_q: 12.350 - prob: 1.000\n",
            "\n",
            "Interval 38781 (387800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 7.484 - mean_q: 12.759 - prob: 1.000\n",
            "\n",
            "Interval 38782 (387810 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 38783 (387820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.368 - mean_q: 12.687 - prob: 1.000\n",
            "\n",
            "Interval 38784 (387830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.502 - mean_q: 13.007 - prob: 1.000\n",
            "\n",
            "Interval 38785 (387840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.383 - mean_q: 12.776 - prob: 1.000\n",
            "\n",
            "Interval 38786 (387850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38787 (387860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 10.000 [8.000, 12.000] - loss: 0.002 - mae: 7.134 - mean_q: 12.319 - prob: 1.000\n",
            "\n",
            "Interval 38788 (387870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38789 (387880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.275 - mean_q: 12.613 - prob: 1.000\n",
            "\n",
            "Interval 38790 (387890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38791 (387900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.002 - mae: 7.318 - mean_q: 12.562 - prob: 1.000\n",
            "\n",
            "Interval 38792 (387910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.198 - mean_q: 12.678 - prob: 1.000\n",
            "\n",
            "Interval 38793 (387920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38794 (387930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.288 - mean_q: 12.516 - prob: 1.000\n",
            "\n",
            "Interval 38795 (387940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.321 - mean_q: 12.643 - prob: 1.000\n",
            "\n",
            "Interval 38796 (387950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 38797 (387960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.004 - mae: 7.421 - mean_q: 12.840 - prob: 1.000\n",
            "\n",
            "Interval 38798 (387970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "10/10 [==============================] - 0s 12ms/step - reward: -1.9000\n",
            "Interval 81034 (810330 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.292 - mean_q: 12.520 - prob: 1.000\n",
            "\n",
            "Interval 81035 (810340 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.292 - mean_q: 12.580 - prob: 1.000\n",
            "\n",
            "Interval 81036 (810350 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.408 - mean_q: 12.787 - prob: 1.000\n",
            "\n",
            "Interval 81037 (810360 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 81038 (810370 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.247 - mean_q: 12.356 - prob: 1.000\n",
            "\n",
            "Interval 81039 (810380 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -24.000 [-24.000, -24.000] - loss: 0.001 - mae: 7.342 - mean_q: 12.641 - prob: 1.000\n",
            "\n",
            "Interval 81040 (810390 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.9000\n",
            "Interval 81041 (810400 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.001 - mae: 7.005 - mean_q: 12.284 - prob: 1.000\n",
            "\n",
            "Interval 81042 (810410 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: -1.9000\n",
            "Interval 81043 (810420 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -19.000 [-19.000, -19.000] - loss: 0.001 - mae: 7.188 - mean_q: 12.459 - prob: 1.000\n",
            "\n",
            "Interval 81044 (810430 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.9000\n",
            "Interval 81045 (810440 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.399 - mean_q: 12.749 - prob: 1.000\n",
            "\n",
            "Interval 81046 (810450 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 81047 (810460 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.218 - mean_q: 12.511 - prob: 1.000\n",
            "\n",
            "Interval 81048 (810470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.129 - mean_q: 12.218 - prob: 1.000\n",
            "\n",
            "Interval 81049 (810480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.427 - mean_q: 12.920 - prob: 1.000\n",
            "\n",
            "Interval 81050 (810490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 81051 (810500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.432 - mean_q: 12.812 - prob: 1.000\n",
            "\n",
            "Interval 81052 (810510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.200 - mean_q: 12.471 - prob: 1.000\n",
            "\n",
            "Interval 81053 (810520 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.002 - mae: 7.202 - mean_q: 12.482 - prob: 1.000\n",
            "\n",
            "Interval 81054 (810530 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 81055 (810540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.638 - mean_q: 13.128 - prob: 1.000\n",
            "\n",
            "Interval 81056 (810550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.199 - mean_q: 12.419 - prob: 1.000\n",
            "\n",
            "Interval 81057 (810560 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 81058 (810570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.186 - mean_q: 12.559 - prob: 1.000\n",
            "\n",
            "Interval 81059 (810580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.470 - mean_q: 12.883 - prob: 1.000\n",
            "\n",
            "Interval 81060 (810590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.247 - mean_q: 12.530 - prob: 1.000\n",
            "\n",
            "Interval 81061 (810600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 81062 (810610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.435 - mean_q: 12.744 - prob: 1.000\n",
            "\n",
            "Interval 81063 (810620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.345 - mean_q: 12.605 - prob: 1.000\n",
            "\n",
            "Interval 81064 (810630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.198 - mean_q: 12.534 - prob: 1.000\n",
            "\n",
            "Interval 81065 (810640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 81066 (810650 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.001 - mae: 7.262 - mean_q: 12.593 - prob: 1.000\n",
            "\n",
            "Interval 81067 (810660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.448 - mean_q: 12.922 - prob: 1.000\n",
            "\n",
            "Interval 81068 (810670 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 81069 (810680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.001 - mae: 7.362 - mean_q: 12.634 - prob: 1.000\n",
            "\n",
            "Interval 81070 (810690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 81071 (810700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.244 - mean_q: 12.660 - prob: 1.000\n",
            "\n",
            "Interval 81072 (810710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.805 - mean_q: 13.339 - prob: 1.000\n",
            "\n",
            "Interval 81073 (810720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 81074 (810730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.266 - mean_q: 12.489 - prob: 1.000\n",
            "\n",
            "Interval 81075 (810740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 81076 (810750 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.342 - mean_q: 12.685 - prob: 1.000\n",
            "\n",
            "Interval 81077 (810760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.625 - mean_q: 13.182 - prob: 1.000\n",
            "\n",
            "Interval 81078 (810770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 81079 (810780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 81080 (810790 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: -2.500 [-19.000, 14.000] - loss: 0.001 - mae: 7.590 - mean_q: 13.008 - prob: 1.000\n",
            "\n",
            "Interval 81081 (810800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 81082 (810810 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.282 - mean_q: 12.631 - prob: 1.000\n",
            "\n",
            "Interval 81083 (810820 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.261 - mean_q: 12.511 - prob: 1.000\n",
            "\n",
            "Interval 81084 (810830 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.467 - mean_q: 12.868 - prob: 1.000\n",
            "\n",
            "Interval 81085 (810840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 81086 (810850 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.491 - mean_q: 12.851 - prob: 1.000\n",
            "\n",
            "Interval 81087 (810860 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.375 - mean_q: 12.836 - prob: 1.000\n",
            "\n",
            "Interval 81088 (810870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 81089 (810880 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.380 - mean_q: 12.713 - prob: 1.000\n",
            "\n",
            "Interval 81090 (810890 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 81091 (810900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.136 - mean_q: 12.337 - prob: 1.000\n",
            "\n",
            "Interval 81092 (810910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 81093 (810920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.215 - mean_q: 12.496 - prob: 1.000\n",
            "\n",
            "Interval 81094 (810930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.308 - mean_q: 12.593 - prob: 1.000\n",
            "\n",
            "Interval 81095 (810940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.600 - mean_q: 13.149 - prob: 1.000\n",
            "\n",
            "Interval 81096 (810950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 81097 (810960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.003 - mae: 7.000 - mean_q: 12.116 - prob: 1.000\n",
            "\n",
            "Interval 81098 (810970 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.490 - mean_q: 12.788 - prob: 1.000\n",
            "\n",
            "Interval 81099 (810980 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 81100 (810990 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.218 - mean_q: 12.574 - prob: 1.000\n",
            "\n",
            "Interval 81101 (811000 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 81102 (811010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.001 - mae: 7.615 - mean_q: 13.049 - prob: 1.000\n",
            "\n",
            "Interval 81103 (811020 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 81104 (811030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.001 - mae: 7.557 - mean_q: 13.165 - prob: 1.000\n",
            "\n",
            "Interval 81105 (811040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.817 - mean_q: 13.264 - prob: 1.000\n",
            "\n",
            "Interval 81106 (811050 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 81107 (811060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.346 - mean_q: 12.674 - prob: 1.000\n",
            "\n",
            "Interval 81108 (811070 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.694 - mean_q: 13.085 - prob: 1.000\n",
            "\n",
            "Interval 81109 (811080 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 81110 (811090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.432 - mean_q: 12.687 - prob: 1.000\n",
            "\n",
            "Interval 81111 (811100 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 81112 (811110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.687 - mean_q: 13.179 - prob: 1.000\n",
            "\n",
            "Interval 81113 (811120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 81114 (811130 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: -0.500 [-16.000, 15.000] - loss: 0.001 - mae: 7.470 - mean_q: 12.924 - prob: 1.000\n",
            "\n",
            "Interval 81115 (811140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 81116 (811150 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 81117 (811160 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 2.000 [-9.000, 13.000] - loss: 0.001 - mae: 7.254 - mean_q: 12.505 - prob: 1.000\n",
            "\n",
            "Interval 81118 (811170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 81119 (811180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.189 - mean_q: 12.651 - prob: 1.000\n",
            "\n",
            "Interval 81120 (811190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 81121 (811200 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.525 - mean_q: 12.963 - prob: 1.000\n",
            "\n",
            "Interval 81122 (811210 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.439 - mean_q: 12.802 - prob: 1.000\n",
            "\n",
            "Interval 81123 (811220 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.349 - mean_q: 12.776 - prob: 1.000\n",
            "\n",
            "Interval 81124 (811230 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.443 - mean_q: 12.726 - prob: 1.000\n",
            "\n",
            "Interval 81125 (811240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.329 - mean_q: 12.666 - prob: 1.000\n",
            "\n",
            "Interval 81126 (811250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 81127 (811260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.001 - mae: 7.214 - mean_q: 12.460 - prob: 1.000\n",
            "\n",
            "Interval 81128 (811270 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 81129 (811280 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.551 - mean_q: 13.005 - prob: 1.000\n",
            "\n",
            "Interval 81130 (811290 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.383 - mean_q: 12.806 - prob: 1.000\n",
            "\n",
            "Interval 81131 (811300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.062 - mean_q: 12.163 - prob: 1.000\n",
            "\n",
            "Interval 81132 (811310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.362 - mean_q: 12.663 - prob: 1.000\n",
            "\n",
            "Interval 81133 (811320 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 81134 (811330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.193 - mean_q: 12.410 - prob: 1.000\n",
            "\n",
            "Interval 81135 (811340 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 81136 (811350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.596 - mean_q: 12.893 - prob: 1.000\n",
            "\n",
            "Interval 81137 (811360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.001 - mae: 7.491 - mean_q: 12.845 - prob: 1.000\n",
            "\n",
            "Interval 81138 (811370 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 81139 (811380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -26.000 [-26.000, -26.000] - loss: 0.001 - mae: 7.058 - mean_q: 12.206 - prob: 1.000\n",
            "\n",
            "Interval 81140 (811390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 81141 (811400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.270 - mean_q: 12.538 - prob: 1.000\n",
            "\n",
            "Interval 81142 (811410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 81143 (811420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.231 - mean_q: 12.471 - prob: 1.000\n",
            "\n",
            "Interval 81144 (811430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 81145 (811440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.096 - mean_q: 12.424 - prob: 1.000\n",
            "\n",
            "Interval 81146 (811450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.415 - mean_q: 12.745 - prob: 1.000\n",
            "\n",
            "Interval 81147 (811460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.367 - mean_q: 12.732 - prob: 1.000\n",
            "\n",
            "Interval 81148 (811470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.662 - mean_q: 13.097 - prob: 1.000\n",
            "\n",
            "Interval 81149 (811480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.197 - mean_q: 12.628 - prob: 1.000\n",
            "\n",
            "Interval 81150 (811490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 81151 (811500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.001 - mae: 7.340 - mean_q: 12.725 - prob: 1.000\n",
            "\n",
            "Interval 81152 (811510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.411 - mean_q: 12.809 - prob: 1.000\n",
            "\n",
            "Interval 81153 (811520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 81154 (811530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.404 - mean_q: 12.758 - prob: 1.000\n",
            "\n",
            "Interval 81155 (811540 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 81156 (811550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.099 - mean_q: 12.358 - prob: 1.000\n",
            "\n",
            "Interval 81157 (811560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.761 - mean_q: 13.272 - prob: 1.000\n",
            "\n",
            "Interval 81158 (811570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 81159 (811580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.264 - mean_q: 12.546 - prob: 1.000\n",
            "\n",
            "Interval 81160 (811590 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.000 - mae: 7.257 - mean_q: 12.545 - prob: 1.000\n",
            "\n",
            "Interval 81161 (811600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 81162 (811610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.708 - mean_q: 13.230 - prob: 1.000\n",
            "\n",
            "Interval 81163 (811620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 81164 (811630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 6.990 - mean_q: 12.133 - prob: 1.000\n",
            "\n",
            "Interval 81165 (811640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.225 - mean_q: 12.564 - prob: 1.000\n",
            "\n",
            "Interval 81166 (811650 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 81167 (811660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.491 - mean_q: 12.917 - prob: 1.000\n",
            "\n",
            "Interval 81168 (811670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.359 - mean_q: 12.852 - prob: 1.000\n",
            "\n",
            "Interval 81169 (811680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 81170 (811690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.056 - mean_q: 12.157 - prob: 1.000\n",
            "\n",
            "Interval 81171 (811700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.538 - mean_q: 12.847 - prob: 1.000\n",
            "\n",
            "Interval 81172 (811710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.351 - mean_q: 12.639 - prob: 1.000\n",
            "\n",
            "Interval 81173 (811720 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 81174 (811730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.482 - mean_q: 12.942 - prob: 1.000\n",
            "\n",
            "Interval 81175 (811740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 81176 (811750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.414 - mean_q: 12.727 - prob: 1.000\n",
            "\n",
            "Interval 81177 (811760 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.268 - mean_q: 12.474 - prob: 1.000\n",
            "\n",
            "Interval 81178 (811770 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 81179 (811780 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -17.000 [-17.000, -17.000] - loss: 0.001 - mae: 7.183 - mean_q: 12.414 - prob: 1.000\n",
            "\n",
            "Interval 81180 (811790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 81181 (811800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.799 - mean_q: 13.474 - prob: 1.000\n",
            "\n",
            "Interval 81182 (811810 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 81183 (811820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.027 - mean_q: 12.177 - prob: 1.000\n",
            "\n",
            "Interval 81184 (811830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 81185 (811840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.180 - mean_q: 12.417 - prob: 1.000\n",
            "\n",
            "Interval 81186 (811850 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 81187 (811860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.214 - mean_q: 12.431 - prob: 1.000\n",
            "\n",
            "Interval 81188 (811870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.416 - mean_q: 12.776 - prob: 1.000\n",
            "\n",
            "Interval 81189 (811880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.465 - mean_q: 12.852 - prob: 1.000\n",
            "\n",
            "Interval 81190 (811890 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 81191 (811900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.171 - mean_q: 12.452 - prob: 1.000\n",
            "\n",
            "Interval 81192 (811910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.200 - mean_q: 12.510 - prob: 1.000\n",
            "\n",
            "Interval 81193 (811920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 81194 (811930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.422 - mean_q: 12.808 - prob: 1.000\n",
            "\n",
            "Interval 81195 (811940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 81196 (811950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.002 - mae: 7.456 - mean_q: 12.910 - prob: 1.000\n",
            "\n",
            "Interval 81197 (811960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.327 - mean_q: 12.613 - prob: 1.000\n",
            "\n",
            "Interval 81198 (811970 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 81199 (811980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -24.000 [-24.000, -24.000] - loss: 0.002 - mae: 7.336 - mean_q: 12.730 - prob: 1.000\n",
            "\n",
            "Interval 81200 (811990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.002 - mae: 7.299 - mean_q: 12.575 - prob: 1.000\n",
            "\n",
            "Interval 81201 (812000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.245 - mean_q: 12.365 - prob: 1.000\n",
            "\n",
            "Interval 81202 (812010 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.164 - mean_q: 12.514 - prob: 1.000\n",
            "\n",
            "Interval 81203 (812020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 81204 (812030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.336 - mean_q: 12.630 - prob: 1.000\n",
            "\n",
            "Interval 81205 (812040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.275 - mean_q: 12.524 - prob: 1.000\n",
            "\n",
            "Interval 81206 (812050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 81207 (812060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.250 - mean_q: 12.564 - prob: 1.000\n",
            "\n",
            "Interval 81208 (812070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.396 - mean_q: 12.852 - prob: 1.000\n",
            "\n",
            "Interval 81209 (812080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.555 - mean_q: 12.956 - prob: 1.000\n",
            "\n",
            "Interval 81210 (812090 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 81211 (812100 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 81212 (812110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -21.000 [-21.000, -21.000] - loss: 0.001 - mae: 7.325 - mean_q: 12.584 - prob: 1.000\n",
            "\n",
            "Interval 81213 (812120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.335 - mean_q: 12.643 - prob: 1.000\n",
            "\n",
            "Interval 81214 (812130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.520 - mean_q: 12.825 - prob: 1.000\n",
            "\n",
            "Interval 81215 (812140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 81216 (812150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.497 - mean_q: 12.863 - prob: 1.000\n",
            "\n",
            "Interval 81217 (812160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.753 - mean_q: 13.209 - prob: 1.000\n",
            "\n",
            "Interval 81218 (812170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.713 - mean_q: 13.352 - prob: 1.000\n",
            "\n",
            "Interval 81219 (812180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.103 - mean_q: 12.445 - prob: 1.000\n",
            "\n",
            "Interval 81220 (812190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 81221 (812200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.111 - mean_q: 12.531 - prob: 1.000\n",
            "\n",
            "Interval 81222 (812210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.426 - mean_q: 12.775 - prob: 1.000\n",
            "\n",
            "Interval 81223 (812220 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 81224 (812230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.148 - mean_q: 12.400 - prob: 1.000\n",
            "\n",
            "Interval 81225 (812240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.206 - mean_q: 12.571 - prob: 1.000\n",
            "\n",
            "Interval 81226 (812250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.585 - mean_q: 13.051 - prob: 1.000\n",
            "\n",
            "Interval 81227 (812260 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 81228 (812270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.468 - mean_q: 12.842 - prob: 1.000\n",
            "\n",
            "Interval 81229 (812280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 81230 (812290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -17.000 [-17.000, -17.000] - loss: 0.003 - mae: 7.326 - mean_q: 12.655 - prob: 1.000\n",
            "\n",
            "Interval 81231 (812300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.300 - mean_q: 12.656 - prob: 1.000\n",
            "\n",
            "Interval 81232 (812310 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 81233 (812320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 81234 (812330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.004 - mae: 7.648 - mean_q: 13.104 - prob: 1.000\n",
            "\n",
            "Interval 81235 (812340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.002 - mae: 7.319 - mean_q: 12.623 - prob: 1.000\n",
            "\n",
            "Interval 81236 (812350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 81237 (812360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.002 - mae: 7.089 - mean_q: 12.342 - prob: 1.000\n",
            "\n",
            "Interval 81238 (812370 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 6.992 - mean_q: 12.097 - prob: 1.000\n",
            "\n",
            "Interval 81239 (812380 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 81240 (812390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.474 - mean_q: 12.703 - prob: 1.000\n",
            "\n",
            "Interval 90183 (901820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.426 - mean_q: 12.772 - prob: 1.000\n",
            "\n",
            "Interval 90184 (901830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 90185 (901840 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.301 - mean_q: 12.683 - prob: 1.000\n",
            "\n",
            "Interval 90186 (901850 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.708 - mean_q: 13.281 - prob: 1.000\n",
            "\n",
            "Interval 90187 (901860 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 90188 (901870 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.136 - mean_q: 12.466 - prob: 1.000\n",
            "\n",
            "Interval 90189 (901880 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 90190 (901890 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.559 - mean_q: 12.979 - prob: 1.000\n",
            "\n",
            "Interval 90191 (901900 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.386 - mean_q: 12.714 - prob: 1.000\n",
            "\n",
            "Interval 90192 (901910 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 90193 (901920 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.001 - mae: 7.418 - mean_q: 12.863 - prob: 1.000\n",
            "\n",
            "Interval 90194 (901930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90195 (901940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.079 - mean_q: 12.305 - prob: 1.000\n",
            "\n",
            "Interval 90196 (901950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.470 - mean_q: 12.827 - prob: 1.000\n",
            "\n",
            "Interval 90197 (901960 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 90198 (901970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.658 - mean_q: 13.075 - prob: 1.000\n",
            "\n",
            "Interval 90199 (901980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 6.976 - mean_q: 12.121 - prob: 1.000\n",
            "\n",
            "Interval 90200 (901990 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 90201 (902000 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.000 - mae: 7.584 - mean_q: 13.020 - prob: 1.000\n",
            "\n",
            "Interval 90202 (902010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90203 (902020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.114 - mean_q: 12.345 - prob: 1.000\n",
            "\n",
            "Interval 90204 (902030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.320 - mean_q: 12.664 - prob: 1.000\n",
            "\n",
            "Interval 90205 (902040 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -3.7000\n",
            "Interval 90206 (902050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -21.000 [-21.000, -21.000] - loss: 0.001 - mae: 7.079 - mean_q: 12.266 - prob: 1.000\n",
            "\n",
            "Interval 90207 (902060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.539 - mean_q: 12.987 - prob: 1.000\n",
            "\n",
            "Interval 90208 (902070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.410 - mean_q: 12.728 - prob: 1.000\n",
            "\n",
            "Interval 90209 (902080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.508 - mean_q: 12.869 - prob: 1.000\n",
            "\n",
            "Interval 90210 (902090 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.263 - mean_q: 12.442 - prob: 1.000\n",
            "\n",
            "Interval 90211 (902100 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 90212 (902110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.479 - mean_q: 12.812 - prob: 1.000\n",
            "\n",
            "Interval 90213 (902120 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 90214 (902130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.180 - mean_q: 12.464 - prob: 1.000\n",
            "\n",
            "Interval 90215 (902140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.346 - mean_q: 12.670 - prob: 1.000\n",
            "\n",
            "Interval 90216 (902150 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 90217 (902160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.001 - mae: 7.445 - mean_q: 12.776 - prob: 1.000\n",
            "\n",
            "Interval 90218 (902170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.483 - mean_q: 12.895 - prob: 1.000\n",
            "\n",
            "Interval 90219 (902180 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.408 - mean_q: 12.639 - prob: 1.000\n",
            "\n",
            "Interval 90220 (902190 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 90221 (902200 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 7.500 [0.000, 15.000] - loss: 0.002 - mae: 7.635 - mean_q: 13.094 - prob: 1.000\n",
            "\n",
            "Interval 90222 (902210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 90223 (902220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -20.000 [-20.000, -20.000] - loss: 0.001 - mae: 7.171 - mean_q: 12.485 - prob: 1.000\n",
            "\n",
            "Interval 90224 (902230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.519 - mean_q: 12.747 - prob: 1.000\n",
            "\n",
            "Interval 90225 (902240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.366 - mean_q: 12.749 - prob: 1.000\n",
            "\n",
            "Interval 90226 (902250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 90227 (902260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.266 - mean_q: 12.635 - prob: 1.000\n",
            "\n",
            "Interval 90228 (902270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.245 - mean_q: 12.514 - prob: 1.000\n",
            "\n",
            "Interval 90229 (902280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90230 (902290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.320 - mean_q: 12.777 - prob: 1.000\n",
            "\n",
            "Interval 90231 (902300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.002 - mae: 7.217 - mean_q: 12.538 - prob: 1.000\n",
            "\n",
            "Interval 90232 (902310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90233 (902320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.422 - mean_q: 12.757 - prob: 1.000\n",
            "\n",
            "Interval 90234 (902330 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.218 - mean_q: 12.552 - prob: 1.000\n",
            "\n",
            "Interval 90235 (902340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 90236 (902350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.160 - mean_q: 12.495 - prob: 1.000\n",
            "\n",
            "Interval 90237 (902360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.237 - mean_q: 12.527 - prob: 1.000\n",
            "\n",
            "Interval 90238 (902370 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 90239 (902380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.300 - mean_q: 12.733 - prob: 1.000\n",
            "\n",
            "Interval 90240 (902390 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 90241 (902400 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.143 - mean_q: 12.419 - prob: 1.000\n",
            "\n",
            "Interval 90242 (902410 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 90243 (902420 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.530 - mean_q: 12.928 - prob: 1.000\n",
            "\n",
            "Interval 90244 (902430 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.163 - mean_q: 12.271 - prob: 1.000\n",
            "\n",
            "Interval 90245 (902440 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -2.8000\n",
            "Interval 90246 (902450 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.001 - mae: 7.534 - mean_q: 12.860 - prob: 1.000\n",
            "\n",
            "Interval 90247 (902460 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.277 - mean_q: 12.566 - prob: 1.000\n",
            "\n",
            "Interval 90248 (902470 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.428 - mean_q: 12.779 - prob: 1.000\n",
            "\n",
            "Interval 90249 (902480 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 90250 (902490 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 90251 (902500 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.001 - mae: 7.393 - mean_q: 12.694 - prob: 1.000\n",
            "\n",
            "Interval 90252 (902510 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.521 - mean_q: 12.896 - prob: 1.000\n",
            "\n",
            "Interval 90253 (902520 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 90254 (902530 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.293 - mean_q: 12.492 - prob: 1.000\n",
            "\n",
            "Interval 90255 (902540 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.000 - mae: 7.482 - mean_q: 12.916 - prob: 1.000\n",
            "\n",
            "Interval 90256 (902550 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 90257 (902560 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.513 - mean_q: 12.982 - prob: 1.000\n",
            "\n",
            "Interval 90258 (902570 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -2.8000\n",
            "Interval 90259 (902580 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -27.000 [-27.000, -27.000] - loss: 0.001 - mae: 7.255 - mean_q: 12.615 - prob: 1.000\n",
            "\n",
            "Interval 90260 (902590 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.218 - mean_q: 12.290 - prob: 1.000\n",
            "\n",
            "Interval 90261 (902600 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 90262 (902610 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.455 - mean_q: 12.655 - prob: 1.000\n",
            "\n",
            "Interval 90263 (902620 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.323 - mean_q: 12.630 - prob: 1.000\n",
            "\n",
            "Interval 90264 (902630 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 90265 (902640 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.323 - mean_q: 12.842 - prob: 1.000\n",
            "\n",
            "Interval 90266 (902650 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.253 - mean_q: 12.586 - prob: 1.000\n",
            "\n",
            "Interval 90267 (902660 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.082 - mean_q: 12.208 - prob: 1.000\n",
            "\n",
            "Interval 90268 (902670 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 90269 (902680 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.551 - mean_q: 13.001 - prob: 1.000\n",
            "\n",
            "Interval 90270 (902690 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.001 - mae: 7.315 - mean_q: 12.595 - prob: 1.000\n",
            "\n",
            "Interval 90271 (902700 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.277 - mean_q: 12.591 - prob: 1.000\n",
            "\n",
            "Interval 90272 (902710 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 90273 (902720 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.309 - mean_q: 12.634 - prob: 1.000\n",
            "\n",
            "Interval 90274 (902730 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.415 - mean_q: 12.757 - prob: 1.000\n",
            "\n",
            "Interval 90275 (902740 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 90276 (902750 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.729 - mean_q: 13.195 - prob: 1.000\n",
            "\n",
            "Interval 90277 (902760 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.437 - mean_q: 12.817 - prob: 1.000\n",
            "\n",
            "Interval 90278 (902770 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.9000\n",
            "Interval 90279 (902780 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.524 - mean_q: 12.820 - prob: 1.000\n",
            "\n",
            "Interval 90280 (902790 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.279 - mean_q: 12.605 - prob: 1.000\n",
            "\n",
            "Interval 90281 (902800 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.227 - mean_q: 12.637 - prob: 1.000\n",
            "\n",
            "Interval 90282 (902810 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.091 - mean_q: 12.365 - prob: 1.000\n",
            "\n",
            "Interval 90283 (902820 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 90284 (902830 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.121 - mean_q: 12.391 - prob: 1.000\n",
            "\n",
            "Interval 90285 (902840 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.507 - mean_q: 12.863 - prob: 1.000\n",
            "\n",
            "Interval 90286 (902850 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 90287 (902860 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.329 - mean_q: 12.675 - prob: 1.000\n",
            "\n",
            "Interval 90288 (902870 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 90289 (902880 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.138 - mean_q: 12.441 - prob: 1.000\n",
            "\n",
            "Interval 90290 (902890 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.273 - mean_q: 12.616 - prob: 1.000\n",
            "\n",
            "Interval 90291 (902900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90292 (902910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.469 - mean_q: 13.054 - prob: 1.000\n",
            "\n",
            "Interval 90293 (902920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 90294 (902930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.213 - mean_q: 12.530 - prob: 1.000\n",
            "\n",
            "Interval 90295 (902940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.437 - mean_q: 12.806 - prob: 1.000\n",
            "\n",
            "Interval 90296 (902950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90297 (902960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.262 - mean_q: 12.585 - prob: 1.000\n",
            "\n",
            "Interval 90298 (902970 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 90299 (902980 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.280 - mean_q: 12.544 - prob: 1.000\n",
            "\n",
            "Interval 90300 (902990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.598 - mean_q: 13.066 - prob: 1.000\n",
            "\n",
            "Interval 90301 (903000 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.525 - mean_q: 12.982 - prob: 1.000\n",
            "\n",
            "Interval 90302 (903010 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 90303 (903020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.002 - mae: 7.183 - mean_q: 12.377 - prob: 1.000\n",
            "\n",
            "Interval 90304 (903030 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 90305 (903040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.279 - mean_q: 12.632 - prob: 1.000\n",
            "\n",
            "Interval 90306 (903050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.268 - mean_q: 12.480 - prob: 1.000\n",
            "\n",
            "Interval 90307 (903060 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 90308 (903070 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.198 - mean_q: 12.594 - prob: 1.000\n",
            "\n",
            "Interval 90309 (903080 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.678 - mean_q: 13.153 - prob: 1.000\n",
            "\n",
            "Interval 90310 (903090 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.205 - mean_q: 12.529 - prob: 1.000\n",
            "\n",
            "Interval 90311 (903100 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 90312 (903110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.255 - mean_q: 12.533 - prob: 1.000\n",
            "\n",
            "Interval 90313 (903120 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.229 - mean_q: 12.512 - prob: 1.000\n",
            "\n",
            "Interval 90314 (903130 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 90315 (903140 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -19.000 [-19.000, -19.000] - loss: 0.001 - mae: 7.462 - mean_q: 12.931 - prob: 1.000\n",
            "\n",
            "Interval 90316 (903150 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 90317 (903160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.366 - mean_q: 12.787 - prob: 1.000\n",
            "\n",
            "Interval 90318 (903170 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.447 - mean_q: 12.839 - prob: 1.000\n",
            "\n",
            "Interval 90319 (903180 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.317 - mean_q: 12.511 - prob: 1.000\n",
            "\n",
            "Interval 90320 (903190 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 90321 (903200 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.278 - mean_q: 12.558 - prob: 1.000\n",
            "\n",
            "Interval 90322 (903210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.355 - mean_q: 12.732 - prob: 1.000\n",
            "\n",
            "Interval 90323 (903220 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 90324 (903230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.371 - mean_q: 12.709 - prob: 1.000\n",
            "\n",
            "Interval 90325 (903240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.001 - mae: 7.660 - mean_q: 13.274 - prob: 1.000\n",
            "\n",
            "Interval 90326 (903250 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 90327 (903260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.151 - mean_q: 12.327 - prob: 1.000\n",
            "\n",
            "Interval 90328 (903270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90329 (903280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.002 - mae: 7.394 - mean_q: 12.588 - prob: 1.000\n",
            "\n",
            "Interval 90330 (903290 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.529 - mean_q: 12.984 - prob: 1.000\n",
            "\n",
            "Interval 90331 (903300 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 90332 (903310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.249 - mean_q: 12.513 - prob: 1.000\n",
            "\n",
            "Interval 90333 (903320 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.248 - mean_q: 12.568 - prob: 1.000\n",
            "\n",
            "Interval 90334 (903330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.000 - mae: 7.297 - mean_q: 12.582 - prob: 1.000\n",
            "\n",
            "Interval 90335 (903340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.372 - mean_q: 12.793 - prob: 1.000\n",
            "\n",
            "Interval 90336 (903350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90337 (903360 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.178 - mean_q: 12.452 - prob: 1.000\n",
            "\n",
            "Interval 90338 (903370 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 90339 (903380 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -29.000 [-29.000, -29.000] - loss: 0.001 - mae: 7.349 - mean_q: 12.728 - prob: 1.000\n",
            "\n",
            "Interval 90340 (903390 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 90341 (903400 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -17.000 [-17.000, -17.000] - loss: 0.001 - mae: 7.278 - mean_q: 12.536 - prob: 1.000\n",
            "\n",
            "Interval 90342 (903410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.189 - mean_q: 12.451 - prob: 1.000\n",
            "\n",
            "Interval 90343 (903420 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 90344 (903430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.230 - mean_q: 12.605 - prob: 1.000\n",
            "\n",
            "Interval 90345 (903440 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 90346 (903450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 10.000 [7.000, 13.000] - loss: 0.001 - mae: 7.197 - mean_q: 12.552 - prob: 1.000\n",
            "\n",
            "Interval 90347 (903460 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 90348 (903470 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.649 - mean_q: 13.105 - prob: 1.000\n",
            "\n",
            "Interval 90349 (903480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.002 - mae: 7.612 - mean_q: 12.888 - prob: 1.000\n",
            "\n",
            "Interval 90350 (903490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.402 - mean_q: 12.703 - prob: 1.000\n",
            "\n",
            "Interval 90351 (903500 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 90352 (903510 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.439 - mean_q: 12.856 - prob: 1.000\n",
            "\n",
            "Interval 90353 (903520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.580 - mean_q: 13.039 - prob: 1.000\n",
            "\n",
            "Interval 90354 (903530 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.149 - mean_q: 12.305 - prob: 1.000\n",
            "\n",
            "Interval 90355 (903540 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 90356 (903550 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.222 - mean_q: 12.395 - prob: 1.000\n",
            "\n",
            "Interval 90357 (903560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.335 - mean_q: 12.768 - prob: 1.000\n",
            "\n",
            "Interval 90358 (903570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.243 - mean_q: 12.590 - prob: 1.000\n",
            "\n",
            "Interval 90359 (903580 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.463 - mean_q: 12.826 - prob: 1.000\n",
            "\n",
            "Interval 90360 (903590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90361 (903600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.060 - mean_q: 12.355 - prob: 1.000\n",
            "\n",
            "Interval 90362 (903610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.167 - mean_q: 12.505 - prob: 1.000\n",
            "\n",
            "Interval 90363 (903620 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.470 - mean_q: 12.800 - prob: 1.000\n",
            "\n",
            "Interval 90364 (903630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 90365 (903640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.142 - mean_q: 12.435 - prob: 1.000\n",
            "\n",
            "Interval 90366 (903650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 90367 (903660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.000 - mae: 7.479 - mean_q: 12.975 - prob: 1.000\n",
            "\n",
            "Interval 90368 (903670 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 90369 (903680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 7.260 - mean_q: 12.670 - prob: 1.000\n",
            "\n",
            "Interval 90370 (903690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.002 - mae: 7.484 - mean_q: 12.930 - prob: 1.000\n",
            "\n",
            "Interval 90371 (903700 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.452 - mean_q: 12.823 - prob: 1.000\n",
            "\n",
            "Interval 90372 (903710 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 90373 (903720 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.385 - mean_q: 12.737 - prob: 1.000\n",
            "\n",
            "Interval 90374 (903730 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 90375 (903740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.428 - mean_q: 12.775 - prob: 1.000\n",
            "\n",
            "Interval 90376 (903750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.001 - mae: 7.217 - mean_q: 12.424 - prob: 1.000\n",
            "\n",
            "Interval 90377 (903760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.458 - mean_q: 12.758 - prob: 1.000\n",
            "\n",
            "Interval 90378 (903770 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 90379 (903780 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.334 - mean_q: 12.705 - prob: 1.000\n",
            "\n",
            "Interval 90380 (903790 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.411 - mean_q: 12.803 - prob: 1.000\n",
            "\n",
            "Interval 90381 (903800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.423 - mean_q: 13.031 - prob: 1.000\n",
            "\n",
            "Interval 90382 (903810 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 90383 (903820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.506 - mean_q: 12.896 - prob: 1.000\n",
            "\n",
            "Interval 90384 (903830 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 90385 (903840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.388 - mean_q: 12.696 - prob: 1.000\n",
            "\n",
            "Interval 90386 (903850 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 90387 (903860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.346 - mean_q: 12.669 - prob: 1.000\n",
            "\n",
            "Interval 90388 (903870 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.371 - mean_q: 12.628 - prob: 1.000\n",
            "\n",
            "Interval 90389 (903880 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.393 - mean_q: 12.700 - prob: 1.000\n",
            "\n",
            "Interval 90390 (903890 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 90391 (903900 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.001 - mae: 7.409 - mean_q: 12.668 - prob: 1.000\n",
            "\n",
            "Interval 90392 (903910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.576 - mean_q: 12.982 - prob: 1.000\n",
            "\n",
            "Interval 90393 (903920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.398 - mean_q: 12.790 - prob: 1.000\n",
            "\n",
            "Interval 90394 (903930 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.479 - mean_q: 12.856 - prob: 1.000\n",
            "\n",
            "Interval 90395 (903940 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 90396 (903950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.099 - mean_q: 12.353 - prob: 1.000\n",
            "\n",
            "Interval 90397 (903960 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -4.6000\n",
            "Interval 90398 (903970 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 90399 (903980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -58.000 [-58.000, -58.000] - loss: 0.001 - mae: 7.368 - mean_q: 12.771 - prob: 1.000\n",
            "\n",
            "Interval 90400 (903990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.475 - mean_q: 12.764 - prob: 1.000\n",
            "\n",
            "Interval 90401 (904000 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 90402 (904010 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.215 - mean_q: 12.502 - prob: 1.000\n",
            "\n",
            "Interval 90403 (904020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.430 - mean_q: 12.953 - prob: 1.000\n",
            "\n",
            "Interval 90404 (904030 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 90405 (904040 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.398 - mean_q: 12.764 - prob: 1.000\n",
            "\n",
            "Interval 90406 (904050 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 90407 (904060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.455 - mean_q: 12.919 - prob: 1.000\n",
            "\n",
            "Interval 90408 (904070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.397 - mean_q: 12.584 - prob: 1.000\n",
            "\n",
            "Interval 90409 (904080 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 90410 (904090 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.323 - mean_q: 12.713 - prob: 1.000\n",
            "\n",
            "Interval 90411 (904100 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.353 - mean_q: 12.754 - prob: 1.000\n",
            "\n",
            "Interval 90412 (904110 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 90413 (904120 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.124 - mean_q: 12.496 - prob: 1.000\n",
            "\n",
            "Interval 90414 (904130 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.424 - mean_q: 12.881 - prob: 1.000\n",
            "\n",
            "Interval 90415 (904140 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 90416 (904150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.404 - mean_q: 12.756 - prob: 1.000\n",
            "\n",
            "Interval 90417 (904160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.590 - mean_q: 13.217 - prob: 1.000\n",
            "\n",
            "Interval 90418 (904170 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 90419 (904180 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.000 - mae: 6.973 - mean_q: 12.215 - prob: 1.000\n",
            "\n",
            "Interval 90420 (904190 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 90421 (904200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 4.500 [-5.000, 14.000] - loss: 0.001 - mae: 7.249 - mean_q: 12.599 - prob: 1.000\n",
            "\n",
            "Interval 90422 (904210 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 90423 (904220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.076 - mean_q: 12.291 - prob: 1.000\n",
            "\n",
            "Interval 90424 (904230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.000 - mae: 7.435 - mean_q: 12.707 - prob: 1.000\n",
            "\n",
            "Interval 90425 (904240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90426 (904250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.687 - mean_q: 13.120 - prob: 1.000\n",
            "\n",
            "Interval 90427 (904260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.602 - mean_q: 13.162 - prob: 1.000\n",
            "\n",
            "Interval 90428 (904270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.178 - mean_q: 12.348 - prob: 1.000\n",
            "\n",
            "Interval 90429 (904280 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 90430 (904290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.225 - mean_q: 12.528 - prob: 1.000\n",
            "\n",
            "Interval 90431 (904300 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 90432 (904310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.409 - mean_q: 12.685 - prob: 1.000\n",
            "\n",
            "Interval 90433 (904320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.466 - mean_q: 12.807 - prob: 1.000\n",
            "\n",
            "Interval 90434 (904330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.763 - mean_q: 13.253 - prob: 1.000\n",
            "\n",
            "Interval 90435 (904340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90436 (904350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.385 - mean_q: 12.711 - prob: 1.000\n",
            "\n",
            "Interval 90437 (904360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90438 (904370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.251 - mean_q: 12.642 - prob: 1.000\n",
            "\n",
            "Interval 90439 (904380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.023 - mean_q: 12.224 - prob: 1.000\n",
            "\n",
            "Interval 90440 (904390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90441 (904400 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.200 - mean_q: 12.520 - prob: 1.000\n",
            "\n",
            "Interval 90442 (904410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90443 (904420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.490 - mean_q: 12.665 - prob: 1.000\n",
            "\n",
            "Interval 90444 (904430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.261 - mean_q: 12.431 - prob: 1.000\n",
            "\n",
            "Interval 90445 (904440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.614 - mean_q: 13.001 - prob: 1.000\n",
            "\n",
            "Interval 90446 (904450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90447 (904460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 90448 (904470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.000 - mae: 7.459 - mean_q: 12.885 - prob: 1.000\n",
            "\n",
            "Interval 90449 (904480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 6.977 - mean_q: 12.119 - prob: 1.000\n",
            "\n",
            "Interval 90450 (904490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90451 (904500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.415 - mean_q: 12.705 - prob: 1.000\n",
            "\n",
            "Interval 90452 (904510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.628 - mean_q: 13.103 - prob: 1.000\n",
            "\n",
            "Interval 90453 (904520 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 6.966 - mean_q: 12.124 - prob: 1.000\n",
            "\n",
            "Interval 90454 (904530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90455 (904540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.420 - mean_q: 12.604 - prob: 1.000\n",
            "\n",
            "Interval 90456 (904550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90457 (904560 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.393 - mean_q: 12.787 - prob: 1.000\n",
            "\n",
            "Interval 90458 (904570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.271 - mean_q: 12.680 - prob: 1.000\n",
            "\n",
            "Interval 90459 (904580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.258 - mean_q: 12.681 - prob: 1.000\n",
            "\n",
            "Interval 90460 (904590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90461 (904600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.484 - mean_q: 12.876 - prob: 1.000\n",
            "\n",
            "Interval 90462 (904610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.420 - mean_q: 12.876 - prob: 1.000\n",
            "\n",
            "Interval 90463 (904620 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 90464 (904630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.280 - mean_q: 12.636 - prob: 1.000\n",
            "\n",
            "Interval 90465 (904640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.602 - mean_q: 13.060 - prob: 1.000\n",
            "\n",
            "Interval 90466 (904650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.559 - mean_q: 12.908 - prob: 1.000\n",
            "\n",
            "Interval 90467 (904660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90468 (904670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.126 - mean_q: 12.233 - prob: 1.000\n",
            "\n",
            "Interval 90469 (904680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90470 (904690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.644 - mean_q: 13.124 - prob: 1.000\n",
            "\n",
            "Interval 90471 (904700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90472 (904710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.533 - mean_q: 12.975 - prob: 1.000\n",
            "\n",
            "Interval 90473 (904720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.438 - mean_q: 12.770 - prob: 1.000\n",
            "\n",
            "Interval 90474 (904730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.380 - mean_q: 12.606 - prob: 1.000\n",
            "\n",
            "Interval 90475 (904740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.150 - mean_q: 12.349 - prob: 1.000\n",
            "\n",
            "Interval 90476 (904750 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 90477 (904760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.443 - mean_q: 12.775 - prob: 1.000\n",
            "\n",
            "Interval 90478 (904770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.443 - mean_q: 12.843 - prob: 1.000\n",
            "\n",
            "Interval 90479 (904780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.476 - mean_q: 12.876 - prob: 1.000\n",
            "\n",
            "Interval 90480 (904790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.161 - mean_q: 12.340 - prob: 1.000\n",
            "\n",
            "Interval 90481 (904800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.000 - mae: 7.372 - mean_q: 12.775 - prob: 1.000\n",
            "\n",
            "Interval 90482 (904810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90483 (904820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.369 - mean_q: 12.768 - prob: 1.000\n",
            "\n",
            "Interval 90484 (904830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90485 (904840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.199 - mean_q: 12.419 - prob: 1.000\n",
            "\n",
            "Interval 90486 (904850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.351 - mean_q: 12.744 - prob: 1.000\n",
            "\n",
            "Interval 90487 (904860 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 90488 (904870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.415 - mean_q: 12.699 - prob: 1.000\n",
            "\n",
            "Interval 90489 (904880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.436 - mean_q: 12.901 - prob: 1.000\n",
            "\n",
            "Interval 90490 (904890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 90491 (904900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.001 - mae: 7.025 - mean_q: 12.204 - prob: 1.000\n",
            "\n",
            "Interval 90492 (904910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.011 - mean_q: 12.243 - prob: 1.000\n",
            "\n",
            "Interval 90493 (904920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90494 (904930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.000 - mae: 7.691 - mean_q: 13.087 - prob: 1.000\n",
            "\n",
            "Interval 90495 (904940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.001 - mean_q: 12.255 - prob: 1.000\n",
            "\n",
            "Interval 90496 (904950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90497 (904960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.550 - mean_q: 12.891 - prob: 1.000\n",
            "\n",
            "Interval 90498 (904970 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 90499 (904980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.274 - mean_q: 12.597 - prob: 1.000\n",
            "\n",
            "Interval 90500 (904990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.413 - mean_q: 12.679 - prob: 1.000\n",
            "\n",
            "Interval 90501 (905000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.071 - mean_q: 12.355 - prob: 1.000\n",
            "\n",
            "Interval 90502 (905010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.484 - mean_q: 12.953 - prob: 1.000\n",
            "\n",
            "Interval 90503 (905020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90504 (905030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.147 - mean_q: 12.376 - prob: 1.000\n",
            "\n",
            "Interval 90505 (905040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.290 - mean_q: 12.674 - prob: 1.000\n",
            "\n",
            "Interval 90506 (905050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 90507 (905060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -23.000 [-23.000, -23.000] - loss: 0.001 - mae: 7.377 - mean_q: 12.666 - prob: 1.000\n",
            "\n",
            "Interval 90508 (905070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.540 - mean_q: 12.962 - prob: 1.000\n",
            "\n",
            "Interval 90509 (905080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90510 (905090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.287 - mean_q: 12.508 - prob: 1.000\n",
            "\n",
            "Interval 90511 (905100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.281 - mean_q: 12.477 - prob: 1.000\n",
            "\n",
            "Interval 90512 (905110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 90513 (905120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.001 - mae: 7.325 - mean_q: 12.619 - prob: 1.000\n",
            "\n",
            "Interval 90514 (905130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 90515 (905140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.6000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.001 - mae: 7.330 - mean_q: 12.552 - prob: 1.000\n",
            "\n",
            "Interval 90516 (905150 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.001 - mae: 7.412 - mean_q: 12.805 - prob: 1.000\n",
            "\n",
            "Interval 90517 (905160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.350 - mean_q: 12.596 - prob: 1.000\n",
            "\n",
            "Interval 90518 (905170 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 90519 (905180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.529 - mean_q: 12.930 - prob: 1.000\n",
            "\n",
            "Interval 90520 (905190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.364 - mean_q: 12.773 - prob: 1.000\n",
            "\n",
            "Interval 90521 (905200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90522 (905210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 90523 (905220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -23.000 [-23.000, -23.000] - loss: 0.001 - mae: 7.225 - mean_q: 12.535 - prob: 1.000\n",
            "\n",
            "Interval 90524 (905230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.249 - mean_q: 12.517 - prob: 1.000\n",
            "\n",
            "Interval 90525 (905240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.137 - mean_q: 12.317 - prob: 1.000\n",
            "\n",
            "Interval 90526 (905250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90527 (905260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.582 - mean_q: 13.023 - prob: 1.000\n",
            "\n",
            "Interval 90528 (905270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90529 (905280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.123 - mean_q: 12.321 - prob: 1.000\n",
            "\n",
            "Interval 90530 (905290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.245 - mean_q: 12.483 - prob: 1.000\n",
            "\n",
            "Interval 90531 (905300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90532 (905310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.195 - mean_q: 12.391 - prob: 1.000\n",
            "\n",
            "Interval 90533 (905320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.526 - mean_q: 12.806 - prob: 1.000\n",
            "\n",
            "Interval 90534 (905330 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.338 - mean_q: 12.678 - prob: 1.000\n",
            "\n",
            "Interval 90535 (905340 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 90536 (905350 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.404 - mean_q: 12.824 - prob: 1.000\n",
            "\n",
            "Interval 90537 (905360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.626 - mean_q: 13.154 - prob: 1.000\n",
            "\n",
            "Interval 90538 (905370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 6.985 - mean_q: 12.151 - prob: 1.000\n",
            "\n",
            "Interval 90539 (905380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90540 (905390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.313 - mean_q: 12.603 - prob: 1.000\n",
            "\n",
            "Interval 90541 (905400 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 90542 (905410 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.319 - mean_q: 12.678 - prob: 1.000\n",
            "\n",
            "Interval 90543 (905420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.855 - mean_q: 13.469 - prob: 1.000\n",
            "\n",
            "Interval 90544 (905430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.161 - mean_q: 12.426 - prob: 1.000\n",
            "\n",
            "Interval 90545 (905440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.290 - mean_q: 12.559 - prob: 1.000\n",
            "\n",
            "Interval 90546 (905450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90547 (905460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.516 - mean_q: 12.977 - prob: 1.000\n",
            "\n",
            "Interval 90548 (905470 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 90549 (905480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.166 - mean_q: 12.292 - prob: 1.000\n",
            "\n",
            "Interval 90550 (905490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.417 - mean_q: 12.728 - prob: 1.000\n",
            "\n",
            "Interval 90551 (905500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90552 (905510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.633 - mean_q: 13.231 - prob: 1.000\n",
            "\n",
            "Interval 90553 (905520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.246 - mean_q: 12.479 - prob: 1.000\n",
            "\n",
            "Interval 90554 (905530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.394 - mean_q: 12.612 - prob: 1.000\n",
            "\n",
            "Interval 90555 (905540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.671 - mean_q: 13.099 - prob: 1.000\n",
            "\n",
            "Interval 90556 (905550 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 90557 (905560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.101 - mean_q: 12.282 - prob: 1.000\n",
            "\n",
            "Interval 90558 (905570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.223 - mean_q: 12.509 - prob: 1.000\n",
            "\n",
            "Interval 90559 (905580 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 90560 (905590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -21.000 [-21.000, -21.000] - loss: 0.001 - mae: 7.175 - mean_q: 12.421 - prob: 1.000\n",
            "\n",
            "Interval 90561 (905600 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.225 - mean_q: 12.485 - prob: 1.000\n",
            "\n",
            "Interval 90562 (905610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90563 (905620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.137 - mean_q: 12.418 - prob: 1.000\n",
            "\n",
            "Interval 90564 (905630 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.123 - mean_q: 12.255 - prob: 1.000\n",
            "\n",
            "Interval 90565 (905640 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.258 - mean_q: 12.734 - prob: 1.000\n",
            "\n",
            "Interval 90566 (905650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90567 (905660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.499 - mean_q: 12.859 - prob: 1.000\n",
            "\n",
            "Interval 90568 (905670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.389 - mean_q: 12.735 - prob: 1.000\n",
            "\n",
            "Interval 90569 (905680 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.680 - mean_q: 13.144 - prob: 1.000\n",
            "\n",
            "Interval 90570 (905690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90571 (905700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.489 - mean_q: 12.856 - prob: 1.000\n",
            "\n",
            "Interval 90572 (905710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.314 - mean_q: 12.577 - prob: 1.000\n",
            "\n",
            "Interval 90573 (905720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90574 (905730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.440 - mean_q: 12.844 - prob: 1.000\n",
            "\n",
            "Interval 90575 (905740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.222 - mean_q: 12.614 - prob: 1.000\n",
            "\n",
            "Interval 90576 (905750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.618 - mean_q: 13.067 - prob: 1.000\n",
            "\n",
            "Interval 90577 (905760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.685 - mean_q: 13.128 - prob: 1.000\n",
            "\n",
            "Interval 90578 (905770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90579 (905780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.149 - mean_q: 12.287 - prob: 1.000\n",
            "\n",
            "Interval 90580 (905790 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 90581 (905800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.395 - mean_q: 12.725 - prob: 1.000\n",
            "\n",
            "Interval 90582 (905810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90583 (905820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.631 - mean_q: 13.065 - prob: 1.000\n",
            "\n",
            "Interval 90584 (905830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 90585 (905840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.789 - mean_q: 13.339 - prob: 1.000\n",
            "\n",
            "Interval 90586 (905850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.461 - mean_q: 12.908 - prob: 1.000\n",
            "\n",
            "Interval 90587 (905860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.837 - mean_q: 13.343 - prob: 1.000\n",
            "\n",
            "Interval 90588 (905870 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 90589 (905880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.305 - mean_q: 12.697 - prob: 1.000\n",
            "\n",
            "Interval 90590 (905890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.310 - mean_q: 12.386 - prob: 1.000\n",
            "\n",
            "Interval 90591 (905900 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 90592 (905910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.002 - mae: 7.497 - mean_q: 12.935 - prob: 1.000\n",
            "\n",
            "Interval 90593 (905920 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.308 - mean_q: 12.584 - prob: 1.000\n",
            "\n",
            "Interval 90594 (905930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 90595 (905940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.287 - mean_q: 12.560 - prob: 1.000\n",
            "\n",
            "Interval 90596 (905950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.293 - mean_q: 12.558 - prob: 1.000\n",
            "\n",
            "Interval 90597 (905960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 90598 (905970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.001 - mae: 7.311 - mean_q: 12.574 - prob: 1.000\n",
            "\n",
            "Interval 90599 (905980 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 90600 (905990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.380 - mean_q: 12.701 - prob: 1.000\n",
            "\n",
            "Interval 90601 (906000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.001 - mae: 7.310 - mean_q: 12.628 - prob: 1.000\n",
            "\n",
            "Interval 90602 (906010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 90603 (906020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.001 - mae: 7.261 - mean_q: 12.537 - prob: 1.000\n",
            "\n",
            "Interval 90604 (906030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90605 (906040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -19.000 [-19.000, -19.000] - loss: 0.001 - mae: 7.157 - mean_q: 12.461 - prob: 1.000\n",
            "\n",
            "Interval 90606 (906050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 90607 (906060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.372 - mean_q: 12.781 - prob: 1.000\n",
            "\n",
            "Interval 90608 (906070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.483 - mean_q: 12.882 - prob: 1.000\n",
            "\n",
            "Interval 90609 (906080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90610 (906090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.315 - mean_q: 12.616 - prob: 1.000\n",
            "\n",
            "Interval 90611 (906100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 90612 (906110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.001 - mae: 7.208 - mean_q: 12.592 - prob: 1.000\n",
            "\n",
            "Interval 90613 (906120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.456 - mean_q: 12.785 - prob: 1.000\n",
            "\n",
            "Interval 90614 (906130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90615 (906140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.221 - mean_q: 12.487 - prob: 1.000\n",
            "\n",
            "Interval 90616 (906150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.519 - mean_q: 12.910 - prob: 1.000\n",
            "\n",
            "Interval 90617 (906160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.240 - mean_q: 12.551 - prob: 1.000\n",
            "\n",
            "Interval 90618 (906170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.345 - mean_q: 12.681 - prob: 1.000\n",
            "\n",
            "Interval 90619 (906180 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 90620 (906190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.440 - mean_q: 12.915 - prob: 1.000\n",
            "\n",
            "Interval 90621 (906200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90622 (906210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.341 - mean_q: 12.614 - prob: 1.000\n",
            "\n",
            "Interval 90623 (906220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.223 - mean_q: 12.423 - prob: 1.000\n",
            "\n",
            "Interval 90624 (906230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 90625 (906240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.506 - mean_q: 12.871 - prob: 1.000\n",
            "\n",
            "Interval 90626 (906250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.587 - mean_q: 13.035 - prob: 1.000\n",
            "\n",
            "Interval 90627 (906260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90628 (906270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.117 - mean_q: 12.417 - prob: 1.000\n",
            "\n",
            "Interval 90629 (906280 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 90630 (906290 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.474 - mean_q: 12.892 - prob: 1.000\n",
            "\n",
            "Interval 90631 (906300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.000 - mae: 7.439 - mean_q: 12.648 - prob: 1.000\n",
            "\n",
            "Interval 90632 (906310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90633 (906320 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.326 - mean_q: 12.625 - prob: 1.000\n",
            "\n",
            "Interval 90634 (906330 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.211 - mean_q: 12.641 - prob: 1.000\n",
            "\n",
            "Interval 90635 (906340 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 90636 (906350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.418 - mean_q: 12.779 - prob: 1.000\n",
            "\n",
            "Interval 90637 (906360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.715 - mean_q: 13.102 - prob: 1.000\n",
            "\n",
            "Interval 90638 (906370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.194 - mean_q: 12.555 - prob: 1.000\n",
            "\n",
            "Interval 90639 (906380 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 90640 (906390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.274 - mean_q: 12.736 - prob: 1.000\n",
            "\n",
            "Interval 90641 (906400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.239 - mean_q: 12.589 - prob: 1.000\n",
            "\n",
            "Interval 90642 (906410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.361 - mean_q: 12.710 - prob: 1.000\n",
            "\n",
            "Interval 90643 (906420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90644 (906430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.214 - mean_q: 12.683 - prob: 1.000\n",
            "\n",
            "Interval 90645 (906440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.107 - mean_q: 12.347 - prob: 1.000\n",
            "\n",
            "Interval 90646 (906450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.270 - mean_q: 12.634 - prob: 1.000\n",
            "\n",
            "Interval 90647 (906460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 90648 (906470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.001 - mae: 7.639 - mean_q: 13.209 - prob: 1.000\n",
            "\n",
            "Interval 90649 (906480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90650 (906490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.001 - mae: 7.095 - mean_q: 12.342 - prob: 1.000\n",
            "\n",
            "Interval 90651 (906500 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.509 - mean_q: 12.862 - prob: 1.000\n",
            "\n",
            "Interval 90652 (906510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90653 (906520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.438 - mean_q: 12.746 - prob: 1.000\n",
            "\n",
            "Interval 90654 (906530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.514 - mean_q: 12.876 - prob: 1.000\n",
            "\n",
            "Interval 90655 (906540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.376 - mean_q: 12.795 - prob: 1.000\n",
            "\n",
            "Interval 90656 (906550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.508 - mean_q: 12.925 - prob: 1.000\n",
            "\n",
            "Interval 90657 (906560 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 90658 (906570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.580 - mean_q: 12.988 - prob: 1.000\n",
            "\n",
            "Interval 90659 (906580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90660 (906590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.135 - mean_q: 12.405 - prob: 1.000\n",
            "\n",
            "Interval 90661 (906600 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.510 - mean_q: 12.882 - prob: 1.000\n",
            "\n",
            "Interval 90662 (906610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.103 - mean_q: 12.365 - prob: 1.000\n",
            "\n",
            "Interval 90663 (906620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 90664 (906630 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.155 - mean_q: 12.452 - prob: 1.000\n",
            "\n",
            "Interval 90665 (906640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.699 - mean_q: 13.218 - prob: 1.000\n",
            "\n",
            "Interval 90666 (906650 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 90667 (906660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.424 - mean_q: 12.840 - prob: 1.000\n",
            "\n",
            "Interval 90668 (906670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90669 (906680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.001 - mae: 7.642 - mean_q: 13.048 - prob: 1.000\n",
            "\n",
            "Interval 90670 (906690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.207 - mean_q: 12.496 - prob: 1.000\n",
            "\n",
            "Interval 90671 (906700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90672 (906710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.001 - mae: 7.173 - mean_q: 12.414 - prob: 1.000\n",
            "\n",
            "Interval 90673 (906720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.221 - mean_q: 12.582 - prob: 1.000\n",
            "\n",
            "Interval 90674 (906730 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 90675 (906740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.104 - mean_q: 12.416 - prob: 1.000\n",
            "\n",
            "Interval 90676 (906750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.383 - mean_q: 12.833 - prob: 1.000\n",
            "\n",
            "Interval 90677 (906760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.549 - mean_q: 12.974 - prob: 1.000\n",
            "\n",
            "Interval 90678 (906770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90679 (906780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.304 - mean_q: 12.509 - prob: 1.000\n",
            "\n",
            "Interval 90680 (906790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.400 - mean_q: 12.695 - prob: 1.000\n",
            "\n",
            "Interval 90681 (906800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.498 - mean_q: 12.995 - prob: 1.000\n",
            "\n",
            "Interval 90682 (906810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90683 (906820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.173 - mean_q: 12.418 - prob: 1.000\n",
            "\n",
            "Interval 90684 (906830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.119 - mean_q: 12.420 - prob: 1.000\n",
            "\n",
            "Interval 90685 (906840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.001 - mae: 7.274 - mean_q: 12.519 - prob: 1.000\n",
            "\n",
            "Interval 90686 (906850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.283 - mean_q: 12.784 - prob: 1.000\n",
            "\n",
            "Interval 90687 (906860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.389 - mean_q: 12.844 - prob: 1.000\n",
            "\n",
            "Interval 90688 (906870 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 90689 (906880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 6.991 - mean_q: 12.104 - prob: 1.000\n",
            "\n",
            "Interval 90690 (906890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.226 - mean_q: 12.513 - prob: 1.000\n",
            "\n",
            "Interval 90691 (906900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.256 - mean_q: 12.596 - prob: 1.000\n",
            "\n",
            "Interval 90692 (906910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90693 (906920 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.628 - mean_q: 13.060 - prob: 1.000\n",
            "\n",
            "Interval 90694 (906930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90695 (906940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 6.960 - mean_q: 12.139 - prob: 1.000\n",
            "\n",
            "Interval 90696 (906950 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 90697 (906960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.316 - mean_q: 12.528 - prob: 1.000\n",
            "\n",
            "Interval 90698 (906970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.481 - mean_q: 12.949 - prob: 1.000\n",
            "\n",
            "Interval 90699 (906980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.514 - mean_q: 12.765 - prob: 1.000\n",
            "\n",
            "Interval 90700 (906990 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 90701 (907000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.001 - mae: 7.463 - mean_q: 12.655 - prob: 1.000\n",
            "\n",
            "Interval 90702 (907010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.240 - mean_q: 12.721 - prob: 1.000\n",
            "\n",
            "Interval 90703 (907020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90704 (907030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.171 - mean_q: 12.502 - prob: 1.000\n",
            "\n",
            "Interval 90705 (907040 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 90706 (907050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.412 - mean_q: 12.680 - prob: 1.000\n",
            "\n",
            "Interval 90707 (907060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.346 - mean_q: 12.631 - prob: 1.000\n",
            "\n",
            "Interval 90708 (907070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90709 (907080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.001 - mae: 7.352 - mean_q: 12.682 - prob: 1.000\n",
            "\n",
            "Interval 90710 (907090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.444 - mean_q: 12.725 - prob: 1.000\n",
            "\n",
            "Interval 90711 (907100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.397 - mean_q: 12.679 - prob: 1.000\n",
            "\n",
            "Interval 90712 (907110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 90713 (907120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.289 - mean_q: 12.563 - prob: 1.000\n",
            "\n",
            "Interval 90714 (907130 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.369 - mean_q: 12.730 - prob: 1.000\n",
            "\n",
            "Interval 90715 (907140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.373 - mean_q: 12.690 - prob: 1.000\n",
            "\n",
            "Interval 90716 (907150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90717 (907160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.360 - mean_q: 12.711 - prob: 1.000\n",
            "\n",
            "Interval 90718 (907170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.282 - mean_q: 12.758 - prob: 1.000\n",
            "\n",
            "Interval 90719 (907180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90720 (907190 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.527 - mean_q: 13.012 - prob: 1.000\n",
            "\n",
            "Interval 90721 (907200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.227 - mean_q: 12.475 - prob: 1.000\n",
            "\n",
            "Interval 90722 (907210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90723 (907220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.000 - mae: 7.308 - mean_q: 12.740 - prob: 1.000\n",
            "\n",
            "Interval 90724 (907230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.274 - mean_q: 12.577 - prob: 1.000\n",
            "\n",
            "Interval 90725 (907240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90726 (907250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.392 - mean_q: 12.811 - prob: 1.000\n",
            "\n",
            "Interval 90727 (907260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90728 (907270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.427 - mean_q: 12.834 - prob: 1.000\n",
            "\n",
            "Interval 90729 (907280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.344 - mean_q: 12.772 - prob: 1.000\n",
            "\n",
            "Interval 90730 (907290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90731 (907300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.486 - mean_q: 12.883 - prob: 1.000\n",
            "\n",
            "Interval 90732 (907310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.590 - mean_q: 12.997 - prob: 1.000\n",
            "\n",
            "Interval 90733 (907320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 90734 (907330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.261 - mean_q: 12.538 - prob: 1.000\n",
            "\n",
            "Interval 90735 (907340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.391 - mean_q: 12.819 - prob: 1.000\n",
            "\n",
            "Interval 90736 (907350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90737 (907360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.750 - mean_q: 13.229 - prob: 1.000\n",
            "\n",
            "Interval 90738 (907370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90739 (907380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.149 - mean_q: 12.372 - prob: 1.000\n",
            "\n",
            "Interval 90740 (907390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90741 (907400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 6.983 - mean_q: 12.189 - prob: 1.000\n",
            "\n",
            "Interval 90742 (907410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90743 (907420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.430 - mean_q: 12.825 - prob: 1.000\n",
            "\n",
            "Interval 90744 (907430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.437 - mean_q: 12.784 - prob: 1.000\n",
            "\n",
            "Interval 90745 (907440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90746 (907450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.273 - mean_q: 12.539 - prob: 1.000\n",
            "\n",
            "Interval 90747 (907460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90748 (907470 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.744 - mean_q: 13.153 - prob: 1.000\n",
            "\n",
            "Interval 90749 (907480 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.493 - mean_q: 12.916 - prob: 1.000\n",
            "\n",
            "Interval 90750 (907490 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 90751 (907500 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 6.961 - mean_q: 12.171 - prob: 1.000\n",
            "\n",
            "Interval 90752 (907510 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 90753 (907520 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.550 - mean_q: 12.975 - prob: 1.000\n",
            "\n",
            "Interval 90754 (907530 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.9000\n",
            "Interval 90755 (907540 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.000 - mae: 7.421 - mean_q: 12.781 - prob: 1.000\n",
            "\n",
            "Interval 90756 (907550 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.000 - mae: 7.464 - mean_q: 12.887 - prob: 1.000\n",
            "\n",
            "Interval 90757 (907560 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 90758 (907570 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.531 - mean_q: 12.983 - prob: 1.000\n",
            "\n",
            "Interval 90759 (907580 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.9000\n",
            "Interval 90760 (907590 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -17.000 [-17.000, -17.000] - loss: 0.000 - mae: 7.273 - mean_q: 12.651 - prob: 1.000\n",
            "\n",
            "Interval 90761 (907600 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.302 - mean_q: 12.723 - prob: 1.000\n",
            "\n",
            "Interval 90762 (907610 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 90763 (907620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.360 - mean_q: 12.657 - prob: 1.000\n",
            "\n",
            "Interval 90764 (907630 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.533 - mean_q: 12.927 - prob: 1.000\n",
            "\n",
            "Interval 90765 (907640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90766 (907650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.336 - mean_q: 12.595 - prob: 1.000\n",
            "\n",
            "Interval 90767 (907660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.350 - mean_q: 12.775 - prob: 1.000\n",
            "\n",
            "Interval 90768 (907670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.001 - mae: 7.328 - mean_q: 12.767 - prob: 1.000\n",
            "\n",
            "Interval 90769 (907680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.356 - mean_q: 12.832 - prob: 1.000\n",
            "\n",
            "Interval 90770 (907690 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 90771 (907700 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.119 - mean_q: 12.566 - prob: 1.000\n",
            "\n",
            "Interval 90772 (907710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.623 - mean_q: 12.936 - prob: 1.000\n",
            "\n",
            "Interval 90773 (907720 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.407 - mean_q: 12.753 - prob: 1.000\n",
            "\n",
            "Interval 90774 (907730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 90775 (907740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.000 - mae: 7.039 - mean_q: 12.352 - prob: 1.000\n",
            "\n",
            "Interval 90776 (907750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.224 - mean_q: 12.627 - prob: 1.000\n",
            "\n",
            "Interval 90777 (907760 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 90778 (907770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90779 (907780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.001 - mae: 7.486 - mean_q: 12.793 - prob: 1.000\n",
            "\n",
            "Interval 90780 (907790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.292 - mean_q: 12.463 - prob: 1.000\n",
            "\n",
            "Interval 90781 (907800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90782 (907810 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.513 - mean_q: 12.935 - prob: 1.000\n",
            "\n",
            "Interval 90783 (907820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.214 - mean_q: 12.479 - prob: 1.000\n",
            "\n",
            "Interval 90784 (907830 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.260 - mean_q: 12.433 - prob: 1.000\n",
            "\n",
            "Interval 90785 (907840 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.001 - mae: 7.496 - mean_q: 12.782 - prob: 1.000\n",
            "\n",
            "Interval 90786 (907850 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 90787 (907860 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.342 - mean_q: 12.694 - prob: 1.000\n",
            "\n",
            "Interval 90788 (907870 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 90789 (907880 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.294 - mean_q: 12.625 - prob: 1.000\n",
            "\n",
            "Interval 90790 (907890 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.327 - mean_q: 12.743 - prob: 1.000\n",
            "\n",
            "Interval 90791 (907900 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.388 - mean_q: 12.667 - prob: 1.000\n",
            "\n",
            "Interval 90792 (907910 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.366 - mean_q: 12.669 - prob: 1.000\n",
            "\n",
            "Interval 90793 (907920 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.481 - mean_q: 12.843 - prob: 1.000\n",
            "\n",
            "Interval 90794 (907930 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.344 - mean_q: 12.695 - prob: 1.000\n",
            "\n",
            "Interval 90795 (907940 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 90796 (907950 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.325 - mean_q: 12.727 - prob: 1.000\n",
            "\n",
            "Interval 90797 (907960 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 11.500 [9.000, 14.000] - loss: 0.001 - mae: 7.303 - mean_q: 12.678 - prob: 1.000\n",
            "\n",
            "Interval 90798 (907970 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.000 - mae: 7.374 - mean_q: 12.634 - prob: 1.000\n",
            "\n",
            "Interval 90799 (907980 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 90800 (907990 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.001 - mae: 7.348 - mean_q: 12.752 - prob: 1.000\n",
            "\n",
            "Interval 90801 (908000 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.0000\n",
            "Interval 90802 (908010 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.341 - mean_q: 12.775 - prob: 1.000\n",
            "\n",
            "Interval 90803 (908020 steps performed)\n",
            "10/10 [==============================] - 0s 16ms/step - reward: -1.0000\n",
            "Interval 90804 (908030 steps performed)\n",
            "10/10 [==============================] - 0s 15ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.591 - mean_q: 13.031 - prob: 1.000\n",
            "\n",
            "Interval 90805 (908040 steps performed)\n",
            "10/10 [==============================] - 0s 20ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.501 - mean_q: 12.861 - prob: 1.000\n",
            "\n",
            "Interval 90806 (908050 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.541 - mean_q: 12.873 - prob: 1.000\n",
            "\n",
            "Interval 90807 (908060 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 90808 (908070 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: -1.9000\n",
            "Interval 90809 (908080 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.001 - mae: 7.523 - mean_q: 12.849 - prob: 1.000\n",
            "\n",
            "Interval 90810 (908090 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.759 - mean_q: 13.274 - prob: 1.000\n",
            "\n",
            "Interval 90811 (908100 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 90812 (908110 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.143 - mean_q: 12.380 - prob: 1.000\n",
            "\n",
            "Interval 90813 (908120 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.610 - mean_q: 12.939 - prob: 1.000\n",
            "\n",
            "Interval 90814 (908130 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 90815 (908140 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.680 - mean_q: 13.074 - prob: 1.000\n",
            "\n",
            "Interval 90816 (908150 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 90817 (908160 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.372 - mean_q: 12.763 - prob: 1.000\n",
            "\n",
            "Interval 90818 (908170 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.408 - mean_q: 12.888 - prob: 1.000\n",
            "\n",
            "Interval 90819 (908180 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 90820 (908190 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.336 - mean_q: 12.582 - prob: 1.000\n",
            "\n",
            "Interval 90821 (908200 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -3.7000\n",
            "Interval 90822 (908210 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 90823 (908220 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -39.000 [-39.000, -39.000] - loss: 0.001 - mae: 7.719 - mean_q: 13.269 - prob: 1.000\n",
            "\n",
            "Interval 90824 (908230 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.001 - mae: 7.128 - mean_q: 12.371 - prob: 1.000\n",
            "\n",
            "Interval 90825 (908240 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.137 - mean_q: 12.393 - prob: 1.000\n",
            "\n",
            "Interval 90826 (908250 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.228 - mean_q: 12.371 - prob: 1.000\n",
            "\n",
            "Interval 90827 (908260 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 90828 (908270 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.216 - mean_q: 12.460 - prob: 1.000\n",
            "\n",
            "Interval 90829 (908280 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 90830 (908290 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 6.928 - mean_q: 11.987 - prob: 1.000\n",
            "\n",
            "Interval 90831 (908300 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.017 - mean_q: 12.189 - prob: 1.000\n",
            "\n",
            "Interval 90832 (908310 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 90833 (908320 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.025 - mean_q: 12.235 - prob: 1.000\n",
            "\n",
            "Interval 90834 (908330 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 90835 (908340 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -18.000 [-18.000, -18.000] - loss: 0.001 - mae: 7.291 - mean_q: 12.594 - prob: 1.000\n",
            "\n",
            "Interval 90836 (908350 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.764 - mean_q: 13.084 - prob: 1.000\n",
            "\n",
            "Interval 90837 (908360 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.001 - mae: 7.383 - mean_q: 12.582 - prob: 1.000\n",
            "\n",
            "Interval 90838 (908370 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 90839 (908380 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.542 - mean_q: 12.914 - prob: 1.000\n",
            "\n",
            "Interval 90840 (908390 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.446 - mean_q: 12.914 - prob: 1.000\n",
            "\n",
            "Interval 90841 (908400 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.563 - mean_q: 12.901 - prob: 1.000\n",
            "\n",
            "Interval 90842 (908410 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 90843 (908420 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.657 - mean_q: 13.010 - prob: 1.000\n",
            "\n",
            "Interval 90844 (908430 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.135 - mean_q: 12.314 - prob: 1.000\n",
            "\n",
            "Interval 90845 (908440 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 90846 (908450 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.498 - mean_q: 12.910 - prob: 1.000\n",
            "\n",
            "Interval 90847 (908460 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 90848 (908470 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.174 - mean_q: 12.592 - prob: 1.000\n",
            "\n",
            "Interval 90849 (908480 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.387 - mean_q: 12.671 - prob: 1.000\n",
            "\n",
            "Interval 90850 (908490 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 90851 (908500 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.056 - mean_q: 12.245 - prob: 1.000\n",
            "\n",
            "Interval 90852 (908510 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.410 - mean_q: 12.820 - prob: 1.000\n",
            "\n",
            "Interval 90853 (908520 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 90854 (908530 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.543 - mean_q: 12.957 - prob: 1.000\n",
            "\n",
            "Interval 90855 (908540 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.529 - mean_q: 12.789 - prob: 1.000\n",
            "\n",
            "Interval 90856 (908550 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 90857 (908560 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.004 - mean_q: 12.132 - prob: 1.000\n",
            "\n",
            "Interval 90858 (908570 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.001 - mae: 7.276 - mean_q: 12.591 - prob: 1.000\n",
            "\n",
            "Interval 90859 (908580 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 90860 (908590 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.003 - mae: 7.464 - mean_q: 12.755 - prob: 1.000\n",
            "\n",
            "Interval 90861 (908600 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.001 - mae: 6.965 - mean_q: 12.051 - prob: 1.000\n",
            "\n",
            "Interval 90862 (908610 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.9000\n",
            "Interval 90863 (908620 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.331 - mean_q: 12.698 - prob: 1.000\n",
            "\n",
            "Interval 90864 (908630 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.474 - mean_q: 12.811 - prob: 1.000\n",
            "\n",
            "Interval 90865 (908640 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.9000\n",
            "Interval 90866 (908650 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.001 - mae: 7.411 - mean_q: 12.746 - prob: 1.000\n",
            "\n",
            "Interval 90867 (908660 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.199 - mean_q: 12.450 - prob: 1.000\n",
            "\n",
            "Interval 90868 (908670 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 90869 (908680 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.426 - mean_q: 12.628 - prob: 1.000\n",
            "\n",
            "Interval 90870 (908690 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 90871 (908700 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.394 - mean_q: 12.788 - prob: 1.000\n",
            "\n",
            "Interval 90872 (908710 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: -1.0000\n",
            "Interval 90873 (908720 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.345 - mean_q: 12.616 - prob: 1.000\n",
            "\n",
            "Interval 90874 (908730 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.9000\n",
            "Interval 90875 (908740 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.001 - mae: 7.543 - mean_q: 12.842 - prob: 1.000\n",
            "\n",
            "Interval 90876 (908750 steps performed)\n",
            "10/10 [==============================] - 0s 14ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.342 - mean_q: 12.574 - prob: 1.000\n",
            "\n",
            "Interval 90877 (908760 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 90878 (908770 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.643 - mean_q: 13.072 - prob: 1.000\n",
            "\n",
            "Interval 90879 (908780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.597 - mean_q: 13.003 - prob: 1.000\n",
            "\n",
            "Interval 90880 (908790 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 90881 (908800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.317 - mean_q: 12.641 - prob: 1.000\n",
            "\n",
            "Interval 90882 (908810 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.466 - mean_q: 12.789 - prob: 1.000\n",
            "\n",
            "Interval 90883 (908820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90884 (908830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.578 - mean_q: 12.924 - prob: 1.000\n",
            "\n",
            "Interval 90885 (908840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.632 - mean_q: 13.160 - prob: 1.000\n",
            "\n",
            "Interval 90886 (908850 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 90887 (908860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.158 - mean_q: 12.457 - prob: 1.000\n",
            "\n",
            "Interval 90888 (908870 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 90889 (908880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.271 - mean_q: 12.608 - prob: 1.000\n",
            "\n",
            "Interval 90890 (908890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.420 - mean_q: 12.861 - prob: 1.000\n",
            "\n",
            "Interval 90891 (908900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.388 - mean_q: 12.606 - prob: 1.000\n",
            "\n",
            "Interval 90892 (908910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.224 - mean_q: 12.525 - prob: 1.000\n",
            "\n",
            "Interval 90893 (908920 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 90894 (908930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.003 - mae: 7.369 - mean_q: 12.660 - prob: 1.000\n",
            "\n",
            "Interval 90895 (908940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.302 - mean_q: 12.503 - prob: 1.000\n",
            "\n",
            "Interval 90896 (908950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.377 - mean_q: 12.708 - prob: 1.000\n",
            "\n",
            "Interval 90897 (908960 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 90898 (908970 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.462 - mean_q: 12.631 - prob: 1.000\n",
            "\n",
            "Interval 90899 (908980 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 90900 (908990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.592 - mean_q: 12.984 - prob: 1.000\n",
            "\n",
            "Interval 90901 (909000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.609 - mean_q: 13.059 - prob: 1.000\n",
            "\n",
            "Interval 90902 (909010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90903 (909020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.388 - mean_q: 12.668 - prob: 1.000\n",
            "\n",
            "Interval 90904 (909030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.586 - mean_q: 12.854 - prob: 1.000\n",
            "\n",
            "Interval 90905 (909040 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 90906 (909050 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.588 - mean_q: 13.048 - prob: 1.000\n",
            "\n",
            "Interval 90907 (909060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.292 - mean_q: 12.602 - prob: 1.000\n",
            "\n",
            "Interval 90908 (909070 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 90909 (909080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.242 - mean_q: 12.587 - prob: 1.000\n",
            "\n",
            "Interval 90910 (909090 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 90911 (909100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.257 - mean_q: 12.530 - prob: 1.000\n",
            "\n",
            "Interval 90912 (909110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.546 - mean_q: 12.935 - prob: 1.000\n",
            "\n",
            "Interval 90913 (909120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.001 - mae: 7.521 - mean_q: 13.022 - prob: 1.000\n",
            "\n",
            "Interval 90914 (909130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.061 - mean_q: 12.356 - prob: 1.000\n",
            "\n",
            "Interval 90915 (909140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90916 (909150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.566 - mean_q: 12.900 - prob: 1.000\n",
            "\n",
            "Interval 90917 (909160 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 90918 (909170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -19.000 [-19.000, -19.000] - loss: 0.001 - mae: 7.386 - mean_q: 12.766 - prob: 1.000\n",
            "\n",
            "Interval 90919 (909180 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 90920 (909190 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.001 - mae: 7.214 - mean_q: 12.476 - prob: 1.000\n",
            "\n",
            "Interval 90921 (909200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.257 - mean_q: 12.457 - prob: 1.000\n",
            "\n",
            "Interval 90922 (909210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.837 - mean_q: 13.362 - prob: 1.000\n",
            "\n",
            "Interval 90923 (909220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 90924 (909230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.045 - mean_q: 12.371 - prob: 1.000\n",
            "\n",
            "Interval 90925 (909240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.256 - mean_q: 12.543 - prob: 1.000\n",
            "\n",
            "Interval 90926 (909250 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.316 - mean_q: 12.621 - prob: 1.000\n",
            "\n",
            "Interval 90927 (909260 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 90928 (909270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.469 - mean_q: 12.865 - prob: 1.000\n",
            "\n",
            "Interval 90929 (909280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90930 (909290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.545 - mean_q: 12.984 - prob: 1.000\n",
            "\n",
            "Interval 90931 (909300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.523 - mean_q: 12.913 - prob: 1.000\n",
            "\n",
            "Interval 90932 (909310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.001 - mae: 7.443 - mean_q: 12.781 - prob: 1.000\n",
            "\n",
            "Interval 90933 (909320 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 90934 (909330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.613 - mean_q: 13.088 - prob: 1.000\n",
            "\n",
            "Interval 90935 (909340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.244 - mean_q: 12.606 - prob: 1.000\n",
            "\n",
            "Interval 90936 (909350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.261 - mean_q: 12.633 - prob: 1.000\n",
            "\n",
            "Interval 90937 (909360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 90938 (909370 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.298 - mean_q: 12.524 - prob: 1.000\n",
            "\n",
            "Interval 90939 (909380 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.448 - mean_q: 12.739 - prob: 1.000\n",
            "\n",
            "Interval 90940 (909390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 90941 (909400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.428 - mean_q: 12.785 - prob: 1.000\n",
            "\n",
            "Interval 90942 (909410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.493 - mean_q: 12.960 - prob: 1.000\n",
            "\n",
            "Interval 90943 (909420 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.479 - mean_q: 12.765 - prob: 1.000\n",
            "\n",
            "Interval 90944 (909430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.824 - mean_q: 13.383 - prob: 1.000\n",
            "\n",
            "Interval 90945 (909440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90946 (909450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.502 - mean_q: 12.907 - prob: 1.000\n",
            "\n",
            "Interval 90947 (909460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.001 - mae: 7.216 - mean_q: 12.435 - prob: 1.000\n",
            "\n",
            "Interval 90948 (909470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90949 (909480 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.253 - mean_q: 12.478 - prob: 1.000\n",
            "\n",
            "Interval 90950 (909490 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 90951 (909500 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 9.000 [3.000, 15.000] - loss: 0.001 - mae: 7.190 - mean_q: 12.567 - prob: 1.000\n",
            "\n",
            "Interval 90952 (909510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90953 (909520 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.282 - mean_q: 12.516 - prob: 1.000\n",
            "\n",
            "Interval 90954 (909530 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.334 - mean_q: 12.560 - prob: 1.000\n",
            "\n",
            "Interval 90955 (909540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90956 (909550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.588 - mean_q: 12.771 - prob: 1.000\n",
            "\n",
            "Interval 90957 (909560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.643 - mean_q: 12.975 - prob: 1.000\n",
            "\n",
            "Interval 90958 (909570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90959 (909580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.846 - mean_q: 13.389 - prob: 1.000\n",
            "\n",
            "Interval 90960 (909590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.477 - mean_q: 12.873 - prob: 1.000\n",
            "\n",
            "Interval 90961 (909600 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 90962 (909610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.001 - mae: 7.394 - mean_q: 12.848 - prob: 1.000\n",
            "\n",
            "Interval 90963 (909620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 6.959 - mean_q: 12.181 - prob: 1.000\n",
            "\n",
            "Interval 90964 (909630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.228 - mean_q: 12.435 - prob: 1.000\n",
            "\n",
            "Interval 90965 (909640 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 90966 (909650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.001 - mae: 6.934 - mean_q: 12.060 - prob: 1.000\n",
            "\n",
            "Interval 90967 (909660 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.502 - mean_q: 12.952 - prob: 1.000\n",
            "\n",
            "Interval 90968 (909670 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 90969 (909680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.703 - mean_q: 13.114 - prob: 1.000\n",
            "\n",
            "Interval 90970 (909690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.603 - mean_q: 13.112 - prob: 1.000\n",
            "\n",
            "Interval 90971 (909700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90972 (909710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.071 - mean_q: 12.367 - prob: 1.000\n",
            "\n",
            "Interval 90973 (909720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90974 (909730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.514 - mean_q: 12.917 - prob: 1.000\n",
            "\n",
            "Interval 90975 (909740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.081 - mean_q: 12.292 - prob: 1.000\n",
            "\n",
            "Interval 90976 (909750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.120 - mean_q: 12.521 - prob: 1.000\n",
            "\n",
            "Interval 90977 (909760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90978 (909770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.179 - mean_q: 12.312 - prob: 1.000\n",
            "\n",
            "Interval 90979 (909780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.249 - mean_q: 12.544 - prob: 1.000\n",
            "\n",
            "Interval 90980 (909790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 90981 (909800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -27.000 [-27.000, -27.000] - loss: 0.001 - mae: 7.568 - mean_q: 13.122 - prob: 1.000\n",
            "\n",
            "Interval 90982 (909810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.404 - mean_q: 12.602 - prob: 1.000\n",
            "\n",
            "Interval 90983 (909820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 90984 (909830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.281 - mean_q: 12.728 - prob: 1.000\n",
            "\n",
            "Interval 90985 (909840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.550 - mean_q: 12.977 - prob: 1.000\n",
            "\n",
            "Interval 90986 (909850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.629 - mean_q: 13.063 - prob: 1.000\n",
            "\n",
            "Interval 90987 (909860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.446 - mean_q: 12.862 - prob: 1.000\n",
            "\n",
            "Interval 90988 (909870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.303 - mean_q: 12.662 - prob: 1.000\n",
            "\n",
            "Interval 90989 (909880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90990 (909890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.154 - mean_q: 12.435 - prob: 1.000\n",
            "\n",
            "Interval 90991 (909900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 90992 (909910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.364 - mean_q: 12.790 - prob: 1.000\n",
            "\n",
            "Interval 90993 (909920 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.152 - mean_q: 12.460 - prob: 1.000\n",
            "\n",
            "Interval 90994 (909930 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 90995 (909940 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.321 - mean_q: 12.688 - prob: 1.000\n",
            "\n",
            "Interval 90996 (909950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.418 - mean_q: 12.688 - prob: 1.000\n",
            "\n",
            "Interval 90997 (909960 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 90998 (909970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.357 - mean_q: 12.671 - prob: 1.000\n",
            "\n",
            "Interval 90999 (909980 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91000 (909990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.707 - mean_q: 13.186 - prob: 1.000\n",
            "\n",
            "Interval 91001 (910000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.453 - mean_q: 12.826 - prob: 1.000\n",
            "\n",
            "Interval 91002 (910010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 91003 (910020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.144 - mean_q: 12.429 - prob: 1.000\n",
            "\n",
            "Interval 91004 (910030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.074 - mean_q: 12.251 - prob: 1.000\n",
            "\n",
            "Interval 91005 (910040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.033 - mean_q: 12.228 - prob: 1.000\n",
            "\n",
            "Interval 91006 (910050 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91007 (910060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.445 - mean_q: 12.678 - prob: 1.000\n",
            "\n",
            "Interval 91008 (910070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.277 - mean_q: 12.648 - prob: 1.000\n",
            "\n",
            "Interval 91009 (910080 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 91010 (910090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.000 - mae: 7.430 - mean_q: 12.826 - prob: 1.000\n",
            "\n",
            "Interval 91011 (910100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.522 - mean_q: 12.961 - prob: 1.000\n",
            "\n",
            "Interval 91012 (910110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.430 - mean_q: 12.666 - prob: 1.000\n",
            "\n",
            "Interval 91013 (910120 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.001 - mae: 7.319 - mean_q: 12.636 - prob: 1.000\n",
            "\n",
            "Interval 91014 (910130 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.240 - mean_q: 12.491 - prob: 1.000\n",
            "\n",
            "Interval 91015 (910140 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91016 (910150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.001 - mae: 7.395 - mean_q: 12.848 - prob: 1.000\n",
            "\n",
            "Interval 91017 (910160 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 91018 (910170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.383 - mean_q: 12.618 - prob: 1.000\n",
            "\n",
            "Interval 91019 (910180 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 91020 (910190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.042 - mean_q: 12.140 - prob: 1.000\n",
            "\n",
            "Interval 91021 (910200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.6000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.330 - mean_q: 12.670 - prob: 1.000\n",
            "\n",
            "Interval 91022 (910210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -21.000 [-21.000, -21.000] - loss: 0.001 - mae: 7.038 - mean_q: 12.206 - prob: 1.000\n",
            "\n",
            "Interval 91023 (910220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 91024 (910230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.255 - mean_q: 12.601 - prob: 1.000\n",
            "\n",
            "Interval 91025 (910240 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.588 - mean_q: 12.918 - prob: 1.000\n",
            "\n",
            "Interval 91026 (910250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.468 - mean_q: 12.945 - prob: 1.000\n",
            "\n",
            "Interval 91027 (910260 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91028 (910270 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.390 - mean_q: 12.912 - prob: 1.000\n",
            "\n",
            "Interval 91029 (910280 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91030 (910290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.286 - mean_q: 12.652 - prob: 1.000\n",
            "\n",
            "Interval 91031 (910300 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.212 - mean_q: 12.594 - prob: 1.000\n",
            "\n",
            "Interval 91032 (910310 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 91033 (910320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.001 - mae: 7.529 - mean_q: 12.829 - prob: 1.000\n",
            "\n",
            "Interval 91034 (910330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 91035 (910340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.001 - mae: 7.604 - mean_q: 13.096 - prob: 1.000\n",
            "\n",
            "Interval 91036 (910350 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 91037 (910360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.001 - mae: 7.398 - mean_q: 12.914 - prob: 1.000\n",
            "\n",
            "Interval 91038 (910370 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.376 - mean_q: 12.677 - prob: 1.000\n",
            "\n",
            "Interval 91039 (910380 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.417 - mean_q: 12.896 - prob: 1.000\n",
            "\n",
            "Interval 91040 (910390 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.288 - mean_q: 12.612 - prob: 1.000\n",
            "\n",
            "Interval 91041 (910400 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91042 (910410 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.477 - mean_q: 12.734 - prob: 1.000\n",
            "\n",
            "Interval 91043 (910420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.001 - mae: 7.243 - mean_q: 12.773 - prob: 1.000\n",
            "\n",
            "Interval 91044 (910430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.307 - mean_q: 12.607 - prob: 1.000\n",
            "\n",
            "Interval 91045 (910440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.668 - mean_q: 13.082 - prob: 1.000\n",
            "\n",
            "Interval 91046 (910450 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91047 (910460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.466 - mean_q: 12.816 - prob: 1.000\n",
            "\n",
            "Interval 91048 (910470 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91049 (910480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.409 - mean_q: 12.734 - prob: 1.000\n",
            "\n",
            "Interval 91050 (910490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.462 - mean_q: 12.839 - prob: 1.000\n",
            "\n",
            "Interval 91051 (910500 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.001 - mae: 7.200 - mean_q: 12.506 - prob: 1.000\n",
            "\n",
            "Interval 91052 (910510 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91053 (910520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.378 - mean_q: 12.691 - prob: 1.000\n",
            "\n",
            "Interval 91054 (910530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.128 - mean_q: 12.325 - prob: 1.000\n",
            "\n",
            "Interval 91055 (910540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 91056 (910550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.096 - mean_q: 12.322 - prob: 1.000\n",
            "\n",
            "Interval 91057 (910560 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91058 (910570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 5.000 [-2.000, 12.000] - loss: 0.001 - mae: 7.530 - mean_q: 13.008 - prob: 1.000\n",
            "\n",
            "Interval 91059 (910580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 91060 (910590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.270 - mean_q: 12.496 - prob: 1.000\n",
            "\n",
            "Interval 91061 (910600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.448 - mean_q: 12.886 - prob: 1.000\n",
            "\n",
            "Interval 91062 (910610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.003 - mae: 7.479 - mean_q: 12.916 - prob: 1.000\n",
            "\n",
            "Interval 91063 (910620 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91064 (910630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.571 - mean_q: 13.038 - prob: 1.000\n",
            "\n",
            "Interval 91065 (910640 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91066 (910650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.001 - mae: 7.102 - mean_q: 12.378 - prob: 1.000\n",
            "\n",
            "Interval 91067 (910660 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 91068 (910670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.002 - mae: 7.181 - mean_q: 12.332 - prob: 1.000\n",
            "\n",
            "Interval 91069 (910680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.339 - mean_q: 12.720 - prob: 1.000\n",
            "\n",
            "Interval 91070 (910690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.413 - mean_q: 12.703 - prob: 1.000\n",
            "\n",
            "Interval 91071 (910700 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91072 (910710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.470 - mean_q: 12.715 - prob: 1.000\n",
            "\n",
            "Interval 91073 (910720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.001 - mae: 7.124 - mean_q: 12.331 - prob: 1.000\n",
            "\n",
            "Interval 91074 (910730 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 91075 (910740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.001 - mae: 7.093 - mean_q: 12.308 - prob: 1.000\n",
            "\n",
            "Interval 91076 (910750 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91077 (910760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 6.847 - mean_q: 12.094 - prob: 1.000\n",
            "\n",
            "Interval 91078 (910770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.334 - mean_q: 12.623 - prob: 1.000\n",
            "\n",
            "Interval 91079 (910780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 91080 (910790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.001 - mae: 7.645 - mean_q: 13.280 - prob: 1.000\n",
            "\n",
            "Interval 91081 (910800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 91082 (910810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.170 - mean_q: 12.505 - prob: 1.000\n",
            "\n",
            "Interval 91083 (910820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.186 - mean_q: 12.559 - prob: 1.000\n",
            "\n",
            "Interval 91084 (910830 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.736 - mean_q: 13.211 - prob: 1.000\n",
            "\n",
            "Interval 91085 (910840 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 91086 (910850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.001 - mae: 7.620 - mean_q: 13.090 - prob: 1.000\n",
            "\n",
            "Interval 91087 (910860 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91088 (910870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.568 - mean_q: 13.025 - prob: 1.000\n",
            "\n",
            "Interval 91089 (910880 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.381 - mean_q: 12.874 - prob: 1.000\n",
            "\n",
            "Interval 91090 (910890 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91091 (910900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.321 - mean_q: 12.501 - prob: 1.000\n",
            "\n",
            "Interval 91092 (910910 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 91093 (910920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.147 - mean_q: 12.409 - prob: 1.000\n",
            "\n",
            "Interval 91094 (910930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.563 - mean_q: 12.997 - prob: 1.000\n",
            "\n",
            "Interval 91095 (910940 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.621 - mean_q: 12.959 - prob: 1.000\n",
            "\n",
            "Interval 91096 (910950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 91097 (910960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.514 - mean_q: 12.997 - prob: 1.000\n",
            "\n",
            "Interval 91098 (910970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.288 - mean_q: 12.597 - prob: 1.000\n",
            "\n",
            "Interval 91099 (910980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.381 - mean_q: 12.856 - prob: 1.000\n",
            "\n",
            "Interval 91100 (910990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.397 - mean_q: 12.635 - prob: 1.000\n",
            "\n",
            "Interval 91101 (911000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 91102 (911010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.013 - mean_q: 12.178 - prob: 1.000\n",
            "\n",
            "Interval 91103 (911020 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.529 - mean_q: 12.782 - prob: 1.000\n",
            "\n",
            "Interval 91104 (911030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 91105 (911040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -20.000 [-20.000, -20.000] - loss: 0.001 - mae: 7.383 - mean_q: 12.686 - prob: 1.000\n",
            "\n",
            "Interval 91106 (911050 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91107 (911060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.510 - mean_q: 12.928 - prob: 1.000\n",
            "\n",
            "Interval 91108 (911070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 91109 (911080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.488 - mean_q: 12.883 - prob: 1.000\n",
            "\n",
            "Interval 91110 (911090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.526 - mean_q: 12.918 - prob: 1.000\n",
            "\n",
            "Interval 91111 (911100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 91112 (911110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.001 - mae: 7.149 - mean_q: 12.491 - prob: 1.000\n",
            "\n",
            "Interval 91113 (911120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.198 - mean_q: 12.440 - prob: 1.000\n",
            "\n",
            "Interval 91114 (911130 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 91115 (911140 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.001 - mae: 7.340 - mean_q: 12.443 - prob: 1.000\n",
            "\n",
            "Interval 91116 (911150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 91117 (911160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.002 - mae: 7.678 - mean_q: 13.136 - prob: 1.000\n",
            "\n",
            "Interval 91118 (911170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 91119 (911180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -19.000 [-19.000, -19.000] - loss: 0.001 - mae: 7.150 - mean_q: 12.343 - prob: 1.000\n",
            "\n",
            "Interval 91120 (911190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 91121 (911200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.003 - mae: 7.370 - mean_q: 12.667 - prob: 1.000\n",
            "\n",
            "Interval 91122 (911210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.002 - mae: 7.122 - mean_q: 12.383 - prob: 1.000\n",
            "\n",
            "Interval 91123 (911220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.349 - mean_q: 12.600 - prob: 1.000\n",
            "\n",
            "Interval 91124 (911230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 91125 (911240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 5.500 [-1.000, 12.000] - loss: 0.001 - mae: 7.411 - mean_q: 12.669 - prob: 1.000\n",
            "\n",
            "Interval 91126 (911250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 91127 (911260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.146 - mean_q: 12.332 - prob: 1.000\n",
            "\n",
            "Interval 91128 (911270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 91129 (911280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.563 - mean_q: 13.005 - prob: 1.000\n",
            "\n",
            "Interval 91130 (911290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.477 - mean_q: 12.759 - prob: 1.000\n",
            "\n",
            "Interval 91131 (911300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 91132 (911310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.645 - mean_q: 13.075 - prob: 1.000\n",
            "\n",
            "Interval 91133 (911320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.196 - mean_q: 12.514 - prob: 1.000\n",
            "\n",
            "Interval 91134 (911330 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91135 (911340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.297 - mean_q: 12.493 - prob: 1.000\n",
            "\n",
            "Interval 91136 (911350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.283 - mean_q: 12.592 - prob: 1.000\n",
            "\n",
            "Interval 91137 (911360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.506 - mean_q: 12.844 - prob: 1.000\n",
            "\n",
            "Interval 91138 (911370 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91139 (911380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.611 - mean_q: 12.982 - prob: 1.000\n",
            "\n",
            "Interval 91140 (911390 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.283 - mean_q: 12.581 - prob: 1.000\n",
            "\n",
            "Interval 91141 (911400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.449 - mean_q: 12.792 - prob: 1.000\n",
            "\n",
            "Interval 91142 (911410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.308 - mean_q: 12.632 - prob: 1.000\n",
            "\n",
            "Interval 91143 (911420 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 91144 (911430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.436 - mean_q: 12.771 - prob: 1.000\n",
            "\n",
            "Interval 91145 (911440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.228 - mean_q: 12.606 - prob: 1.000\n",
            "\n",
            "Interval 91146 (911450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.6000\n",
            "1 episodes - episode_reward: -29.000 [-29.000, -29.000] - loss: 0.001 - mae: 7.482 - mean_q: 12.984 - prob: 1.000\n",
            "\n",
            "Interval 91147 (911460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 91148 (911470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.335 - mean_q: 12.698 - prob: 1.000\n",
            "\n",
            "Interval 91149 (911480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 91150 (911490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.000 - mae: 7.229 - mean_q: 12.545 - prob: 1.000\n",
            "\n",
            "Interval 91151 (911500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.474 - mean_q: 12.859 - prob: 1.000\n",
            "\n",
            "Interval 91152 (911510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.606 - mean_q: 13.073 - prob: 1.000\n",
            "\n",
            "Interval 91153 (911520 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91154 (911530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.476 - mean_q: 12.855 - prob: 1.000\n",
            "\n",
            "Interval 91155 (911540 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.622 - mean_q: 13.341 - prob: 1.000\n",
            "\n",
            "Interval 91156 (911550 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 91157 (911560 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.001 - mae: 7.387 - mean_q: 12.813 - prob: 1.000\n",
            "\n",
            "Interval 91158 (911570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.300 - mean_q: 12.660 - prob: 1.000\n",
            "\n",
            "Interval 91159 (911580 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 91160 (911590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.000 - mae: 7.367 - mean_q: 12.570 - prob: 1.000\n",
            "\n",
            "Interval 91161 (911600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 91162 (911610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.466 - mean_q: 12.778 - prob: 1.000\n",
            "\n",
            "Interval 91163 (911620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.261 - mean_q: 12.532 - prob: 1.000\n",
            "\n",
            "Interval 91164 (911630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.355 - mean_q: 12.721 - prob: 1.000\n",
            "\n",
            "Interval 91165 (911640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.000 - mae: 7.487 - mean_q: 12.922 - prob: 1.000\n",
            "\n",
            "Interval 91166 (911650 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91167 (911660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.440 - mean_q: 12.840 - prob: 1.000\n",
            "\n",
            "Interval 91168 (911670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.000 - mae: 7.158 - mean_q: 12.312 - prob: 1.000\n",
            "\n",
            "Interval 91169 (911680 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 91170 (911690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.672 - mean_q: 13.040 - prob: 1.000\n",
            "\n",
            "Interval 91171 (911700 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.449 - mean_q: 12.959 - prob: 1.000\n",
            "\n",
            "Interval 91172 (911710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.000 - mae: 7.261 - mean_q: 12.637 - prob: 1.000\n",
            "\n",
            "Interval 91173 (911720 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91174 (911730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.399 - mean_q: 12.678 - prob: 1.000\n",
            "\n",
            "Interval 91175 (911740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.429 - mean_q: 12.804 - prob: 1.000\n",
            "\n",
            "Interval 91176 (911750 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 91177 (911760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.533 - mean_q: 13.086 - prob: 1.000\n",
            "\n",
            "Interval 91178 (911770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.380 - mean_q: 12.610 - prob: 1.000\n",
            "\n",
            "Interval 91179 (911780 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91180 (911790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.309 - mean_q: 12.660 - prob: 1.000\n",
            "\n",
            "Interval 91181 (911800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.328 - mean_q: 12.810 - prob: 1.000\n",
            "\n",
            "Interval 91182 (911810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 91183 (911820 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.000 - mae: 7.227 - mean_q: 12.422 - prob: 1.000\n",
            "\n",
            "Interval 91184 (911830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 91185 (911840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.422 - mean_q: 12.908 - prob: 1.000\n",
            "\n",
            "Interval 91186 (911850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.289 - mean_q: 12.625 - prob: 1.000\n",
            "\n",
            "Interval 91187 (911860 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.465 - mean_q: 12.855 - prob: 1.000\n",
            "\n",
            "Interval 91188 (911870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 91189 (911880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.337 - mean_q: 12.798 - prob: 1.000\n",
            "\n",
            "Interval 91190 (911890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 91191 (911900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.000 - mae: 7.197 - mean_q: 12.548 - prob: 1.000\n",
            "\n",
            "Interval 91192 (911910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.502 - mean_q: 12.935 - prob: 1.000\n",
            "\n",
            "Interval 91193 (911920 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.157 - mean_q: 12.304 - prob: 1.000\n",
            "\n",
            "Interval 91194 (911930 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91195 (911940 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91196 (911950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.590 - mean_q: 13.081 - prob: 1.000\n",
            "\n",
            "Interval 91197 (911960 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.521 - mean_q: 12.782 - prob: 1.000\n",
            "\n",
            "Interval 91198 (911970 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 91199 (911980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.238 - mean_q: 12.546 - prob: 1.000\n",
            "\n",
            "Interval 91200 (911990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.367 - mean_q: 12.856 - prob: 1.000\n",
            "\n",
            "Interval 91201 (912000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 91202 (912010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.192 - mean_q: 12.428 - prob: 1.000\n",
            "\n",
            "Interval 91203 (912020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 8.063 - mean_q: 13.656 - prob: 1.000\n",
            "\n",
            "Interval 91204 (912030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 91205 (912040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.000 - mae: 7.667 - mean_q: 13.025 - prob: 1.000\n",
            "\n",
            "Interval 91206 (912050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 91207 (912060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -17.000 [-17.000, -17.000] - loss: 0.002 - mae: 7.219 - mean_q: 12.366 - prob: 1.000\n",
            "\n",
            "Interval 91208 (912070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.001 - mae: 7.506 - mean_q: 12.975 - prob: 1.000\n",
            "\n",
            "Interval 91209 (912080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 11.500 [8.000, 15.000] - loss: 0.001 - mae: 7.405 - mean_q: 12.835 - prob: 1.000\n",
            "\n",
            "Interval 91210 (912090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 91211 (912100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.363 - mean_q: 12.742 - prob: 1.000\n",
            "\n",
            "Interval 91212 (912110 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 91213 (912120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.001 - mae: 7.123 - mean_q: 12.467 - prob: 1.000\n",
            "\n",
            "Interval 91214 (912130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.361 - mean_q: 12.770 - prob: 1.000\n",
            "\n",
            "Interval 91215 (912140 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91216 (912150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.518 - mean_q: 12.924 - prob: 1.000\n",
            "\n",
            "Interval 91217 (912160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.550 - mean_q: 12.947 - prob: 1.000\n",
            "\n",
            "Interval 91218 (912170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.836 - mean_q: 13.477 - prob: 1.000\n",
            "\n",
            "Interval 91219 (912180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.266 - mean_q: 12.627 - prob: 1.000\n",
            "\n",
            "Interval 91220 (912190 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91221 (912200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.517 - mean_q: 12.922 - prob: 1.000\n",
            "\n",
            "Interval 91222 (912210 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.329 - mean_q: 12.599 - prob: 1.000\n",
            "\n",
            "Interval 91223 (912220 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 91224 (912230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.002 - mae: 7.315 - mean_q: 12.601 - prob: 1.000\n",
            "\n",
            "Interval 91225 (912240 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91226 (912250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.482 - mean_q: 12.838 - prob: 1.000\n",
            "\n",
            "Interval 91227 (912260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.394 - mean_q: 12.526 - prob: 1.000\n",
            "\n",
            "Interval 91228 (912270 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91229 (912280 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.416 - mean_q: 12.661 - prob: 1.000\n",
            "\n",
            "Interval 91230 (912290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 6.985 - mean_q: 12.218 - prob: 1.000\n",
            "\n",
            "Interval 91231 (912300 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 91232 (912310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.6000\n",
            "1 episodes - episode_reward: -36.000 [-36.000, -36.000] - loss: 0.001 - mae: 7.447 - mean_q: 12.737 - prob: 1.000\n",
            "\n",
            "Interval 91233 (912320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 91234 (912330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.395 - mean_q: 12.807 - prob: 1.000\n",
            "\n",
            "Interval 91235 (912340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.075 - mean_q: 12.367 - prob: 1.000\n",
            "\n",
            "Interval 91236 (912350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.398 - mean_q: 12.727 - prob: 1.000\n",
            "\n",
            "Interval 91237 (912360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 91238 (912370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.463 - mean_q: 13.035 - prob: 1.000\n",
            "\n",
            "Interval 91239 (912380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.273 - mean_q: 12.657 - prob: 1.000\n",
            "\n",
            "Interval 91240 (912390 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.229 - mean_q: 12.455 - prob: 1.000\n",
            "\n",
            "Interval 91241 (912400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 91242 (912410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 6.950 - mean_q: 12.223 - prob: 1.000\n",
            "\n",
            "Interval 91243 (912420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.253 - mean_q: 12.521 - prob: 1.000\n",
            "\n",
            "Interval 91244 (912430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.377 - mean_q: 12.601 - prob: 1.000\n",
            "\n",
            "Interval 91245 (912440 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91246 (912450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.386 - mean_q: 12.702 - prob: 1.000\n",
            "\n",
            "Interval 91247 (912460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.659 - mean_q: 13.228 - prob: 1.000\n",
            "\n",
            "Interval 91248 (912470 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91249 (912480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.315 - mean_q: 12.696 - prob: 1.000\n",
            "\n",
            "Interval 91250 (912490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 91251 (912500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.574 - mean_q: 13.088 - prob: 1.000\n",
            "\n",
            "Interval 91252 (912510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.360 - mean_q: 12.661 - prob: 1.000\n",
            "\n",
            "Interval 91253 (912520 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.667 - mean_q: 13.220 - prob: 1.000\n",
            "\n",
            "Interval 91254 (912530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 91255 (912540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.540 - mean_q: 13.017 - prob: 1.000\n",
            "\n",
            "Interval 91256 (912550 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 91257 (912560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -21.000 [-21.000, -21.000] - loss: 0.002 - mae: 7.216 - mean_q: 12.506 - prob: 1.000\n",
            "\n",
            "Interval 91258 (912570 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 91259 (912580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.001 - mae: 7.109 - mean_q: 12.163 - prob: 1.000\n",
            "\n",
            "Interval 91260 (912590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.447 - mean_q: 12.822 - prob: 1.000\n",
            "\n",
            "Interval 91261 (912600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 91262 (912610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.000 - mae: 7.359 - mean_q: 12.679 - prob: 1.000\n",
            "\n",
            "Interval 91263 (912620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.446 - mean_q: 12.797 - prob: 1.000\n",
            "\n",
            "Interval 91264 (912630 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 91265 (912640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.385 - mean_q: 12.708 - prob: 1.000\n",
            "\n",
            "Interval 91266 (912650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.276 - mean_q: 12.615 - prob: 1.000\n",
            "\n",
            "Interval 91267 (912660 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91268 (912670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.478 - mean_q: 12.772 - prob: 1.000\n",
            "\n",
            "Interval 91269 (912680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.002 - mean_q: 12.053 - prob: 1.000\n",
            "\n",
            "Interval 91270 (912690 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.744 - mean_q: 13.251 - prob: 1.000\n",
            "\n",
            "Interval 91271 (912700 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -3.7000\n",
            "Interval 91272 (912710 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -22.000 [-22.000, -22.000] - loss: 0.001 - mae: 7.171 - mean_q: 12.384 - prob: 1.000\n",
            "\n",
            "Interval 91273 (912720 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.379 - mean_q: 12.661 - prob: 1.000\n",
            "\n",
            "Interval 91274 (912730 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.565 - mean_q: 12.972 - prob: 1.000\n",
            "\n",
            "Interval 91275 (912740 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.001 - mae: 7.399 - mean_q: 12.715 - prob: 1.000\n",
            "\n",
            "Interval 91276 (912750 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 91277 (912760 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.660 - mean_q: 13.121 - prob: 1.000\n",
            "\n",
            "Interval 91278 (912770 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 91279 (912780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.493 - mean_q: 12.829 - prob: 1.000\n",
            "\n",
            "Interval 91280 (912790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 91281 (912800 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.502 - mean_q: 12.857 - prob: 1.000\n",
            "\n",
            "Interval 91282 (912810 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.560 - mean_q: 12.905 - prob: 1.000\n",
            "\n",
            "Interval 91283 (912820 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91284 (912830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.450 - mean_q: 12.824 - prob: 1.000\n",
            "\n",
            "Interval 91285 (912840 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91286 (912850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.396 - mean_q: 12.820 - prob: 1.000\n",
            "\n",
            "Interval 91287 (912860 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 91288 (912870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -17.000 [-17.000, -17.000] - loss: 0.001 - mae: 6.933 - mean_q: 12.163 - prob: 1.000\n",
            "\n",
            "Interval 91289 (912880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.691 - mean_q: 13.174 - prob: 1.000\n",
            "\n",
            "Interval 91290 (912890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.504 - mean_q: 13.021 - prob: 1.000\n",
            "\n",
            "Interval 91291 (912900 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.502 - mean_q: 13.005 - prob: 1.000\n",
            "\n",
            "Interval 91292 (912910 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91293 (912920 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.385 - mean_q: 12.630 - prob: 1.000\n",
            "\n",
            "Interval 91294 (912930 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 91295 (912940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.001 - mae: 7.334 - mean_q: 12.646 - prob: 1.000\n",
            "\n",
            "Interval 91296 (912950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.208 - mean_q: 12.479 - prob: 1.000\n",
            "\n",
            "Interval 91297 (912960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 91298 (912970 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.721 - mean_q: 13.225 - prob: 1.000\n",
            "\n",
            "Interval 91299 (912980 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.334 - mean_q: 12.744 - prob: 1.000\n",
            "\n",
            "Interval 91300 (912990 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.218 - mean_q: 12.517 - prob: 1.000\n",
            "\n",
            "Interval 91301 (913000 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 91302 (913010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.245 - mean_q: 12.598 - prob: 1.000\n",
            "\n",
            "Interval 91303 (913020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.001 - mae: 7.499 - mean_q: 12.859 - prob: 1.000\n",
            "\n",
            "Interval 91304 (913030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.241 - mean_q: 12.560 - prob: 1.000\n",
            "\n",
            "Interval 91305 (913040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 91306 (913050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.162 - mean_q: 12.352 - prob: 1.000\n",
            "\n",
            "Interval 91307 (913060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.088 - mean_q: 12.301 - prob: 1.000\n",
            "\n",
            "Interval 91308 (913070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 91309 (913080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.871 - mean_q: 13.329 - prob: 1.000\n",
            "\n",
            "Interval 91310 (913090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.156 - mean_q: 12.403 - prob: 1.000\n",
            "\n",
            "Interval 91311 (913100 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91312 (913110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.442 - mean_q: 12.657 - prob: 1.000\n",
            "\n",
            "Interval 91313 (913120 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91314 (913130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.455 - mean_q: 12.890 - prob: 1.000\n",
            "\n",
            "Interval 91315 (913140 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91316 (913150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.510 - mean_q: 12.887 - prob: 1.000\n",
            "\n",
            "Interval 91317 (913160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 91318 (913170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.275 - mean_q: 12.679 - prob: 1.000\n",
            "\n",
            "Interval 91319 (913180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.415 - mean_q: 12.805 - prob: 1.000\n",
            "\n",
            "Interval 91320 (913190 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91321 (913200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.292 - mean_q: 12.692 - prob: 1.000\n",
            "\n",
            "Interval 91322 (913210 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.145 - mean_q: 12.365 - prob: 1.000\n",
            "\n",
            "Interval 91323 (913220 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 91324 (913230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.234 - mean_q: 12.496 - prob: 1.000\n",
            "\n",
            "Interval 91325 (913240 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91326 (913250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.259 - mean_q: 12.480 - prob: 1.000\n",
            "\n",
            "Interval 91327 (913260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.348 - mean_q: 12.571 - prob: 1.000\n",
            "\n",
            "Interval 91328 (913270 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 91329 (913280 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.509 - mean_q: 12.962 - prob: 1.000\n",
            "\n",
            "Interval 91330 (913290 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 91331 (913300 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.759 - mean_q: 13.317 - prob: 1.000\n",
            "\n",
            "Interval 91332 (913310 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.137 - mean_q: 12.543 - prob: 1.000\n",
            "\n",
            "Interval 91333 (913320 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 91334 (913330 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.456 - mean_q: 12.813 - prob: 1.000\n",
            "\n",
            "Interval 91335 (913340 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 91336 (913350 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.421 - mean_q: 12.864 - prob: 1.000\n",
            "\n",
            "Interval 91337 (913360 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.463 - mean_q: 12.816 - prob: 1.000\n",
            "\n",
            "Interval 91338 (913370 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 91339 (913380 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.705 - mean_q: 13.204 - prob: 1.000\n",
            "\n",
            "Interval 91340 (913390 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 91341 (913400 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.000 - mae: 7.409 - mean_q: 12.843 - prob: 1.000\n",
            "\n",
            "Interval 91342 (913410 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 91343 (913420 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.462 - mean_q: 12.850 - prob: 1.000\n",
            "\n",
            "Interval 91344 (913430 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.426 - mean_q: 12.778 - prob: 1.000\n",
            "\n",
            "Interval 91345 (913440 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 91346 (913450 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.547 - mean_q: 12.938 - prob: 1.000\n",
            "\n",
            "Interval 91347 (913460 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.258 - mean_q: 12.592 - prob: 1.000\n",
            "\n",
            "Interval 91348 (913470 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 91349 (913480 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.523 - mean_q: 12.952 - prob: 1.000\n",
            "\n",
            "Interval 91350 (913490 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.331 - mean_q: 12.549 - prob: 1.000\n",
            "\n",
            "Interval 91351 (913500 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 91352 (913510 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.121 - mean_q: 12.411 - prob: 1.000\n",
            "\n",
            "Interval 91353 (913520 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 91354 (913530 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.474 - mean_q: 12.988 - prob: 1.000\n",
            "\n",
            "Interval 91355 (913540 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.001 - mae: 7.383 - mean_q: 12.693 - prob: 1.000\n",
            "\n",
            "Interval 91356 (913550 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 91357 (913560 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.437 - mean_q: 12.753 - prob: 1.000\n",
            "\n",
            "Interval 91358 (913570 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 91359 (913580 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.230 - mean_q: 12.386 - prob: 1.000\n",
            "\n",
            "Interval 91360 (913590 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.033 - mean_q: 12.121 - prob: 1.000\n",
            "\n",
            "Interval 91361 (913600 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 91362 (913610 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.191 - mean_q: 12.529 - prob: 1.000\n",
            "\n",
            "Interval 91363 (913620 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.439 - mean_q: 12.940 - prob: 1.000\n",
            "\n",
            "Interval 91364 (913630 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 91365 (913640 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.426 - mean_q: 12.860 - prob: 1.000\n",
            "\n",
            "Interval 91366 (913650 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.000 - mae: 7.488 - mean_q: 12.895 - prob: 1.000\n",
            "\n",
            "Interval 91367 (913660 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.136 - mean_q: 12.350 - prob: 1.000\n",
            "\n",
            "Interval 91368 (913670 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.226 - mean_q: 12.549 - prob: 1.000\n",
            "\n",
            "Interval 91369 (913680 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -2.8000\n",
            "Interval 91370 (913690 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 91371 (913700 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -21.000 [-21.000, -21.000] - loss: 0.001 - mae: 7.449 - mean_q: 12.857 - prob: 1.000\n",
            "\n",
            "Interval 91372 (913710 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.179 - mean_q: 12.502 - prob: 1.000\n",
            "\n",
            "Interval 91373 (913720 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.245 - mean_q: 12.382 - prob: 1.000\n",
            "\n",
            "Interval 91374 (913730 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.261 - mean_q: 12.663 - prob: 1.000\n",
            "\n",
            "Interval 91375 (913740 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.804 - mean_q: 13.341 - prob: 1.000\n",
            "\n",
            "Interval 91376 (913750 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.385 - mean_q: 12.791 - prob: 1.000\n",
            "\n",
            "Interval 91377 (913760 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 91378 (913770 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.511 - mean_q: 12.763 - prob: 1.000\n",
            "\n",
            "Interval 91379 (913780 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.459 - mean_q: 12.868 - prob: 1.000\n",
            "\n",
            "Interval 91380 (913790 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 91381 (913800 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.000 - mae: 7.645 - mean_q: 13.179 - prob: 1.000\n",
            "\n",
            "Interval 91382 (913810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 91383 (913820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.196 - mean_q: 12.520 - prob: 1.000\n",
            "\n",
            "Interval 91384 (913830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.325 - mean_q: 12.526 - prob: 1.000\n",
            "\n",
            "Interval 91385 (913840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 91386 (913850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -3.7000\n",
            "Interval 91387 (913860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -36.000 [-36.000, -36.000] - loss: 0.001 - mae: 7.142 - mean_q: 12.323 - prob: 1.000\n",
            "\n",
            "Interval 91388 (913870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.268 - mean_q: 12.461 - prob: 1.000\n",
            "\n",
            "Interval 91389 (913880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.488 - mean_q: 12.971 - prob: 1.000\n",
            "\n",
            "Interval 91390 (913890 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91391 (913900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.514 - mean_q: 12.886 - prob: 1.000\n",
            "\n",
            "Interval 91392 (913910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.459 - mean_q: 12.997 - prob: 1.000\n",
            "\n",
            "Interval 91393 (913920 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91394 (913930 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.538 - mean_q: 12.984 - prob: 1.000\n",
            "\n",
            "Interval 91395 (913940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.174 - mean_q: 12.462 - prob: 1.000\n",
            "\n",
            "Interval 91396 (913950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.347 - mean_q: 12.695 - prob: 1.000\n",
            "\n",
            "Interval 91397 (913960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 91398 (913970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.540 - mean_q: 12.926 - prob: 1.000\n",
            "\n",
            "Interval 91399 (913980 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 91400 (913990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.388 - mean_q: 12.718 - prob: 1.000\n",
            "\n",
            "Interval 91401 (914000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.214 - mean_q: 12.497 - prob: 1.000\n",
            "\n",
            "Interval 91402 (914010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.587 - mean_q: 13.025 - prob: 1.000\n",
            "\n",
            "Interval 91403 (914020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.001 - mae: 7.307 - mean_q: 12.694 - prob: 1.000\n",
            "\n",
            "Interval 91404 (914030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 91405 (914040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.527 - mean_q: 12.872 - prob: 1.000\n",
            "\n",
            "Interval 91406 (914050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 91407 (914060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.304 - mean_q: 12.660 - prob: 1.000\n",
            "\n",
            "Interval 91408 (914070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.574 - mean_q: 12.887 - prob: 1.000\n",
            "\n",
            "Interval 91409 (914080 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 91410 (914090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.210 - mean_q: 12.412 - prob: 1.000\n",
            "\n",
            "Interval 91411 (914100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.463 - mean_q: 12.864 - prob: 1.000\n",
            "\n",
            "Interval 91412 (914110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.284 - mean_q: 12.542 - prob: 1.000\n",
            "\n",
            "Interval 91413 (914120 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91414 (914130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.856 - mean_q: 13.372 - prob: 1.000\n",
            "\n",
            "Interval 91415 (914140 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91416 (914150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.344 - mean_q: 12.756 - prob: 1.000\n",
            "\n",
            "Interval 91417 (914160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.533 - mean_q: 12.949 - prob: 1.000\n",
            "\n",
            "Interval 91418 (914170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 91419 (914180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.001 - mae: 7.439 - mean_q: 12.781 - prob: 1.000\n",
            "\n",
            "Interval 91420 (914190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 91421 (914200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.001 - mae: 7.337 - mean_q: 12.657 - prob: 1.000\n",
            "\n",
            "Interval 91422 (914210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.301 - mean_q: 12.666 - prob: 1.000\n",
            "\n",
            "Interval 91423 (914220 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91424 (914230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.389 - mean_q: 12.779 - prob: 1.000\n",
            "\n",
            "Interval 91425 (914240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 91426 (914250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.001 - mae: 7.214 - mean_q: 12.481 - prob: 1.000\n",
            "\n",
            "Interval 91427 (914260 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91428 (914270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.455 - mean_q: 12.889 - prob: 1.000\n",
            "\n",
            "Interval 91429 (914280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.464 - mean_q: 12.953 - prob: 1.000\n",
            "\n",
            "Interval 91430 (914290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 91431 (914300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.177 - mean_q: 12.360 - prob: 1.000\n",
            "\n",
            "Interval 91432 (914310 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 91433 (914320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.466 - mean_q: 12.964 - prob: 1.000\n",
            "\n",
            "Interval 91434 (914330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.474 - mean_q: 12.810 - prob: 1.000\n",
            "\n",
            "Interval 91435 (914340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.162 - mean_q: 12.543 - prob: 1.000\n",
            "\n",
            "Interval 91436 (914350 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 91437 (914360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -19.000 [-19.000, -19.000] - loss: 0.001 - mae: 7.506 - mean_q: 13.000 - prob: 1.000\n",
            "\n",
            "Interval 91438 (914370 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 2.3000\n",
            "2 episodes - episode_reward: 9.000 [4.000, 14.000] - loss: 0.002 - mae: 7.301 - mean_q: 12.582 - prob: 1.000\n",
            "\n",
            "Interval 91439 (914380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 91440 (914390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.657 - mean_q: 13.237 - prob: 1.000\n",
            "\n",
            "Interval 91441 (914400 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 91442 (914410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.545 - mean_q: 12.938 - prob: 1.000\n",
            "\n",
            "Interval 91443 (914420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.001 - mae: 7.472 - mean_q: 12.820 - prob: 1.000\n",
            "\n",
            "Interval 91444 (914430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 91445 (914440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.489 - mean_q: 12.895 - prob: 1.000\n",
            "\n",
            "Interval 91446 (914450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 91447 (914460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.371 - mean_q: 12.736 - prob: 1.000\n",
            "\n",
            "Interval 91448 (914470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.334 - mean_q: 12.651 - prob: 1.000\n",
            "\n",
            "Interval 91449 (914480 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.003 - mae: 7.438 - mean_q: 12.872 - prob: 1.000\n",
            "\n",
            "Interval 91450 (914490 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 91451 (914500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.003 - mae: 7.224 - mean_q: 12.389 - prob: 1.000\n",
            "\n",
            "Interval 91452 (914510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 91453 (914520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.416 - mean_q: 12.766 - prob: 1.000\n",
            "\n",
            "Interval 91454 (914530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.209 - mean_q: 12.588 - prob: 1.000\n",
            "\n",
            "Interval 91455 (914540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 91456 (914550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.002 - mae: 7.326 - mean_q: 12.586 - prob: 1.000\n",
            "\n",
            "Interval 91457 (914560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.046 - mean_q: 12.331 - prob: 1.000\n",
            "\n",
            "Interval 91458 (914570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.415 - mean_q: 12.671 - prob: 1.000\n",
            "\n",
            "Interval 91459 (914580 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 91460 (914590 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.474 - mean_q: 12.864 - prob: 1.000\n",
            "\n",
            "Interval 91461 (914600 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.002 - mae: 7.442 - mean_q: 12.853 - prob: 1.000\n",
            "\n",
            "Interval 91462 (914610 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91463 (914620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.314 - mean_q: 12.576 - prob: 1.000\n",
            "\n",
            "Interval 91464 (914630 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91465 (914640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.339 - mean_q: 12.604 - prob: 1.000\n",
            "\n",
            "Interval 91466 (914650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.381 - mean_q: 12.735 - prob: 1.000\n",
            "\n",
            "Interval 91467 (914660 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.531 - mean_q: 12.863 - prob: 1.000\n",
            "\n",
            "Interval 91468 (914670 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91469 (914680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.426 - mean_q: 12.771 - prob: 1.000\n",
            "\n",
            "Interval 91470 (914690 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91471 (914700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.350 - mean_q: 12.461 - prob: 1.000\n",
            "\n",
            "Interval 91472 (914710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.306 - mean_q: 12.575 - prob: 1.000\n",
            "\n",
            "Interval 91473 (914720 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.425 - mean_q: 12.745 - prob: 1.000\n",
            "\n",
            "Interval 91474 (914730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 91475 (914740 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -17.000 [-17.000, -17.000] - loss: 0.001 - mae: 7.267 - mean_q: 12.652 - prob: 1.000\n",
            "\n",
            "Interval 91476 (914750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 91477 (914760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.600 - mean_q: 13.023 - prob: 1.000\n",
            "\n",
            "Interval 91478 (914770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 91479 (914780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.436 - mean_q: 12.784 - prob: 1.000\n",
            "\n",
            "Interval 91480 (914790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.524 - mean_q: 12.926 - prob: 1.000\n",
            "\n",
            "Interval 91481 (914800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.001 - mae: 7.493 - mean_q: 12.979 - prob: 1.000\n",
            "\n",
            "Interval 91482 (914810 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91483 (914820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.438 - mean_q: 12.743 - prob: 1.000\n",
            "\n",
            "Interval 91484 (914830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 91485 (914840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.386 - mean_q: 12.737 - prob: 1.000\n",
            "\n",
            "Interval 91486 (914850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.531 - mean_q: 12.803 - prob: 1.000\n",
            "\n",
            "Interval 91487 (914860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.216 - mean_q: 12.550 - prob: 1.000\n",
            "\n",
            "Interval 91488 (914870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 91489 (914880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.001 - mae: 7.385 - mean_q: 12.722 - prob: 1.000\n",
            "\n",
            "Interval 91490 (914890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.001 - mae: 7.334 - mean_q: 12.523 - prob: 1.000\n",
            "\n",
            "Interval 91491 (914900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.289 - mean_q: 12.566 - prob: 1.000\n",
            "\n",
            "Interval 91492 (914910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.370 - mean_q: 12.673 - prob: 1.000\n",
            "\n",
            "Interval 91493 (914920 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91494 (914930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.566 - mean_q: 12.889 - prob: 1.000\n",
            "\n",
            "Interval 91495 (914940 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91496 (914950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.000 - mae: 7.403 - mean_q: 12.673 - prob: 1.000\n",
            "\n",
            "Interval 91497 (914960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.138 - mean_q: 12.296 - prob: 1.000\n",
            "\n",
            "Interval 91498 (914970 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91499 (914980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.114 - mean_q: 12.345 - prob: 1.000\n",
            "\n",
            "Interval 91500 (914990 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.269 - mean_q: 12.552 - prob: 1.000\n",
            "\n",
            "Interval 91501 (915000 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91502 (915010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.512 - mean_q: 13.061 - prob: 1.000\n",
            "\n",
            "Interval 91503 (915020 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.442 - mean_q: 12.843 - prob: 1.000\n",
            "\n",
            "Interval 91504 (915030 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91505 (915040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.319 - mean_q: 12.721 - prob: 1.000\n",
            "\n",
            "Interval 91506 (915050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.403 - mean_q: 12.667 - prob: 1.000\n",
            "\n",
            "Interval 91507 (915060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.001 - mae: 7.382 - mean_q: 12.818 - prob: 1.000\n",
            "\n",
            "Interval 91508 (915070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 91509 (915080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.373 - mean_q: 12.781 - prob: 1.000\n",
            "\n",
            "Interval 91510 (915090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.391 - mean_q: 12.641 - prob: 1.000\n",
            "\n",
            "Interval 91511 (915100 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91512 (915110 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 10.000 [7.000, 13.000] - loss: 0.001 - mae: 7.324 - mean_q: 12.547 - prob: 1.000\n",
            "\n",
            "Interval 91513 (915120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 91514 (915130 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.329 - mean_q: 12.628 - prob: 1.000\n",
            "\n",
            "Interval 91515 (915140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.002 - mean_q: 12.196 - prob: 1.000\n",
            "\n",
            "Interval 91516 (915150 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.250 - mean_q: 12.497 - prob: 1.000\n",
            "\n",
            "Interval 91517 (915160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 91518 (915170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.446 - mean_q: 12.834 - prob: 1.000\n",
            "\n",
            "Interval 91519 (915180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.292 - mean_q: 12.618 - prob: 1.000\n",
            "\n",
            "Interval 91520 (915190 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91521 (915200 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.406 - mean_q: 12.821 - prob: 1.000\n",
            "\n",
            "Interval 91522 (915210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 91523 (915220 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.273 - mean_q: 12.604 - prob: 1.000\n",
            "\n",
            "Interval 91524 (915230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.580 - mean_q: 13.090 - prob: 1.000\n",
            "\n",
            "Interval 91525 (915240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.278 - mean_q: 12.737 - prob: 1.000\n",
            "\n",
            "Interval 91526 (915250 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 91527 (915260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.555 - mean_q: 12.955 - prob: 1.000\n",
            "\n",
            "Interval 91528 (915270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 91529 (915280 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.310 - mean_q: 12.434 - prob: 1.000\n",
            "\n",
            "Interval 91530 (915290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.341 - mean_q: 12.760 - prob: 1.000\n",
            "\n",
            "Interval 91531 (915300 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91532 (915310 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.270 - mean_q: 12.423 - prob: 1.000\n",
            "\n",
            "Interval 91533 (915320 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.000 - mae: 7.054 - mean_q: 12.291 - prob: 1.000\n",
            "\n",
            "Interval 91534 (915330 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.092 - mean_q: 12.402 - prob: 1.000\n",
            "\n",
            "Interval 91535 (915340 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91536 (915350 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.366 - mean_q: 12.749 - prob: 1.000\n",
            "\n",
            "Interval 91537 (915360 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.693 - mean_q: 13.091 - prob: 1.000\n",
            "\n",
            "Interval 91538 (915370 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91539 (915380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.196 - mean_q: 12.517 - prob: 1.000\n",
            "\n",
            "Interval 91540 (915390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.337 - mean_q: 12.710 - prob: 1.000\n",
            "\n",
            "Interval 91541 (915400 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.459 - mean_q: 12.902 - prob: 1.000\n",
            "\n",
            "Interval 91542 (915410 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 91543 (915420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.390 - mean_q: 12.731 - prob: 1.000\n",
            "\n",
            "Interval 91544 (915430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.125 - mean_q: 12.188 - prob: 1.000\n",
            "\n",
            "Interval 91545 (915440 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 91546 (915450 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.514 - mean_q: 12.893 - prob: 1.000\n",
            "\n",
            "Interval 91547 (915460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.260 - mean_q: 12.631 - prob: 1.000\n",
            "\n",
            "Interval 91548 (915470 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 91549 (915480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -17.000 [-17.000, -17.000] - loss: 0.001 - mae: 7.598 - mean_q: 12.953 - prob: 1.000\n",
            "\n",
            "Interval 91550 (915490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.485 - mean_q: 12.908 - prob: 1.000\n",
            "\n",
            "Interval 91551 (915500 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91552 (915510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.103 - mean_q: 12.304 - prob: 1.000\n",
            "\n",
            "Interval 91553 (915520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.251 - mean_q: 12.637 - prob: 1.000\n",
            "\n",
            "Interval 91554 (915530 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91555 (915540 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.606 - mean_q: 13.028 - prob: 1.000\n",
            "\n",
            "Interval 91556 (915550 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 91557 (915560 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.002 - mae: 7.495 - mean_q: 12.837 - prob: 1.000\n",
            "\n",
            "Interval 91558 (915570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.363 - mean_q: 12.837 - prob: 1.000\n",
            "\n",
            "Interval 91559 (915580 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91560 (915590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.150 - mean_q: 12.437 - prob: 1.000\n",
            "\n",
            "Interval 91561 (915600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.582 - mean_q: 13.125 - prob: 1.000\n",
            "\n",
            "Interval 91562 (915610 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91563 (915620 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.309 - mean_q: 12.706 - prob: 1.000\n",
            "\n",
            "Interval 91564 (915630 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.568 - mean_q: 12.996 - prob: 1.000\n",
            "\n",
            "Interval 91565 (915640 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91566 (915650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.393 - mean_q: 12.660 - prob: 1.000\n",
            "\n",
            "Interval 91567 (915660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.356 - mean_q: 12.704 - prob: 1.000\n",
            "\n",
            "Interval 91568 (915670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 91569 (915680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.184 - mean_q: 12.497 - prob: 1.000\n",
            "\n",
            "Interval 91570 (915690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.412 - mean_q: 12.688 - prob: 1.000\n",
            "\n",
            "Interval 91571 (915700 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.066 - mean_q: 12.282 - prob: 1.000\n",
            "\n",
            "Interval 91572 (915710 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.174 - mean_q: 12.375 - prob: 1.000\n",
            "\n",
            "Interval 91573 (915720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 91574 (915730 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.042 - mean_q: 12.210 - prob: 1.000\n",
            "\n",
            "Interval 91575 (915740 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.484 - mean_q: 12.954 - prob: 1.000\n",
            "\n",
            "Interval 91576 (915750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 91577 (915760 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.099 - mean_q: 12.358 - prob: 1.000\n",
            "\n",
            "Interval 91578 (915770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 91579 (915780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.264 - mean_q: 12.711 - prob: 1.000\n",
            "\n",
            "Interval 91580 (915790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.000 - mae: 7.605 - mean_q: 13.028 - prob: 1.000\n",
            "\n",
            "Interval 91581 (915800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.289 - mean_q: 12.760 - prob: 1.000\n",
            "\n",
            "Interval 91582 (915810 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 91583 (915820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.310 - mean_q: 12.593 - prob: 1.000\n",
            "\n",
            "Interval 91584 (915830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 91585 (915840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.302 - mean_q: 12.673 - prob: 1.000\n",
            "\n",
            "Interval 91586 (915850 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91587 (915860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.545 - mean_q: 13.072 - prob: 1.000\n",
            "\n",
            "Interval 91588 (915870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.492 - mean_q: 12.974 - prob: 1.000\n",
            "\n",
            "Interval 91589 (915880 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91590 (915890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.369 - mean_q: 12.691 - prob: 1.000\n",
            "\n",
            "Interval 91591 (915900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.296 - mean_q: 12.625 - prob: 1.000\n",
            "\n",
            "Interval 91592 (915910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.280 - mean_q: 12.613 - prob: 1.000\n",
            "\n",
            "Interval 91593 (915920 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91594 (915930 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.513 - mean_q: 12.872 - prob: 1.000\n",
            "\n",
            "Interval 91595 (915940 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 91596 (915950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -17.000 [-17.000, -17.000] - loss: 0.001 - mae: 7.204 - mean_q: 12.548 - prob: 1.000\n",
            "\n",
            "Interval 91597 (915960 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91598 (915970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.353 - mean_q: 12.700 - prob: 1.000\n",
            "\n",
            "Interval 91599 (915980 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.584 - mean_q: 12.997 - prob: 1.000\n",
            "\n",
            "Interval 91600 (915990 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 91601 (916000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.001 - mae: 7.493 - mean_q: 12.927 - prob: 1.000\n",
            "\n",
            "Interval 91602 (916010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.260 - mean_q: 12.506 - prob: 1.000\n",
            "\n",
            "Interval 91603 (916020 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 91604 (916030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.001 - mae: 7.575 - mean_q: 13.017 - prob: 1.000\n",
            "\n",
            "Interval 91605 (916040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.506 - mean_q: 12.925 - prob: 1.000\n",
            "\n",
            "Interval 91606 (916050 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91607 (916060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.178 - mean_q: 12.318 - prob: 1.000\n",
            "\n",
            "Interval 91608 (916070 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.281 - mean_q: 12.495 - prob: 1.000\n",
            "\n",
            "Interval 91609 (916080 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.373 - mean_q: 12.766 - prob: 1.000\n",
            "\n",
            "Interval 91610 (916090 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91611 (916100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.060 - mean_q: 12.346 - prob: 1.000\n",
            "\n",
            "Interval 91612 (916110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 91613 (916120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -20.000 [-20.000, -20.000] - loss: 0.001 - mae: 7.217 - mean_q: 12.409 - prob: 1.000\n",
            "\n",
            "Interval 91614 (916130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 91615 (916140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.355 - mean_q: 12.781 - prob: 1.000\n",
            "\n",
            "Interval 91616 (916150 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.277 - mean_q: 12.621 - prob: 1.000\n",
            "\n",
            "Interval 91617 (916160 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 91618 (916170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.559 - mean_q: 12.892 - prob: 1.000\n",
            "\n",
            "Interval 91619 (916180 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.396 - mean_q: 12.749 - prob: 1.000\n",
            "\n",
            "Interval 91620 (916190 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91621 (916200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.535 - mean_q: 12.967 - prob: 1.000\n",
            "\n",
            "Interval 91622 (916210 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.491 - mean_q: 12.859 - prob: 1.000\n",
            "\n",
            "Interval 91623 (916220 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91624 (916230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.311 - mean_q: 12.644 - prob: 1.000\n",
            "\n",
            "Interval 91625 (916240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.376 - mean_q: 12.762 - prob: 1.000\n",
            "\n",
            "Interval 91626 (916250 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 91627 (916260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 9.500 [5.000, 14.000] - loss: 0.001 - mae: 7.414 - mean_q: 12.735 - prob: 1.000\n",
            "\n",
            "Interval 91628 (916270 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 91629 (916280 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Gi93ToWJk-1F",
        "colab": {},
        "outputId": "8feb637b-ada5-4563-85cc-c94baa0c70af"
      },
      "source": [
        "dqn_only_embedding.test(env, nb_episodes=5, visualize=True, nb_max_episode_steps=99)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing for 5 episodes ...\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "|\u001b[43m \u001b[0m| : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : | : : |\n",
            "|\u001b[43m \u001b[0m: : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : | : : |\n",
            "| :\u001b[43m \u001b[0m: : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : :\u001b[43m \u001b[0m: : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : :\u001b[43m \u001b[0m: |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : :\u001b[43m \u001b[0m|\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : | : :\u001b[43m \u001b[0m|\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|R: | : :\u001b[42mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "+---------+\n",
            "|R: | :\u001b[42m_\u001b[0m:G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | :\u001b[42m_\u001b[0m: |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : :\u001b[42m_\u001b[0m: |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : |\u001b[42m_\u001b[0m: |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35m\u001b[42mB\u001b[0m\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35m\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "Episode 1: reward: 6.000, steps: 15\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : :\u001b[43m \u001b[0m|\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : | : :\u001b[43m \u001b[0m|\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :\u001b[42mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : :\u001b[42m_\u001b[0m|\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | :\u001b[42m_\u001b[0m: |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : |\u001b[42m_\u001b[0m: : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : :\u001b[42m_\u001b[0m: : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| :\u001b[42m_\u001b[0m: : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "|\u001b[42m_\u001b[0m: : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "|\u001b[42m_\u001b[0m: | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|\u001b[35m\u001b[42mR\u001b[0m\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "Episode 2: reward: 8.000, steps: 13\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : |\u001b[43m \u001b[0m: : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : :\u001b[43m \u001b[0m: : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| :\u001b[43m \u001b[0m: : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| :\u001b[43m \u001b[0m| : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m:\u001b[43m \u001b[0m| : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "+---------+\n",
            "|\u001b[42mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "|\u001b[42m_\u001b[0m: | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "|\u001b[42m_\u001b[0m: : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "|\u001b[42m_\u001b[0m| : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35m\u001b[42mY\u001b[0m\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35m\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "Episode 3: reward: 9.000, steps: 12\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : |\u001b[43m \u001b[0m: : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : :\u001b[43m \u001b[0m: : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| :\u001b[43m \u001b[0m: : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| :\u001b[43m \u001b[0m| : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m:\u001b[43m \u001b[0m| : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "+---------+\n",
            "|\u001b[42mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "|\u001b[42m_\u001b[0m: | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "|\u001b[42m_\u001b[0m: : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "|\u001b[42m_\u001b[0m| : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35m\u001b[42mY\u001b[0m\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35m\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "Episode 4: reward: 9.000, steps: 12\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : | : :\u001b[43m \u001b[0m|\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|R: | : :\u001b[42mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "+---------+\n",
            "|R: | :\u001b[42m_\u001b[0m:G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "+---------+\n",
            "|R: |\u001b[42m_\u001b[0m: :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : |\u001b[42m_\u001b[0m: : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : :\u001b[42m_\u001b[0m: : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| :\u001b[42m_\u001b[0m: : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "|\u001b[42m_\u001b[0m: : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "|\u001b[42m_\u001b[0m| : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35m\u001b[42mY\u001b[0m\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35m\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "Episode 5: reward: 9.000, steps: 12\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f6794038e48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0f_kpxJwVqvD",
        "colab": {},
        "outputId": "01645312-86c9-46bf-ecb6-3f86219d55b9"
      },
      "source": [
        "####################################################\n",
        "memory = SequentialMemory(limit=50000, window_length=1)\n",
        "policy = EpsGreedyQPolicy()\n",
        "dqn = DQNAgent(model=model, nb_actions=action_size, memory=memory, nb_steps_warmup=500, target_model_update=1e-2, policy=policy)\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
        "dqn.fit(env, nb_steps=1000000, visualize=False, verbose=1, nb_max_episode_steps=99, log_interval=10)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for 1000000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10/10 [==============================] - 0s 14ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - prob: 1.000\n",
            "\n",
            "Interval 2 (10 steps performed)\n",
            "10/10 [==============================] - 0s 663us/step - reward: -1.0000\n",
            "Interval 3 (20 steps performed)\n",
            "10/10 [==============================] - 0s 663us/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - prob: 1.000\n",
            "\n",
            "Interval 4 (30 steps performed)\n",
            "10/10 [==============================] - 0s 696us/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - prob: 1.000\n",
            "\n",
            "Interval 5 (40 steps performed)\n",
            "10/10 [==============================] - 0s 622us/step - reward: -1.9000\n",
            "Interval 6 (50 steps performed)\n",
            "10/10 [==============================] - 0s 755us/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - prob: 1.000\n",
            "\n",
            "Interval 7 (60 steps performed)\n",
            "10/10 [==============================] - 0s 628us/step - reward: -1.9000\n",
            "Interval 8 (70 steps performed)\n",
            "10/10 [==============================] - 0s 745us/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - prob: 1.000\n",
            "\n",
            "Interval 9 (80 steps performed)\n",
            "10/10 [==============================] - 0s 719us/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - prob: 1.000\n",
            "\n",
            "Interval 10 (90 steps performed)\n",
            "10/10 [==============================] - 0s 778us/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - prob: 1.000\n",
            "\n",
            "Interval 11 (100 steps performed)\n",
            "10/10 [==============================] - 0s 685us/step - reward: -1.0000\n",
            "Interval 12 (110 steps performed)\n",
            "10/10 [==============================] - 0s 722us/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - prob: 1.000\n",
            "\n",
            "Interval 13 (120 steps performed)\n",
            "10/10 [==============================] - 0s 609us/step - reward: -1.0000\n",
            "Interval 14 (130 steps performed)\n",
            "10/10 [==============================] - 0s 753us/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - prob: 1.000\n",
            "\n",
            "Interval 15 (140 steps performed)\n",
            "10/10 [==============================] - 0s 760us/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - prob: 1.000\n",
            "\n",
            "Interval 16 (150 steps performed)\n",
            "10/10 [==============================] - 0s 769us/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - prob: 1.000\n",
            "\n",
            "Interval 17 (160 steps performed)\n",
            "10/10 [==============================] - 0s 702us/step - reward: -1.0000\n",
            "Interval 18 (170 steps performed)\n",
            "10/10 [==============================] - 0s 739us/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - prob: 1.000\n",
            "\n",
            "Interval 19 (180 steps performed)\n",
            "10/10 [==============================] - 0s 714us/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - prob: 1.000\n",
            "\n",
            "Interval 20 (190 steps performed)\n",
            "10/10 [==============================] - 0s 677us/step - reward: -1.9000\n",
            "Interval 21 (200 steps performed)\n",
            "10/10 [==============================] - 0s 734us/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - prob: 1.000\n",
            "\n",
            "Interval 22 (210 steps performed)\n",
            "10/10 [==============================] - 0s 686us/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - prob: 1.000\n",
            "\n",
            "Interval 23 (220 steps performed)\n",
            "10/10 [==============================] - 0s 684us/step - reward: -1.0000\n",
            "Interval 24 (230 steps performed)\n",
            "10/10 [==============================] - 0s 729us/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - prob: 1.000\n",
            "\n",
            "Interval 25 (240 steps performed)\n",
            "10/10 [==============================] - 0s 750us/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - prob: 1.000\n",
            "\n",
            "Interval 26 (250 steps performed)\n",
            "10/10 [==============================] - 0s 711us/step - reward: -1.0000\n",
            "Interval 27 (260 steps performed)\n",
            "10/10 [==============================] - 0s 844us/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - prob: 1.000\n",
            "\n",
            "Interval 28 (270 steps performed)\n",
            "10/10 [==============================] - 0s 717us/step - reward: -1.0000\n",
            "Interval 29 (280 steps performed)\n",
            "10/10 [==============================] - 0s 868us/step - reward: 2.3000\n",
            "2 episodes - episode_reward: -6.000 [-16.000, 4.000] - prob: 1.000\n",
            "\n",
            "Interval 30 (290 steps performed)\n",
            "10/10 [==============================] - 0s 804us/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - prob: 1.000\n",
            "\n",
            "Interval 31 (300 steps performed)\n",
            "10/10 [==============================] - 0s 743us/step - reward: -1.0000\n",
            "Interval 32 (310 steps performed)\n",
            "10/10 [==============================] - 0s 766us/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - prob: 1.000\n",
            "\n",
            "Interval 33 (320 steps performed)\n",
            "10/10 [==============================] - 0s 813us/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - prob: 1.000\n",
            "\n",
            "Interval 34 (330 steps performed)\n",
            "10/10 [==============================] - 0s 764us/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - prob: 1.000\n",
            "\n",
            "Interval 35 (340 steps performed)\n",
            "10/10 [==============================] - 0s 739us/step - reward: -1.0000\n",
            "Interval 36 (350 steps performed)\n",
            "10/10 [==============================] - 0s 805us/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - prob: 1.000\n",
            "\n",
            "Interval 37 (360 steps performed)\n",
            "10/10 [==============================] - 0s 628us/step - reward: -1.9000\n",
            "Interval 38 (370 steps performed)\n",
            "10/10 [==============================] - 0s 762us/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -19.000 [-19.000, -19.000] - prob: 1.000\n",
            "\n",
            "Interval 39 (380 steps performed)\n",
            "10/10 [==============================] - 0s 760us/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - prob: 1.000\n",
            "\n",
            "Interval 40 (390 steps performed)\n",
            "10/10 [==============================] - 0s 718us/step - reward: -1.0000\n",
            "Interval 41 (400 steps performed)\n",
            "10/10 [==============================] - 0s 710us/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - prob: 1.000\n",
            "\n",
            "Interval 42 (410 steps performed)\n",
            "10/10 [==============================] - 0s 758us/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - prob: 1.000\n",
            "\n",
            "Interval 43 (420 steps performed)\n",
            "10/10 [==============================] - 0s 728us/step - reward: -1.0000\n",
            "Interval 44 (430 steps performed)\n",
            "10/10 [==============================] - 0s 764us/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - prob: 1.000\n",
            "\n",
            "Interval 45 (440 steps performed)\n",
            "10/10 [==============================] - 0s 779us/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - prob: 1.000\n",
            "\n",
            "Interval 46 (450 steps performed)\n",
            "10/10 [==============================] - 0s 724us/step - reward: -1.9000\n",
            "Interval 47 (460 steps performed)\n",
            "10/10 [==============================] - 0s 788us/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - prob: 1.000\n",
            "\n",
            "Interval 48 (470 steps performed)\n",
            "10/10 [==============================] - 0s 715us/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - prob: 1.000\n",
            "\n",
            "Interval 49 (480 steps performed)\n",
            "10/10 [==============================] - 0s 709us/step - reward: -1.0000\n",
            "Interval 50 (490 steps performed)\n",
            "10/10 [==============================] - 0s 744us/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - prob: 1.000\n",
            "\n",
            "Interval 51 (500 steps performed)\n",
            "10/10 [==============================] - 1s 102ms/step - reward: -1.0000\n",
            "Interval 52 (510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.332 - mean_q: 12.656 - prob: 1.000\n",
            "\n",
            "Interval 53 (520 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 54 (530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.467 - mean_q: 12.999 - prob: 1.000\n",
            "\n",
            "Interval 55 (540 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.171 - mean_q: 12.446 - prob: 1.000\n",
            "\n",
            "Interval 56 (550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 57 (560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.000 - mae: 7.529 - mean_q: 13.042 - prob: 1.000\n",
            "\n",
            "Interval 58 (570 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 59 (580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.156 - mean_q: 12.474 - prob: 1.000\n",
            "\n",
            "Interval 60 (590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.179 - mean_q: 12.589 - prob: 1.000\n",
            "\n",
            "Interval 61 (600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.323 - mean_q: 12.749 - prob: 1.000\n",
            "\n",
            "Interval 62 (610 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.185 - mean_q: 12.505 - prob: 1.000\n",
            "\n",
            "Interval 63 (620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 64 (630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.000 - mae: 7.094 - mean_q: 12.388 - prob: 1.000\n",
            "\n",
            "Interval 65 (640 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 66 (650 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.229 - mean_q: 12.542 - prob: 1.000\n",
            "\n",
            "Interval 67 (660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 68 (670 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.000 - mae: 7.337 - mean_q: 12.695 - prob: 1.000\n",
            "\n",
            "Interval 69 (680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.219 - mean_q: 12.484 - prob: 1.000\n",
            "\n",
            "Interval 70 (690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.046 - mean_q: 12.299 - prob: 1.000\n",
            "\n",
            "Interval 71 (700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.164 - mean_q: 12.397 - prob: 1.000\n",
            "\n",
            "Interval 72 (710 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 73 (720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.000 - mae: 7.425 - mean_q: 12.902 - prob: 1.000\n",
            "\n",
            "Interval 74 (730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 75 (740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.111 - mean_q: 12.488 - prob: 1.000\n",
            "\n",
            "Interval 76 (750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.513 - mean_q: 13.006 - prob: 1.000\n",
            "\n",
            "Interval 77 (760 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.316 - mean_q: 12.635 - prob: 1.000\n",
            "\n",
            "Interval 78 (770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 79 (780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.001 - mae: 7.022 - mean_q: 12.193 - prob: 1.000\n",
            "\n",
            "Interval 80 (790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.263 - mean_q: 12.633 - prob: 1.000\n",
            "\n",
            "Interval 81 (800 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 82 (810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.452 - mean_q: 12.892 - prob: 1.000\n",
            "\n",
            "Interval 83 (820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.478 - mean_q: 12.973 - prob: 1.000\n",
            "\n",
            "Interval 84 (830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 85 (840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.113 - mean_q: 12.472 - prob: 1.000\n",
            "\n",
            "Interval 86 (850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 87 (860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.360 - mean_q: 12.809 - prob: 1.000\n",
            "\n",
            "Interval 88 (870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.365 - mean_q: 12.894 - prob: 1.000\n",
            "\n",
            "Interval 89 (880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.291 - mean_q: 12.767 - prob: 1.000\n",
            "\n",
            "Interval 90 (890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 91 (900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.091 - mean_q: 12.409 - prob: 1.000\n",
            "\n",
            "Interval 92 (910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.385 - mean_q: 12.782 - prob: 1.000\n",
            "\n",
            "Interval 93 (920 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.193 - mean_q: 12.710 - prob: 1.000\n",
            "\n",
            "Interval 94 (930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 95 (940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.520 - mean_q: 12.985 - prob: 1.000\n",
            "\n",
            "Interval 96 (950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.521 - mean_q: 13.031 - prob: 1.000\n",
            "\n",
            "Interval 97 (960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.289 - mean_q: 12.463 - prob: 1.000\n",
            "\n",
            "Interval 98 (970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 99 (980 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.563 - mean_q: 13.072 - prob: 1.000\n",
            "\n",
            "Interval 100 (990 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.013 - mean_q: 12.389 - prob: 1.000\n",
            "\n",
            "Interval 101 (1000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 102 (1010 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.498 - mean_q: 12.996 - prob: 1.000\n",
            "\n",
            "Interval 103 (1020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 104 (1030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.168 - mean_q: 12.541 - prob: 1.000\n",
            "\n",
            "Interval 105 (1040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 106 (1050 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.000 - mae: 7.250 - mean_q: 12.630 - prob: 1.000\n",
            "\n",
            "Interval 107 (1060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 6.878 - mean_q: 12.194 - prob: 1.000\n",
            "\n",
            "Interval 108 (1070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.059 - mean_q: 12.353 - prob: 1.000\n",
            "\n",
            "Interval 109 (1080 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.376 - mean_q: 12.766 - prob: 1.000\n",
            "\n",
            "Interval 110 (1090 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 111 (1100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.501 - mean_q: 12.993 - prob: 1.000\n",
            "\n",
            "Interval 112 (1110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.433 - mean_q: 12.849 - prob: 1.000\n",
            "\n",
            "Interval 113 (1120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.390 - mean_q: 12.914 - prob: 1.000\n",
            "\n",
            "Interval 114 (1130 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 115 (1140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.296 - mean_q: 12.808 - prob: 1.000\n",
            "\n",
            "Interval 116 (1150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.282 - mean_q: 12.655 - prob: 1.000\n",
            "\n",
            "Interval 117 (1160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 118 (1170 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.038 - mean_q: 12.362 - prob: 1.000\n",
            "\n",
            "Interval 119 (1180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.481 - mean_q: 12.856 - prob: 1.000\n",
            "\n",
            "Interval 120 (1190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 121 (1200 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.001 - mae: 7.404 - mean_q: 12.818 - prob: 1.000\n",
            "\n",
            "Interval 122 (1210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.552 - mean_q: 13.091 - prob: 1.000\n",
            "\n",
            "Interval 123 (1220 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.414 - mean_q: 12.794 - prob: 1.000\n",
            "\n",
            "Interval 124 (1230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 125 (1240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.062 - mean_q: 12.485 - prob: 1.000\n",
            "\n",
            "Interval 126 (1250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.355 - mean_q: 12.845 - prob: 1.000\n",
            "\n",
            "Interval 127 (1260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 128 (1270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.000 - mae: 7.527 - mean_q: 12.916 - prob: 1.000\n",
            "\n",
            "Interval 129 (1280 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 130 (1290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.000 - mae: 7.553 - mean_q: 13.127 - prob: 1.000\n",
            "\n",
            "Interval 131 (1300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.456 - mean_q: 13.022 - prob: 1.000\n",
            "\n",
            "Interval 132 (1310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.383 - mean_q: 12.811 - prob: 1.000\n",
            "\n",
            "Interval 133 (1320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 134 (1330 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.448 - mean_q: 12.856 - prob: 1.000\n",
            "\n",
            "Interval 135 (1340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 136 (1350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.538 - mean_q: 12.956 - prob: 1.000\n",
            "\n",
            "Interval 137 (1360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.376 - mean_q: 12.774 - prob: 1.000\n",
            "\n",
            "Interval 138 (1370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 139 (1380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.521 - mean_q: 12.953 - prob: 1.000\n",
            "\n",
            "Interval 140 (1390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.547 - mean_q: 12.934 - prob: 1.000\n",
            "\n",
            "Interval 141 (1400 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.206 - mean_q: 12.486 - prob: 1.000\n",
            "\n",
            "Interval 142 (1410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.000 - mae: 7.535 - mean_q: 12.985 - prob: 1.000\n",
            "\n",
            "Interval 143 (1420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.000 - mae: 7.529 - mean_q: 13.001 - prob: 1.000\n",
            "\n",
            "Interval 144 (1430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.000 - mae: 7.510 - mean_q: 12.988 - prob: 1.000\n",
            "\n",
            "Interval 145 (1440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.210 - mean_q: 12.543 - prob: 1.000\n",
            "\n",
            "Interval 146 (1450 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 147 (1460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.000 - mae: 7.433 - mean_q: 12.905 - prob: 1.000\n",
            "\n",
            "Interval 148 (1470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.193 - mean_q: 12.643 - prob: 1.000\n",
            "\n",
            "Interval 149 (1480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 150 (1490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.302 - mean_q: 12.566 - prob: 1.000\n",
            "\n",
            "Interval 151 (1500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.506 - mean_q: 13.042 - prob: 1.000\n",
            "\n",
            "Interval 152 (1510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.603 - mean_q: 12.991 - prob: 1.000\n",
            "\n",
            "Interval 153 (1520 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 154 (1530 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.475 - mean_q: 12.964 - prob: 1.000\n",
            "\n",
            "Interval 155 (1540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.285 - mean_q: 12.845 - prob: 1.000\n",
            "\n",
            "Interval 156 (1550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 157 (1560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.070 - mean_q: 12.343 - prob: 1.000\n",
            "\n",
            "Interval 158 (1570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.279 - mean_q: 12.659 - prob: 1.000\n",
            "\n",
            "Interval 159 (1580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 160 (1590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.288 - mean_q: 12.747 - prob: 1.000\n",
            "\n",
            "Interval 161 (1600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 162 (1610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.115 - mean_q: 12.426 - prob: 1.000\n",
            "\n",
            "Interval 163 (1620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.586 - mean_q: 12.921 - prob: 1.000\n",
            "\n",
            "Interval 164 (1630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.421 - mean_q: 13.005 - prob: 1.000\n",
            "\n",
            "Interval 165 (1640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 166 (1650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.189 - mean_q: 12.598 - prob: 1.000\n",
            "\n",
            "Interval 167 (1660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 168 (1670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.302 - mean_q: 12.663 - prob: 1.000\n",
            "\n",
            "Interval 169 (1680 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 170 (1690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.233 - mean_q: 12.752 - prob: 1.000\n",
            "\n",
            "Interval 171 (1700 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.474 - mean_q: 12.998 - prob: 1.000\n",
            "\n",
            "Interval 172 (1710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 173 (1720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 10.000 [8.000, 12.000] - loss: 0.001 - mae: 7.484 - mean_q: 12.903 - prob: 1.000\n",
            "\n",
            "Interval 174 (1730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 175 (1740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.254 - mean_q: 12.639 - prob: 1.000\n",
            "\n",
            "Interval 176 (1750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.379 - mean_q: 12.638 - prob: 1.000\n",
            "\n",
            "Interval 177 (1760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.001 - mae: 7.328 - mean_q: 12.674 - prob: 1.000\n",
            "\n",
            "Interval 178 (1770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.568 - mean_q: 12.984 - prob: 1.000\n",
            "\n",
            "Interval 179 (1780 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 180 (1790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.571 - mean_q: 12.915 - prob: 1.000\n",
            "\n",
            "Interval 181 (1800 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 182 (1810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.561 - mean_q: 12.965 - prob: 1.000\n",
            "\n",
            "Interval 183 (1820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.001 - mae: 7.318 - mean_q: 12.858 - prob: 1.000\n",
            "\n",
            "Interval 184 (1830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.319 - mean_q: 12.675 - prob: 1.000\n",
            "\n",
            "Interval 185 (1840 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 186 (1850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.339 - mean_q: 12.773 - prob: 1.000\n",
            "\n",
            "Interval 187 (1860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.175 - mean_q: 12.473 - prob: 1.000\n",
            "\n",
            "Interval 188 (1870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.365 - mean_q: 12.756 - prob: 1.000\n",
            "\n",
            "Interval 189 (1880 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 190 (1890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.000 - mae: 7.144 - mean_q: 12.446 - prob: 1.000\n",
            "\n",
            "Interval 191 (1900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.527 - mean_q: 13.025 - prob: 1.000\n",
            "\n",
            "Interval 192 (1910 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.342 - mean_q: 12.706 - prob: 1.000\n",
            "\n",
            "Interval 193 (1920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 194 (1930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.228 - mean_q: 12.652 - prob: 1.000\n",
            "\n",
            "Interval 195 (1940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 196 (1950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.486 - mean_q: 12.807 - prob: 1.000\n",
            "\n",
            "Interval 197 (1960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.182 - mean_q: 12.576 - prob: 1.000\n",
            "\n",
            "Interval 198 (1970 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.445 - mean_q: 12.914 - prob: 1.000\n",
            "\n",
            "Interval 199 (1980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 200 (1990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.541 - mean_q: 12.939 - prob: 1.000\n",
            "\n",
            "Interval 201 (2000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.399 - mean_q: 12.847 - prob: 1.000\n",
            "\n",
            "Interval 202 (2010 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.630 - mean_q: 13.171 - prob: 1.000\n",
            "\n",
            "Interval 203 (2020 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 204 (2030 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.390 - mean_q: 12.711 - prob: 1.000\n",
            "\n",
            "Interval 205 (2040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 206 (2050 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.306 - mean_q: 12.779 - prob: 1.000\n",
            "\n",
            "Interval 207 (2060 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.432 - mean_q: 12.683 - prob: 1.000\n",
            "\n",
            "Interval 208 (2070 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.483 - mean_q: 12.732 - prob: 1.000\n",
            "\n",
            "Interval 209 (2080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 210 (2090 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.356 - mean_q: 12.760 - prob: 1.000\n",
            "\n",
            "Interval 211 (2100 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.474 - mean_q: 12.936 - prob: 1.000\n",
            "\n",
            "Interval 212 (2110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 213 (2120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 214 (2130 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -28.000 [-28.000, -28.000] - loss: 0.001 - mae: 7.362 - mean_q: 12.498 - prob: 1.000\n",
            "\n",
            "Interval 215 (2140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.291 - mean_q: 12.602 - prob: 1.000\n",
            "\n",
            "Interval 216 (2150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 217 (2160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.496 - mean_q: 12.922 - prob: 1.000\n",
            "\n",
            "Interval 218 (2170 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 219 (2180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.251 - mean_q: 12.703 - prob: 1.000\n",
            "\n",
            "Interval 220 (2190 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -25.000 [-25.000, -25.000] - loss: 0.001 - mae: 7.140 - mean_q: 12.440 - prob: 1.000\n",
            "\n",
            "Interval 221 (2200 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 222 (2210 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 223 (2220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.000 - mae: 7.016 - mean_q: 12.299 - prob: 1.000\n",
            "\n",
            "Interval 224 (2230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.350 - mean_q: 12.759 - prob: 1.000\n",
            "\n",
            "Interval 225 (2240 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 226 (2250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.230 - mean_q: 12.521 - prob: 1.000\n",
            "\n",
            "Interval 227 (2260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.399 - mean_q: 12.623 - prob: 1.000\n",
            "\n",
            "Interval 228 (2270 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 229 (2280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.001 - mae: 7.156 - mean_q: 12.456 - prob: 1.000\n",
            "\n",
            "Interval 230 (2290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 231 (2300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 6.811 - mean_q: 12.075 - prob: 1.000\n",
            "\n",
            "Interval 232 (2310 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.405 - mean_q: 12.795 - prob: 1.000\n",
            "\n",
            "Interval 233 (2320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 234 (2330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.340 - mean_q: 12.758 - prob: 1.000\n",
            "\n",
            "Interval 235 (2340 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 236 (2350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.344 - mean_q: 12.689 - prob: 1.000\n",
            "\n",
            "Interval 237 (2360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.350 - mean_q: 12.677 - prob: 1.000\n",
            "\n",
            "Interval 238 (2370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.284 - mean_q: 12.619 - prob: 1.000\n",
            "\n",
            "Interval 239 (2380 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 240 (2390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -19.000 [-19.000, -19.000] - loss: 0.000 - mae: 7.422 - mean_q: 12.804 - prob: 1.000\n",
            "\n",
            "Interval 241 (2400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 242 (2410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.359 - mean_q: 12.813 - prob: 1.000\n",
            "\n",
            "Interval 243 (2420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.239 - mean_q: 12.405 - prob: 1.000\n",
            "\n",
            "Interval 244 (2430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 245 (2440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 6.952 - mean_q: 12.214 - prob: 1.000\n",
            "\n",
            "Interval 246 (2450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.295 - mean_q: 12.599 - prob: 1.000\n",
            "\n",
            "Interval 247 (2460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 248 (2470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.282 - mean_q: 12.649 - prob: 1.000\n",
            "\n",
            "Interval 249 (2480 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 250 (2490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.001 - mae: 7.439 - mean_q: 12.870 - prob: 1.000\n",
            "\n",
            "Interval 251 (2500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.600 - mean_q: 13.158 - prob: 1.000\n",
            "\n",
            "Interval 252 (2510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 253 (2520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.336 - mean_q: 12.609 - prob: 1.000\n",
            "\n",
            "Interval 254 (2530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.205 - mean_q: 12.602 - prob: 1.000\n",
            "\n",
            "Interval 255 (2540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 256 (2550 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.000 - mae: 7.281 - mean_q: 12.702 - prob: 1.000\n",
            "\n",
            "Interval 257 (2560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.021 - mean_q: 12.252 - prob: 1.000\n",
            "\n",
            "Interval 258 (2570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.135 - mean_q: 12.452 - prob: 1.000\n",
            "\n",
            "Interval 259 (2580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 260 (2590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.362 - mean_q: 12.717 - prob: 1.000\n",
            "\n",
            "Interval 261 (2600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 262 (2610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.371 - mean_q: 12.808 - prob: 1.000\n",
            "\n",
            "Interval 263 (2620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 264 (2630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.373 - mean_q: 12.794 - prob: 1.000\n",
            "\n",
            "Interval 265 (2640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.330 - mean_q: 12.694 - prob: 1.000\n",
            "\n",
            "Interval 266 (2650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.217 - mean_q: 12.647 - prob: 1.000\n",
            "\n",
            "Interval 267 (2660 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 268 (2670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.007 - mean_q: 12.344 - prob: 1.000\n",
            "\n",
            "Interval 269 (2680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.495 - mean_q: 13.017 - prob: 1.000\n",
            "\n",
            "Interval 270 (2690 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 271 (2700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.001 - mae: 7.774 - mean_q: 13.414 - prob: 1.000\n",
            "\n",
            "Interval 272 (2710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.322 - mean_q: 12.661 - prob: 1.000\n",
            "\n",
            "Interval 273 (2720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 274 (2730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.564 - mean_q: 13.026 - prob: 1.000\n",
            "\n",
            "Interval 275 (2740 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 276 (2750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.167 - mean_q: 12.490 - prob: 1.000\n",
            "\n",
            "Interval 277 (2760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.269 - mean_q: 12.628 - prob: 1.000\n",
            "\n",
            "Interval 278 (2770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 279 (2780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.281 - mean_q: 12.575 - prob: 1.000\n",
            "\n",
            "Interval 280 (2790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.272 - mean_q: 12.620 - prob: 1.000\n",
            "\n",
            "Interval 281 (2800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.131 - mean_q: 12.486 - prob: 1.000\n",
            "\n",
            "Interval 282 (2810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.237 - mean_q: 12.624 - prob: 1.000\n",
            "\n",
            "Interval 283 (2820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 284 (2830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.421 - mean_q: 12.848 - prob: 1.000\n",
            "\n",
            "Interval 285 (2840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.053 - mean_q: 12.305 - prob: 1.000\n",
            "\n",
            "Interval 286 (2850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 287 (2860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.463 - mean_q: 12.917 - prob: 1.000\n",
            "\n",
            "Interval 288 (2870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.001 - mae: 7.517 - mean_q: 13.071 - prob: 1.000\n",
            "\n",
            "Interval 289 (2880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.400 - mean_q: 12.769 - prob: 1.000\n",
            "\n",
            "Interval 290 (2890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.286 - mean_q: 12.572 - prob: 1.000\n",
            "\n",
            "Interval 291 (2900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.721 - mean_q: 13.295 - prob: 1.000\n",
            "\n",
            "Interval 292 (2910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 293 (2920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.397 - mean_q: 12.963 - prob: 1.000\n",
            "\n",
            "Interval 294 (2930 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 295 (2940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.001 - mae: 7.189 - mean_q: 12.411 - prob: 1.000\n",
            "\n",
            "Interval 296 (2950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 297 (2960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.147 - mean_q: 12.484 - prob: 1.000\n",
            "\n",
            "Interval 298 (2970 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 6.948 - mean_q: 12.080 - prob: 1.000\n",
            "\n",
            "Interval 299 (2980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.257 - mean_q: 12.544 - prob: 1.000\n",
            "\n",
            "Interval 300 (2990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.564 - mean_q: 12.895 - prob: 1.000\n",
            "\n",
            "Interval 301 (3000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.420 - mean_q: 12.762 - prob: 1.000\n",
            "\n",
            "Interval 302 (3010 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 303 (3020 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.001 - mae: 7.237 - mean_q: 12.593 - prob: 1.000\n",
            "\n",
            "Interval 304 (3030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -3.7000\n",
            "Interval 305 (3040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -24.000 [-24.000, -24.000] - loss: 0.000 - mae: 7.203 - mean_q: 12.569 - prob: 1.000\n",
            "\n",
            "Interval 306 (3050 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 307 (3060 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.013 - mean_q: 12.309 - prob: 1.000\n",
            "\n",
            "Interval 308 (3070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.195 - mean_q: 12.578 - prob: 1.000\n",
            "\n",
            "Interval 309 (3080 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 310 (3090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.494 - mean_q: 12.815 - prob: 1.000\n",
            "\n",
            "Interval 311 (3100 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.367 - mean_q: 12.761 - prob: 1.000\n",
            "\n",
            "Interval 312 (3110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 313 (3120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.350 - mean_q: 12.784 - prob: 1.000\n",
            "\n",
            "Interval 314 (3130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.082 - mean_q: 12.226 - prob: 1.000\n",
            "\n",
            "Interval 315 (3140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 316 (3150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.326 - mean_q: 12.731 - prob: 1.000\n",
            "\n",
            "Interval 317 (3160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.326 - mean_q: 12.661 - prob: 1.000\n",
            "\n",
            "Interval 318 (3170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 319 (3180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.536 - mean_q: 13.089 - prob: 1.000\n",
            "\n",
            "Interval 320 (3190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.252 - mean_q: 12.485 - prob: 1.000\n",
            "\n",
            "Interval 321 (3200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 322 (3210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.550 - mean_q: 12.996 - prob: 1.000\n",
            "\n",
            "Interval 323 (3220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.069 - mean_q: 12.293 - prob: 1.000\n",
            "\n",
            "Interval 324 (3230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.404 - mean_q: 12.860 - prob: 1.000\n",
            "\n",
            "Interval 325 (3240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 326 (3250 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.187 - mean_q: 12.468 - prob: 1.000\n",
            "\n",
            "Interval 327 (3260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.158 - mean_q: 12.435 - prob: 1.000\n",
            "\n",
            "Interval 328 (3270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 329 (3280 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.000 - mae: 7.120 - mean_q: 12.439 - prob: 1.000\n",
            "\n",
            "Interval 330 (3290 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.002 - mae: 7.222 - mean_q: 12.512 - prob: 1.000\n",
            "\n",
            "Interval 331 (3300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.640 - mean_q: 13.141 - prob: 1.000\n",
            "\n",
            "Interval 332 (3310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 333 (3320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.272 - mean_q: 12.634 - prob: 1.000\n",
            "\n",
            "Interval 334 (3330 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.437 - mean_q: 12.957 - prob: 1.000\n",
            "\n",
            "Interval 335 (3340 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 336 (3350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.209 - mean_q: 12.646 - prob: 1.000\n",
            "\n",
            "Interval 337 (3360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 338 (3370 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.322 - mean_q: 12.832 - prob: 1.000\n",
            "\n",
            "Interval 339 (3380 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.447 - mean_q: 12.931 - prob: 1.000\n",
            "\n",
            "Interval 340 (3390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 341 (3400 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.000 - mae: 7.265 - mean_q: 12.583 - prob: 1.000\n",
            "\n",
            "Interval 342 (3410 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.236 - mean_q: 12.510 - prob: 1.000\n",
            "\n",
            "Interval 343 (3420 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.273 - mean_q: 12.729 - prob: 1.000\n",
            "\n",
            "Interval 344 (3430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 345 (3440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.278 - mean_q: 12.589 - prob: 1.000\n",
            "\n",
            "Interval 346 (3450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 347 (3460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.001 - mae: 7.260 - mean_q: 12.691 - prob: 1.000\n",
            "\n",
            "Interval 348 (3470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.223 - mean_q: 12.451 - prob: 1.000\n",
            "\n",
            "Interval 349 (3480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.001 - mae: 7.551 - mean_q: 12.929 - prob: 1.000\n",
            "\n",
            "Interval 350 (3490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.449 - mean_q: 12.819 - prob: 1.000\n",
            "\n",
            "Interval 351 (3500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.331 - mean_q: 12.804 - prob: 1.000\n",
            "\n",
            "Interval 352 (3510 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 353 (3520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.781 - mean_q: 13.409 - prob: 1.000\n",
            "\n",
            "Interval 354 (3530 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.277 - mean_q: 12.734 - prob: 1.000\n",
            "\n",
            "Interval 355 (3540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 356 (3550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.171 - mean_q: 12.484 - prob: 1.000\n",
            "\n",
            "Interval 357 (3560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.496 - mean_q: 12.911 - prob: 1.000\n",
            "\n",
            "Interval 358 (3570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 359 (3580 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.373 - mean_q: 12.750 - prob: 1.000\n",
            "\n",
            "Interval 360 (3590 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.145 - mean_q: 12.381 - prob: 1.000\n",
            "\n",
            "Interval 361 (3600 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.348 - mean_q: 12.576 - prob: 1.000\n",
            "\n",
            "Interval 362 (3610 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.534 - mean_q: 12.931 - prob: 1.000\n",
            "\n",
            "Interval 363 (3620 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.484 - mean_q: 12.941 - prob: 1.000\n",
            "\n",
            "Interval 364 (3630 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.104 - mean_q: 12.433 - prob: 1.000\n",
            "\n",
            "Interval 365 (3640 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 366 (3650 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.047 - mean_q: 12.182 - prob: 1.000\n",
            "\n",
            "Interval 367 (3660 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.245 - mean_q: 12.660 - prob: 1.000\n",
            "\n",
            "Interval 368 (3670 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.0000\n",
            "Interval 369 (3680 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.522 - mean_q: 13.027 - prob: 1.000\n",
            "\n",
            "Interval 370 (3690 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.227 - mean_q: 12.584 - prob: 1.000\n",
            "\n",
            "Interval 371 (3700 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 372 (3710 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.435 - mean_q: 12.954 - prob: 1.000\n",
            "\n",
            "Interval 373 (3720 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.001 - mae: 7.516 - mean_q: 12.978 - prob: 1.000\n",
            "\n",
            "Interval 374 (3730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.327 - mean_q: 12.782 - prob: 1.000\n",
            "\n",
            "Interval 375 (3740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 376 (3750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 377 (3760 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -22.000 [-22.000, -22.000] - loss: 0.000 - mae: 7.335 - mean_q: 12.723 - prob: 1.000\n",
            "\n",
            "Interval 378 (3770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.376 - mean_q: 12.747 - prob: 1.000\n",
            "\n",
            "Interval 379 (3780 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.140 - mean_q: 12.568 - prob: 1.000\n",
            "\n",
            "Interval 380 (3790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 381 (3800 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.439 - mean_q: 12.712 - prob: 1.000\n",
            "\n",
            "Interval 382 (3810 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.163 - mean_q: 12.454 - prob: 1.000\n",
            "\n",
            "Interval 383 (3820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 384 (3830 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.360 - mean_q: 12.792 - prob: 1.000\n",
            "\n",
            "Interval 385 (3840 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.474 - mean_q: 12.704 - prob: 1.000\n",
            "\n",
            "Interval 386 (3850 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 387 (3860 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.158 - mean_q: 12.449 - prob: 1.000\n",
            "\n",
            "Interval 388 (3870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 389 (3880 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.590 - mean_q: 13.035 - prob: 1.000\n",
            "\n",
            "Interval 390 (3890 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.220 - mean_q: 12.529 - prob: 1.000\n",
            "\n",
            "Interval 391 (3900 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.367 - mean_q: 12.822 - prob: 1.000\n",
            "\n",
            "Interval 392 (3910 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.399 - mean_q: 12.823 - prob: 1.000\n",
            "\n",
            "Interval 393 (3920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 394 (3930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.192 - mean_q: 12.450 - prob: 1.000\n",
            "\n",
            "Interval 395 (3940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.279 - mean_q: 12.698 - prob: 1.000\n",
            "\n",
            "Interval 396 (3950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.471 - mean_q: 12.930 - prob: 1.000\n",
            "\n",
            "Interval 397 (3960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 398 (3970 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.526 - mean_q: 12.968 - prob: 1.000\n",
            "\n",
            "Interval 399 (3980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.159 - mean_q: 12.479 - prob: 1.000\n",
            "\n",
            "Interval 400 (3990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.292 - mean_q: 12.715 - prob: 1.000\n",
            "\n",
            "Interval 401 (4000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 402 (4010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.397 - mean_q: 12.773 - prob: 1.000\n",
            "\n",
            "Interval 403 (4020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.213 - mean_q: 12.500 - prob: 1.000\n",
            "\n",
            "Interval 404 (4030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 405 (4040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.604 - mean_q: 13.109 - prob: 1.000\n",
            "\n",
            "Interval 406 (4050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 407 (4060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.440 - mean_q: 12.861 - prob: 1.000\n",
            "\n",
            "Interval 408 (4070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.405 - mean_q: 12.837 - prob: 1.000\n",
            "\n",
            "Interval 409 (4080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 410 (4090 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.438 - mean_q: 12.755 - prob: 1.000\n",
            "\n",
            "Interval 411 (4100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 412 (4110 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.390 - mean_q: 12.645 - prob: 1.000\n",
            "\n",
            "Interval 413 (4120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.519 - mean_q: 13.033 - prob: 1.000\n",
            "\n",
            "Interval 414 (4130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.524 - mean_q: 13.116 - prob: 1.000\n",
            "\n",
            "Interval 415 (4140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.001 - mae: 7.442 - mean_q: 12.744 - prob: 1.000\n",
            "\n",
            "Interval 416 (4150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.370 - mean_q: 12.830 - prob: 1.000\n",
            "\n",
            "Interval 417 (4160 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 418 (4170 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.011 - mae: 7.464 - mean_q: 12.916 - prob: 1.000\n",
            "\n",
            "Interval 419 (4180 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 420 (4190 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.004 - mae: 7.152 - mean_q: 12.409 - prob: 1.000\n",
            "\n",
            "Interval 421 (4200 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.005 - mae: 7.245 - mean_q: 12.631 - prob: 1.000\n",
            "\n",
            "Interval 422 (4210 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 423 (4220 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.003 - mae: 7.819 - mean_q: 13.420 - prob: 1.000\n",
            "\n",
            "Interval 424 (4230 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 425 (4240 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.153 - mean_q: 12.371 - prob: 1.000\n",
            "\n",
            "Interval 426 (4250 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.298 - mean_q: 12.572 - prob: 1.000\n",
            "\n",
            "Interval 427 (4260 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 428 (4270 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.477 - mean_q: 12.906 - prob: 1.000\n",
            "\n",
            "Interval 429 (4280 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.315 - mean_q: 12.692 - prob: 1.000\n",
            "\n",
            "Interval 430 (4290 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 431 (4300 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 9.000 [5.000, 13.000] - loss: 0.001 - mae: 7.513 - mean_q: 12.941 - prob: 1.000\n",
            "\n",
            "Interval 432 (4310 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 433 (4320 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.606 - mean_q: 13.026 - prob: 1.000\n",
            "\n",
            "Interval 434 (4330 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 435 (4340 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.706 - mean_q: 13.176 - prob: 1.000\n",
            "\n",
            "Interval 436 (4350 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.361 - mean_q: 12.782 - prob: 1.000\n",
            "\n",
            "Interval 437 (4360 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 438 (4370 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.022 - mean_q: 12.254 - prob: 1.000\n",
            "\n",
            "Interval 439 (4380 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.297 - mean_q: 12.603 - prob: 1.000\n",
            "\n",
            "Interval 440 (4390 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 441 (4400 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.544 - mean_q: 12.988 - prob: 1.000\n",
            "\n",
            "Interval 442 (4410 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.313 - mean_q: 12.487 - prob: 1.000\n",
            "\n",
            "Interval 443 (4420 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 444 (4430 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.487 - mean_q: 12.931 - prob: 1.000\n",
            "\n",
            "Interval 445 (4440 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.638 - mean_q: 13.112 - prob: 1.000\n",
            "\n",
            "Interval 446 (4450 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 447 (4460 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.666 - mean_q: 13.174 - prob: 1.000\n",
            "\n",
            "Interval 448 (4470 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 449 (4480 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.215 - mean_q: 12.513 - prob: 1.000\n",
            "\n",
            "Interval 450 (4490 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.580 - mean_q: 13.022 - prob: 1.000\n",
            "\n",
            "Interval 451 (4500 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 452 (4510 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.762 - mean_q: 13.352 - prob: 1.000\n",
            "\n",
            "Interval 453 (4520 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.180 - mean_q: 12.435 - prob: 1.000\n",
            "\n",
            "Interval 454 (4530 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.492 - mean_q: 13.064 - prob: 1.000\n",
            "\n",
            "Interval 455 (4540 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.000 - mae: 7.705 - mean_q: 13.144 - prob: 1.000\n",
            "\n",
            "Interval 456 (4550 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.9000\n",
            "Interval 457 (4560 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.145 - mean_q: 12.429 - prob: 1.000\n",
            "\n",
            "Interval 458 (4570 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 459 (4580 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.000 - mae: 7.110 - mean_q: 12.498 - prob: 1.000\n",
            "\n",
            "Interval 460 (4590 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.374 - mean_q: 12.792 - prob: 1.000\n",
            "\n",
            "Interval 461 (4600 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.388 - mean_q: 12.791 - prob: 1.000\n",
            "\n",
            "Interval 462 (4610 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.000 - mae: 7.020 - mean_q: 12.241 - prob: 1.000\n",
            "\n",
            "Interval 463 (4620 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 464 (4630 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.202 - mean_q: 12.574 - prob: 1.000\n",
            "\n",
            "Interval 465 (4640 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 466 (4650 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.045 - mean_q: 12.357 - prob: 1.000\n",
            "\n",
            "Interval 467 (4660 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.337 - mean_q: 12.779 - prob: 1.000\n",
            "\n",
            "Interval 468 (4670 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: -1.9000\n",
            "Interval 469 (4680 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.253 - mean_q: 12.564 - prob: 1.000\n",
            "\n",
            "Interval 470 (4690 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.541 - mean_q: 13.051 - prob: 1.000\n",
            "\n",
            "Interval 471 (4700 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.452 - mean_q: 12.883 - prob: 1.000\n",
            "\n",
            "Interval 472 (4710 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 473 (4720 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.218 - mean_q: 12.614 - prob: 1.000\n",
            "\n",
            "Interval 474 (4730 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.301 - mean_q: 12.680 - prob: 1.000\n",
            "\n",
            "Interval 475 (4740 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 476 (4750 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.001 - mae: 7.490 - mean_q: 12.815 - prob: 1.000\n",
            "\n",
            "Interval 477 (4760 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 478 (4770 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.455 - mean_q: 12.843 - prob: 1.000\n",
            "\n",
            "Interval 479 (4780 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.458 - mean_q: 12.951 - prob: 1.000\n",
            "\n",
            "Interval 480 (4790 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.408 - mean_q: 12.771 - prob: 1.000\n",
            "\n",
            "Interval 481 (4800 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -3.7000\n",
            "Interval 482 (4810 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 483 (4820 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -33.000 [-33.000, -33.000] - loss: 0.001 - mae: 7.603 - mean_q: 13.089 - prob: 1.000\n",
            "\n",
            "Interval 484 (4830 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.266 - mean_q: 12.541 - prob: 1.000\n",
            "\n",
            "Interval 485 (4840 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.477 - mean_q: 12.809 - prob: 1.000\n",
            "\n",
            "Interval 486 (4850 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 487 (4860 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.466 - mean_q: 12.912 - prob: 1.000\n",
            "\n",
            "Interval 488 (4870 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 489 (4880 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.368 - mean_q: 12.759 - prob: 1.000\n",
            "\n",
            "Interval 490 (4890 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.647 - mean_q: 13.118 - prob: 1.000\n",
            "\n",
            "Interval 491 (4900 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.324 - mean_q: 12.742 - prob: 1.000\n",
            "\n",
            "Interval 492 (4910 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.247 - mean_q: 12.465 - prob: 1.000\n",
            "\n",
            "Interval 493 (4920 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 494 (4930 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.000 - mae: 7.362 - mean_q: 12.603 - prob: 1.000\n",
            "\n",
            "Interval 495 (4940 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.133 - mean_q: 12.404 - prob: 1.000\n",
            "\n",
            "Interval 496 (4950 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 497 (4960 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.241 - mean_q: 12.480 - prob: 1.000\n",
            "\n",
            "Interval 498 (4970 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 499 (4980 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.582 - mean_q: 13.007 - prob: 1.000\n",
            "\n",
            "Interval 500 (4990 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 501 (5000 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.000 - mae: 7.590 - mean_q: 13.037 - prob: 1.000\n",
            "\n",
            "Interval 502 (5010 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.563 - mean_q: 13.007 - prob: 1.000\n",
            "\n",
            "Interval 503 (5020 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 504 (5030 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 11.000 [9.000, 13.000] - loss: 0.000 - mae: 7.396 - mean_q: 12.801 - prob: 1.000\n",
            "\n",
            "Interval 505 (5040 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 506 (5050 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 6.780 - mean_q: 11.961 - prob: 1.000\n",
            "\n",
            "Interval 507 (5060 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.000 - mae: 7.295 - mean_q: 12.668 - prob: 1.000\n",
            "\n",
            "Interval 508 (5070 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 509 (5080 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.349 - mean_q: 12.601 - prob: 1.000\n",
            "\n",
            "Interval 510 (5090 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.427 - mean_q: 12.741 - prob: 1.000\n",
            "\n",
            "Interval 511 (5100 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.000 - mae: 7.394 - mean_q: 12.791 - prob: 1.000\n",
            "\n",
            "Interval 512 (5110 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 513 (5120 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 514 (5130 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.004 - mae: 7.604 - mean_q: 12.945 - prob: 1.000\n",
            "\n",
            "Interval 515 (5140 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.266 - mean_q: 12.585 - prob: 1.000\n",
            "\n",
            "Interval 516 (5150 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 517 (5160 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.352 - mean_q: 12.656 - prob: 1.000\n",
            "\n",
            "Interval 518 (5170 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.401 - mean_q: 12.898 - prob: 1.000\n",
            "\n",
            "Interval 519 (5180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 520 (5190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.456 - mean_q: 12.952 - prob: 1.000\n",
            "\n",
            "Interval 521 (5200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.383 - mean_q: 12.835 - prob: 1.000\n",
            "\n",
            "Interval 522 (5210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.320 - mean_q: 12.699 - prob: 1.000\n",
            "\n",
            "Interval 523 (5220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 524 (5230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.457 - mean_q: 12.905 - prob: 1.000\n",
            "\n",
            "Interval 525 (5240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.690 - mean_q: 13.174 - prob: 1.000\n",
            "\n",
            "Interval 526 (5250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.238 - mean_q: 12.621 - prob: 1.000\n",
            "\n",
            "Interval 527 (5260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.272 - mean_q: 12.606 - prob: 1.000\n",
            "\n",
            "Interval 528 (5270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 529 (5280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.112 - mean_q: 12.382 - prob: 1.000\n",
            "\n",
            "Interval 530 (5290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.470 - mean_q: 13.018 - prob: 1.000\n",
            "\n",
            "Interval 531 (5300 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 532 (5310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 10.500 [9.000, 12.000] - loss: 0.001 - mae: 7.287 - mean_q: 12.537 - prob: 1.000\n",
            "\n",
            "Interval 533 (5320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.000 - mae: 8.004 - mean_q: 13.716 - prob: 1.000\n",
            "\n",
            "Interval 534 (5330 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 535 (5340 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.278 - mean_q: 12.550 - prob: 1.000\n",
            "\n",
            "Interval 536 (5350 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.463 - mean_q: 12.992 - prob: 1.000\n",
            "\n",
            "Interval 537 (5360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.326 - mean_q: 12.616 - prob: 1.000\n",
            "\n",
            "Interval 538 (5370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.437 - mean_q: 12.881 - prob: 1.000\n",
            "\n",
            "Interval 539 (5380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 540 (5390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.367 - mean_q: 12.840 - prob: 1.000\n",
            "\n",
            "Interval 541 (5400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.449 - mean_q: 12.852 - prob: 1.000\n",
            "\n",
            "Interval 542 (5410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.147 - mean_q: 12.414 - prob: 1.000\n",
            "\n",
            "Interval 543 (5420 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 544 (5430 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.348 - mean_q: 12.754 - prob: 1.000\n",
            "\n",
            "Interval 545 (5440 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 546 (5450 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.511 - mean_q: 12.963 - prob: 1.000\n",
            "\n",
            "Interval 547 (5460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.464 - mean_q: 12.911 - prob: 1.000\n",
            "\n",
            "Interval 548 (5470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.000 - mae: 7.470 - mean_q: 12.935 - prob: 1.000\n",
            "\n",
            "Interval 549 (5480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.262 - mean_q: 12.606 - prob: 1.000\n",
            "\n",
            "Interval 550 (5490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 551 (5500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.312 - mean_q: 12.806 - prob: 1.000\n",
            "\n",
            "Interval 552 (5510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.365 - mean_q: 12.728 - prob: 1.000\n",
            "\n",
            "Interval 553 (5520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.121 - mean_q: 12.455 - prob: 1.000\n",
            "\n",
            "Interval 554 (5530 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.858 - mean_q: 13.558 - prob: 1.000\n",
            "\n",
            "Interval 555 (5540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 556 (5550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.314 - mean_q: 12.654 - prob: 1.000\n",
            "\n",
            "Interval 557 (5560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.384 - mean_q: 12.737 - prob: 1.000\n",
            "\n",
            "Interval 558 (5570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 559 (5580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.480 - mean_q: 12.901 - prob: 1.000\n",
            "\n",
            "Interval 560 (5590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.495 - mean_q: 12.994 - prob: 1.000\n",
            "\n",
            "Interval 561 (5600 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 562 (5610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.000 - mae: 7.330 - mean_q: 12.775 - prob: 1.000\n",
            "\n",
            "Interval 563 (5620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.324 - mean_q: 12.739 - prob: 1.000\n",
            "\n",
            "Interval 564 (5630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.000 - mae: 7.143 - mean_q: 12.519 - prob: 1.000\n",
            "\n",
            "Interval 565 (5640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.000 - mae: 7.472 - mean_q: 12.899 - prob: 1.000\n",
            "\n",
            "Interval 566 (5650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.411 - mean_q: 12.755 - prob: 1.000\n",
            "\n",
            "Interval 567 (5660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 568 (5670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.295 - mean_q: 12.643 - prob: 1.000\n",
            "\n",
            "Interval 569 (5680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.195 - mean_q: 12.503 - prob: 1.000\n",
            "\n",
            "Interval 570 (5690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 571 (5700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.318 - mean_q: 12.777 - prob: 1.000\n",
            "\n",
            "Interval 572 (5710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 573 (5720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.000 - mae: 7.062 - mean_q: 12.294 - prob: 1.000\n",
            "\n",
            "Interval 574 (5730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.367 - mean_q: 12.798 - prob: 1.000\n",
            "\n",
            "Interval 575 (5740 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.358 - mean_q: 12.731 - prob: 1.000\n",
            "\n",
            "Interval 576 (5750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 577 (5760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.719 - mean_q: 13.161 - prob: 1.000\n",
            "\n",
            "Interval 578 (5770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.234 - mean_q: 12.499 - prob: 1.000\n",
            "\n",
            "Interval 579 (5780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.101 - mean_q: 12.354 - prob: 1.000\n",
            "\n",
            "Interval 580 (5790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.000 - mae: 7.529 - mean_q: 13.079 - prob: 1.000\n",
            "\n",
            "Interval 581 (5800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 582 (5810 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.535 - mean_q: 12.947 - prob: 1.000\n",
            "\n",
            "Interval 583 (5820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 584 (5830 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.000 - mae: 7.102 - mean_q: 12.325 - prob: 1.000\n",
            "\n",
            "Interval 585 (5840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 586 (5850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.189 - mean_q: 12.687 - prob: 1.000\n",
            "\n",
            "Interval 587 (5860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.045 - mean_q: 12.348 - prob: 1.000\n",
            "\n",
            "Interval 588 (5870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.001 - mae: 7.735 - mean_q: 13.451 - prob: 1.000\n",
            "\n",
            "Interval 589 (5880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.506 - mean_q: 12.900 - prob: 1.000\n",
            "\n",
            "Interval 590 (5890 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 591 (5900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 592 (5910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.463 - mean_q: 12.943 - prob: 1.000\n",
            "\n",
            "Interval 593 (5920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.230 - mean_q: 12.574 - prob: 1.000\n",
            "\n",
            "Interval 594 (5930 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.468 - mean_q: 12.915 - prob: 1.000\n",
            "\n",
            "Interval 595 (5940 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.386 - mean_q: 12.713 - prob: 1.000\n",
            "\n",
            "Interval 596 (5950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.346 - mean_q: 12.723 - prob: 1.000\n",
            "\n",
            "Interval 597 (5960 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 598 (5970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 599 (5980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.549 - mean_q: 13.001 - prob: 1.000\n",
            "\n",
            "Interval 600 (5990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.222 - mean_q: 12.682 - prob: 1.000\n",
            "\n",
            "Interval 601 (6000 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 602 (6010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.327 - mean_q: 12.616 - prob: 1.000\n",
            "\n",
            "Interval 603 (6020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 604 (6030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.451 - mean_q: 12.745 - prob: 1.000\n",
            "\n",
            "Interval 605 (6040 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.309 - mean_q: 12.763 - prob: 1.000\n",
            "\n",
            "Interval 606 (6050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 607 (6060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.335 - mean_q: 12.639 - prob: 1.000\n",
            "\n",
            "Interval 608 (6070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.702 - mean_q: 13.297 - prob: 1.000\n",
            "\n",
            "Interval 609 (6080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.237 - mean_q: 12.529 - prob: 1.000\n",
            "\n",
            "Interval 610 (6090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.378 - mean_q: 12.648 - prob: 1.000\n",
            "\n",
            "Interval 611 (6100 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 612 (6110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.171 - mean_q: 12.511 - prob: 1.000\n",
            "\n",
            "Interval 613 (6120 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 614 (6130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.332 - mean_q: 12.603 - prob: 1.000\n",
            "\n",
            "Interval 615 (6140 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.540 - mean_q: 13.067 - prob: 1.000\n",
            "\n",
            "Interval 616 (6150 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.002 - mae: 7.247 - mean_q: 12.608 - prob: 1.000\n",
            "\n",
            "Interval 617 (6160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 618 (6170 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.486 - mean_q: 12.993 - prob: 1.000\n",
            "\n",
            "Interval 619 (6180 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.396 - mean_q: 12.826 - prob: 1.000\n",
            "\n",
            "Interval 620 (6190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 621 (6200 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.120 - mean_q: 12.433 - prob: 1.000\n",
            "\n",
            "Interval 622 (6210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.316 - mean_q: 12.646 - prob: 1.000\n",
            "\n",
            "Interval 623 (6220 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 624 (6230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.002 - mae: 7.536 - mean_q: 13.013 - prob: 1.000\n",
            "\n",
            "Interval 625 (6240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.354 - mean_q: 12.608 - prob: 1.000\n",
            "\n",
            "Interval 626 (6250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 627 (6260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.395 - mean_q: 12.618 - prob: 1.000\n",
            "\n",
            "Interval 628 (6270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.289 - mean_q: 12.383 - prob: 1.000\n",
            "\n",
            "Interval 629 (6280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 630 (6290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.143 - mean_q: 12.581 - prob: 1.000\n",
            "\n",
            "Interval 631 (6300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 632 (6310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.010 - mae: 7.549 - mean_q: 13.009 - prob: 1.000\n",
            "\n",
            "Interval 633 (6320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.003 - mae: 7.426 - mean_q: 12.875 - prob: 1.000\n",
            "\n",
            "Interval 634 (6330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 635 (6340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.324 - mean_q: 12.749 - prob: 1.000\n",
            "\n",
            "Interval 636 (6350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 637 (6360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.002 - mae: 7.342 - mean_q: 12.636 - prob: 1.000\n",
            "\n",
            "Interval 638 (6370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.425 - mean_q: 12.778 - prob: 1.000\n",
            "\n",
            "Interval 639 (6380 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.002 - mae: 7.609 - mean_q: 13.191 - prob: 1.000\n",
            "\n",
            "Interval 640 (6390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.266 - mean_q: 12.624 - prob: 1.000\n",
            "\n",
            "Interval 641 (6400 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 642 (6410 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.006 - mae: 7.159 - mean_q: 12.485 - prob: 1.000\n",
            "\n",
            "Interval 643 (6420 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 644 (6430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 7.270 - mean_q: 12.669 - prob: 1.000\n",
            "\n",
            "Interval 645 (6440 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.002 - mae: 7.164 - mean_q: 12.576 - prob: 1.000\n",
            "\n",
            "Interval 646 (6450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 647 (6460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.332 - mean_q: 12.869 - prob: 1.000\n",
            "\n",
            "Interval 648 (6470 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.002 - mae: 7.327 - mean_q: 12.600 - prob: 1.000\n",
            "\n",
            "Interval 649 (6480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 650 (6490 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.614 - mean_q: 13.087 - prob: 1.000\n",
            "\n",
            "Interval 651 (6500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.340 - mean_q: 12.679 - prob: 1.000\n",
            "\n",
            "Interval 652 (6510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.374 - mean_q: 12.790 - prob: 1.000\n",
            "\n",
            "Interval 653 (6520 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 654 (6530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.192 - mean_q: 12.599 - prob: 1.000\n",
            "\n",
            "Interval 655 (6540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 656 (6550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.388 - mean_q: 12.792 - prob: 1.000\n",
            "\n",
            "Interval 657 (6560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 658 (6570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.366 - mean_q: 12.796 - prob: 1.000\n",
            "\n",
            "Interval 659 (6580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.491 - mean_q: 12.820 - prob: 1.000\n",
            "\n",
            "Interval 660 (6590 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.465 - mean_q: 12.907 - prob: 1.000\n",
            "\n",
            "Interval 661 (6600 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 662 (6610 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.330 - mean_q: 12.695 - prob: 1.000\n",
            "\n",
            "Interval 663 (6620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 664 (6630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.001 - mae: 7.647 - mean_q: 13.157 - prob: 1.000\n",
            "\n",
            "Interval 665 (6640 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.464 - mean_q: 12.904 - prob: 1.000\n",
            "\n",
            "Interval 666 (6650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.564 - mean_q: 13.001 - prob: 1.000\n",
            "\n",
            "Interval 667 (6660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 668 (6670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.156 - mean_q: 12.319 - prob: 1.000\n",
            "\n",
            "Interval 669 (6680 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.418 - mean_q: 12.845 - prob: 1.000\n",
            "\n",
            "Interval 670 (6690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 671 (6700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.000 - mae: 7.274 - mean_q: 12.569 - prob: 1.000\n",
            "\n",
            "Interval 672 (6710 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 673 (6720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.389 - mean_q: 12.653 - prob: 1.000\n",
            "\n",
            "Interval 674 (6730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.614 - mean_q: 13.061 - prob: 1.000\n",
            "\n",
            "Interval 675 (6740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.489 - mean_q: 12.867 - prob: 1.000\n",
            "\n",
            "Interval 676 (6750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.572 - mean_q: 13.174 - prob: 1.000\n",
            "\n",
            "Interval 677 (6760 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 678 (6770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.001 - mae: 7.659 - mean_q: 13.291 - prob: 1.000\n",
            "\n",
            "Interval 679 (6780 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 680 (6790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.154 - mean_q: 12.433 - prob: 1.000\n",
            "\n",
            "Interval 681 (6800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.464 - mean_q: 12.832 - prob: 1.000\n",
            "\n",
            "Interval 682 (6810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 683 (6820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.479 - mean_q: 12.856 - prob: 1.000\n",
            "\n",
            "Interval 684 (6830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.450 - mean_q: 12.972 - prob: 1.000\n",
            "\n",
            "Interval 685 (6840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.571 - mean_q: 13.083 - prob: 1.000\n",
            "\n",
            "Interval 686 (6850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.607 - mean_q: 13.076 - prob: 1.000\n",
            "\n",
            "Interval 687 (6860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 688 (6870 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 10.000 [6.000, 14.000] - loss: 0.000 - mae: 7.487 - mean_q: 13.101 - prob: 1.000\n",
            "\n",
            "Interval 689 (6880 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.156 - mean_q: 12.534 - prob: 1.000\n",
            "\n",
            "Interval 690 (6890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 691 (6900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 692 (6910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.300 - mean_q: 12.647 - prob: 1.000\n",
            "\n",
            "Interval 693 (6920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.357 - mean_q: 12.830 - prob: 1.000\n",
            "\n",
            "Interval 694 (6930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 695 (6940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.167 - mean_q: 12.188 - prob: 1.000\n",
            "\n",
            "Interval 696 (6950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.620 - mean_q: 13.099 - prob: 1.000\n",
            "\n",
            "Interval 697 (6960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 698 (6970 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.298 - mean_q: 12.621 - prob: 1.000\n",
            "\n",
            "Interval 699 (6980 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.854 - mean_q: 13.357 - prob: 1.000\n",
            "\n",
            "Interval 700 (6990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 701 (7000 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.610 - mean_q: 13.153 - prob: 1.000\n",
            "\n",
            "Interval 702 (7010 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.307 - mean_q: 12.603 - prob: 1.000\n",
            "\n",
            "Interval 703 (7020 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.559 - mean_q: 12.972 - prob: 1.000\n",
            "\n",
            "Interval 704 (7030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.525 - mean_q: 12.955 - prob: 1.000\n",
            "\n",
            "Interval 705 (7040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 706 (7050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.000 - mae: 7.145 - mean_q: 12.361 - prob: 1.000\n",
            "\n",
            "Interval 707 (7060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 708 (7070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.560 - mean_q: 12.878 - prob: 1.000\n",
            "\n",
            "Interval 709 (7080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.000 - mae: 7.651 - mean_q: 13.263 - prob: 1.000\n",
            "\n",
            "Interval 710 (7090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 711 (7100 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -17.000 [-17.000, -17.000] - loss: 0.000 - mae: 7.379 - mean_q: 12.875 - prob: 1.000\n",
            "\n",
            "Interval 712 (7110 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.558 - mean_q: 13.137 - prob: 1.000\n",
            "\n",
            "Interval 713 (7120 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 714 (7130 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.334 - mean_q: 12.692 - prob: 1.000\n",
            "\n",
            "Interval 715 (7140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.410 - mean_q: 12.773 - prob: 1.000\n",
            "\n",
            "Interval 716 (7150 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.571 - mean_q: 13.035 - prob: 1.000\n",
            "\n",
            "Interval 717 (7160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 718 (7170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 719 (7180 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.488 - mean_q: 12.970 - prob: 1.000\n",
            "\n",
            "Interval 720 (7190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.235 - mean_q: 12.590 - prob: 1.000\n",
            "\n",
            "Interval 721 (7200 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 722 (7210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.000 - mae: 7.660 - mean_q: 13.140 - prob: 1.000\n",
            "\n",
            "Interval 723 (7220 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 724 (7230 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -25.000 [-25.000, -25.000] - loss: 0.000 - mae: 7.136 - mean_q: 12.449 - prob: 1.000\n",
            "\n",
            "Interval 725 (7240 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.397 - mean_q: 12.775 - prob: 1.000\n",
            "\n",
            "Interval 726 (7250 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 727 (7260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.193 - mean_q: 12.521 - prob: 1.000\n",
            "\n",
            "Interval 728 (7270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 729 (7280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.271 - mean_q: 12.552 - prob: 1.000\n",
            "\n",
            "Interval 730 (7290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.370 - mean_q: 12.833 - prob: 1.000\n",
            "\n",
            "Interval 731 (7300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 732 (7310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.471 - mean_q: 12.955 - prob: 1.000\n",
            "\n",
            "Interval 733 (7320 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.368 - mean_q: 12.764 - prob: 1.000\n",
            "\n",
            "Interval 734 (7330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 735 (7340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.000 - mae: 7.130 - mean_q: 12.337 - prob: 1.000\n",
            "\n",
            "Interval 736 (7350 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.231 - mean_q: 12.546 - prob: 1.000\n",
            "\n",
            "Interval 737 (7360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 738 (7370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.564 - mean_q: 12.965 - prob: 1.000\n",
            "\n",
            "Interval 739 (7380 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.254 - mean_q: 12.486 - prob: 1.000\n",
            "\n",
            "Interval 740 (7390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.401 - mean_q: 12.838 - prob: 1.000\n",
            "\n",
            "Interval 741 (7400 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.649 - mean_q: 13.075 - prob: 1.000\n",
            "\n",
            "Interval 742 (7410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 743 (7420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.125 - mean_q: 12.409 - prob: 1.000\n",
            "\n",
            "Interval 744 (7430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.546 - mean_q: 12.958 - prob: 1.000\n",
            "\n",
            "Interval 745 (7440 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 746 (7450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -19.000 [-19.000, -19.000] - loss: 0.001 - mae: 7.168 - mean_q: 12.588 - prob: 1.000\n",
            "\n",
            "Interval 747 (7460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.169 - mean_q: 12.564 - prob: 1.000\n",
            "\n",
            "Interval 748 (7470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 749 (7480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.443 - mean_q: 12.898 - prob: 1.000\n",
            "\n",
            "Interval 750 (7490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.188 - mean_q: 12.479 - prob: 1.000\n",
            "\n",
            "Interval 751 (7500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 752 (7510 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.573 - mean_q: 13.051 - prob: 1.000\n",
            "\n",
            "Interval 753 (7520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 754 (7530 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.000 - mae: 7.337 - mean_q: 12.621 - prob: 1.000\n",
            "\n",
            "Interval 755 (7540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.572 - mean_q: 12.943 - prob: 1.000\n",
            "\n",
            "Interval 756 (7550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 757 (7560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.000 - mae: 7.860 - mean_q: 13.528 - prob: 1.000\n",
            "\n",
            "Interval 758 (7570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 759 (7580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -25.000 [-25.000, -25.000] - loss: 0.000 - mae: 7.587 - mean_q: 13.094 - prob: 1.000\n",
            "\n",
            "Interval 760 (7590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.000 - mae: 7.375 - mean_q: 12.685 - prob: 1.000\n",
            "\n",
            "Interval 761 (7600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 762 (7610 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 10.500 [7.000, 14.000] - loss: 0.001 - mae: 7.529 - mean_q: 12.857 - prob: 1.000\n",
            "\n",
            "Interval 763 (7620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 764 (7630 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.486 - mean_q: 12.934 - prob: 1.000\n",
            "\n",
            "Interval 765 (7640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 766 (7650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -22.000 [-22.000, -22.000] - loss: 0.000 - mae: 7.222 - mean_q: 12.515 - prob: 1.000\n",
            "\n",
            "Interval 767 (7660 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.274 - mean_q: 12.683 - prob: 1.000\n",
            "\n",
            "Interval 768 (7670 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.001 - mae: 7.141 - mean_q: 12.476 - prob: 1.000\n",
            "\n",
            "Interval 769 (7680 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.000 - mae: 7.400 - mean_q: 12.721 - prob: 1.000\n",
            "\n",
            "Interval 770 (7690 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.644 - mean_q: 13.083 - prob: 1.000\n",
            "\n",
            "Interval 771 (7700 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.672 - mean_q: 13.169 - prob: 1.000\n",
            "\n",
            "Interval 772 (7710 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.587 - mean_q: 13.200 - prob: 1.000\n",
            "\n",
            "Interval 773 (7720 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.000 - mae: 7.486 - mean_q: 12.970 - prob: 1.000\n",
            "\n",
            "Interval 774 (7730 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.181 - mean_q: 12.545 - prob: 1.000\n",
            "\n",
            "Interval 775 (7740 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 776 (7750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 6.962 - mean_q: 12.189 - prob: 1.000\n",
            "\n",
            "Interval 777 (7760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.538 - mean_q: 13.044 - prob: 1.000\n",
            "\n",
            "Interval 778 (7770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 779 (7780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.477 - mean_q: 12.932 - prob: 1.000\n",
            "\n",
            "Interval 780 (7790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.163 - mean_q: 12.572 - prob: 1.000\n",
            "\n",
            "Interval 781 (7800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.688 - mean_q: 13.331 - prob: 1.000\n",
            "\n",
            "Interval 782 (7810 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 783 (7820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.394 - mean_q: 12.887 - prob: 1.000\n",
            "\n",
            "Interval 784 (7830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.513 - mean_q: 12.930 - prob: 1.000\n",
            "\n",
            "Interval 785 (7840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 786 (7850 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.074 - mean_q: 12.367 - prob: 1.000\n",
            "\n",
            "Interval 787 (7860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 788 (7870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.405 - mean_q: 12.861 - prob: 1.000\n",
            "\n",
            "Interval 789 (7880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.709 - mean_q: 13.302 - prob: 1.000\n",
            "\n",
            "Interval 790 (7890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 791 (7900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.681 - mean_q: 13.076 - prob: 1.000\n",
            "\n",
            "Interval 792 (7910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 793 (7920 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.419 - mean_q: 12.882 - prob: 1.000\n",
            "\n",
            "Interval 794 (7930 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.264 - mean_q: 12.470 - prob: 1.000\n",
            "\n",
            "Interval 795 (7940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 796 (7950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.253 - mean_q: 12.600 - prob: 1.000\n",
            "\n",
            "Interval 797 (7960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 798 (7970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -17.000 [-17.000, -17.000] - loss: 0.001 - mae: 7.382 - mean_q: 12.703 - prob: 1.000\n",
            "\n",
            "Interval 799 (7980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 800 (7990 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -28.000 [-28.000, -28.000] - loss: 0.001 - mae: 7.520 - mean_q: 13.044 - prob: 1.000\n",
            "\n",
            "Interval 801 (8000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.649 - mean_q: 13.148 - prob: 1.000\n",
            "\n",
            "Interval 802 (8010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 803 (8020 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 6.987 - mean_q: 12.158 - prob: 1.000\n",
            "\n",
            "Interval 804 (8030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.219 - mean_q: 12.618 - prob: 1.000\n",
            "\n",
            "Interval 805 (8040 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.670 - mean_q: 13.069 - prob: 1.000\n",
            "\n",
            "Interval 806 (8050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 807 (8060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.582 - mean_q: 13.118 - prob: 1.000\n",
            "\n",
            "Interval 808 (8070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.300 - mean_q: 12.828 - prob: 1.000\n",
            "\n",
            "Interval 809 (8080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 810 (8090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.382 - mean_q: 12.661 - prob: 1.000\n",
            "\n",
            "Interval 811 (8100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 812 (8110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.550 - mean_q: 13.014 - prob: 1.000\n",
            "\n",
            "Interval 813 (8120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.478 - mean_q: 12.848 - prob: 1.000\n",
            "\n",
            "Interval 814 (8130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.336 - mean_q: 12.664 - prob: 1.000\n",
            "\n",
            "Interval 815 (8140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 816 (8150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.623 - mean_q: 13.140 - prob: 1.000\n",
            "\n",
            "Interval 817 (8160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.559 - mean_q: 12.987 - prob: 1.000\n",
            "\n",
            "Interval 818 (8170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 819 (8180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.214 - mean_q: 12.588 - prob: 1.000\n",
            "\n",
            "Interval 820 (8190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 821 (8200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.4000\n",
            "2 episodes - episode_reward: 6.500 [-2.000, 15.000] - loss: 0.001 - mae: 7.290 - mean_q: 12.739 - prob: 1.000\n",
            "\n",
            "Interval 822 (8210 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.279 - mean_q: 12.668 - prob: 1.000\n",
            "\n",
            "Interval 823 (8220 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 824 (8230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 825 (8240 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -19.000 [-19.000, -19.000] - loss: 0.001 - mae: 7.560 - mean_q: 12.982 - prob: 1.000\n",
            "\n",
            "Interval 826 (8250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.340 - mean_q: 12.800 - prob: 1.000\n",
            "\n",
            "Interval 827 (8260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.485 - mean_q: 13.086 - prob: 1.000\n",
            "\n",
            "Interval 828 (8270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 829 (8280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.342 - mean_q: 12.780 - prob: 1.000\n",
            "\n",
            "Interval 830 (8290 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 831 (8300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.351 - mean_q: 12.666 - prob: 1.000\n",
            "\n",
            "Interval 832 (8310 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 833 (8320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.403 - mean_q: 12.917 - prob: 1.000\n",
            "\n",
            "Interval 834 (8330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.311 - mean_q: 12.747 - prob: 1.000\n",
            "\n",
            "Interval 835 (8340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.418 - mean_q: 12.780 - prob: 1.000\n",
            "\n",
            "Interval 836 (8350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 837 (8360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.281 - mean_q: 12.516 - prob: 1.000\n",
            "\n",
            "Interval 838 (8370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 6.977 - mean_q: 12.169 - prob: 1.000\n",
            "\n",
            "Interval 839 (8380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 840 (8390 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.277 - mean_q: 12.615 - prob: 1.000\n",
            "\n",
            "Interval 841 (8400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.556 - mean_q: 13.099 - prob: 1.000\n",
            "\n",
            "Interval 842 (8410 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 843 (8420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.498 - mean_q: 13.005 - prob: 1.000\n",
            "\n",
            "Interval 844 (8430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.222 - mean_q: 12.684 - prob: 1.000\n",
            "\n",
            "Interval 845 (8440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.532 - mean_q: 13.023 - prob: 1.000\n",
            "\n",
            "Interval 846 (8450 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.329 - mean_q: 12.694 - prob: 1.000\n",
            "\n",
            "Interval 847 (8460 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 848 (8470 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.694 - mean_q: 13.125 - prob: 1.000\n",
            "\n",
            "Interval 849 (8480 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 6.976 - mean_q: 12.165 - prob: 1.000\n",
            "\n",
            "Interval 850 (8490 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 851 (8500 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.459 - mean_q: 12.932 - prob: 1.000\n",
            "\n",
            "Interval 852 (8510 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.393 - mean_q: 12.725 - prob: 1.000\n",
            "\n",
            "Interval 853 (8520 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 854 (8530 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.611 - mean_q: 13.158 - prob: 1.000\n",
            "\n",
            "Interval 855 (8540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.000 - mae: 7.379 - mean_q: 12.672 - prob: 1.000\n",
            "\n",
            "Interval 856 (8550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 857 (8560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.147 - mean_q: 12.454 - prob: 1.000\n",
            "\n",
            "Interval 858 (8570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.364 - mean_q: 12.845 - prob: 1.000\n",
            "\n",
            "Interval 859 (8580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 860 (8590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.615 - mean_q: 13.023 - prob: 1.000\n",
            "\n",
            "Interval 861 (8600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.157 - mean_q: 12.546 - prob: 1.000\n",
            "\n",
            "Interval 862 (8610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 863 (8620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.462 - mean_q: 12.825 - prob: 1.000\n",
            "\n",
            "Interval 864 (8630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.864 - mean_q: 13.442 - prob: 1.000\n",
            "\n",
            "Interval 865 (8640 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 866 (8650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.001 - mae: 7.350 - mean_q: 12.660 - prob: 1.000\n",
            "\n",
            "Interval 867 (8660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.001 - mae: 7.243 - mean_q: 12.645 - prob: 1.000\n",
            "\n",
            "Interval 868 (8670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.494 - mean_q: 12.933 - prob: 1.000\n",
            "\n",
            "Interval 869 (8680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.070 - mean_q: 12.407 - prob: 1.000\n",
            "\n",
            "Interval 870 (8690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 871 (8700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.288 - mean_q: 12.602 - prob: 1.000\n",
            "\n",
            "Interval 872 (8710 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.399 - mean_q: 12.834 - prob: 1.000\n",
            "\n",
            "Interval 873 (8720 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 874 (8730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.337 - mean_q: 12.671 - prob: 1.000\n",
            "\n",
            "Interval 875 (8740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.493 - mean_q: 12.878 - prob: 1.000\n",
            "\n",
            "Interval 876 (8750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 877 (8760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.093 - mean_q: 12.389 - prob: 1.000\n",
            "\n",
            "Interval 878 (8770 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.497 - mean_q: 13.036 - prob: 1.000\n",
            "\n",
            "Interval 879 (8780 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 880 (8790 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.622 - mean_q: 13.173 - prob: 1.000\n",
            "\n",
            "Interval 881 (8800 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.247 - mean_q: 12.588 - prob: 1.000\n",
            "\n",
            "Interval 882 (8810 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -20.000 [-20.000, -20.000] - loss: 0.001 - mae: 7.708 - mean_q: 13.221 - prob: 1.000\n",
            "\n",
            "Interval 883 (8820 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 884 (8830 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.567 - mean_q: 13.091 - prob: 1.000\n",
            "\n",
            "Interval 885 (8840 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 886 (8850 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.380 - mean_q: 12.873 - prob: 1.000\n",
            "\n",
            "Interval 887 (8860 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.302 - mean_q: 12.521 - prob: 1.000\n",
            "\n",
            "Interval 888 (8870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 889 (8880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.001 - mae: 7.317 - mean_q: 12.727 - prob: 1.000\n",
            "\n",
            "Interval 890 (8890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.477 - mean_q: 12.784 - prob: 1.000\n",
            "\n",
            "Interval 891 (8900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 892 (8910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.813 - mean_q: 13.253 - prob: 1.000\n",
            "\n",
            "Interval 893 (8920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.002 - mae: 7.629 - mean_q: 13.017 - prob: 1.000\n",
            "\n",
            "Interval 894 (8930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 895 (8940 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.183 - mean_q: 12.500 - prob: 1.000\n",
            "\n",
            "Interval 896 (8950 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 897 (8960 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.683 - mean_q: 13.184 - prob: 1.000\n",
            "\n",
            "Interval 898 (8970 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.566 - mean_q: 12.982 - prob: 1.000\n",
            "\n",
            "Interval 899 (8980 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 900 (8990 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.000 - mae: 7.437 - mean_q: 12.833 - prob: 1.000\n",
            "\n",
            "Interval 901 (9000 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.352 - mean_q: 12.793 - prob: 1.000\n",
            "\n",
            "Interval 902 (9010 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.093 - mean_q: 12.296 - prob: 1.000\n",
            "\n",
            "Interval 903 (9020 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.462 - mean_q: 12.998 - prob: 1.000\n",
            "\n",
            "Interval 904 (9030 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 905 (9040 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.429 - mean_q: 12.862 - prob: 1.000\n",
            "\n",
            "Interval 906 (9050 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.651 - mean_q: 13.149 - prob: 1.000\n",
            "\n",
            "Interval 907 (9060 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.386 - mean_q: 12.765 - prob: 1.000\n",
            "\n",
            "Interval 908 (9070 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 909 (9080 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.267 - mean_q: 12.510 - prob: 1.000\n",
            "\n",
            "Interval 910 (9090 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.549 - mean_q: 13.011 - prob: 1.000\n",
            "\n",
            "Interval 911 (9100 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 912 (9110 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.503 - mean_q: 12.765 - prob: 1.000\n",
            "\n",
            "Interval 913 (9120 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.617 - mean_q: 13.042 - prob: 1.000\n",
            "\n",
            "Interval 914 (9130 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -2.8000\n",
            "Interval 915 (9140 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 916 (9150 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -30.000 [-30.000, -30.000] - loss: 0.002 - mae: 7.647 - mean_q: 13.084 - prob: 1.000\n",
            "\n",
            "Interval 917 (9160 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 8.002 - mean_q: 13.563 - prob: 1.000\n",
            "\n",
            "Interval 918 (9170 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.598 - mean_q: 13.030 - prob: 1.000\n",
            "\n",
            "Interval 919 (9180 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 920 (9190 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.312 - mean_q: 12.667 - prob: 1.000\n",
            "\n",
            "Interval 921 (9200 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 922 (9210 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.398 - mean_q: 12.741 - prob: 1.000\n",
            "\n",
            "Interval 923 (9220 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.577 - mean_q: 12.926 - prob: 1.000\n",
            "\n",
            "Interval 924 (9230 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 925 (9240 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.666 - mean_q: 13.129 - prob: 1.000\n",
            "\n",
            "Interval 926 (9250 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.674 - mean_q: 13.205 - prob: 1.000\n",
            "\n",
            "Interval 927 (9260 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 928 (9270 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.409 - mean_q: 12.841 - prob: 1.000\n",
            "\n",
            "Interval 929 (9280 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.311 - mean_q: 12.569 - prob: 1.000\n",
            "\n",
            "Interval 930 (9290 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.841 - mean_q: 13.219 - prob: 1.000\n",
            "\n",
            "Interval 931 (9300 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 932 (9310 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.198 - mean_q: 12.451 - prob: 1.000\n",
            "\n",
            "Interval 933 (9320 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 934 (9330 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.471 - mean_q: 12.806 - prob: 1.000\n",
            "\n",
            "Interval 935 (9340 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 936 (9350 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.001 - mae: 7.663 - mean_q: 13.185 - prob: 1.000\n",
            "\n",
            "Interval 937 (9360 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.449 - mean_q: 12.928 - prob: 1.000\n",
            "\n",
            "Interval 938 (9370 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 939 (9380 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.402 - mean_q: 12.827 - prob: 1.000\n",
            "\n",
            "Interval 940 (9390 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.341 - mean_q: 12.830 - prob: 1.000\n",
            "\n",
            "Interval 941 (9400 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.245 - mean_q: 12.751 - prob: 1.000\n",
            "\n",
            "Interval 942 (9410 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.000 - mae: 7.384 - mean_q: 12.701 - prob: 1.000\n",
            "\n",
            "Interval 943 (9420 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 944 (9430 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.726 - mean_q: 13.176 - prob: 1.000\n",
            "\n",
            "Interval 945 (9440 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.062 - mean_q: 12.346 - prob: 1.000\n",
            "\n",
            "Interval 946 (9450 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 947 (9460 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 948 (9470 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.645 - mean_q: 13.042 - prob: 1.000\n",
            "\n",
            "Interval 949 (9480 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.000 - mae: 7.332 - mean_q: 12.747 - prob: 1.000\n",
            "\n",
            "Interval 950 (9490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 951 (9500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.260 - mean_q: 12.698 - prob: 1.000\n",
            "\n",
            "Interval 952 (9510 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 953 (9520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.322 - mean_q: 12.565 - prob: 1.000\n",
            "\n",
            "Interval 954 (9530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.000 - mae: 7.266 - mean_q: 12.451 - prob: 1.000\n",
            "\n",
            "Interval 955 (9540 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 956 (9550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.000 - mae: 7.018 - mean_q: 12.306 - prob: 1.000\n",
            "\n",
            "Interval 957 (9560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.539 - mean_q: 13.088 - prob: 1.000\n",
            "\n",
            "Interval 958 (9570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 959 (9580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -18.000 [-18.000, -18.000] - loss: 0.000 - mae: 7.227 - mean_q: 12.526 - prob: 1.000\n",
            "\n",
            "Interval 960 (9590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 961 (9600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 2.3000\n",
            "2 episodes - episode_reward: 6.500 [0.000, 13.000] - loss: 0.001 - mae: 7.239 - mean_q: 12.530 - prob: 1.000\n",
            "\n",
            "Interval 962 (9610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 963 (9620 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 964 (9630 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.000 - mae: 7.561 - mean_q: 12.982 - prob: 1.000\n",
            "\n",
            "Interval 965 (9640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.816 - mean_q: 13.241 - prob: 1.000\n",
            "\n",
            "Interval 966 (9650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.220 - mean_q: 12.425 - prob: 1.000\n",
            "\n",
            "Interval 967 (9660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 968 (9670 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.374 - mean_q: 12.608 - prob: 1.000\n",
            "\n",
            "Interval 969 (9680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.135 - mean_q: 12.366 - prob: 1.000\n",
            "\n",
            "Interval 970 (9690 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 971 (9700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 8.500 [3.000, 14.000] - loss: 0.000 - mae: 7.299 - mean_q: 12.481 - prob: 1.000\n",
            "\n",
            "Interval 972 (9710 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 973 (9720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.000 - mae: 7.218 - mean_q: 12.433 - prob: 1.000\n",
            "\n",
            "Interval 974 (9730 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.000 - mae: 7.210 - mean_q: 12.505 - prob: 1.000\n",
            "\n",
            "Interval 975 (9740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 976 (9750 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.464 - mean_q: 12.903 - prob: 1.000\n",
            "\n",
            "Interval 977 (9760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 978 (9770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.479 - mean_q: 12.906 - prob: 1.000\n",
            "\n",
            "Interval 979 (9780 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.376 - mean_q: 12.744 - prob: 1.000\n",
            "\n",
            "Interval 980 (9790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 981 (9800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.421 - mean_q: 12.810 - prob: 1.000\n",
            "\n",
            "Interval 982 (9810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.710 - mean_q: 13.155 - prob: 1.000\n",
            "\n",
            "Interval 983 (9820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 984 (9830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.000 - mae: 7.399 - mean_q: 12.758 - prob: 1.000\n",
            "\n",
            "Interval 985 (9840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.088 - mean_q: 12.471 - prob: 1.000\n",
            "\n",
            "Interval 986 (9850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.271 - mean_q: 12.484 - prob: 1.000\n",
            "\n",
            "Interval 987 (9860 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 988 (9870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.198 - mean_q: 12.547 - prob: 1.000\n",
            "\n",
            "Interval 989 (9880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.081 - mean_q: 12.493 - prob: 1.000\n",
            "\n",
            "Interval 990 (9890 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.503 - mean_q: 12.919 - prob: 1.000\n",
            "\n",
            "Interval 991 (9900 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 992 (9910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.336 - mean_q: 12.540 - prob: 1.000\n",
            "\n",
            "Interval 993 (9920 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 994 (9930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.000 - mae: 7.643 - mean_q: 13.037 - prob: 1.000\n",
            "\n",
            "Interval 995 (9940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.537 - mean_q: 12.870 - prob: 1.000\n",
            "\n",
            "Interval 996 (9950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.254 - mean_q: 12.607 - prob: 1.000\n",
            "\n",
            "Interval 997 (9960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.000 - mae: 7.559 - mean_q: 13.028 - prob: 1.000\n",
            "\n",
            "Interval 998 (9970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 999 (9980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.331 - mean_q: 12.745 - prob: 1.000\n",
            "\n",
            "Interval 1000 (9990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.770 - mean_q: 13.267 - prob: 1.000\n",
            "\n",
            "Interval 1001 (10000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1002 (10010 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 10.000 [7.000, 13.000] - loss: 0.001 - mae: 7.666 - mean_q: 13.091 - prob: 1.000\n",
            "\n",
            "Interval 1003 (10020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1004 (10030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.527 - mean_q: 12.839 - prob: 1.000\n",
            "\n",
            "Interval 1005 (10040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.323 - mean_q: 12.559 - prob: 1.000\n",
            "\n",
            "Interval 1006 (10050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.269 - mean_q: 12.757 - prob: 1.000\n",
            "\n",
            "Interval 1007 (10060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.000 - mae: 7.231 - mean_q: 12.590 - prob: 1.000\n",
            "\n",
            "Interval 1008 (10070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1009 (10080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.414 - mean_q: 13.010 - prob: 1.000\n",
            "\n",
            "Interval 1010 (10090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1011 (10100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.001 - mae: 7.689 - mean_q: 13.307 - prob: 1.000\n",
            "\n",
            "Interval 1012 (10110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1013 (10120 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.481 - mean_q: 12.864 - prob: 1.000\n",
            "\n",
            "Interval 1014 (10130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.481 - mean_q: 12.893 - prob: 1.000\n",
            "\n",
            "Interval 1015 (10140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.567 - mean_q: 13.030 - prob: 1.000\n",
            "\n",
            "Interval 1016 (10150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1017 (10160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.420 - mean_q: 12.743 - prob: 1.000\n",
            "\n",
            "Interval 1018 (10170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.499 - mean_q: 12.881 - prob: 1.000\n",
            "\n",
            "Interval 1019 (10180 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1020 (10190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.471 - mean_q: 12.936 - prob: 1.000\n",
            "\n",
            "Interval 1021 (10200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1022 (10210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.001 - mae: 7.347 - mean_q: 12.755 - prob: 1.000\n",
            "\n",
            "Interval 1023 (10220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.359 - mean_q: 12.701 - prob: 1.000\n",
            "\n",
            "Interval 1024 (10230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.127 - mean_q: 12.502 - prob: 1.000\n",
            "\n",
            "Interval 1025 (10240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1026 (10250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.237 - mean_q: 12.616 - prob: 1.000\n",
            "\n",
            "Interval 1027 (10260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.421 - mean_q: 12.798 - prob: 1.000\n",
            "\n",
            "Interval 1028 (10270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.328 - mean_q: 12.586 - prob: 1.000\n",
            "\n",
            "Interval 1029 (10280 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.393 - mean_q: 12.703 - prob: 1.000\n",
            "\n",
            "Interval 1030 (10290 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1031 (10300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.787 - mean_q: 13.315 - prob: 1.000\n",
            "\n",
            "Interval 1032 (10310 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1033 (10320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.442 - mean_q: 12.692 - prob: 1.000\n",
            "\n",
            "Interval 1034 (10330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.451 - mean_q: 12.929 - prob: 1.000\n",
            "\n",
            "Interval 1035 (10340 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 1036 (10350 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -25.000 [-25.000, -25.000] - loss: 0.001 - mae: 7.645 - mean_q: 13.042 - prob: 1.000\n",
            "\n",
            "Interval 1037 (10360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1038 (10370 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.659 - mean_q: 13.149 - prob: 1.000\n",
            "\n",
            "Interval 1039 (10380 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 1040 (10390 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 8.000 [4.000, 12.000] - loss: 0.001 - mae: 7.450 - mean_q: 12.908 - prob: 1.000\n",
            "\n",
            "Interval 1041 (10400 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 1042 (10410 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 1043 (10420 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.344 - mean_q: 12.768 - prob: 1.000\n",
            "\n",
            "Interval 1044 (10430 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 1045 (10440 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.325 - mean_q: 12.774 - prob: 1.000\n",
            "\n",
            "Interval 1046 (10450 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 1047 (10460 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.398 - mean_q: 12.854 - prob: 1.000\n",
            "\n",
            "Interval 1048 (10470 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -2.8000\n",
            "Interval 1049 (10480 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -25.000 [-25.000, -25.000] - loss: 0.001 - mae: 7.613 - mean_q: 13.002 - prob: 1.000\n",
            "\n",
            "Interval 1050 (10490 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.454 - mean_q: 12.712 - prob: 1.000\n",
            "\n",
            "Interval 1051 (10500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.484 - mean_q: 12.871 - prob: 1.000\n",
            "\n",
            "Interval 1052 (10510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1053 (10520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.322 - mean_q: 12.640 - prob: 1.000\n",
            "\n",
            "Interval 1054 (10530 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.598 - mean_q: 12.979 - prob: 1.000\n",
            "\n",
            "Interval 1055 (10540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.384 - mean_q: 12.695 - prob: 1.000\n",
            "\n",
            "Interval 1056 (10550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1057 (10560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.287 - mean_q: 12.629 - prob: 1.000\n",
            "\n",
            "Interval 1058 (10570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.579 - mean_q: 13.027 - prob: 1.000\n",
            "\n",
            "Interval 1059 (10580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1060 (10590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1061 (10600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1062 (10610 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -27.000 [-27.000, -27.000] - loss: 0.001 - mae: 7.270 - mean_q: 12.581 - prob: 1.000\n",
            "\n",
            "Interval 1063 (10620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1064 (10630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.364 - mean_q: 12.607 - prob: 1.000\n",
            "\n",
            "Interval 1065 (10640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1066 (10650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.413 - mean_q: 12.837 - prob: 1.000\n",
            "\n",
            "Interval 1067 (10660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.280 - mean_q: 12.563 - prob: 1.000\n",
            "\n",
            "Interval 1068 (10670 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1069 (10680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.339 - mean_q: 12.740 - prob: 1.000\n",
            "\n",
            "Interval 1070 (10690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1071 (10700 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 6.937 - mean_q: 12.127 - prob: 1.000\n",
            "\n",
            "Interval 1072 (10710 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.410 - mean_q: 12.792 - prob: 1.000\n",
            "\n",
            "Interval 1073 (10720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.302 - mean_q: 12.688 - prob: 1.000\n",
            "\n",
            "Interval 1074 (10730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 1075 (10740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.001 - mae: 7.408 - mean_q: 12.804 - prob: 1.000\n",
            "\n",
            "Interval 1076 (10750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.6000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.264 - mean_q: 12.734 - prob: 1.000\n",
            "\n",
            "Interval 1077 (10760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1078 (10770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -29.000 [-29.000, -29.000] - loss: 0.002 - mae: 7.328 - mean_q: 12.618 - prob: 1.000\n",
            "\n",
            "Interval 1079 (10780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1080 (10790 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.545 - mean_q: 13.039 - prob: 1.000\n",
            "\n",
            "Interval 1081 (10800 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.001 - mae: 7.385 - mean_q: 12.757 - prob: 1.000\n",
            "\n",
            "Interval 1082 (10810 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.281 - mean_q: 12.539 - prob: 1.000\n",
            "\n",
            "Interval 1083 (10820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.646 - mean_q: 13.069 - prob: 1.000\n",
            "\n",
            "Interval 1084 (10830 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1085 (10840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.686 - mean_q: 13.205 - prob: 1.000\n",
            "\n",
            "Interval 1086 (10850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.021 - mean_q: 12.295 - prob: 1.000\n",
            "\n",
            "Interval 1087 (10860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 1088 (10870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -27.000 [-27.000, -27.000] - loss: 0.001 - mae: 7.184 - mean_q: 12.384 - prob: 1.000\n",
            "\n",
            "Interval 1089 (10880 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1090 (10890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -18.000 [-18.000, -18.000] - loss: 0.001 - mae: 7.529 - mean_q: 12.941 - prob: 1.000\n",
            "\n",
            "Interval 1091 (10900 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.703 - mean_q: 13.121 - prob: 1.000\n",
            "\n",
            "Interval 1092 (10910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1093 (10920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.581 - mean_q: 13.173 - prob: 1.000\n",
            "\n",
            "Interval 1094 (10930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.304 - mean_q: 12.661 - prob: 1.000\n",
            "\n",
            "Interval 1095 (10940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1096 (10950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.421 - mean_q: 12.829 - prob: 1.000\n",
            "\n",
            "Interval 1097 (10960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1098 (10970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.612 - mean_q: 13.106 - prob: 1.000\n",
            "\n",
            "Interval 1099 (10980 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1100 (10990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.930 - mean_q: 13.562 - prob: 1.000\n",
            "\n",
            "Interval 1101 (11000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1102 (11010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.375 - mean_q: 12.667 - prob: 1.000\n",
            "\n",
            "Interval 1103 (11020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 6.948 - mean_q: 12.157 - prob: 1.000\n",
            "\n",
            "Interval 1104 (11030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1105 (11040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.000 - mae: 7.527 - mean_q: 12.919 - prob: 1.000\n",
            "\n",
            "Interval 1106 (11050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1107 (11060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.000 - mae: 7.231 - mean_q: 12.690 - prob: 1.000\n",
            "\n",
            "Interval 1108 (11070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.336 - mean_q: 12.769 - prob: 1.000\n",
            "\n",
            "Interval 1109 (11080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1110 (11090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.093 - mean_q: 12.305 - prob: 1.000\n",
            "\n",
            "Interval 1111 (11100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.254 - mean_q: 12.498 - prob: 1.000\n",
            "\n",
            "Interval 1112 (11110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.464 - mean_q: 12.828 - prob: 1.000\n",
            "\n",
            "Interval 1113 (11120 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1114 (11130 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1115 (11140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.000 - mae: 7.274 - mean_q: 12.522 - prob: 1.000\n",
            "\n",
            "Interval 1116 (11150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1117 (11160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.000 - mae: 7.213 - mean_q: 12.561 - prob: 1.000\n",
            "\n",
            "Interval 1118 (11170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -22.000 [-22.000, -22.000] - loss: 0.000 - mae: 7.672 - mean_q: 13.116 - prob: 1.000\n",
            "\n",
            "Interval 1119 (11180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1120 (11190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.536 - mean_q: 12.881 - prob: 1.000\n",
            "\n",
            "Interval 1121 (11200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.160 - mean_q: 12.467 - prob: 1.000\n",
            "\n",
            "Interval 1122 (11210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.447 - mean_q: 12.869 - prob: 1.000\n",
            "\n",
            "Interval 1123 (11220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1124 (11230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.410 - mean_q: 12.789 - prob: 1.000\n",
            "\n",
            "Interval 1125 (11240 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1126 (11250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.296 - mean_q: 12.514 - prob: 1.000\n",
            "\n",
            "Interval 1127 (11260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.826 - mean_q: 13.398 - prob: 1.000\n",
            "\n",
            "Interval 1128 (11270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.017 - mean_q: 12.180 - prob: 1.000\n",
            "\n",
            "Interval 1129 (11280 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1130 (11290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.000 - mae: 7.247 - mean_q: 12.520 - prob: 1.000\n",
            "\n",
            "Interval 1131 (11300 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 1132 (11310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1133 (11320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -22.000 [-22.000, -22.000] - loss: 0.001 - mae: 7.246 - mean_q: 12.748 - prob: 1.000\n",
            "\n",
            "Interval 1134 (11330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1135 (11340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -27.000 [-27.000, -27.000] - loss: 0.001 - mae: 7.301 - mean_q: 12.595 - prob: 1.000\n",
            "\n",
            "Interval 1136 (11350 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1137 (11360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1138 (11370 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -21.000 [-21.000, -21.000] - loss: 0.001 - mae: 7.093 - mean_q: 12.329 - prob: 1.000\n",
            "\n",
            "Interval 1139 (11380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.646 - mean_q: 13.092 - prob: 1.000\n",
            "\n",
            "Interval 1140 (11390 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.453 - mean_q: 12.829 - prob: 1.000\n",
            "\n",
            "Interval 1141 (11400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1142 (11410 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.078 - mean_q: 12.183 - prob: 1.000\n",
            "\n",
            "Interval 1143 (11420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1144 (11430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.514 - mean_q: 13.014 - prob: 1.000\n",
            "\n",
            "Interval 1145 (11440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1146 (11450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.171 - mean_q: 12.482 - prob: 1.000\n",
            "\n",
            "Interval 1147 (11460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.796 - mean_q: 13.279 - prob: 1.000\n",
            "\n",
            "Interval 1148 (11470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.701 - mean_q: 13.218 - prob: 1.000\n",
            "\n",
            "Interval 1149 (11480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1150 (11490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.502 - mean_q: 12.825 - prob: 1.000\n",
            "\n",
            "Interval 1151 (11500 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1152 (11510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.269 - mean_q: 12.565 - prob: 1.000\n",
            "\n",
            "Interval 1153 (11520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.289 - mean_q: 12.464 - prob: 1.000\n",
            "\n",
            "Interval 1154 (11530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.464 - mean_q: 12.742 - prob: 1.000\n",
            "\n",
            "Interval 1155 (11540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1156 (11550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.311 - mean_q: 12.522 - prob: 1.000\n",
            "\n",
            "Interval 1157 (11560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1158 (11570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.347 - mean_q: 12.617 - prob: 1.000\n",
            "\n",
            "Interval 1159 (11580 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.570 - mean_q: 13.110 - prob: 1.000\n",
            "\n",
            "Interval 1160 (11590 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1161 (11600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.329 - mean_q: 12.758 - prob: 1.000\n",
            "\n",
            "Interval 1162 (11610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.382 - mean_q: 12.778 - prob: 1.000\n",
            "\n",
            "Interval 1163 (11620 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1164 (11630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.131 - mean_q: 12.359 - prob: 1.000\n",
            "\n",
            "Interval 1165 (11640 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1166 (11650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.479 - mean_q: 12.972 - prob: 1.000\n",
            "\n",
            "Interval 1167 (11660 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 6.933 - mean_q: 11.815 - prob: 1.000\n",
            "\n",
            "Interval 1168 (11670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.464 - mean_q: 12.811 - prob: 1.000\n",
            "\n",
            "Interval 1169 (11680 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1170 (11690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.519 - mean_q: 12.835 - prob: 1.000\n",
            "\n",
            "Interval 1171 (11700 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.327 - mean_q: 12.699 - prob: 1.000\n",
            "\n",
            "Interval 1172 (11710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 1173 (11720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.001 - mae: 7.471 - mean_q: 12.904 - prob: 1.000\n",
            "\n",
            "Interval 1174 (11730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.000 - mae: 7.485 - mean_q: 12.902 - prob: 1.000\n",
            "\n",
            "Interval 1175 (11740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.000 - mae: 7.296 - mean_q: 12.598 - prob: 1.000\n",
            "\n",
            "Interval 1176 (11750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1177 (11760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.000 - mae: 7.412 - mean_q: 12.802 - prob: 1.000\n",
            "\n",
            "Interval 1178 (11770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1179 (11780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.201 - mean_q: 12.473 - prob: 1.000\n",
            "\n",
            "Interval 1180 (11790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.123 - mean_q: 12.353 - prob: 1.000\n",
            "\n",
            "Interval 1181 (11800 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1182 (11810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.508 - mean_q: 13.033 - prob: 1.000\n",
            "\n",
            "Interval 1183 (11820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.250 - mean_q: 12.631 - prob: 1.000\n",
            "\n",
            "Interval 1184 (11830 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 1185 (11840 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.324 - mean_q: 12.656 - prob: 1.000\n",
            "\n",
            "Interval 1186 (11850 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1187 (11860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.570 - mean_q: 12.976 - prob: 1.000\n",
            "\n",
            "Interval 1188 (11870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.152 - mean_q: 12.453 - prob: 1.000\n",
            "\n",
            "Interval 1189 (11880 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1190 (11890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.328 - mean_q: 12.727 - prob: 1.000\n",
            "\n",
            "Interval 1191 (11900 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1192 (11910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.428 - mean_q: 12.793 - prob: 1.000\n",
            "\n",
            "Interval 1193 (11920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.311 - mean_q: 12.541 - prob: 1.000\n",
            "\n",
            "Interval 1194 (11930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1195 (11940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.534 - mean_q: 12.817 - prob: 1.000\n",
            "\n",
            "Interval 1196 (11950 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1197 (11960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 5.000 [-4.000, 14.000] - loss: 0.002 - mae: 7.669 - mean_q: 13.158 - prob: 1.000\n",
            "\n",
            "Interval 1198 (11970 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1199 (11980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.499 - mean_q: 12.808 - prob: 1.000\n",
            "\n",
            "Interval 1200 (11990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.448 - mean_q: 12.929 - prob: 1.000\n",
            "\n",
            "Interval 1201 (12000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1202 (12010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -26.000 [-26.000, -26.000] - loss: 0.001 - mae: 7.495 - mean_q: 12.757 - prob: 1.000\n",
            "\n",
            "Interval 1203 (12020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1204 (12030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.354 - mean_q: 12.694 - prob: 1.000\n",
            "\n",
            "Interval 1205 (12040 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.001 - mae: 7.295 - mean_q: 12.635 - prob: 1.000\n",
            "\n",
            "Interval 1206 (12050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.614 - mean_q: 13.094 - prob: 1.000\n",
            "\n",
            "Interval 1207 (12060 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1208 (12070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.002 - mae: 7.300 - mean_q: 12.450 - prob: 1.000\n",
            "\n",
            "Interval 1209 (12080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.491 - mean_q: 12.706 - prob: 1.000\n",
            "\n",
            "Interval 1210 (12090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.398 - mean_q: 12.765 - prob: 1.000\n",
            "\n",
            "Interval 1211 (12100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 6.995 - mean_q: 12.209 - prob: 1.000\n",
            "\n",
            "Interval 1212 (12110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1213 (12120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.430 - mean_q: 12.796 - prob: 1.000\n",
            "\n",
            "Interval 1214 (12130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.581 - mean_q: 12.998 - prob: 1.000\n",
            "\n",
            "Interval 1215 (12140 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1216 (12150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.441 - mean_q: 12.918 - prob: 1.000\n",
            "\n",
            "Interval 1217 (12160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.448 - mean_q: 12.883 - prob: 1.000\n",
            "\n",
            "Interval 1218 (12170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1219 (12180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.346 - mean_q: 12.624 - prob: 1.000\n",
            "\n",
            "Interval 1220 (12190 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1221 (12200 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.001 - mae: 7.307 - mean_q: 12.650 - prob: 1.000\n",
            "\n",
            "Interval 1222 (12210 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1223 (12220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.397 - mean_q: 12.721 - prob: 1.000\n",
            "\n",
            "Interval 1224 (12230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.002 - mae: 7.406 - mean_q: 12.698 - prob: 1.000\n",
            "\n",
            "Interval 1225 (12240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1226 (12250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.381 - mean_q: 12.666 - prob: 1.000\n",
            "\n",
            "Interval 1227 (12260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.449 - mean_q: 12.792 - prob: 1.000\n",
            "\n",
            "Interval 1228 (12270 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.027 - mean_q: 12.170 - prob: 1.000\n",
            "\n",
            "Interval 1229 (12280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1230 (12290 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.711 - mean_q: 13.190 - prob: 1.000\n",
            "\n",
            "Interval 1231 (12300 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.558 - mean_q: 13.041 - prob: 1.000\n",
            "\n",
            "Interval 1232 (12310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1233 (12320 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.080 - mean_q: 12.301 - prob: 1.000\n",
            "\n",
            "Interval 1234 (12330 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.001 - mae: 7.319 - mean_q: 12.691 - prob: 1.000\n",
            "\n",
            "Interval 1235 (12340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1236 (12350 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.473 - mean_q: 12.941 - prob: 1.000\n",
            "\n",
            "Interval 1237 (12360 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.496 - mean_q: 12.804 - prob: 1.000\n",
            "\n",
            "Interval 1238 (12370 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.029 - mean_q: 12.303 - prob: 1.000\n",
            "\n",
            "Interval 1239 (12380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1240 (12390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.407 - mean_q: 12.787 - prob: 1.000\n",
            "\n",
            "Interval 1241 (12400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.525 - mean_q: 13.015 - prob: 1.000\n",
            "\n",
            "Interval 1242 (12410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1243 (12420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.529 - mean_q: 12.939 - prob: 1.000\n",
            "\n",
            "Interval 1244 (12430 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.245 - mean_q: 12.482 - prob: 1.000\n",
            "\n",
            "Interval 1245 (12440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1246 (12450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.191 - mean_q: 12.459 - prob: 1.000\n",
            "\n",
            "Interval 1247 (12460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.391 - mean_q: 12.797 - prob: 1.000\n",
            "\n",
            "Interval 1248 (12470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1249 (12480 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.388 - mean_q: 12.726 - prob: 1.000\n",
            "\n",
            "Interval 1250 (12490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.459 - mean_q: 12.806 - prob: 1.000\n",
            "\n",
            "Interval 1251 (12500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.725 - mean_q: 13.095 - prob: 1.000\n",
            "\n",
            "Interval 1252 (12510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.575 - mean_q: 12.990 - prob: 1.000\n",
            "\n",
            "Interval 1253 (12520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1254 (12530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.366 - mean_q: 12.610 - prob: 1.000\n",
            "\n",
            "Interval 1255 (12540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.343 - mean_q: 12.604 - prob: 1.000\n",
            "\n",
            "Interval 1256 (12550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1257 (12560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.429 - mean_q: 12.794 - prob: 1.000\n",
            "\n",
            "Interval 1258 (12570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.360 - mean_q: 12.717 - prob: 1.000\n",
            "\n",
            "Interval 1259 (12580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.286 - mean_q: 12.503 - prob: 1.000\n",
            "\n",
            "Interval 1260 (12590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 1261 (12600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.001 - mae: 7.088 - mean_q: 12.305 - prob: 1.000\n",
            "\n",
            "Interval 1262 (12610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.573 - mean_q: 12.973 - prob: 1.000\n",
            "\n",
            "Interval 1263 (12620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.118 - mean_q: 12.207 - prob: 1.000\n",
            "\n",
            "Interval 1264 (12630 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.348 - mean_q: 12.510 - prob: 1.000\n",
            "\n",
            "Interval 1265 (12640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.000 - mae: 7.363 - mean_q: 12.772 - prob: 1.000\n",
            "\n",
            "Interval 1266 (12650 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1267 (12660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.590 - mean_q: 13.101 - prob: 1.000\n",
            "\n",
            "Interval 1268 (12670 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1269 (12680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 9.000 [6.000, 12.000] - loss: 0.001 - mae: 7.302 - mean_q: 12.621 - prob: 1.000\n",
            "\n",
            "Interval 1270 (12690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1271 (12700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.144 - mean_q: 12.595 - prob: 1.000\n",
            "\n",
            "Interval 1272 (12710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.688 - mean_q: 13.242 - prob: 1.000\n",
            "\n",
            "Interval 1273 (12720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.210 - mean_q: 12.481 - prob: 1.000\n",
            "\n",
            "Interval 1274 (12730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1275 (12740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.417 - mean_q: 12.699 - prob: 1.000\n",
            "\n",
            "Interval 1276 (12750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.434 - mean_q: 12.742 - prob: 1.000\n",
            "\n",
            "Interval 1277 (12760 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1278 (12770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.429 - mean_q: 12.792 - prob: 1.000\n",
            "\n",
            "Interval 1279 (12780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1280 (12790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -18.000 [-18.000, -18.000] - loss: 0.000 - mae: 7.072 - mean_q: 12.254 - prob: 1.000\n",
            "\n",
            "Interval 1281 (12800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1282 (12810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.768 - mean_q: 13.257 - prob: 1.000\n",
            "\n",
            "Interval 1283 (12820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.000 - mae: 7.040 - mean_q: 12.299 - prob: 1.000\n",
            "\n",
            "Interval 1284 (12830 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1285 (12840 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.174 - mean_q: 12.479 - prob: 1.000\n",
            "\n",
            "Interval 1286 (12850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.663 - mean_q: 13.052 - prob: 1.000\n",
            "\n",
            "Interval 1287 (12860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.000 - mae: 7.494 - mean_q: 12.857 - prob: 1.000\n",
            "\n",
            "Interval 1288 (12870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1289 (12880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.000 - mae: 7.692 - mean_q: 13.302 - prob: 1.000\n",
            "\n",
            "Interval 1290 (12890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1291 (12900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.282 - mean_q: 12.477 - prob: 1.000\n",
            "\n",
            "Interval 1292 (12910 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1293 (12920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.000 - mae: 7.606 - mean_q: 13.088 - prob: 1.000\n",
            "\n",
            "Interval 1294 (12930 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1295 (12940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.000 - mae: 7.369 - mean_q: 12.782 - prob: 1.000\n",
            "\n",
            "Interval 1296 (12950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.297 - mean_q: 12.591 - prob: 1.000\n",
            "\n",
            "Interval 1297 (12960 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1298 (12970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 6.940 - mean_q: 12.217 - prob: 1.000\n",
            "\n",
            "Interval 1299 (12980 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1300 (12990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.000 - mae: 7.366 - mean_q: 12.768 - prob: 1.000\n",
            "\n",
            "Interval 1301 (13000 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1302 (13010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.457 - mean_q: 12.733 - prob: 1.000\n",
            "\n",
            "Interval 1303 (13020 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1304 (13030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 8.500 [5.000, 12.000] - loss: 0.000 - mae: 7.257 - mean_q: 12.385 - prob: 1.000\n",
            "\n",
            "Interval 1305 (13040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1306 (13050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.429 - mean_q: 12.688 - prob: 1.000\n",
            "\n",
            "Interval 1307 (13060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1308 (13070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.433 - mean_q: 12.770 - prob: 1.000\n",
            "\n",
            "Interval 1309 (13080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.325 - mean_q: 12.493 - prob: 1.000\n",
            "\n",
            "Interval 1310 (13090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1311 (13100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.277 - mean_q: 12.496 - prob: 1.000\n",
            "\n",
            "Interval 1312 (13110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1313 (13120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.447 - mean_q: 12.731 - prob: 1.000\n",
            "\n",
            "Interval 1314 (13130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.018 - mean_q: 12.217 - prob: 1.000\n",
            "\n",
            "Interval 1315 (13140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1316 (13150 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.419 - mean_q: 12.692 - prob: 1.000\n",
            "\n",
            "Interval 1317 (13160 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 1318 (13170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.567 - mean_q: 12.944 - prob: 1.000\n",
            "\n",
            "Interval 1319 (13180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.001 - mae: 7.500 - mean_q: 12.869 - prob: 1.000\n",
            "\n",
            "Interval 1320 (13190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1321 (13200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.189 - mean_q: 12.489 - prob: 1.000\n",
            "\n",
            "Interval 1322 (13210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.132 - mean_q: 12.311 - prob: 1.000\n",
            "\n",
            "Interval 1323 (13220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.388 - mean_q: 12.737 - prob: 1.000\n",
            "\n",
            "Interval 1324 (13230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.321 - mean_q: 12.776 - prob: 1.000\n",
            "\n",
            "Interval 1325 (13240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1326 (13250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1327 (13260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.001 - mae: 7.628 - mean_q: 13.077 - prob: 1.000\n",
            "\n",
            "Interval 1328 (13270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.130 - mean_q: 12.497 - prob: 1.000\n",
            "\n",
            "Interval 1329 (13280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.525 - mean_q: 12.845 - prob: 1.000\n",
            "\n",
            "Interval 1330 (13290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.089 - mean_q: 12.405 - prob: 1.000\n",
            "\n",
            "Interval 1331 (13300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.197 - mean_q: 12.424 - prob: 1.000\n",
            "\n",
            "Interval 1332 (13310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1333 (13320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.399 - mean_q: 12.609 - prob: 1.000\n",
            "\n",
            "Interval 1334 (13330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.534 - mean_q: 12.997 - prob: 1.000\n",
            "\n",
            "Interval 1335 (13340 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1336 (13350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.226 - mean_q: 12.462 - prob: 1.000\n",
            "\n",
            "Interval 1337 (13360 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.293 - mean_q: 12.558 - prob: 1.000\n",
            "\n",
            "Interval 1338 (13370 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1339 (13380 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1340 (13390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.453 - mean_q: 12.802 - prob: 1.000\n",
            "\n",
            "Interval 1341 (13400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.340 - mean_q: 12.593 - prob: 1.000\n",
            "\n",
            "Interval 1342 (13410 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.286 - mean_q: 12.502 - prob: 1.000\n",
            "\n",
            "Interval 1343 (13420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1344 (13430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.387 - mean_q: 12.599 - prob: 1.000\n",
            "\n",
            "Interval 1345 (13440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.380 - mean_q: 12.818 - prob: 1.000\n",
            "\n",
            "Interval 1346 (13450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.607 - mean_q: 13.162 - prob: 1.000\n",
            "\n",
            "Interval 1347 (13460 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1348 (13470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.090 - mean_q: 12.337 - prob: 1.000\n",
            "\n",
            "Interval 1349 (13480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.629 - mean_q: 13.112 - prob: 1.000\n",
            "\n",
            "Interval 1350 (13490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1351 (13500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.267 - mean_q: 12.508 - prob: 1.000\n",
            "\n",
            "Interval 1352 (13510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.000 - mae: 6.990 - mean_q: 12.146 - prob: 1.000\n",
            "\n",
            "Interval 1353 (13520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1354 (13530 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.469 - mean_q: 12.945 - prob: 1.000\n",
            "\n",
            "Interval 1355 (13540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.217 - mean_q: 12.466 - prob: 1.000\n",
            "\n",
            "Interval 1356 (13550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.256 - mean_q: 12.620 - prob: 1.000\n",
            "\n",
            "Interval 1357 (13560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1358 (13570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.321 - mean_q: 12.502 - prob: 1.000\n",
            "\n",
            "Interval 1359 (13580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.639 - mean_q: 13.085 - prob: 1.000\n",
            "\n",
            "Interval 1360 (13590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.504 - mean_q: 12.960 - prob: 1.000\n",
            "\n",
            "Interval 1361 (13600 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1362 (13610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.553 - mean_q: 12.917 - prob: 1.000\n",
            "\n",
            "Interval 1363 (13620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1364 (13630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.000 - mae: 7.463 - mean_q: 12.821 - prob: 1.000\n",
            "\n",
            "Interval 1365 (13640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.547 - mean_q: 12.990 - prob: 1.000\n",
            "\n",
            "Interval 1366 (13650 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1367 (13660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.000 - mae: 7.502 - mean_q: 12.823 - prob: 1.000\n",
            "\n",
            "Interval 1368 (13670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.143 - mean_q: 12.486 - prob: 1.000\n",
            "\n",
            "Interval 1369 (13680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1370 (13690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.126 - mean_q: 12.445 - prob: 1.000\n",
            "\n",
            "Interval 1371 (13700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.034 - mean_q: 12.164 - prob: 1.000\n",
            "\n",
            "Interval 1372 (13710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.677 - mean_q: 13.214 - prob: 1.000\n",
            "\n",
            "Interval 1373 (13720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1374 (13730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.644 - mean_q: 13.019 - prob: 1.000\n",
            "\n",
            "Interval 1375 (13740 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.000 - mae: 7.422 - mean_q: 12.778 - prob: 1.000\n",
            "\n",
            "Interval 1376 (13750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.646 - mean_q: 12.992 - prob: 1.000\n",
            "\n",
            "Interval 1377 (13760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.545 - mean_q: 13.053 - prob: 1.000\n",
            "\n",
            "Interval 1378 (13770 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1379 (13780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.339 - mean_q: 12.610 - prob: 1.000\n",
            "\n",
            "Interval 1380 (13790 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 1381 (13800 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.000 - mae: 7.416 - mean_q: 12.729 - prob: 1.000\n",
            "\n",
            "Interval 1382 (13810 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.337 - mean_q: 12.674 - prob: 1.000\n",
            "\n",
            "Interval 1383 (13820 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.579 - mean_q: 12.945 - prob: 1.000\n",
            "\n",
            "Interval 1384 (13830 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.9000\n",
            "Interval 1385 (13840 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.429 - mean_q: 12.809 - prob: 1.000\n",
            "\n",
            "Interval 1386 (13850 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: -1.0000\n",
            "Interval 1387 (13860 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.476 - mean_q: 12.970 - prob: 1.000\n",
            "\n",
            "Interval 1388 (13870 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.584 - mean_q: 13.074 - prob: 1.000\n",
            "\n",
            "Interval 1389 (13880 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.332 - mean_q: 12.617 - prob: 1.000\n",
            "\n",
            "Interval 1390 (13890 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 1391 (13900 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.000 - mae: 7.516 - mean_q: 12.884 - prob: 1.000\n",
            "\n",
            "Interval 1392 (13910 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.185 - mean_q: 12.573 - prob: 1.000\n",
            "\n",
            "Interval 1393 (13920 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 1394 (13930 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.392 - mean_q: 12.916 - prob: 1.000\n",
            "\n",
            "Interval 1395 (13940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.523 - mean_q: 12.927 - prob: 1.000\n",
            "\n",
            "Interval 1396 (13950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.271 - mean_q: 12.574 - prob: 1.000\n",
            "\n",
            "Interval 1397 (13960 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.001 - mae: 7.163 - mean_q: 12.425 - prob: 1.000\n",
            "\n",
            "Interval 1398 (13970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1399 (13980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.596 - mean_q: 13.034 - prob: 1.000\n",
            "\n",
            "Interval 1400 (13990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1401 (14000 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.656 - mean_q: 13.155 - prob: 1.000\n",
            "\n",
            "Interval 1402 (14010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1403 (14020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.437 - mean_q: 12.787 - prob: 1.000\n",
            "\n",
            "Interval 1404 (14030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.326 - mean_q: 12.647 - prob: 1.000\n",
            "\n",
            "Interval 1405 (14040 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.018 - mean_q: 12.203 - prob: 1.000\n",
            "\n",
            "Interval 1406 (14050 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1407 (14060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.019 - mean_q: 12.081 - prob: 1.000\n",
            "\n",
            "Interval 1408 (14070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.483 - mean_q: 12.919 - prob: 1.000\n",
            "\n",
            "Interval 1409 (14080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1410 (14090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.565 - mean_q: 13.024 - prob: 1.000\n",
            "\n",
            "Interval 1411 (14100 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.001 - mae: 7.374 - mean_q: 12.751 - prob: 1.000\n",
            "\n",
            "Interval 1412 (14110 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.449 - mean_q: 12.823 - prob: 1.000\n",
            "\n",
            "Interval 1413 (14120 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 1414 (14130 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.511 - mean_q: 12.917 - prob: 1.000\n",
            "\n",
            "Interval 1415 (14140 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 1416 (14150 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.410 - mean_q: 12.881 - prob: 1.000\n",
            "\n",
            "Interval 1417 (14160 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.277 - mean_q: 12.629 - prob: 1.000\n",
            "\n",
            "Interval 1418 (14170 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 1419 (14180 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.212 - mean_q: 12.398 - prob: 1.000\n",
            "\n",
            "Interval 1420 (14190 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.001 - mae: 7.493 - mean_q: 12.853 - prob: 1.000\n",
            "\n",
            "Interval 1421 (14200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.463 - mean_q: 12.873 - prob: 1.000\n",
            "\n",
            "Interval 1422 (14210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.134 - mean_q: 12.458 - prob: 1.000\n",
            "\n",
            "Interval 1423 (14220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1424 (14230 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1425 (14240 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.395 - mean_q: 12.717 - prob: 1.000\n",
            "\n",
            "Interval 1426 (14250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.402 - mean_q: 12.581 - prob: 1.000\n",
            "\n",
            "Interval 1427 (14260 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.457 - mean_q: 12.848 - prob: 1.000\n",
            "\n",
            "Interval 1428 (14270 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.9000\n",
            "Interval 1429 (14280 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.001 - mae: 7.548 - mean_q: 12.947 - prob: 1.000\n",
            "\n",
            "Interval 1430 (14290 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 1431 (14300 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.000 - mae: 7.362 - mean_q: 12.743 - prob: 1.000\n",
            "\n",
            "Interval 1432 (14310 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 6.979 - mean_q: 12.235 - prob: 1.000\n",
            "\n",
            "Interval 1433 (14320 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 1434 (14330 steps performed)\n",
            "10/10 [==============================] - 0s 14ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.685 - mean_q: 13.258 - prob: 1.000\n",
            "\n",
            "Interval 1435 (14340 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 1436 (14350 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.263 - mean_q: 12.486 - prob: 1.000\n",
            "\n",
            "Interval 1437 (14360 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.468 - mean_q: 12.815 - prob: 1.000\n",
            "\n",
            "Interval 1438 (14370 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 6.995 - mean_q: 12.140 - prob: 1.000\n",
            "\n",
            "Interval 1439 (14380 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.9000\n",
            "Interval 1440 (14390 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.000 - mae: 7.360 - mean_q: 12.687 - prob: 1.000\n",
            "\n",
            "Interval 1441 (14400 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.356 - mean_q: 12.726 - prob: 1.000\n",
            "\n",
            "Interval 1442 (14410 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.462 - mean_q: 12.854 - prob: 1.000\n",
            "\n",
            "Interval 1443 (14420 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: -1.0000\n",
            "Interval 1444 (14430 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.588 - mean_q: 13.041 - prob: 1.000\n",
            "\n",
            "Interval 1445 (14440 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.530 - mean_q: 12.800 - prob: 1.000\n",
            "\n",
            "Interval 1446 (14450 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 1447 (14460 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.343 - mean_q: 12.579 - prob: 1.000\n",
            "\n",
            "Interval 1448 (14470 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.9000\n",
            "Interval 1449 (14480 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.620 - mean_q: 13.043 - prob: 1.000\n",
            "\n",
            "Interval 1450 (14490 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.505 - mean_q: 12.876 - prob: 1.000\n",
            "\n",
            "Interval 1451 (14500 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.269 - mean_q: 12.606 - prob: 1.000\n",
            "\n",
            "Interval 1452 (14510 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.459 - mean_q: 12.813 - prob: 1.000\n",
            "\n",
            "Interval 1453 (14520 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 1454 (14530 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.436 - mean_q: 12.694 - prob: 1.000\n",
            "\n",
            "Interval 1455 (14540 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.328 - mean_q: 12.659 - prob: 1.000\n",
            "\n",
            "Interval 1456 (14550 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.387 - mean_q: 12.646 - prob: 1.000\n",
            "\n",
            "Interval 1457 (14560 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 1458 (14570 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.235 - mean_q: 12.499 - prob: 1.000\n",
            "\n",
            "Interval 1459 (14580 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.040 - mean_q: 12.199 - prob: 1.000\n",
            "\n",
            "Interval 1460 (14590 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 1461 (14600 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.423 - mean_q: 12.832 - prob: 1.000\n",
            "\n",
            "Interval 1462 (14610 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 1463 (14620 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.001 - mae: 7.321 - mean_q: 12.633 - prob: 1.000\n",
            "\n",
            "Interval 1464 (14630 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 1465 (14640 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.001 - mae: 7.286 - mean_q: 12.567 - prob: 1.000\n",
            "\n",
            "Interval 1466 (14650 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 1467 (14660 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.003 - mean_q: 12.266 - prob: 1.000\n",
            "\n",
            "Interval 1468 (14670 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 1469 (14680 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.524 - mean_q: 12.842 - prob: 1.000\n",
            "\n",
            "Interval 1470 (14690 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -2.8000\n",
            "Interval 1471 (14700 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -19.000 [-19.000, -19.000] - loss: 0.000 - mae: 7.406 - mean_q: 12.909 - prob: 1.000\n",
            "\n",
            "Interval 1472 (14710 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.196 - mean_q: 12.453 - prob: 1.000\n",
            "\n",
            "Interval 1473 (14720 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 1474 (14730 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 1475 (14740 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 6.996 - mean_q: 12.034 - prob: 1.000\n",
            "\n",
            "Interval 1476 (14750 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.289 - mean_q: 12.506 - prob: 1.000\n",
            "\n",
            "Interval 1477 (14760 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -2.8000\n",
            "Interval 1478 (14770 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -26.000 [-26.000, -26.000] - loss: 0.000 - mae: 7.208 - mean_q: 12.612 - prob: 1.000\n",
            "\n",
            "Interval 1479 (14780 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.0000\n",
            "Interval 1480 (14790 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.321 - mean_q: 12.751 - prob: 1.000\n",
            "\n",
            "Interval 1481 (14800 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.654 - mean_q: 13.169 - prob: 1.000\n",
            "\n",
            "Interval 1482 (14810 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.9000\n",
            "Interval 1483 (14820 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.164 - mean_q: 12.439 - prob: 1.000\n",
            "\n",
            "Interval 1484 (14830 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.526 - mean_q: 12.821 - prob: 1.000\n",
            "\n",
            "Interval 1485 (14840 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.9000\n",
            "Interval 1486 (14850 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.001 - mae: 7.203 - mean_q: 12.451 - prob: 1.000\n",
            "\n",
            "Interval 1487 (14860 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.9000\n",
            "Interval 1488 (14870 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.425 - mean_q: 12.779 - prob: 1.000\n",
            "\n",
            "Interval 1489 (14880 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.001 - mae: 7.200 - mean_q: 12.526 - prob: 1.000\n",
            "\n",
            "Interval 1490 (14890 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.141 - mean_q: 12.462 - prob: 1.000\n",
            "\n",
            "Interval 1491 (14900 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 1492 (14910 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 6.992 - mean_q: 12.234 - prob: 1.000\n",
            "\n",
            "Interval 1493 (14920 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 1494 (14930 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.249 - mean_q: 12.547 - prob: 1.000\n",
            "\n",
            "Interval 1495 (14940 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.001 - mae: 7.141 - mean_q: 12.380 - prob: 1.000\n",
            "\n",
            "Interval 1496 (14950 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 1497 (14960 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.000 - mae: 7.257 - mean_q: 12.446 - prob: 1.000\n",
            "\n",
            "Interval 1498 (14970 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.330 - mean_q: 12.749 - prob: 1.000\n",
            "\n",
            "Interval 1499 (14980 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 1500 (14990 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.0000\n",
            "Interval 1501 (15000 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.522 - mean_q: 12.957 - prob: 1.000\n",
            "\n",
            "Interval 1502 (15010 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.260 - mean_q: 12.527 - prob: 1.000\n",
            "\n",
            "Interval 1503 (15020 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.490 - mean_q: 12.853 - prob: 1.000\n",
            "\n",
            "Interval 1504 (15030 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: -1.0000\n",
            "Interval 1505 (15040 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.474 - mean_q: 12.674 - prob: 1.000\n",
            "\n",
            "Interval 1506 (15050 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.666 - mean_q: 13.197 - prob: 1.000\n",
            "\n",
            "Interval 1507 (15060 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.364 - mean_q: 12.389 - prob: 1.000\n",
            "\n",
            "Interval 1508 (15070 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 1509 (15080 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.001 - mae: 7.409 - mean_q: 12.654 - prob: 1.000\n",
            "\n",
            "Interval 1510 (15090 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.461 - mean_q: 12.891 - prob: 1.000\n",
            "\n",
            "Interval 1511 (15100 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 1512 (15110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.001 - mae: 7.307 - mean_q: 12.476 - prob: 1.000\n",
            "\n",
            "Interval 1513 (15120 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1514 (15130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.399 - mean_q: 12.784 - prob: 1.000\n",
            "\n",
            "Interval 1515 (15140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.524 - mean_q: 12.930 - prob: 1.000\n",
            "\n",
            "Interval 1516 (15150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1517 (15160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.499 - mean_q: 12.928 - prob: 1.000\n",
            "\n",
            "Interval 1518 (15170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.227 - mean_q: 12.565 - prob: 1.000\n",
            "\n",
            "Interval 1519 (15180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1520 (15190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.395 - mean_q: 12.840 - prob: 1.000\n",
            "\n",
            "Interval 1521 (15200 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.299 - mean_q: 12.713 - prob: 1.000\n",
            "\n",
            "Interval 1522 (15210 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.441 - mean_q: 12.789 - prob: 1.000\n",
            "\n",
            "Interval 1523 (15220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1524 (15230 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.262 - mean_q: 12.540 - prob: 1.000\n",
            "\n",
            "Interval 1525 (15240 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.352 - mean_q: 12.740 - prob: 1.000\n",
            "\n",
            "Interval 1526 (15250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.367 - mean_q: 12.686 - prob: 1.000\n",
            "\n",
            "Interval 1527 (15260 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -2.8000\n",
            "Interval 1528 (15270 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.001 - mae: 7.548 - mean_q: 13.028 - prob: 1.000\n",
            "\n",
            "Interval 1529 (15280 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.314 - mean_q: 12.580 - prob: 1.000\n",
            "\n",
            "Interval 1530 (15290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1531 (15300 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.236 - mean_q: 12.659 - prob: 1.000\n",
            "\n",
            "Interval 1532 (15310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1533 (15320 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.000 - mae: 7.321 - mean_q: 12.663 - prob: 1.000\n",
            "\n",
            "Interval 1534 (15330 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.299 - mean_q: 12.618 - prob: 1.000\n",
            "\n",
            "Interval 1535 (15340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1536 (15350 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.181 - mean_q: 12.438 - prob: 1.000\n",
            "\n",
            "Interval 1537 (15360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.311 - mean_q: 12.589 - prob: 1.000\n",
            "\n",
            "Interval 1538 (15370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1539 (15380 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.374 - mean_q: 12.709 - prob: 1.000\n",
            "\n",
            "Interval 1540 (15390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1541 (15400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.385 - mean_q: 12.736 - prob: 1.000\n",
            "\n",
            "Interval 1542 (15410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1543 (15420 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.364 - mean_q: 12.604 - prob: 1.000\n",
            "\n",
            "Interval 1544 (15430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.249 - mean_q: 12.664 - prob: 1.000\n",
            "\n",
            "Interval 1545 (15440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1546 (15450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 6.987 - mean_q: 12.016 - prob: 1.000\n",
            "\n",
            "Interval 1547 (15460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.261 - mean_q: 12.584 - prob: 1.000\n",
            "\n",
            "Interval 1548 (15470 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1549 (15480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.337 - mean_q: 12.749 - prob: 1.000\n",
            "\n",
            "Interval 1550 (15490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.376 - mean_q: 12.682 - prob: 1.000\n",
            "\n",
            "Interval 1551 (15500 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.424 - mean_q: 12.950 - prob: 1.000\n",
            "\n",
            "Interval 1552 (15510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1553 (15520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.298 - mean_q: 12.605 - prob: 1.000\n",
            "\n",
            "Interval 1554 (15530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.499 - mean_q: 12.891 - prob: 1.000\n",
            "\n",
            "Interval 1555 (15540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.521 - mean_q: 12.921 - prob: 1.000\n",
            "\n",
            "Interval 1556 (15550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.259 - mean_q: 12.557 - prob: 1.000\n",
            "\n",
            "Interval 1557 (15560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1558 (15570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.341 - mean_q: 12.584 - prob: 1.000\n",
            "\n",
            "Interval 1559 (15580 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.539 - mean_q: 12.801 - prob: 1.000\n",
            "\n",
            "Interval 1560 (15590 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 1561 (15600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.072 - mean_q: 12.270 - prob: 1.000\n",
            "\n",
            "Interval 1562 (15610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.203 - mean_q: 12.461 - prob: 1.000\n",
            "\n",
            "Interval 1563 (15620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1564 (15630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.900 - mean_q: 13.491 - prob: 1.000\n",
            "\n",
            "Interval 1565 (15640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.520 - mean_q: 12.953 - prob: 1.000\n",
            "\n",
            "Interval 1566 (15650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.184 - mean_q: 12.434 - prob: 1.000\n",
            "\n",
            "Interval 1567 (15660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.000 - mae: 7.089 - mean_q: 12.296 - prob: 1.000\n",
            "\n",
            "Interval 1568 (15670 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1569 (15680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.429 - mean_q: 12.748 - prob: 1.000\n",
            "\n",
            "Interval 1570 (15690 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1571 (15700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.254 - mean_q: 12.408 - prob: 1.000\n",
            "\n",
            "Interval 1572 (15710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.517 - mean_q: 12.925 - prob: 1.000\n",
            "\n",
            "Interval 1573 (15720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1574 (15730 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.001 - mean_q: 12.147 - prob: 1.000\n",
            "\n",
            "Interval 1575 (15740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.366 - mean_q: 12.688 - prob: 1.000\n",
            "\n",
            "Interval 1576 (15750 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1577 (15760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.068 - mean_q: 12.337 - prob: 1.000\n",
            "\n",
            "Interval 1578 (15770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.456 - mean_q: 12.690 - prob: 1.000\n",
            "\n",
            "Interval 1579 (15780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.466 - mean_q: 12.844 - prob: 1.000\n",
            "\n",
            "Interval 1580 (15790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.301 - mean_q: 12.747 - prob: 1.000\n",
            "\n",
            "Interval 1581 (15800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1582 (15810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.222 - mean_q: 12.416 - prob: 1.000\n",
            "\n",
            "Interval 1583 (15820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1584 (15830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 6.844 - mean_q: 11.903 - prob: 1.000\n",
            "\n",
            "Interval 1585 (15840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1586 (15850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.284 - mean_q: 12.575 - prob: 1.000\n",
            "\n",
            "Interval 1587 (15860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.401 - mean_q: 12.875 - prob: 1.000\n",
            "\n",
            "Interval 1588 (15870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.001 - mae: 7.501 - mean_q: 12.950 - prob: 1.000\n",
            "\n",
            "Interval 1589 (15880 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.337 - mean_q: 12.584 - prob: 1.000\n",
            "\n",
            "Interval 1590 (15890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1591 (15900 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.095 - mean_q: 12.264 - prob: 1.000\n",
            "\n",
            "Interval 1592 (15910 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1593 (15920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.171 - mean_q: 12.465 - prob: 1.000\n",
            "\n",
            "Interval 1594 (15930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.464 - mean_q: 12.842 - prob: 1.000\n",
            "\n",
            "Interval 1595 (15940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1596 (15950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.183 - mean_q: 12.504 - prob: 1.000\n",
            "\n",
            "Interval 1597 (15960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.320 - mean_q: 12.687 - prob: 1.000\n",
            "\n",
            "Interval 1598 (15970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1599 (15980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.692 - mean_q: 13.073 - prob: 1.000\n",
            "\n",
            "Interval 1600 (15990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.241 - mean_q: 12.409 - prob: 1.000\n",
            "\n",
            "Interval 1601 (16000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.862 - mean_q: 13.359 - prob: 1.000\n",
            "\n",
            "Interval 1602 (16010 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1603 (16020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.238 - mean_q: 12.558 - prob: 1.000\n",
            "\n",
            "Interval 1604 (16030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.429 - mean_q: 12.832 - prob: 1.000\n",
            "\n",
            "Interval 1605 (16040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.239 - mean_q: 12.465 - prob: 1.000\n",
            "\n",
            "Interval 1606 (16050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 1607 (16060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.000 - mae: 7.409 - mean_q: 12.750 - prob: 1.000\n",
            "\n",
            "Interval 1608 (16070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.501 - mean_q: 12.818 - prob: 1.000\n",
            "\n",
            "Interval 1609 (16080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1610 (16090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.001 - mae: 7.438 - mean_q: 12.895 - prob: 1.000\n",
            "\n",
            "Interval 1611 (16100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1612 (16110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.553 - mean_q: 12.981 - prob: 1.000\n",
            "\n",
            "Interval 1613 (16120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.640 - mean_q: 13.107 - prob: 1.000\n",
            "\n",
            "Interval 1614 (16130 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.549 - mean_q: 13.086 - prob: 1.000\n",
            "\n",
            "Interval 1615 (16140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.115 - mean_q: 12.345 - prob: 1.000\n",
            "\n",
            "Interval 1616 (16150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1617 (16160 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.115 - mean_q: 12.223 - prob: 1.000\n",
            "\n",
            "Interval 1618 (16170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.310 - mean_q: 12.698 - prob: 1.000\n",
            "\n",
            "Interval 1619 (16180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1620 (16190 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.395 - mean_q: 12.768 - prob: 1.000\n",
            "\n",
            "Interval 1621 (16200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.001 - mae: 7.231 - mean_q: 12.529 - prob: 1.000\n",
            "\n",
            "Interval 1622 (16210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.415 - mean_q: 12.867 - prob: 1.000\n",
            "\n",
            "Interval 1623 (16220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1624 (16230 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.336 - mean_q: 12.606 - prob: 1.000\n",
            "\n",
            "Interval 1625 (16240 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 6.896 - mean_q: 12.044 - prob: 1.000\n",
            "\n",
            "Interval 1626 (16250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1627 (16260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.001 - mae: 7.336 - mean_q: 12.600 - prob: 1.000\n",
            "\n",
            "Interval 1628 (16270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1629 (16280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.103 - mean_q: 12.308 - prob: 1.000\n",
            "\n",
            "Interval 1630 (16290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1631 (16300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.272 - mean_q: 12.559 - prob: 1.000\n",
            "\n",
            "Interval 1632 (16310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.477 - mean_q: 12.779 - prob: 1.000\n",
            "\n",
            "Interval 1633 (16320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1634 (16330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.135 - mean_q: 12.324 - prob: 1.000\n",
            "\n",
            "Interval 1635 (16340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.444 - mean_q: 12.778 - prob: 1.000\n",
            "\n",
            "Interval 1636 (16350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.003 - mae: 7.441 - mean_q: 12.684 - prob: 1.000\n",
            "\n",
            "Interval 1637 (16360 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1638 (16370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.372 - mean_q: 12.651 - prob: 1.000\n",
            "\n",
            "Interval 1639 (16380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.003 - mae: 7.662 - mean_q: 13.225 - prob: 1.000\n",
            "\n",
            "Interval 1640 (16390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.425 - mean_q: 12.761 - prob: 1.000\n",
            "\n",
            "Interval 1641 (16400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.002 - mae: 7.190 - mean_q: 12.412 - prob: 1.000\n",
            "\n",
            "Interval 1642 (16410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.002 - mae: 7.442 - mean_q: 12.742 - prob: 1.000\n",
            "\n",
            "Interval 1643 (16420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1644 (16430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.002 - mae: 7.387 - mean_q: 12.624 - prob: 1.000\n",
            "\n",
            "Interval 1645 (16440 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1646 (16450 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.001 - mae: 7.270 - mean_q: 12.523 - prob: 1.000\n",
            "\n",
            "Interval 1647 (16460 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 1648 (16470 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -27.000 [-27.000, -27.000] - loss: 0.001 - mae: 7.417 - mean_q: 12.867 - prob: 1.000\n",
            "\n",
            "Interval 1649 (16480 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.002 - mae: 7.465 - mean_q: 12.824 - prob: 1.000\n",
            "\n",
            "Interval 1650 (16490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1651 (16500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.493 - mean_q: 12.832 - prob: 1.000\n",
            "\n",
            "Interval 1652 (16510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.004 - mae: 7.453 - mean_q: 12.859 - prob: 1.000\n",
            "\n",
            "Interval 1653 (16520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1654 (16530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.335 - mean_q: 12.601 - prob: 1.000\n",
            "\n",
            "Interval 1655 (16540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.250 - mean_q: 12.669 - prob: 1.000\n",
            "\n",
            "Interval 1656 (16550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1657 (16560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.002 - mae: 7.367 - mean_q: 12.588 - prob: 1.000\n",
            "\n",
            "Interval 1658 (16570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.156 - mean_q: 12.570 - prob: 1.000\n",
            "\n",
            "Interval 1659 (16580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.423 - mean_q: 12.892 - prob: 1.000\n",
            "\n",
            "Interval 1660 (16590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1661 (16600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.313 - mean_q: 12.685 - prob: 1.000\n",
            "\n",
            "Interval 1662 (16610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.414 - mean_q: 12.753 - prob: 1.000\n",
            "\n",
            "Interval 1663 (16620 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1664 (16630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.050 - mean_q: 12.230 - prob: 1.000\n",
            "\n",
            "Interval 1665 (16640 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.251 - mean_q: 12.501 - prob: 1.000\n",
            "\n",
            "Interval 1666 (16650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1667 (16660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.001 - mae: 7.345 - mean_q: 12.708 - prob: 1.000\n",
            "\n",
            "Interval 1668 (16670 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1669 (16680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.329 - mean_q: 12.755 - prob: 1.000\n",
            "\n",
            "Interval 1670 (16690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.000 - mae: 7.269 - mean_q: 12.577 - prob: 1.000\n",
            "\n",
            "Interval 1671 (16700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.495 - mean_q: 12.755 - prob: 1.000\n",
            "\n",
            "Interval 1672 (16710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.295 - mean_q: 12.596 - prob: 1.000\n",
            "\n",
            "Interval 1673 (16720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.187 - mean_q: 12.484 - prob: 1.000\n",
            "\n",
            "Interval 1674 (16730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1675 (16740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.427 - mean_q: 12.811 - prob: 1.000\n",
            "\n",
            "Interval 1676 (16750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.520 - mean_q: 13.097 - prob: 1.000\n",
            "\n",
            "Interval 1677 (16760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1678 (16770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.000 - mae: 7.653 - mean_q: 13.147 - prob: 1.000\n",
            "\n",
            "Interval 1679 (16780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1680 (16790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.681 - mean_q: 13.034 - prob: 1.000\n",
            "\n",
            "Interval 1681 (16800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.474 - mean_q: 12.789 - prob: 1.000\n",
            "\n",
            "Interval 1682 (16810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.396 - mean_q: 12.761 - prob: 1.000\n",
            "\n",
            "Interval 1683 (16820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1684 (16830 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.231 - mean_q: 12.607 - prob: 1.000\n",
            "\n",
            "Interval 1685 (16840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.602 - mean_q: 13.112 - prob: 1.000\n",
            "\n",
            "Interval 1686 (16850 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1687 (16860 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.510 - mean_q: 12.870 - prob: 1.000\n",
            "\n",
            "Interval 1688 (16870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.197 - mean_q: 12.592 - prob: 1.000\n",
            "\n",
            "Interval 1689 (16880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1690 (16890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.291 - mean_q: 12.661 - prob: 1.000\n",
            "\n",
            "Interval 1691 (16900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1692 (16910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.241 - mean_q: 12.662 - prob: 1.000\n",
            "\n",
            "Interval 1693 (16920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.308 - mean_q: 12.470 - prob: 1.000\n",
            "\n",
            "Interval 1694 (16930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.000 - mae: 7.356 - mean_q: 12.696 - prob: 1.000\n",
            "\n",
            "Interval 1695 (16940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1696 (16950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.426 - mean_q: 12.736 - prob: 1.000\n",
            "\n",
            "Interval 1697 (16960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1698 (16970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.000 - mae: 7.116 - mean_q: 12.378 - prob: 1.000\n",
            "\n",
            "Interval 1699 (16980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1700 (16990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.279 - mean_q: 12.622 - prob: 1.000\n",
            "\n",
            "Interval 1701 (17000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.000 - mae: 7.005 - mean_q: 12.240 - prob: 1.000\n",
            "\n",
            "Interval 1702 (17010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1703 (17020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.344 - mean_q: 12.507 - prob: 1.000\n",
            "\n",
            "Interval 1704 (17030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.086 - mean_q: 12.246 - prob: 1.000\n",
            "\n",
            "Interval 1705 (17040 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1706 (17050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -18.000 [-18.000, -18.000] - loss: 0.000 - mae: 7.236 - mean_q: 12.543 - prob: 1.000\n",
            "\n",
            "Interval 1707 (17060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1708 (17070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.234 - mean_q: 12.522 - prob: 1.000\n",
            "\n",
            "Interval 1709 (17080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.539 - mean_q: 12.878 - prob: 1.000\n",
            "\n",
            "Interval 1710 (17090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.115 - mean_q: 12.317 - prob: 1.000\n",
            "\n",
            "Interval 1711 (17100 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1712 (17110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.243 - mean_q: 12.529 - prob: 1.000\n",
            "\n",
            "Interval 1713 (17120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.516 - mean_q: 12.917 - prob: 1.000\n",
            "\n",
            "Interval 1714 (17130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1715 (17140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.402 - mean_q: 12.717 - prob: 1.000\n",
            "\n",
            "Interval 1716 (17150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1717 (17160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1718 (17170 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.001 - mae: 7.197 - mean_q: 12.646 - prob: 1.000\n",
            "\n",
            "Interval 1719 (17180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.391 - mean_q: 12.670 - prob: 1.000\n",
            "\n",
            "Interval 1720 (17190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1721 (17200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.290 - mean_q: 12.570 - prob: 1.000\n",
            "\n",
            "Interval 1722 (17210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.541 - mean_q: 13.016 - prob: 1.000\n",
            "\n",
            "Interval 1723 (17220 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1724 (17230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.167 - mean_q: 12.476 - prob: 1.000\n",
            "\n",
            "Interval 1725 (17240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.394 - mean_q: 12.719 - prob: 1.000\n",
            "\n",
            "Interval 1726 (17250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.202 - mean_q: 12.426 - prob: 1.000\n",
            "\n",
            "Interval 1727 (17260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.505 - mean_q: 13.055 - prob: 1.000\n",
            "\n",
            "Interval 1728 (17270 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1729 (17280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.328 - mean_q: 12.613 - prob: 1.000\n",
            "\n",
            "Interval 1730 (17290 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1731 (17300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.624 - mean_q: 13.158 - prob: 1.000\n",
            "\n",
            "Interval 1732 (17310 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.647 - mean_q: 13.066 - prob: 1.000\n",
            "\n",
            "Interval 1733 (17320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.292 - mean_q: 12.583 - prob: 1.000\n",
            "\n",
            "Interval 1734 (17330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1735 (17340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.500 - mean_q: 12.861 - prob: 1.000\n",
            "\n",
            "Interval 1736 (17350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.000 - mae: 7.338 - mean_q: 12.766 - prob: 1.000\n",
            "\n",
            "Interval 1737 (17360 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.267 - mean_q: 12.585 - prob: 1.000\n",
            "\n",
            "Interval 1738 (17370 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.133 - mean_q: 12.440 - prob: 1.000\n",
            "\n",
            "Interval 1739 (17380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1740 (17390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.513 - mean_q: 12.909 - prob: 1.000\n",
            "\n",
            "Interval 1741 (17400 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.173 - mean_q: 12.344 - prob: 1.000\n",
            "\n",
            "Interval 1742 (17410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.677 - mean_q: 13.207 - prob: 1.000\n",
            "\n",
            "Interval 1743 (17420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.347 - mean_q: 12.613 - prob: 1.000\n",
            "\n",
            "Interval 1744 (17430 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1745 (17440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.180 - mean_q: 12.373 - prob: 1.000\n",
            "\n",
            "Interval 1746 (17450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.368 - mean_q: 12.783 - prob: 1.000\n",
            "\n",
            "Interval 1747 (17460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.335 - mean_q: 12.687 - prob: 1.000\n",
            "\n",
            "Interval 1748 (17470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1749 (17480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.296 - mean_q: 12.556 - prob: 1.000\n",
            "\n",
            "Interval 1750 (17490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.001 - mae: 7.189 - mean_q: 12.465 - prob: 1.000\n",
            "\n",
            "Interval 1751 (17500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1752 (17510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.204 - mean_q: 12.453 - prob: 1.000\n",
            "\n",
            "Interval 1753 (17520 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1754 (17530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -18.000 [-18.000, -18.000] - loss: 0.001 - mae: 7.508 - mean_q: 13.032 - prob: 1.000\n",
            "\n",
            "Interval 1755 (17540 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1756 (17550 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.156 - mean_q: 12.404 - prob: 1.000\n",
            "\n",
            "Interval 1757 (17560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.120 - mean_q: 12.248 - prob: 1.000\n",
            "\n",
            "Interval 1758 (17570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.303 - mean_q: 12.655 - prob: 1.000\n",
            "\n",
            "Interval 1759 (17580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1760 (17590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.497 - mean_q: 12.997 - prob: 1.000\n",
            "\n",
            "Interval 1761 (17600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.439 - mean_q: 12.680 - prob: 1.000\n",
            "\n",
            "Interval 1762 (17610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1763 (17620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.527 - mean_q: 13.059 - prob: 1.000\n",
            "\n",
            "Interval 1764 (17630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.359 - mean_q: 12.681 - prob: 1.000\n",
            "\n",
            "Interval 1765 (17640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.623 - mean_q: 13.113 - prob: 1.000\n",
            "\n",
            "Interval 1766 (17650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.346 - mean_q: 12.680 - prob: 1.000\n",
            "\n",
            "Interval 1767 (17660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1768 (17670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.398 - mean_q: 12.732 - prob: 1.000\n",
            "\n",
            "Interval 1769 (17680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.198 - mean_q: 12.441 - prob: 1.000\n",
            "\n",
            "Interval 1770 (17690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1771 (17700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.396 - mean_q: 12.581 - prob: 1.000\n",
            "\n",
            "Interval 1772 (17710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.319 - mean_q: 12.715 - prob: 1.000\n",
            "\n",
            "Interval 1773 (17720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.467 - mean_q: 12.790 - prob: 1.000\n",
            "\n",
            "Interval 1774 (17730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1775 (17740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.369 - mean_q: 12.667 - prob: 1.000\n",
            "\n",
            "Interval 1776 (17750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.000 - mae: 7.300 - mean_q: 12.650 - prob: 1.000\n",
            "\n",
            "Interval 1777 (17760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.270 - mean_q: 12.536 - prob: 1.000\n",
            "\n",
            "Interval 1778 (17770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.000 - mae: 7.064 - mean_q: 12.303 - prob: 1.000\n",
            "\n",
            "Interval 1779 (17780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.215 - mean_q: 12.485 - prob: 1.000\n",
            "\n",
            "Interval 1780 (17790 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -2.8000\n",
            "Interval 1781 (17800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -21.000 [-21.000, -21.000] - loss: 0.000 - mae: 7.559 - mean_q: 12.999 - prob: 1.000\n",
            "\n",
            "Interval 1782 (17810 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.548 - mean_q: 12.913 - prob: 1.000\n",
            "\n",
            "Interval 1783 (17820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.303 - mean_q: 12.585 - prob: 1.000\n",
            "\n",
            "Interval 1784 (17830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1785 (17840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.299 - mean_q: 12.611 - prob: 1.000\n",
            "\n",
            "Interval 1786 (17850 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1787 (17860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.398 - mean_q: 12.696 - prob: 1.000\n",
            "\n",
            "Interval 1788 (17870 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.397 - mean_q: 12.776 - prob: 1.000\n",
            "\n",
            "Interval 1789 (17880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1790 (17890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.813 - mean_q: 13.391 - prob: 1.000\n",
            "\n",
            "Interval 1791 (17900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1792 (17910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.362 - mean_q: 12.728 - prob: 1.000\n",
            "\n",
            "Interval 1793 (17920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.000 - mae: 7.204 - mean_q: 12.437 - prob: 1.000\n",
            "\n",
            "Interval 1794 (17930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.352 - mean_q: 12.736 - prob: 1.000\n",
            "\n",
            "Interval 1795 (17940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1796 (17950 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.595 - mean_q: 13.014 - prob: 1.000\n",
            "\n",
            "Interval 1797 (17960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.082 - mean_q: 12.288 - prob: 1.000\n",
            "\n",
            "Interval 1798 (17970 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1799 (17980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.311 - mean_q: 12.542 - prob: 1.000\n",
            "\n",
            "Interval 1800 (17990 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.249 - mean_q: 12.559 - prob: 1.000\n",
            "\n",
            "Interval 1801 (18000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1802 (18010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.202 - mean_q: 12.471 - prob: 1.000\n",
            "\n",
            "Interval 1803 (18020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.345 - mean_q: 12.811 - prob: 1.000\n",
            "\n",
            "Interval 1804 (18030 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1805 (18040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1806 (18050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.001 - mae: 7.394 - mean_q: 12.685 - prob: 1.000\n",
            "\n",
            "Interval 1807 (18060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1808 (18070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.594 - mean_q: 13.008 - prob: 1.000\n",
            "\n",
            "Interval 1809 (18080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1810 (18090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.001 - mae: 7.632 - mean_q: 13.159 - prob: 1.000\n",
            "\n",
            "Interval 1811 (18100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.606 - mean_q: 13.014 - prob: 1.000\n",
            "\n",
            "Interval 1812 (18110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1813 (18120 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.000 - mae: 7.350 - mean_q: 12.603 - prob: 1.000\n",
            "\n",
            "Interval 1814 (18130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.277 - mean_q: 12.663 - prob: 1.000\n",
            "\n",
            "Interval 1815 (18140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1816 (18150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.000 - mae: 7.303 - mean_q: 12.606 - prob: 1.000\n",
            "\n",
            "Interval 1817 (18160 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.497 - mean_q: 12.870 - prob: 1.000\n",
            "\n",
            "Interval 1818 (18170 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1819 (18180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.000 - mae: 7.414 - mean_q: 12.870 - prob: 1.000\n",
            "\n",
            "Interval 1820 (18190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.470 - mean_q: 12.765 - prob: 1.000\n",
            "\n",
            "Interval 1821 (18200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1822 (18210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.421 - mean_q: 12.843 - prob: 1.000\n",
            "\n",
            "Interval 1823 (18220 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.000 - mae: 7.186 - mean_q: 12.406 - prob: 1.000\n",
            "\n",
            "Interval 1824 (18230 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1825 (18240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.639 - mean_q: 13.144 - prob: 1.000\n",
            "\n",
            "Interval 1826 (18250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.196 - mean_q: 12.414 - prob: 1.000\n",
            "\n",
            "Interval 1827 (18260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1828 (18270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.289 - mean_q: 12.720 - prob: 1.000\n",
            "\n",
            "Interval 1829 (18280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.223 - mean_q: 12.483 - prob: 1.000\n",
            "\n",
            "Interval 1830 (18290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.000 - mae: 7.014 - mean_q: 12.159 - prob: 1.000\n",
            "\n",
            "Interval 1831 (18300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1832 (18310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.441 - mean_q: 12.875 - prob: 1.000\n",
            "\n",
            "Interval 1833 (18320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1834 (18330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.000 - mae: 7.560 - mean_q: 12.951 - prob: 1.000\n",
            "\n",
            "Interval 1835 (18340 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 1836 (18350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.306 - mean_q: 12.560 - prob: 1.000\n",
            "\n",
            "Interval 1837 (18360 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1838 (18370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.000 - mae: 7.305 - mean_q: 12.477 - prob: 1.000\n",
            "\n",
            "Interval 1839 (18380 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1840 (18390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.222 - mean_q: 12.481 - prob: 1.000\n",
            "\n",
            "Interval 1841 (18400 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1842 (18410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.418 - mean_q: 12.761 - prob: 1.000\n",
            "\n",
            "Interval 1843 (18420 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1844 (18430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.427 - mean_q: 12.731 - prob: 1.000\n",
            "\n",
            "Interval 1845 (18440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.731 - mean_q: 13.194 - prob: 1.000\n",
            "\n",
            "Interval 1846 (18450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1847 (18460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.772 - mean_q: 13.330 - prob: 1.000\n",
            "\n",
            "Interval 1848 (18470 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1849 (18480 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.348 - mean_q: 12.711 - prob: 1.000\n",
            "\n",
            "Interval 1850 (18490 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1851 (18500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.377 - mean_q: 12.590 - prob: 1.000\n",
            "\n",
            "Interval 1852 (18510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1853 (18520 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.520 - mean_q: 12.826 - prob: 1.000\n",
            "\n",
            "Interval 1854 (18530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1855 (18540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.002 - mae: 7.523 - mean_q: 12.922 - prob: 1.000\n",
            "\n",
            "Interval 1856 (18550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.451 - mean_q: 12.741 - prob: 1.000\n",
            "\n",
            "Interval 1857 (18560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1858 (18570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.133 - mean_q: 12.398 - prob: 1.000\n",
            "\n",
            "Interval 1859 (18580 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.414 - mean_q: 12.701 - prob: 1.000\n",
            "\n",
            "Interval 1860 (18590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.415 - mean_q: 12.866 - prob: 1.000\n",
            "\n",
            "Interval 1861 (18600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1862 (18610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.152 - mean_q: 12.386 - prob: 1.000\n",
            "\n",
            "Interval 1863 (18620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.177 - mean_q: 12.257 - prob: 1.000\n",
            "\n",
            "Interval 1864 (18630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.001 - mae: 7.434 - mean_q: 12.776 - prob: 1.000\n",
            "\n",
            "Interval 1865 (18640 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1866 (18650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.385 - mean_q: 12.826 - prob: 1.000\n",
            "\n",
            "Interval 1867 (18660 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1868 (18670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.489 - mean_q: 12.873 - prob: 1.000\n",
            "\n",
            "Interval 1869 (18680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.635 - mean_q: 13.206 - prob: 1.000\n",
            "\n",
            "Interval 1870 (18690 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.286 - mean_q: 12.499 - prob: 1.000\n",
            "\n",
            "Interval 1871 (18700 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1872 (18710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.303 - mean_q: 12.622 - prob: 1.000\n",
            "\n",
            "Interval 1873 (18720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.231 - mean_q: 12.388 - prob: 1.000\n",
            "\n",
            "Interval 1874 (18730 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.632 - mean_q: 13.095 - prob: 1.000\n",
            "\n",
            "Interval 1875 (18740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1876 (18750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.498 - mean_q: 12.951 - prob: 1.000\n",
            "\n",
            "Interval 1877 (18760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.522 - mean_q: 12.883 - prob: 1.000\n",
            "\n",
            "Interval 1878 (18770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1879 (18780 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.442 - mean_q: 12.788 - prob: 1.000\n",
            "\n",
            "Interval 1880 (18790 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.485 - mean_q: 12.850 - prob: 1.000\n",
            "\n",
            "Interval 1881 (18800 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 1882 (18810 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.199 - mean_q: 12.427 - prob: 1.000\n",
            "\n",
            "Interval 1883 (18820 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.392 - mean_q: 12.890 - prob: 1.000\n",
            "\n",
            "Interval 1884 (18830 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.475 - mean_q: 12.824 - prob: 1.000\n",
            "\n",
            "Interval 1885 (18840 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 1886 (18850 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.337 - mean_q: 12.622 - prob: 1.000\n",
            "\n",
            "Interval 1887 (18860 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 1888 (18870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: -1.000 [-17.000, 15.000] - loss: 0.000 - mae: 7.438 - mean_q: 12.957 - prob: 1.000\n",
            "\n",
            "Interval 1889 (18880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1890 (18890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.461 - mean_q: 12.877 - prob: 1.000\n",
            "\n",
            "Interval 1891 (18900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.508 - mean_q: 12.942 - prob: 1.000\n",
            "\n",
            "Interval 1892 (18910 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.257 - mean_q: 12.568 - prob: 1.000\n",
            "\n",
            "Interval 1893 (18920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.135 - mean_q: 12.296 - prob: 1.000\n",
            "\n",
            "Interval 1894 (18930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1895 (18940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1896 (18950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.205 - mean_q: 12.329 - prob: 1.000\n",
            "\n",
            "Interval 1897 (18960 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.624 - mean_q: 12.963 - prob: 1.000\n",
            "\n",
            "Interval 1898 (18970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.000 - mae: 7.489 - mean_q: 12.788 - prob: 1.000\n",
            "\n",
            "Interval 1899 (18980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.093 - mean_q: 12.261 - prob: 1.000\n",
            "\n",
            "Interval 1900 (18990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1901 (19000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.392 - mean_q: 12.608 - prob: 1.000\n",
            "\n",
            "Interval 1902 (19010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.331 - mean_q: 12.691 - prob: 1.000\n",
            "\n",
            "Interval 1903 (19020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1904 (19030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.267 - mean_q: 12.648 - prob: 1.000\n",
            "\n",
            "Interval 1905 (19040 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.218 - mean_q: 12.617 - prob: 1.000\n",
            "\n",
            "Interval 1906 (19050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1907 (19060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.419 - mean_q: 12.751 - prob: 1.000\n",
            "\n",
            "Interval 1908 (19070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 6.943 - mean_q: 12.072 - prob: 1.000\n",
            "\n",
            "Interval 1909 (19080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1910 (19090 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.289 - mean_q: 12.537 - prob: 1.000\n",
            "\n",
            "Interval 1911 (19100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.070 - mean_q: 12.315 - prob: 1.000\n",
            "\n",
            "Interval 1912 (19110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.470 - mean_q: 12.965 - prob: 1.000\n",
            "\n",
            "Interval 1913 (19120 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1914 (19130 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.261 - mean_q: 12.656 - prob: 1.000\n",
            "\n",
            "Interval 1915 (19140 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.325 - mean_q: 12.700 - prob: 1.000\n",
            "\n",
            "Interval 1916 (19150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1917 (19160 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.125 - mean_q: 12.536 - prob: 1.000\n",
            "\n",
            "Interval 1918 (19170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1919 (19180 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.386 - mean_q: 12.622 - prob: 1.000\n",
            "\n",
            "Interval 1920 (19190 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.334 - mean_q: 12.701 - prob: 1.000\n",
            "\n",
            "Interval 1921 (19200 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 1922 (19210 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.412 - mean_q: 12.718 - prob: 1.000\n",
            "\n",
            "Interval 1923 (19220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1924 (19230 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.437 - mean_q: 12.841 - prob: 1.000\n",
            "\n",
            "Interval 1925 (19240 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.190 - mean_q: 12.421 - prob: 1.000\n",
            "\n",
            "Interval 1926 (19250 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 1927 (19260 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.276 - mean_q: 12.665 - prob: 1.000\n",
            "\n",
            "Interval 1928 (19270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1929 (19280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.439 - mean_q: 12.838 - prob: 1.000\n",
            "\n",
            "Interval 1930 (19290 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.348 - mean_q: 12.781 - prob: 1.000\n",
            "\n",
            "Interval 1931 (19300 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 1932 (19310 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.604 - mean_q: 12.946 - prob: 1.000\n",
            "\n",
            "Interval 1933 (19320 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.433 - mean_q: 12.772 - prob: 1.000\n",
            "\n",
            "Interval 1934 (19330 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.535 - mean_q: 12.979 - prob: 1.000\n",
            "\n",
            "Interval 1935 (19340 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 1936 (19350 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.158 - mean_q: 12.505 - prob: 1.000\n",
            "\n",
            "Interval 1937 (19360 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.321 - mean_q: 12.540 - prob: 1.000\n",
            "\n",
            "Interval 1938 (19370 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.214 - mean_q: 12.351 - prob: 1.000\n",
            "\n",
            "Interval 1939 (19380 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 1940 (19390 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.232 - mean_q: 12.458 - prob: 1.000\n",
            "\n",
            "Interval 1941 (19400 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.363 - mean_q: 12.591 - prob: 1.000\n",
            "\n",
            "Interval 1942 (19410 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 1943 (19420 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.155 - mean_q: 12.451 - prob: 1.000\n",
            "\n",
            "Interval 1944 (19430 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 1945 (19440 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.462 - mean_q: 12.827 - prob: 1.000\n",
            "\n",
            "Interval 1946 (19450 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.551 - mean_q: 12.851 - prob: 1.000\n",
            "\n",
            "Interval 1947 (19460 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 1948 (19470 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.286 - mean_q: 12.512 - prob: 1.000\n",
            "\n",
            "Interval 1949 (19480 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 1950 (19490 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.288 - mean_q: 12.559 - prob: 1.000\n",
            "\n",
            "Interval 1951 (19500 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.211 - mean_q: 12.497 - prob: 1.000\n",
            "\n",
            "Interval 1952 (19510 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.552 - mean_q: 12.963 - prob: 1.000\n",
            "\n",
            "Interval 1953 (19520 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.511 - mean_q: 12.847 - prob: 1.000\n",
            "\n",
            "Interval 1954 (19530 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 1955 (19540 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.614 - mean_q: 12.944 - prob: 1.000\n",
            "\n",
            "Interval 1956 (19550 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.077 - mean_q: 12.344 - prob: 1.000\n",
            "\n",
            "Interval 1957 (19560 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 1958 (19570 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.552 - mean_q: 13.102 - prob: 1.000\n",
            "\n",
            "Interval 1959 (19580 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.542 - mean_q: 12.903 - prob: 1.000\n",
            "\n",
            "Interval 1960 (19590 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.548 - mean_q: 12.857 - prob: 1.000\n",
            "\n",
            "Interval 1961 (19600 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 1962 (19610 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.614 - mean_q: 13.125 - prob: 1.000\n",
            "\n",
            "Interval 1963 (19620 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.403 - mean_q: 12.733 - prob: 1.000\n",
            "\n",
            "Interval 1964 (19630 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 1965 (19640 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.295 - mean_q: 12.615 - prob: 1.000\n",
            "\n",
            "Interval 1966 (19650 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 1967 (19660 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.626 - mean_q: 13.049 - prob: 1.000\n",
            "\n",
            "Interval 1968 (19670 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 1969 (19680 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.317 - mean_q: 12.681 - prob: 1.000\n",
            "\n",
            "Interval 1970 (19690 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.105 - mean_q: 12.407 - prob: 1.000\n",
            "\n",
            "Interval 1971 (19700 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 1972 (19710 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.370 - mean_q: 12.763 - prob: 1.000\n",
            "\n",
            "Interval 1973 (19720 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 1974 (19730 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.446 - mean_q: 12.959 - prob: 1.000\n",
            "\n",
            "Interval 1975 (19740 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.670 - mean_q: 13.149 - prob: 1.000\n",
            "\n",
            "Interval 1976 (19750 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.419 - mean_q: 12.746 - prob: 1.000\n",
            "\n",
            "Interval 1977 (19760 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 1978 (19770 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.438 - mean_q: 12.841 - prob: 1.000\n",
            "\n",
            "Interval 1979 (19780 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.129 - mean_q: 12.303 - prob: 1.000\n",
            "\n",
            "Interval 1980 (19790 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.199 - mean_q: 12.578 - prob: 1.000\n",
            "\n",
            "Interval 1981 (19800 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 1982 (19810 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.442 - mean_q: 12.936 - prob: 1.000\n",
            "\n",
            "Interval 1983 (19820 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.154 - mean_q: 12.309 - prob: 1.000\n",
            "\n",
            "Interval 1984 (19830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1985 (19840 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -17.000 [-17.000, -17.000] - loss: 0.001 - mae: 6.985 - mean_q: 12.131 - prob: 1.000\n",
            "\n",
            "Interval 1986 (19850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 1987 (19860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.504 - mean_q: 12.977 - prob: 1.000\n",
            "\n",
            "Interval 1988 (19870 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.410 - mean_q: 12.716 - prob: 1.000\n",
            "\n",
            "Interval 1989 (19880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 1990 (19890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.296 - mean_q: 12.699 - prob: 1.000\n",
            "\n",
            "Interval 1991 (19900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.547 - mean_q: 12.948 - prob: 1.000\n",
            "\n",
            "Interval 1992 (19910 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1993 (19920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.272 - mean_q: 12.513 - prob: 1.000\n",
            "\n",
            "Interval 1994 (19930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.340 - mean_q: 12.674 - prob: 1.000\n",
            "\n",
            "Interval 1995 (19940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.393 - mean_q: 12.613 - prob: 1.000\n",
            "\n",
            "Interval 1996 (19950 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 1997 (19960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.133 - mean_q: 12.257 - prob: 1.000\n",
            "\n",
            "Interval 1998 (19970 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 1999 (19980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.187 - mean_q: 12.446 - prob: 1.000\n",
            "\n",
            "Interval 2000 (19990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.298 - mean_q: 12.728 - prob: 1.000\n",
            "\n",
            "Interval 2001 (20000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.333 - mean_q: 12.743 - prob: 1.000\n",
            "\n",
            "Interval 2002 (20010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.000 - mae: 7.085 - mean_q: 12.272 - prob: 1.000\n",
            "\n",
            "Interval 2003 (20020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2004 (20030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.001 - mae: 7.066 - mean_q: 12.289 - prob: 1.000\n",
            "\n",
            "Interval 2005 (20040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2006 (20050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.001 - mae: 7.313 - mean_q: 12.568 - prob: 1.000\n",
            "\n",
            "Interval 2007 (20060 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 2008 (20070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.475 - mean_q: 12.902 - prob: 1.000\n",
            "\n",
            "Interval 2009 (20080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.419 - mean_q: 12.655 - prob: 1.000\n",
            "\n",
            "Interval 2010 (20090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.241 - mean_q: 12.616 - prob: 1.000\n",
            "\n",
            "Interval 2011 (20100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.515 - mean_q: 12.786 - prob: 1.000\n",
            "\n",
            "Interval 2012 (20110 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 2013 (20120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.250 - mean_q: 12.585 - prob: 1.000\n",
            "\n",
            "Interval 2014 (20130 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.413 - mean_q: 12.773 - prob: 1.000\n",
            "\n",
            "Interval 2015 (20140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2016 (20150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2017 (20160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.620 - mean_q: 13.053 - prob: 1.000\n",
            "\n",
            "Interval 2018 (20170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.346 - mean_q: 12.592 - prob: 1.000\n",
            "\n",
            "Interval 2019 (20180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2020 (20190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.176 - mean_q: 12.522 - prob: 1.000\n",
            "\n",
            "Interval 2021 (20200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.208 - mean_q: 12.609 - prob: 1.000\n",
            "\n",
            "Interval 2022 (20210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.000 - mae: 7.598 - mean_q: 12.984 - prob: 1.000\n",
            "\n",
            "Interval 2023 (20220 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 2024 (20230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.001 - mae: 7.395 - mean_q: 12.775 - prob: 1.000\n",
            "\n",
            "Interval 2025 (20240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.603 - mean_q: 13.140 - prob: 1.000\n",
            "\n",
            "Interval 2026 (20250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.133 - mean_q: 12.370 - prob: 1.000\n",
            "\n",
            "Interval 2027 (20260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.483 - mean_q: 12.913 - prob: 1.000\n",
            "\n",
            "Interval 2028 (20270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.273 - mean_q: 12.585 - prob: 1.000\n",
            "\n",
            "Interval 2029 (20280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2030 (20290 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.399 - mean_q: 12.740 - prob: 1.000\n",
            "\n",
            "Interval 2031 (20300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.377 - mean_q: 12.639 - prob: 1.000\n",
            "\n",
            "Interval 2032 (20310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2033 (20320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.400 - mean_q: 12.660 - prob: 1.000\n",
            "\n",
            "Interval 2034 (20330 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.674 - mean_q: 13.230 - prob: 1.000\n",
            "\n",
            "Interval 2035 (20340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.398 - mean_q: 12.884 - prob: 1.000\n",
            "\n",
            "Interval 2036 (20350 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 2037 (20360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.052 - mean_q: 12.280 - prob: 1.000\n",
            "\n",
            "Interval 2038 (20370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.388 - mean_q: 12.725 - prob: 1.000\n",
            "\n",
            "Interval 2039 (20380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.397 - mean_q: 12.684 - prob: 1.000\n",
            "\n",
            "Interval 2040 (20390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.000 - mae: 7.390 - mean_q: 12.683 - prob: 1.000\n",
            "\n",
            "Interval 2041 (20400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.363 - mean_q: 12.759 - prob: 1.000\n",
            "\n",
            "Interval 2042 (20410 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 2043 (20420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.370 - mean_q: 12.631 - prob: 1.000\n",
            "\n",
            "Interval 2044 (20430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.413 - mean_q: 12.676 - prob: 1.000\n",
            "\n",
            "Interval 2045 (20440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.000 - mae: 7.552 - mean_q: 12.934 - prob: 1.000\n",
            "\n",
            "Interval 2046 (20450 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 2047 (20460 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.352 - mean_q: 12.736 - prob: 1.000\n",
            "\n",
            "Interval 2048 (20470 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.558 - mean_q: 12.872 - prob: 1.000\n",
            "\n",
            "Interval 2049 (20480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.180 - mean_q: 12.530 - prob: 1.000\n",
            "\n",
            "Interval 2050 (20490 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.117 - mean_q: 12.391 - prob: 1.000\n",
            "\n",
            "Interval 2051 (20500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2052 (20510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.465 - mean_q: 12.861 - prob: 1.000\n",
            "\n",
            "Interval 2053 (20520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.285 - mean_q: 12.653 - prob: 1.000\n",
            "\n",
            "Interval 2054 (20530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2055 (20540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.000 - mae: 7.040 - mean_q: 12.254 - prob: 1.000\n",
            "\n",
            "Interval 2056 (20550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2057 (20560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.000 - mae: 7.154 - mean_q: 12.429 - prob: 1.000\n",
            "\n",
            "Interval 2058 (20570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2059 (20580 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.594 - mean_q: 12.961 - prob: 1.000\n",
            "\n",
            "Interval 2060 (20590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.540 - mean_q: 12.925 - prob: 1.000\n",
            "\n",
            "Interval 2061 (20600 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.377 - mean_q: 12.690 - prob: 1.000\n",
            "\n",
            "Interval 2062 (20610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 2063 (20620 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -21.000 [-21.000, -21.000] - loss: 0.001 - mae: 7.318 - mean_q: 12.508 - prob: 1.000\n",
            "\n",
            "Interval 2064 (20630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.155 - mean_q: 12.596 - prob: 1.000\n",
            "\n",
            "Interval 2065 (20640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.950 - mean_q: 13.515 - prob: 1.000\n",
            "\n",
            "Interval 2066 (20650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2067 (20660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2068 (20670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.000 - mae: 7.175 - mean_q: 12.266 - prob: 1.000\n",
            "\n",
            "Interval 2069 (20680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.463 - mean_q: 12.827 - prob: 1.000\n",
            "\n",
            "Interval 2070 (20690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.359 - mean_q: 12.665 - prob: 1.000\n",
            "\n",
            "Interval 2071 (20700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.626 - mean_q: 13.091 - prob: 1.000\n",
            "\n",
            "Interval 2072 (20710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2073 (20720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.315 - mean_q: 12.755 - prob: 1.000\n",
            "\n",
            "Interval 2074 (20730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.278 - mean_q: 12.596 - prob: 1.000\n",
            "\n",
            "Interval 2075 (20740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.428 - mean_q: 13.076 - prob: 1.000\n",
            "\n",
            "Interval 2076 (20750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2077 (20760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -17.000 [-17.000, -17.000] - loss: 0.000 - mae: 7.025 - mean_q: 12.230 - prob: 1.000\n",
            "\n",
            "Interval 2078 (20770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2079 (20780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2080 (20790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -19.000 [-19.000, -19.000] - loss: 0.001 - mae: 7.484 - mean_q: 12.894 - prob: 1.000\n",
            "\n",
            "Interval 2081 (20800 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.240 - mean_q: 12.438 - prob: 1.000\n",
            "\n",
            "Interval 2082 (20810 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.003 - mean_q: 12.207 - prob: 1.000\n",
            "\n",
            "Interval 2083 (20820 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.334 - mean_q: 12.595 - prob: 1.000\n",
            "\n",
            "Interval 2084 (20830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2085 (20840 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.120 - mean_q: 12.333 - prob: 1.000\n",
            "\n",
            "Interval 2086 (20850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2087 (20860 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 8.000 [1.000, 15.000] - loss: 0.001 - mae: 7.058 - mean_q: 12.192 - prob: 1.000\n",
            "\n",
            "Interval 2088 (20870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2089 (20880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.362 - mean_q: 12.652 - prob: 1.000\n",
            "\n",
            "Interval 2090 (20890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2091 (20900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.446 - mean_q: 12.770 - prob: 1.000\n",
            "\n",
            "Interval 2092 (20910 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.611 - mean_q: 12.997 - prob: 1.000\n",
            "\n",
            "Interval 2093 (20920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.394 - mean_q: 12.768 - prob: 1.000\n",
            "\n",
            "Interval 2094 (20930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2095 (20940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.090 - mean_q: 12.340 - prob: 1.000\n",
            "\n",
            "Interval 2096 (20950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2097 (20960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.225 - mean_q: 12.507 - prob: 1.000\n",
            "\n",
            "Interval 2098 (20970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.346 - mean_q: 12.598 - prob: 1.000\n",
            "\n",
            "Interval 2099 (20980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.502 - mean_q: 12.769 - prob: 1.000\n",
            "\n",
            "Interval 2100 (20990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2101 (21000 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.398 - mean_q: 12.635 - prob: 1.000\n",
            "\n",
            "Interval 2102 (21010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2103 (21020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.440 - mean_q: 12.815 - prob: 1.000\n",
            "\n",
            "Interval 2104 (21030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.657 - mean_q: 13.045 - prob: 1.000\n",
            "\n",
            "Interval 2105 (21040 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 2106 (21050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.002 - mae: 7.562 - mean_q: 12.975 - prob: 1.000\n",
            "\n",
            "Interval 2107 (21060 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 2108 (21070 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -17.000 [-17.000, -17.000] - loss: 0.001 - mae: 7.276 - mean_q: 12.493 - prob: 1.000\n",
            "\n",
            "Interval 2109 (21080 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 2110 (21090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.163 - mean_q: 12.509 - prob: 1.000\n",
            "\n",
            "Interval 2111 (21100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.318 - mean_q: 12.675 - prob: 1.000\n",
            "\n",
            "Interval 2112 (21110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2113 (21120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.158 - mean_q: 12.371 - prob: 1.000\n",
            "\n",
            "Interval 2114 (21130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.528 - mean_q: 13.076 - prob: 1.000\n",
            "\n",
            "Interval 2115 (21140 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.001 - mae: 7.081 - mean_q: 12.149 - prob: 1.000\n",
            "\n",
            "Interval 2116 (21150 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.001 - mae: 7.336 - mean_q: 12.732 - prob: 1.000\n",
            "\n",
            "Interval 2117 (21160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2118 (21170 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 11.500 [10.000, 13.000] - loss: 0.001 - mae: 7.455 - mean_q: 12.767 - prob: 1.000\n",
            "\n",
            "Interval 2119 (21180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.109 - mean_q: 12.396 - prob: 1.000\n",
            "\n",
            "Interval 2120 (21190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2121 (21200 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.142 - mean_q: 12.354 - prob: 1.000\n",
            "\n",
            "Interval 2122 (21210 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.080 - mean_q: 12.225 - prob: 1.000\n",
            "\n",
            "Interval 2123 (21220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.587 - mean_q: 12.939 - prob: 1.000\n",
            "\n",
            "Interval 2124 (21230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2125 (21240 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.845 - mean_q: 13.348 - prob: 1.000\n",
            "\n",
            "Interval 2126 (21250 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 2127 (21260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.001 - mae: 7.205 - mean_q: 12.407 - prob: 1.000\n",
            "\n",
            "Interval 2128 (21270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2129 (21280 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.527 - mean_q: 12.891 - prob: 1.000\n",
            "\n",
            "Interval 2130 (21290 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.345 - mean_q: 12.705 - prob: 1.000\n",
            "\n",
            "Interval 2131 (21300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2132 (21310 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.404 - mean_q: 12.723 - prob: 1.000\n",
            "\n",
            "Interval 2133 (21320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2134 (21330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.001 - mae: 7.170 - mean_q: 12.442 - prob: 1.000\n",
            "\n",
            "Interval 2135 (21340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.169 - mean_q: 12.410 - prob: 1.000\n",
            "\n",
            "Interval 2136 (21350 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 2137 (21360 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.305 - mean_q: 12.530 - prob: 1.000\n",
            "\n",
            "Interval 2138 (21370 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.066 - mean_q: 12.261 - prob: 1.000\n",
            "\n",
            "Interval 2139 (21380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2140 (21390 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.205 - mean_q: 12.560 - prob: 1.000\n",
            "\n",
            "Interval 2141 (21400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.293 - mean_q: 12.600 - prob: 1.000\n",
            "\n",
            "Interval 2142 (21410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2143 (21420 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.166 - mean_q: 12.560 - prob: 1.000\n",
            "\n",
            "Interval 2144 (21430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.109 - mean_q: 12.342 - prob: 1.000\n",
            "\n",
            "Interval 2145 (21440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.047 - mean_q: 12.243 - prob: 1.000\n",
            "\n",
            "Interval 2146 (21450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2147 (21460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2148 (21470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.575 - mean_q: 12.960 - prob: 1.000\n",
            "\n",
            "Interval 2149 (21480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.210 - mean_q: 12.560 - prob: 1.000\n",
            "\n",
            "Interval 2150 (21490 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.483 - mean_q: 12.850 - prob: 1.000\n",
            "\n",
            "Interval 2151 (21500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2152 (21510 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.204 - mean_q: 12.527 - prob: 1.000\n",
            "\n",
            "Interval 2153 (21520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.058 - mean_q: 12.294 - prob: 1.000\n",
            "\n",
            "Interval 2154 (21530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.000 - mae: 7.296 - mean_q: 12.628 - prob: 1.000\n",
            "\n",
            "Interval 2155 (21540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2156 (21550 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.225 - mean_q: 12.434 - prob: 1.000\n",
            "\n",
            "Interval 2157 (21560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.005 - mean_q: 12.297 - prob: 1.000\n",
            "\n",
            "Interval 2158 (21570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.000 - mae: 7.130 - mean_q: 12.354 - prob: 1.000\n",
            "\n",
            "Interval 2159 (21580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2160 (21590 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.001 - mae: 7.360 - mean_q: 12.573 - prob: 1.000\n",
            "\n",
            "Interval 2161 (21600 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.408 - mean_q: 12.566 - prob: 1.000\n",
            "\n",
            "Interval 2162 (21610 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.000 - mae: 7.289 - mean_q: 12.700 - prob: 1.000\n",
            "\n",
            "Interval 2163 (21620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.569 - mean_q: 13.005 - prob: 1.000\n",
            "\n",
            "Interval 2164 (21630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2165 (21640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.044 - mean_q: 12.251 - prob: 1.000\n",
            "\n",
            "Interval 2166 (21650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.137 - mean_q: 12.517 - prob: 1.000\n",
            "\n",
            "Interval 2167 (21660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2168 (21670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -18.000 [-18.000, -18.000] - loss: 0.000 - mae: 7.436 - mean_q: 12.733 - prob: 1.000\n",
            "\n",
            "Interval 2169 (21680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.000 - mae: 7.281 - mean_q: 12.523 - prob: 1.000\n",
            "\n",
            "Interval 2170 (21690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2171 (21700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.085 - mean_q: 12.221 - prob: 1.000\n",
            "\n",
            "Interval 2172 (21710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -34.000 [-34.000, -34.000] - loss: 0.000 - mae: 7.503 - mean_q: 12.865 - prob: 1.000\n",
            "\n",
            "Interval 2173 (21720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2174 (21730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.471 - mean_q: 12.873 - prob: 1.000\n",
            "\n",
            "Interval 2175 (21740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.444 - mean_q: 12.760 - prob: 1.000\n",
            "\n",
            "Interval 2176 (21750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2177 (21760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.432 - mean_q: 12.815 - prob: 1.000\n",
            "\n",
            "Interval 2178 (21770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.084 - mean_q: 12.287 - prob: 1.000\n",
            "\n",
            "Interval 2179 (21780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2180 (21790 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.000 - mae: 7.475 - mean_q: 12.835 - prob: 1.000\n",
            "\n",
            "Interval 2181 (21800 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.571 - mean_q: 13.099 - prob: 1.000\n",
            "\n",
            "Interval 2182 (21810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.367 - mean_q: 12.715 - prob: 1.000\n",
            "\n",
            "Interval 2183 (21820 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 2184 (21830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2185 (21840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.001 - mae: 7.179 - mean_q: 12.371 - prob: 1.000\n",
            "\n",
            "Interval 2186 (21850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.218 - mean_q: 12.427 - prob: 1.000\n",
            "\n",
            "Interval 2187 (21860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.386 - mean_q: 12.754 - prob: 1.000\n",
            "\n",
            "Interval 2188 (21870 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 2189 (21880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -24.000 [-24.000, -24.000] - loss: 0.001 - mae: 7.030 - mean_q: 12.270 - prob: 1.000\n",
            "\n",
            "Interval 2190 (21890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.250 - mean_q: 12.595 - prob: 1.000\n",
            "\n",
            "Interval 2191 (21900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.450 - mean_q: 12.703 - prob: 1.000\n",
            "\n",
            "Interval 2192 (21910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2193 (21920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.554 - mean_q: 12.897 - prob: 1.000\n",
            "\n",
            "Interval 2194 (21930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.259 - mean_q: 12.506 - prob: 1.000\n",
            "\n",
            "Interval 2195 (21940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2196 (21950 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.379 - mean_q: 12.739 - prob: 1.000\n",
            "\n",
            "Interval 2197 (21960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.308 - mean_q: 12.618 - prob: 1.000\n",
            "\n",
            "Interval 2198 (21970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2199 (21980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.000 - mae: 7.388 - mean_q: 12.756 - prob: 1.000\n",
            "\n",
            "Interval 2200 (21990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.000 - mae: 7.480 - mean_q: 12.887 - prob: 1.000\n",
            "\n",
            "Interval 2201 (22000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2202 (22010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.000 - mae: 6.995 - mean_q: 12.313 - prob: 1.000\n",
            "\n",
            "Interval 2203 (22020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2204 (22030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -20.000 [-20.000, -20.000] - loss: 0.000 - mae: 7.060 - mean_q: 12.149 - prob: 1.000\n",
            "\n",
            "Interval 2205 (22040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2206 (22050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.067 - mean_q: 12.179 - prob: 1.000\n",
            "\n",
            "Interval 2207 (22060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.430 - mean_q: 12.773 - prob: 1.000\n",
            "\n",
            "Interval 2208 (22070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.396 - mean_q: 12.619 - prob: 1.000\n",
            "\n",
            "Interval 2209 (22080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.511 - mean_q: 12.953 - prob: 1.000\n",
            "\n",
            "Interval 2210 (22090 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 2211 (22100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.353 - mean_q: 12.677 - prob: 1.000\n",
            "\n",
            "Interval 2212 (22110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2213 (22120 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.513 - mean_q: 12.927 - prob: 1.000\n",
            "\n",
            "Interval 2214 (22130 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.208 - mean_q: 12.484 - prob: 1.000\n",
            "\n",
            "Interval 2215 (22140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2216 (22150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.390 - mean_q: 12.700 - prob: 1.000\n",
            "\n",
            "Interval 2217 (22160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2218 (22170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: -1.000 [-14.000, 12.000] - loss: 0.001 - mae: 7.253 - mean_q: 12.666 - prob: 1.000\n",
            "\n",
            "Interval 2219 (22180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2220 (22190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.190 - mean_q: 12.423 - prob: 1.000\n",
            "\n",
            "Interval 2221 (22200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2222 (22210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.000 - mae: 7.506 - mean_q: 12.967 - prob: 1.000\n",
            "\n",
            "Interval 2223 (22220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2224 (22230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.412 - mean_q: 12.797 - prob: 1.000\n",
            "\n",
            "Interval 2225 (22240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.334 - mean_q: 12.766 - prob: 1.000\n",
            "\n",
            "Interval 2226 (22250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2227 (22260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -23.000 [-23.000, -23.000] - loss: 0.001 - mae: 7.513 - mean_q: 12.957 - prob: 1.000\n",
            "\n",
            "Interval 2228 (22270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2229 (22280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.023 - mean_q: 12.143 - prob: 1.000\n",
            "\n",
            "Interval 2230 (22290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.408 - mean_q: 12.786 - prob: 1.000\n",
            "\n",
            "Interval 2231 (22300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.310 - mean_q: 12.562 - prob: 1.000\n",
            "\n",
            "Interval 2232 (22310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.064 - mean_q: 12.258 - prob: 1.000\n",
            "\n",
            "Interval 2233 (22320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2234 (22330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.341 - mean_q: 12.723 - prob: 1.000\n",
            "\n",
            "Interval 2235 (22340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2236 (22350 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.568 - mean_q: 12.891 - prob: 1.000\n",
            "\n",
            "Interval 2237 (22360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.740 - mean_q: 13.221 - prob: 1.000\n",
            "\n",
            "Interval 2238 (22370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2239 (22380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.653 - mean_q: 13.214 - prob: 1.000\n",
            "\n",
            "Interval 2240 (22390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.496 - mean_q: 12.936 - prob: 1.000\n",
            "\n",
            "Interval 2241 (22400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.273 - mean_q: 12.638 - prob: 1.000\n",
            "\n",
            "Interval 2242 (22410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2243 (22420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.051 - mean_q: 12.250 - prob: 1.000\n",
            "\n",
            "Interval 2244 (22430 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.282 - mean_q: 12.683 - prob: 1.000\n",
            "\n",
            "Interval 2245 (22440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2246 (22450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.409 - mean_q: 12.872 - prob: 1.000\n",
            "\n",
            "Interval 2247 (22460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.322 - mean_q: 12.545 - prob: 1.000\n",
            "\n",
            "Interval 2248 (22470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.304 - mean_q: 12.579 - prob: 1.000\n",
            "\n",
            "Interval 2249 (22480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2250 (22490 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.077 - mean_q: 12.162 - prob: 1.000\n",
            "\n",
            "Interval 2251 (22500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.626 - mean_q: 13.060 - prob: 1.000\n",
            "\n",
            "Interval 2252 (22510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2253 (22520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.452 - mean_q: 12.775 - prob: 1.000\n",
            "\n",
            "Interval 2254 (22530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.513 - mean_q: 12.857 - prob: 1.000\n",
            "\n",
            "Interval 2255 (22540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2256 (22550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.268 - mean_q: 12.525 - prob: 1.000\n",
            "\n",
            "Interval 2257 (22560 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.308 - mean_q: 12.726 - prob: 1.000\n",
            "\n",
            "Interval 2258 (22570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.805 - mean_q: 13.422 - prob: 1.000\n",
            "\n",
            "Interval 2259 (22580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2260 (22590 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.351 - mean_q: 12.856 - prob: 1.000\n",
            "\n",
            "Interval 2261 (22600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2262 (22610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.480 - mean_q: 12.920 - prob: 1.000\n",
            "\n",
            "Interval 2263 (22620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.230 - mean_q: 12.522 - prob: 1.000\n",
            "\n",
            "Interval 2264 (22630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.186 - mean_q: 12.501 - prob: 1.000\n",
            "\n",
            "Interval 2265 (22640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.350 - mean_q: 12.641 - prob: 1.000\n",
            "\n",
            "Interval 2266 (22650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2267 (22660 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.350 - mean_q: 12.629 - prob: 1.000\n",
            "\n",
            "Interval 2268 (22670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.579 - mean_q: 13.045 - prob: 1.000\n",
            "\n",
            "Interval 2269 (22680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.512 - mean_q: 12.924 - prob: 1.000\n",
            "\n",
            "Interval 2270 (22690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 2271 (22700 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.001 - mae: 7.514 - mean_q: 12.857 - prob: 1.000\n",
            "\n",
            "Interval 2272 (22710 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.325 - mean_q: 12.662 - prob: 1.000\n",
            "\n",
            "Interval 2273 (22720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.142 - mean_q: 12.400 - prob: 1.000\n",
            "\n",
            "Interval 2274 (22730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2275 (22740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.564 - mean_q: 12.987 - prob: 1.000\n",
            "\n",
            "Interval 2276 (22750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 6.865 - mean_q: 12.115 - prob: 1.000\n",
            "\n",
            "Interval 2277 (22760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2278 (22770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.334 - mean_q: 12.814 - prob: 1.000\n",
            "\n",
            "Interval 2279 (22780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2280 (22790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.426 - mean_q: 12.718 - prob: 1.000\n",
            "\n",
            "Interval 2281 (22800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.219 - mean_q: 12.576 - prob: 1.000\n",
            "\n",
            "Interval 2282 (22810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2283 (22820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.000 - mae: 7.578 - mean_q: 12.991 - prob: 1.000\n",
            "\n",
            "Interval 2284 (22830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.402 - mean_q: 12.778 - prob: 1.000\n",
            "\n",
            "Interval 2285 (22840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.209 - mean_q: 12.540 - prob: 1.000\n",
            "\n",
            "Interval 2286 (22850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.350 - mean_q: 12.771 - prob: 1.000\n",
            "\n",
            "Interval 2287 (22860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2288 (22870 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.736 - mean_q: 13.229 - prob: 1.000\n",
            "\n",
            "Interval 2289 (22880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.315 - mean_q: 12.618 - prob: 1.000\n",
            "\n",
            "Interval 2290 (22890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2291 (22900 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.000 - mae: 7.591 - mean_q: 12.979 - prob: 1.000\n",
            "\n",
            "Interval 2292 (22910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.000 - mae: 7.559 - mean_q: 13.102 - prob: 1.000\n",
            "\n",
            "Interval 2293 (22920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.000 - mae: 7.221 - mean_q: 12.582 - prob: 1.000\n",
            "\n",
            "Interval 2294 (22930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2295 (22940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.000 - mae: 7.535 - mean_q: 12.934 - prob: 1.000\n",
            "\n",
            "Interval 2296 (22950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.193 - mean_q: 12.464 - prob: 1.000\n",
            "\n",
            "Interval 2297 (22960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -3.7000\n",
            "Interval 2298 (22970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -34.000 [-34.000, -34.000] - loss: 0.000 - mae: 7.467 - mean_q: 12.916 - prob: 1.000\n",
            "\n",
            "Interval 2299 (22980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.323 - mean_q: 12.664 - prob: 1.000\n",
            "\n",
            "Interval 2300 (22990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.387 - mean_q: 12.763 - prob: 1.000\n",
            "\n",
            "Interval 2301 (23000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2302 (23010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.264 - mean_q: 12.646 - prob: 1.000\n",
            "\n",
            "Interval 2303 (23020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.000 - mae: 7.226 - mean_q: 12.545 - prob: 1.000\n",
            "\n",
            "Interval 2304 (23030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.250 - mean_q: 12.564 - prob: 1.000\n",
            "\n",
            "Interval 2305 (23040 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.001 - mae: 7.247 - mean_q: 12.509 - prob: 1.000\n",
            "\n",
            "Interval 2306 (23050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2307 (23060 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.341 - mean_q: 12.750 - prob: 1.000\n",
            "\n",
            "Interval 2308 (23070 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.395 - mean_q: 12.753 - prob: 1.000\n",
            "\n",
            "Interval 2309 (23080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.343 - mean_q: 12.616 - prob: 1.000\n",
            "\n",
            "Interval 2310 (23090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2311 (23100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.165 - mean_q: 12.473 - prob: 1.000\n",
            "\n",
            "Interval 2312 (23110 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.001 - mae: 7.298 - mean_q: 12.511 - prob: 1.000\n",
            "\n",
            "Interval 2313 (23120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.544 - mean_q: 12.974 - prob: 1.000\n",
            "\n",
            "Interval 2314 (23130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2315 (23140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.642 - mean_q: 13.147 - prob: 1.000\n",
            "\n",
            "Interval 2316 (23150 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.207 - mean_q: 12.346 - prob: 1.000\n",
            "\n",
            "Interval 2317 (23160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2318 (23170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.000 - mae: 6.951 - mean_q: 12.080 - prob: 1.000\n",
            "\n",
            "Interval 2319 (23180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2320 (23190 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.000 - mae: 7.244 - mean_q: 12.445 - prob: 1.000\n",
            "\n",
            "Interval 2321 (23200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2322 (23210 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.269 - mean_q: 12.638 - prob: 1.000\n",
            "\n",
            "Interval 2323 (23220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2324 (23230 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.326 - mean_q: 12.681 - prob: 1.000\n",
            "\n",
            "Interval 2325 (23240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.407 - mean_q: 12.812 - prob: 1.000\n",
            "\n",
            "Interval 2326 (23250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.638 - mean_q: 13.080 - prob: 1.000\n",
            "\n",
            "Interval 2327 (23260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2328 (23270 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.838 - mean_q: 13.431 - prob: 1.000\n",
            "\n",
            "Interval 2329 (23280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.207 - mean_q: 12.423 - prob: 1.000\n",
            "\n",
            "Interval 2330 (23290 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.001 - mae: 7.408 - mean_q: 12.724 - prob: 1.000\n",
            "\n",
            "Interval 2331 (23300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2332 (23310 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.330 - mean_q: 12.701 - prob: 1.000\n",
            "\n",
            "Interval 2333 (23320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2334 (23330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.553 - mean_q: 12.897 - prob: 1.000\n",
            "\n",
            "Interval 2335 (23340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.557 - mean_q: 12.970 - prob: 1.000\n",
            "\n",
            "Interval 2336 (23350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2337 (23360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.439 - mean_q: 12.931 - prob: 1.000\n",
            "\n",
            "Interval 2338 (23370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.330 - mean_q: 12.785 - prob: 1.000\n",
            "\n",
            "Interval 2339 (23380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.479 - mean_q: 12.901 - prob: 1.000\n",
            "\n",
            "Interval 2340 (23390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.301 - mean_q: 12.618 - prob: 1.000\n",
            "\n",
            "Interval 2341 (23400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.345 - mean_q: 12.717 - prob: 1.000\n",
            "\n",
            "Interval 2342 (23410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2343 (23420 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.000 - mae: 7.402 - mean_q: 12.737 - prob: 1.000\n",
            "\n",
            "Interval 2344 (23430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.364 - mean_q: 12.726 - prob: 1.000\n",
            "\n",
            "Interval 2345 (23440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.000 - mae: 7.283 - mean_q: 12.499 - prob: 1.000\n",
            "\n",
            "Interval 2346 (23450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2347 (23460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.219 - mean_q: 12.513 - prob: 1.000\n",
            "\n",
            "Interval 2348 (23470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.159 - mean_q: 12.530 - prob: 1.000\n",
            "\n",
            "Interval 2349 (23480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.390 - mean_q: 12.885 - prob: 1.000\n",
            "\n",
            "Interval 2350 (23490 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.001 - mae: 7.387 - mean_q: 12.858 - prob: 1.000\n",
            "\n",
            "Interval 2351 (23500 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.412 - mean_q: 12.789 - prob: 1.000\n",
            "\n",
            "Interval 2352 (23510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.389 - mean_q: 12.839 - prob: 1.000\n",
            "\n",
            "Interval 2353 (23520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2354 (23530 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.388 - mean_q: 12.821 - prob: 1.000\n",
            "\n",
            "Interval 2355 (23540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2356 (23550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.246 - mean_q: 12.422 - prob: 1.000\n",
            "\n",
            "Interval 2357 (23560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.675 - mean_q: 13.122 - prob: 1.000\n",
            "\n",
            "Interval 2358 (23570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2359 (23580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.240 - mean_q: 12.568 - prob: 1.000\n",
            "\n",
            "Interval 2360 (23590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.364 - mean_q: 12.803 - prob: 1.000\n",
            "\n",
            "Interval 2361 (23600 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.002 - mae: 7.008 - mean_q: 12.215 - prob: 1.000\n",
            "\n",
            "Interval 2362 (23610 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.302 - mean_q: 12.743 - prob: 1.000\n",
            "\n",
            "Interval 2363 (23620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.002 - mae: 7.742 - mean_q: 13.179 - prob: 1.000\n",
            "\n",
            "Interval 2364 (23630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2365 (23640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.638 - mean_q: 13.116 - prob: 1.000\n",
            "\n",
            "Interval 2366 (23650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.243 - mean_q: 12.583 - prob: 1.000\n",
            "\n",
            "Interval 2367 (23660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.085 - mean_q: 12.321 - prob: 1.000\n",
            "\n",
            "Interval 2368 (23670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2369 (23680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.540 - mean_q: 12.985 - prob: 1.000\n",
            "\n",
            "Interval 2370 (23690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.583 - mean_q: 13.195 - prob: 1.000\n",
            "\n",
            "Interval 2371 (23700 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.001 - mae: 7.532 - mean_q: 12.915 - prob: 1.000\n",
            "\n",
            "Interval 2372 (23710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2373 (23720 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -22.000 [-22.000, -22.000] - loss: 0.001 - mae: 7.498 - mean_q: 12.850 - prob: 1.000\n",
            "\n",
            "Interval 2374 (23730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2375 (23740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.162 - mean_q: 12.467 - prob: 1.000\n",
            "\n",
            "Interval 2376 (23750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.344 - mean_q: 12.578 - prob: 1.000\n",
            "\n",
            "Interval 2377 (23760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2378 (23770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.532 - mean_q: 13.010 - prob: 1.000\n",
            "\n",
            "Interval 2379 (23780 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.263 - mean_q: 12.507 - prob: 1.000\n",
            "\n",
            "Interval 2380 (23790 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.024 - mean_q: 12.346 - prob: 1.000\n",
            "\n",
            "Interval 2381 (23800 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -2.8000\n",
            "Interval 2382 (23810 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 0.500 [-13.000, 14.000] - loss: 0.001 - mae: 7.072 - mean_q: 12.226 - prob: 1.000\n",
            "\n",
            "Interval 2383 (23820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2384 (23830 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.392 - mean_q: 12.760 - prob: 1.000\n",
            "\n",
            "Interval 2385 (23840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2386 (23850 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.295 - mean_q: 12.574 - prob: 1.000\n",
            "\n",
            "Interval 2387 (23860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.710 - mean_q: 13.094 - prob: 1.000\n",
            "\n",
            "Interval 2388 (23870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2389 (23880 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.211 - mean_q: 12.595 - prob: 1.000\n",
            "\n",
            "Interval 2390 (23890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.244 - mean_q: 12.604 - prob: 1.000\n",
            "\n",
            "Interval 2391 (23900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.133 - mean_q: 12.395 - prob: 1.000\n",
            "\n",
            "Interval 2392 (23910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2393 (23920 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.538 - mean_q: 12.832 - prob: 1.000\n",
            "\n",
            "Interval 2394 (23930 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.070 - mean_q: 12.373 - prob: 1.000\n",
            "\n",
            "Interval 2395 (23940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2396 (23950 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.474 - mean_q: 12.991 - prob: 1.000\n",
            "\n",
            "Interval 2397 (23960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.240 - mean_q: 12.600 - prob: 1.000\n",
            "\n",
            "Interval 2398 (23970 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.523 - mean_q: 12.829 - prob: 1.000\n",
            "\n",
            "Interval 2399 (23980 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.285 - mean_q: 12.637 - prob: 1.000\n",
            "\n",
            "Interval 2400 (23990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.509 - mean_q: 13.134 - prob: 1.000\n",
            "\n",
            "Interval 2401 (24000 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.602 - mean_q: 13.007 - prob: 1.000\n",
            "\n",
            "Interval 2402 (24010 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.9000\n",
            "Interval 2403 (24020 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.9000\n",
            "Interval 2404 (24030 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -19.000 [-19.000, -19.000] - loss: 0.001 - mae: 7.185 - mean_q: 12.400 - prob: 1.000\n",
            "\n",
            "Interval 2405 (24040 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.412 - mean_q: 12.907 - prob: 1.000\n",
            "\n",
            "Interval 2406 (24050 steps performed)\n",
            "10/10 [==============================] - 0s 19ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 6.892 - mean_q: 11.965 - prob: 1.000\n",
            "\n",
            "Interval 2407 (24060 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.9000\n",
            "Interval 2408 (24070 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -18.000 [-18.000, -18.000] - loss: 0.001 - mae: 7.530 - mean_q: 12.906 - prob: 1.000\n",
            "\n",
            "Interval 2409 (24080 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 2410 (24090 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.394 - mean_q: 12.688 - prob: 1.000\n",
            "\n",
            "Interval 2411 (24100 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 2412 (24110 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.293 - mean_q: 12.528 - prob: 1.000\n",
            "\n",
            "Interval 2413 (24120 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.460 - mean_q: 12.882 - prob: 1.000\n",
            "\n",
            "Interval 2414 (24130 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 2415 (24140 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.001 - mae: 7.148 - mean_q: 12.257 - prob: 1.000\n",
            "\n",
            "Interval 2416 (24150 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.396 - mean_q: 12.769 - prob: 1.000\n",
            "\n",
            "Interval 2417 (24160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2418 (24170 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.224 - mean_q: 12.489 - prob: 1.000\n",
            "\n",
            "Interval 2419 (24180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.449 - mean_q: 12.676 - prob: 1.000\n",
            "\n",
            "Interval 2420 (24190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.331 - mean_q: 12.811 - prob: 1.000\n",
            "\n",
            "Interval 2421 (24200 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.002 - mae: 7.457 - mean_q: 12.742 - prob: 1.000\n",
            "\n",
            "Interval 2422 (24210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2423 (24220 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.382 - mean_q: 12.775 - prob: 1.000\n",
            "\n",
            "Interval 2424 (24230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2425 (24240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.387 - mean_q: 12.662 - prob: 1.000\n",
            "\n",
            "Interval 2426 (24250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.185 - mean_q: 12.494 - prob: 1.000\n",
            "\n",
            "Interval 2427 (24260 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 2428 (24270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.197 - mean_q: 12.439 - prob: 1.000\n",
            "\n",
            "Interval 2429 (24280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.271 - mean_q: 12.525 - prob: 1.000\n",
            "\n",
            "Interval 2430 (24290 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.627 - mean_q: 13.143 - prob: 1.000\n",
            "\n",
            "Interval 2431 (24300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2432 (24310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.001 - mae: 6.952 - mean_q: 12.118 - prob: 1.000\n",
            "\n",
            "Interval 2433 (24320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.389 - mean_q: 12.618 - prob: 1.000\n",
            "\n",
            "Interval 2434 (24330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2435 (24340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.624 - mean_q: 12.982 - prob: 1.000\n",
            "\n",
            "Interval 2436 (24350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2437 (24360 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.151 - mean_q: 12.359 - prob: 1.000\n",
            "\n",
            "Interval 2438 (24370 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.562 - mean_q: 13.001 - prob: 1.000\n",
            "\n",
            "Interval 2439 (24380 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 2440 (24390 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.365 - mean_q: 12.794 - prob: 1.000\n",
            "\n",
            "Interval 2441 (24400 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.000 - mae: 7.129 - mean_q: 12.408 - prob: 1.000\n",
            "\n",
            "Interval 2442 (24410 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.269 - mean_q: 12.517 - prob: 1.000\n",
            "\n",
            "Interval 2443 (24420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -3.7000\n",
            "Interval 2444 (24430 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -35.000 [-35.000, -35.000] - loss: 0.000 - mae: 7.532 - mean_q: 13.008 - prob: 1.000\n",
            "\n",
            "Interval 2445 (24440 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.373 - mean_q: 12.681 - prob: 1.000\n",
            "\n",
            "Interval 2446 (24450 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 2447 (24460 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.000 - mae: 7.027 - mean_q: 12.197 - prob: 1.000\n",
            "\n",
            "Interval 2448 (24470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2449 (24480 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.351 - mean_q: 12.587 - prob: 1.000\n",
            "\n",
            "Interval 2450 (24490 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.375 - mean_q: 12.718 - prob: 1.000\n",
            "\n",
            "Interval 2451 (24500 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.000 - mae: 7.200 - mean_q: 12.592 - prob: 1.000\n",
            "\n",
            "Interval 2452 (24510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2453 (24520 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.000 - mae: 7.355 - mean_q: 12.759 - prob: 1.000\n",
            "\n",
            "Interval 2454 (24530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2455 (24540 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.001 - mae: 7.056 - mean_q: 12.288 - prob: 1.000\n",
            "\n",
            "Interval 2456 (24550 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.046 - mean_q: 12.435 - prob: 1.000\n",
            "\n",
            "Interval 2457 (24560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.325 - mean_q: 12.616 - prob: 1.000\n",
            "\n",
            "Interval 2458 (24570 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 2459 (24580 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.001 - mae: 7.361 - mean_q: 12.632 - prob: 1.000\n",
            "\n",
            "Interval 2460 (24590 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.192 - mean_q: 12.312 - prob: 1.000\n",
            "\n",
            "Interval 2461 (24600 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.572 - mean_q: 13.040 - prob: 1.000\n",
            "\n",
            "Interval 2462 (24610 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 2463 (24620 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.341 - mean_q: 12.561 - prob: 1.000\n",
            "\n",
            "Interval 2464 (24630 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.196 - mean_q: 12.534 - prob: 1.000\n",
            "\n",
            "Interval 2465 (24640 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 2466 (24650 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.399 - mean_q: 12.765 - prob: 1.000\n",
            "\n",
            "Interval 2467 (24660 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.347 - mean_q: 12.627 - prob: 1.000\n",
            "\n",
            "Interval 2468 (24670 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 2469 (24680 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.779 - mean_q: 13.184 - prob: 1.000\n",
            "\n",
            "Interval 2470 (24690 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.193 - mean_q: 12.430 - prob: 1.000\n",
            "\n",
            "Interval 2471 (24700 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 2472 (24710 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.002 - mae: 7.458 - mean_q: 13.004 - prob: 1.000\n",
            "\n",
            "Interval 2473 (24720 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.625 - mean_q: 13.186 - prob: 1.000\n",
            "\n",
            "Interval 2474 (24730 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.002 - mae: 7.179 - mean_q: 12.408 - prob: 1.000\n",
            "\n",
            "Interval 2475 (24740 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 2476 (24750 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.251 - mean_q: 12.582 - prob: 1.000\n",
            "\n",
            "Interval 2477 (24760 steps performed)\n",
            "10/10 [==============================] - 0s 19ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.412 - mean_q: 12.824 - prob: 1.000\n",
            "\n",
            "Interval 2478 (24770 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: -1.0000\n",
            "Interval 2479 (24780 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.342 - mean_q: 12.597 - prob: 1.000\n",
            "\n",
            "Interval 2480 (24790 steps performed)\n",
            "10/10 [==============================] - 0s 16ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.338 - mean_q: 12.703 - prob: 1.000\n",
            "\n",
            "Interval 2481 (24800 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.0000\n",
            "Interval 2482 (24810 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.488 - mean_q: 12.887 - prob: 1.000\n",
            "\n",
            "Interval 2483 (24820 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.477 - mean_q: 12.903 - prob: 1.000\n",
            "\n",
            "Interval 2484 (24830 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 2485 (24840 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -2.8000\n",
            "Interval 2486 (24850 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -20.000 [-20.000, -20.000] - loss: 0.001 - mae: 7.144 - mean_q: 12.383 - prob: 1.000\n",
            "\n",
            "Interval 2487 (24860 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.261 - mean_q: 12.525 - prob: 1.000\n",
            "\n",
            "Interval 2488 (24870 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 2489 (24880 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.531 - mean_q: 13.012 - prob: 1.000\n",
            "\n",
            "Interval 2490 (24890 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 2491 (24900 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.443 - mean_q: 12.683 - prob: 1.000\n",
            "\n",
            "Interval 2492 (24910 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.0000\n",
            "Interval 2493 (24920 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.513 - mean_q: 12.967 - prob: 1.000\n",
            "\n",
            "Interval 2494 (24930 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.606 - mean_q: 13.081 - prob: 1.000\n",
            "\n",
            "Interval 2495 (24940 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.456 - mean_q: 12.885 - prob: 1.000\n",
            "\n",
            "Interval 2496 (24950 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 2497 (24960 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.486 - mean_q: 12.889 - prob: 1.000\n",
            "\n",
            "Interval 2498 (24970 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.365 - mean_q: 12.737 - prob: 1.000\n",
            "\n",
            "Interval 2499 (24980 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.171 - mean_q: 12.496 - prob: 1.000\n",
            "\n",
            "Interval 2500 (24990 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.612 - mean_q: 13.029 - prob: 1.000\n",
            "\n",
            "Interval 2501 (25000 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 2502 (25010 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.367 - mean_q: 12.631 - prob: 1.000\n",
            "\n",
            "Interval 2503 (25020 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.176 - mean_q: 12.520 - prob: 1.000\n",
            "\n",
            "Interval 2504 (25030 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: -1.0000\n",
            "Interval 2505 (25040 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.000 - mae: 7.834 - mean_q: 13.476 - prob: 1.000\n",
            "\n",
            "Interval 2506 (25050 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.9000\n",
            "Interval 2507 (25060 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.468 - mean_q: 12.754 - prob: 1.000\n",
            "\n",
            "Interval 2508 (25070 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.277 - mean_q: 12.536 - prob: 1.000\n",
            "\n",
            "Interval 2509 (25080 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 2510 (25090 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.504 - mean_q: 12.935 - prob: 1.000\n",
            "\n",
            "Interval 2511 (25100 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.347 - mean_q: 12.593 - prob: 1.000\n",
            "\n",
            "Interval 2512 (25110 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 2513 (25120 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.257 - mean_q: 12.644 - prob: 1.000\n",
            "\n",
            "Interval 2514 (25130 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.483 - mean_q: 12.949 - prob: 1.000\n",
            "\n",
            "Interval 2515 (25140 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.351 - mean_q: 12.817 - prob: 1.000\n",
            "\n",
            "Interval 2516 (25150 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.438 - mean_q: 12.663 - prob: 1.000\n",
            "\n",
            "Interval 2517 (25160 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 2518 (25170 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.000 - mae: 7.026 - mean_q: 12.129 - prob: 1.000\n",
            "\n",
            "Interval 2519 (25180 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.9000\n",
            "Interval 2520 (25190 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -25.000 [-25.000, -25.000] - loss: 0.001 - mae: 7.325 - mean_q: 12.586 - prob: 1.000\n",
            "\n",
            "Interval 2521 (25200 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 2522 (25210 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.000 - mae: 7.387 - mean_q: 12.646 - prob: 1.000\n",
            "\n",
            "Interval 2523 (25220 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 2524 (25230 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.585 - mean_q: 13.053 - prob: 1.000\n",
            "\n",
            "Interval 2525 (25240 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.351 - mean_q: 12.603 - prob: 1.000\n",
            "\n",
            "Interval 2526 (25250 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.363 - mean_q: 12.690 - prob: 1.000\n",
            "\n",
            "Interval 2527 (25260 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 2528 (25270 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.460 - mean_q: 12.946 - prob: 1.000\n",
            "\n",
            "Interval 2529 (25280 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.642 - mean_q: 13.090 - prob: 1.000\n",
            "\n",
            "Interval 2530 (25290 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.303 - mean_q: 12.541 - prob: 1.000\n",
            "\n",
            "Interval 2531 (25300 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 2532 (25310 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.402 - mean_q: 12.587 - prob: 1.000\n",
            "\n",
            "Interval 2533 (25320 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 2534 (25330 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.442 - mean_q: 12.836 - prob: 1.000\n",
            "\n",
            "Interval 2535 (25340 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.388 - mean_q: 12.530 - prob: 1.000\n",
            "\n",
            "Interval 2536 (25350 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.406 - mean_q: 12.777 - prob: 1.000\n",
            "\n",
            "Interval 2537 (25360 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 2538 (25370 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.354 - mean_q: 12.738 - prob: 1.000\n",
            "\n",
            "Interval 2539 (25380 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.428 - mean_q: 12.976 - prob: 1.000\n",
            "\n",
            "Interval 2540 (25390 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.583 - mean_q: 13.107 - prob: 1.000\n",
            "\n",
            "Interval 2541 (25400 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 2542 (25410 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.211 - mean_q: 12.366 - prob: 1.000\n",
            "\n",
            "Interval 2543 (25420 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 2544 (25430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.220 - mean_q: 12.443 - prob: 1.000\n",
            "\n",
            "Interval 2545 (25440 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.001 - mae: 7.681 - mean_q: 13.148 - prob: 1.000\n",
            "\n",
            "Interval 2546 (25450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.256 - mean_q: 12.590 - prob: 1.000\n",
            "\n",
            "Interval 2547 (25460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2548 (25470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.266 - mean_q: 12.550 - prob: 1.000\n",
            "\n",
            "Interval 2549 (25480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.402 - mean_q: 12.705 - prob: 1.000\n",
            "\n",
            "Interval 2550 (25490 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.472 - mean_q: 12.955 - prob: 1.000\n",
            "\n",
            "Interval 2551 (25500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2552 (25510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.288 - mean_q: 12.547 - prob: 1.000\n",
            "\n",
            "Interval 2553 (25520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 2554 (25530 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.001 - mae: 7.480 - mean_q: 12.858 - prob: 1.000\n",
            "\n",
            "Interval 2555 (25540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.131 - mean_q: 12.392 - prob: 1.000\n",
            "\n",
            "Interval 2556 (25550 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 2557 (25560 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.367 - mean_q: 12.740 - prob: 1.000\n",
            "\n",
            "Interval 2558 (25570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2559 (25580 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.580 - mean_q: 12.990 - prob: 1.000\n",
            "\n",
            "Interval 2560 (25590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.510 - mean_q: 13.029 - prob: 1.000\n",
            "\n",
            "Interval 2561 (25600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2562 (25610 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.435 - mean_q: 12.798 - prob: 1.000\n",
            "\n",
            "Interval 2563 (25620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2564 (25630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -19.000 [-19.000, -19.000] - loss: 0.001 - mae: 7.135 - mean_q: 12.412 - prob: 1.000\n",
            "\n",
            "Interval 2565 (25640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2566 (25650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.000 - mae: 7.209 - mean_q: 12.518 - prob: 1.000\n",
            "\n",
            "Interval 2567 (25660 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.390 - mean_q: 12.768 - prob: 1.000\n",
            "\n",
            "Interval 2568 (25670 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 2569 (25680 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 9.500 [7.000, 12.000] - loss: 0.001 - mae: 7.343 - mean_q: 12.741 - prob: 1.000\n",
            "\n",
            "Interval 2570 (25690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2571 (25700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.006 - mae: 7.647 - mean_q: 12.956 - prob: 1.000\n",
            "\n",
            "Interval 2572 (25710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2573 (25720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.301 - mean_q: 12.569 - prob: 1.000\n",
            "\n",
            "Interval 2574 (25730 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.802 - mean_q: 13.411 - prob: 1.000\n",
            "\n",
            "Interval 2575 (25740 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 2576 (25750 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.594 - mean_q: 13.077 - prob: 1.000\n",
            "\n",
            "Interval 2577 (25760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2578 (25770 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.260 - mean_q: 12.699 - prob: 1.000\n",
            "\n",
            "Interval 2579 (25780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.001 - mae: 7.251 - mean_q: 12.497 - prob: 1.000\n",
            "\n",
            "Interval 2580 (25790 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 2581 (25800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.320 - mean_q: 12.571 - prob: 1.000\n",
            "\n",
            "Interval 2582 (25810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2583 (25820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.000 - mae: 7.530 - mean_q: 12.860 - prob: 1.000\n",
            "\n",
            "Interval 2584 (25830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.626 - mean_q: 13.045 - prob: 1.000\n",
            "\n",
            "Interval 2585 (25840 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 2586 (25850 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.001 - mae: 7.324 - mean_q: 12.759 - prob: 1.000\n",
            "\n",
            "Interval 2587 (25860 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.572 - mean_q: 13.050 - prob: 1.000\n",
            "\n",
            "Interval 2588 (25870 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.169 - mean_q: 12.628 - prob: 1.000\n",
            "\n",
            "Interval 2589 (25880 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.086 - mean_q: 12.303 - prob: 1.000\n",
            "\n",
            "Interval 2590 (25890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2591 (25900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.103 - mean_q: 12.297 - prob: 1.000\n",
            "\n",
            "Interval 2592 (25910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.388 - mean_q: 12.639 - prob: 1.000\n",
            "\n",
            "Interval 2593 (25920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2594 (25930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.002 - mae: 7.249 - mean_q: 12.640 - prob: 1.000\n",
            "\n",
            "Interval 2595 (25940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.001 - mae: 7.174 - mean_q: 12.357 - prob: 1.000\n",
            "\n",
            "Interval 2596 (25950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.520 - mean_q: 13.058 - prob: 1.000\n",
            "\n",
            "Interval 2597 (25960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2598 (25970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.347 - mean_q: 12.727 - prob: 1.000\n",
            "\n",
            "Interval 2599 (25980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.300 - mean_q: 12.629 - prob: 1.000\n",
            "\n",
            "Interval 2600 (25990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.561 - mean_q: 12.942 - prob: 1.000\n",
            "\n",
            "Interval 2601 (26000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.441 - mean_q: 12.664 - prob: 1.000\n",
            "\n",
            "Interval 2602 (26010 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 2603 (26020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.540 - mean_q: 12.963 - prob: 1.000\n",
            "\n",
            "Interval 2604 (26030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.610 - mean_q: 13.192 - prob: 1.000\n",
            "\n",
            "Interval 2605 (26040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2606 (26050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.269 - mean_q: 12.593 - prob: 1.000\n",
            "\n",
            "Interval 2607 (26060 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.117 - mean_q: 12.406 - prob: 1.000\n",
            "\n",
            "Interval 2608 (26070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2609 (26080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.337 - mean_q: 12.743 - prob: 1.000\n",
            "\n",
            "Interval 2610 (26090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.220 - mean_q: 12.584 - prob: 1.000\n",
            "\n",
            "Interval 2611 (26100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2612 (26110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.335 - mean_q: 12.795 - prob: 1.000\n",
            "\n",
            "Interval 2613 (26120 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.537 - mean_q: 12.864 - prob: 1.000\n",
            "\n",
            "Interval 2614 (26130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.253 - mean_q: 12.644 - prob: 1.000\n",
            "\n",
            "Interval 2615 (26140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.256 - mean_q: 12.563 - prob: 1.000\n",
            "\n",
            "Interval 2616 (26150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2617 (26160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2618 (26170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.333 - mean_q: 12.703 - prob: 1.000\n",
            "\n",
            "Interval 2619 (26180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.513 - mean_q: 12.828 - prob: 1.000\n",
            "\n",
            "Interval 2620 (26190 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 2621 (26200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -20.000 [-20.000, -20.000] - loss: 0.001 - mae: 7.860 - mean_q: 13.362 - prob: 1.000\n",
            "\n",
            "Interval 2622 (26210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.000 - mae: 7.281 - mean_q: 12.572 - prob: 1.000\n",
            "\n",
            "Interval 2623 (26220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.409 - mean_q: 12.741 - prob: 1.000\n",
            "\n",
            "Interval 2624 (26230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 6.984 - mean_q: 12.198 - prob: 1.000\n",
            "\n",
            "Interval 2625 (26240 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 2626 (26250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -17.000 [-17.000, -17.000] - loss: 0.001 - mae: 7.303 - mean_q: 12.683 - prob: 1.000\n",
            "\n",
            "Interval 2627 (26260 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 2628 (26270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.397 - mean_q: 12.697 - prob: 1.000\n",
            "\n",
            "Interval 2629 (26280 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.301 - mean_q: 12.643 - prob: 1.000\n",
            "\n",
            "Interval 2630 (26290 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.329 - mean_q: 12.573 - prob: 1.000\n",
            "\n",
            "Interval 2631 (26300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.389 - mean_q: 12.612 - prob: 1.000\n",
            "\n",
            "Interval 2632 (26310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.562 - mean_q: 12.891 - prob: 1.000\n",
            "\n",
            "Interval 2633 (26320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2634 (26330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.405 - mean_q: 12.735 - prob: 1.000\n",
            "\n",
            "Interval 2635 (26340 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 2636 (26350 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.127 - mean_q: 12.555 - prob: 1.000\n",
            "\n",
            "Interval 2637 (26360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2638 (26370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -25.000 [-25.000, -25.000] - loss: 0.001 - mae: 7.388 - mean_q: 12.724 - prob: 1.000\n",
            "\n",
            "Interval 2639 (26380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.416 - mean_q: 12.796 - prob: 1.000\n",
            "\n",
            "Interval 2640 (26390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2641 (26400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2642 (26410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.703 - mean_q: 13.172 - prob: 1.000\n",
            "\n",
            "Interval 2643 (26420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.254 - mean_q: 12.654 - prob: 1.000\n",
            "\n",
            "Interval 2644 (26430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.577 - mean_q: 12.943 - prob: 1.000\n",
            "\n",
            "Interval 2645 (26440 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.226 - mean_q: 12.457 - prob: 1.000\n",
            "\n",
            "Interval 2646 (26450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2647 (26460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.608 - mean_q: 13.138 - prob: 1.000\n",
            "\n",
            "Interval 2648 (26470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2649 (26480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.002 - mae: 7.723 - mean_q: 13.070 - prob: 1.000\n",
            "\n",
            "Interval 2650 (26490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.239 - mean_q: 12.427 - prob: 1.000\n",
            "\n",
            "Interval 2651 (26500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.438 - mean_q: 12.758 - prob: 1.000\n",
            "\n",
            "Interval 2652 (26510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.003 - mae: 7.452 - mean_q: 12.779 - prob: 1.000\n",
            "\n",
            "Interval 2653 (26520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2654 (26530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.220 - mean_q: 12.485 - prob: 1.000\n",
            "\n",
            "Interval 2655 (26540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.485 - mean_q: 12.828 - prob: 1.000\n",
            "\n",
            "Interval 2656 (26550 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 2657 (26560 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.345 - mean_q: 12.726 - prob: 1.000\n",
            "\n",
            "Interval 2658 (26570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2659 (26580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.562 - mean_q: 13.088 - prob: 1.000\n",
            "\n",
            "Interval 2660 (26590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2661 (26600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.019 - mean_q: 12.300 - prob: 1.000\n",
            "\n",
            "Interval 2662 (26610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.518 - mean_q: 12.886 - prob: 1.000\n",
            "\n",
            "Interval 2663 (26620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2664 (26630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.299 - mean_q: 12.640 - prob: 1.000\n",
            "\n",
            "Interval 2665 (26640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.465 - mean_q: 12.892 - prob: 1.000\n",
            "\n",
            "Interval 2666 (26650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.462 - mean_q: 12.974 - prob: 1.000\n",
            "\n",
            "Interval 2667 (26660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2668 (26670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.004 - mae: 7.142 - mean_q: 12.467 - prob: 1.000\n",
            "\n",
            "Interval 2669 (26680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.417 - mean_q: 12.626 - prob: 1.000\n",
            "\n",
            "Interval 2670 (26690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2671 (26700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.422 - mean_q: 12.788 - prob: 1.000\n",
            "\n",
            "Interval 2672 (26710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2673 (26720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.002 - mae: 7.592 - mean_q: 12.981 - prob: 1.000\n",
            "\n",
            "Interval 2674 (26730 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 2675 (26740 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.579 - mean_q: 12.930 - prob: 1.000\n",
            "\n",
            "Interval 2676 (26750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.371 - mean_q: 12.812 - prob: 1.000\n",
            "\n",
            "Interval 2677 (26760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2678 (26770 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.668 - mean_q: 13.140 - prob: 1.000\n",
            "\n",
            "Interval 2679 (26780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2680 (26790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.258 - mean_q: 12.562 - prob: 1.000\n",
            "\n",
            "Interval 2681 (26800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.447 - mean_q: 12.783 - prob: 1.000\n",
            "\n",
            "Interval 2682 (26810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.157 - mean_q: 12.384 - prob: 1.000\n",
            "\n",
            "Interval 2683 (26820 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 2684 (26830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.467 - mean_q: 12.869 - prob: 1.000\n",
            "\n",
            "Interval 2685 (26840 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 2686 (26850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.266 - mean_q: 12.523 - prob: 1.000\n",
            "\n",
            "Interval 2687 (26860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.053 - mean_q: 12.486 - prob: 1.000\n",
            "\n",
            "Interval 2688 (26870 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 2689 (26880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.236 - mean_q: 12.621 - prob: 1.000\n",
            "\n",
            "Interval 2690 (26890 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.424 - mean_q: 12.643 - prob: 1.000\n",
            "\n",
            "Interval 2691 (26900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.179 - mean_q: 12.509 - prob: 1.000\n",
            "\n",
            "Interval 2692 (26910 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 2693 (26920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.240 - mean_q: 12.495 - prob: 1.000\n",
            "\n",
            "Interval 2694 (26930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.551 - mean_q: 13.030 - prob: 1.000\n",
            "\n",
            "Interval 2695 (26940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -21.000 [-21.000, -21.000] - loss: 0.001 - mae: 7.250 - mean_q: 12.488 - prob: 1.000\n",
            "\n",
            "Interval 2696 (26950 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 2697 (26960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2698 (26970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.001 - mae: 7.570 - mean_q: 13.047 - prob: 1.000\n",
            "\n",
            "Interval 2699 (26980 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.068 - mean_q: 12.358 - prob: 1.000\n",
            "\n",
            "Interval 2700 (26990 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.130 - mean_q: 12.335 - prob: 1.000\n",
            "\n",
            "Interval 2701 (27000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2702 (27010 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.595 - mean_q: 13.096 - prob: 1.000\n",
            "\n",
            "Interval 2703 (27020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 2704 (27030 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -28.000 [-28.000, -28.000] - loss: 0.001 - mae: 7.358 - mean_q: 12.740 - prob: 1.000\n",
            "\n",
            "Interval 2705 (27040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.368 - mean_q: 12.780 - prob: 1.000\n",
            "\n",
            "Interval 2706 (27050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2707 (27060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.327 - mean_q: 12.602 - prob: 1.000\n",
            "\n",
            "Interval 2708 (27070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.316 - mean_q: 12.643 - prob: 1.000\n",
            "\n",
            "Interval 2709 (27080 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 2710 (27090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.430 - mean_q: 12.724 - prob: 1.000\n",
            "\n",
            "Interval 2711 (27100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.359 - mean_q: 12.636 - prob: 1.000\n",
            "\n",
            "Interval 2712 (27110 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.475 - mean_q: 12.860 - prob: 1.000\n",
            "\n",
            "Interval 2713 (27120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2714 (27130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.439 - mean_q: 12.897 - prob: 1.000\n",
            "\n",
            "Interval 2715 (27140 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 2716 (27150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.293 - mean_q: 12.511 - prob: 1.000\n",
            "\n",
            "Interval 2717 (27160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.468 - mean_q: 12.879 - prob: 1.000\n",
            "\n",
            "Interval 2718 (27170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2719 (27180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.330 - mean_q: 12.612 - prob: 1.000\n",
            "\n",
            "Interval 2720 (27190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2721 (27200 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.280 - mean_q: 12.587 - prob: 1.000\n",
            "\n",
            "Interval 2722 (27210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.152 - mean_q: 12.415 - prob: 1.000\n",
            "\n",
            "Interval 2723 (27220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 2724 (27230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2725 (27240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -29.000 [-29.000, -29.000] - loss: 0.001 - mae: 7.377 - mean_q: 12.786 - prob: 1.000\n",
            "\n",
            "Interval 2726 (27250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.651 - mean_q: 13.112 - prob: 1.000\n",
            "\n",
            "Interval 2727 (27260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.471 - mean_q: 12.932 - prob: 1.000\n",
            "\n",
            "Interval 2728 (27270 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 2729 (27280 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.703 - mean_q: 13.122 - prob: 1.000\n",
            "\n",
            "Interval 2730 (27290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.083 - mean_q: 12.419 - prob: 1.000\n",
            "\n",
            "Interval 2731 (27300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2732 (27310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.464 - mean_q: 12.873 - prob: 1.000\n",
            "\n",
            "Interval 2733 (27320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.165 - mean_q: 12.532 - prob: 1.000\n",
            "\n",
            "Interval 2734 (27330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.349 - mean_q: 12.767 - prob: 1.000\n",
            "\n",
            "Interval 2735 (27340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2736 (27350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2737 (27360 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -21.000 [-21.000, -21.000] - loss: 0.001 - mae: 7.557 - mean_q: 13.092 - prob: 1.000\n",
            "\n",
            "Interval 2738 (27370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.337 - mean_q: 12.621 - prob: 1.000\n",
            "\n",
            "Interval 2739 (27380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2740 (27390 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.184 - mean_q: 12.402 - prob: 1.000\n",
            "\n",
            "Interval 2741 (27400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 2742 (27410 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: -4.500 [-21.000, 12.000] - loss: 0.001 - mae: 7.394 - mean_q: 12.843 - prob: 1.000\n",
            "\n",
            "Interval 2743 (27420 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 2744 (27430 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.027 - mean_q: 12.366 - prob: 1.000\n",
            "\n",
            "Interval 2745 (27440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2746 (27450 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.123 - mean_q: 12.307 - prob: 1.000\n",
            "\n",
            "Interval 2747 (27460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.399 - mean_q: 12.680 - prob: 1.000\n",
            "\n",
            "Interval 2748 (27470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2749 (27480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.201 - mean_q: 12.354 - prob: 1.000\n",
            "\n",
            "Interval 2750 (27490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.744 - mean_q: 13.186 - prob: 1.000\n",
            "\n",
            "Interval 2751 (27500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2752 (27510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.000 - mae: 7.373 - mean_q: 12.752 - prob: 1.000\n",
            "\n",
            "Interval 2753 (27520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.592 - mean_q: 13.085 - prob: 1.000\n",
            "\n",
            "Interval 2754 (27530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2755 (27540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.294 - mean_q: 12.568 - prob: 1.000\n",
            "\n",
            "Interval 2756 (27550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2757 (27560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.493 - mean_q: 12.936 - prob: 1.000\n",
            "\n",
            "Interval 2758 (27570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.098 - mean_q: 12.252 - prob: 1.000\n",
            "\n",
            "Interval 2759 (27580 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.503 - mean_q: 12.994 - prob: 1.000\n",
            "\n",
            "Interval 2760 (27590 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 2761 (27600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.067 - mean_q: 12.237 - prob: 1.000\n",
            "\n",
            "Interval 2762 (27610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.117 - mean_q: 12.390 - prob: 1.000\n",
            "\n",
            "Interval 2763 (27620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2764 (27630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.298 - mean_q: 12.538 - prob: 1.000\n",
            "\n",
            "Interval 2765 (27640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.291 - mean_q: 12.586 - prob: 1.000\n",
            "\n",
            "Interval 2766 (27650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.515 - mean_q: 12.937 - prob: 1.000\n",
            "\n",
            "Interval 2767 (27660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2768 (27670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.295 - mean_q: 12.609 - prob: 1.000\n",
            "\n",
            "Interval 2769 (27680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2770 (27690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.000 - mae: 7.317 - mean_q: 12.609 - prob: 1.000\n",
            "\n",
            "Interval 2771 (27700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.149 - mean_q: 12.427 - prob: 1.000\n",
            "\n",
            "Interval 2772 (27710 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 2773 (27720 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 2774 (27730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.414 - mean_q: 12.850 - prob: 1.000\n",
            "\n",
            "Interval 2775 (27740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.377 - mean_q: 12.614 - prob: 1.000\n",
            "\n",
            "Interval 2776 (27750 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 2777 (27760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.184 - mean_q: 12.438 - prob: 1.000\n",
            "\n",
            "Interval 2778 (27770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2779 (27780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.356 - mean_q: 12.671 - prob: 1.000\n",
            "\n",
            "Interval 2780 (27790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.089 - mean_q: 12.276 - prob: 1.000\n",
            "\n",
            "Interval 2781 (27800 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.767 - mean_q: 13.154 - prob: 1.000\n",
            "\n",
            "Interval 2782 (27810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2783 (27820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.575 - mean_q: 12.977 - prob: 1.000\n",
            "\n",
            "Interval 2784 (27830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.023 - mean_q: 12.254 - prob: 1.000\n",
            "\n",
            "Interval 2785 (27840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2786 (27850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.001 - mae: 7.425 - mean_q: 12.794 - prob: 1.000\n",
            "\n",
            "Interval 2787 (27860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.253 - mean_q: 12.605 - prob: 1.000\n",
            "\n",
            "Interval 2788 (27870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2789 (27880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.295 - mean_q: 12.646 - prob: 1.000\n",
            "\n",
            "Interval 2790 (27890 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 2791 (27900 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.315 - mean_q: 12.585 - prob: 1.000\n",
            "\n",
            "Interval 2792 (27910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.480 - mean_q: 12.923 - prob: 1.000\n",
            "\n",
            "Interval 2793 (27920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.493 - mean_q: 12.911 - prob: 1.000\n",
            "\n",
            "Interval 2794 (27930 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 2795 (27940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.422 - mean_q: 12.826 - prob: 1.000\n",
            "\n",
            "Interval 2796 (27950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.305 - mean_q: 12.810 - prob: 1.000\n",
            "\n",
            "Interval 2797 (27960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2798 (27970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.522 - mean_q: 12.856 - prob: 1.000\n",
            "\n",
            "Interval 2799 (27980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2800 (27990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.254 - mean_q: 12.573 - prob: 1.000\n",
            "\n",
            "Interval 2801 (28000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.690 - mean_q: 13.038 - prob: 1.000\n",
            "\n",
            "Interval 2802 (28010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.496 - mean_q: 12.909 - prob: 1.000\n",
            "\n",
            "Interval 2803 (28020 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.032 - mean_q: 12.262 - prob: 1.000\n",
            "\n",
            "Interval 2804 (28030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.269 - mean_q: 12.704 - prob: 1.000\n",
            "\n",
            "Interval 2805 (28040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2806 (28050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.282 - mean_q: 12.615 - prob: 1.000\n",
            "\n",
            "Interval 2807 (28060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.330 - mean_q: 12.800 - prob: 1.000\n",
            "\n",
            "Interval 2808 (28070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.705 - mean_q: 13.119 - prob: 1.000\n",
            "\n",
            "Interval 2809 (28080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.388 - mean_q: 12.802 - prob: 1.000\n",
            "\n",
            "Interval 2810 (28090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2811 (28100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.264 - mean_q: 12.447 - prob: 1.000\n",
            "\n",
            "Interval 2812 (28110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.103 - mean_q: 12.399 - prob: 1.000\n",
            "\n",
            "Interval 2813 (28120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.303 - mean_q: 12.572 - prob: 1.000\n",
            "\n",
            "Interval 2814 (28130 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.386 - mean_q: 12.724 - prob: 1.000\n",
            "\n",
            "Interval 2815 (28140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2816 (28150 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -26.000 [-26.000, -26.000] - loss: 0.001 - mae: 7.623 - mean_q: 12.978 - prob: 1.000\n",
            "\n",
            "Interval 2817 (28160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 2818 (28170 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.000 - mae: 7.195 - mean_q: 12.339 - prob: 1.000\n",
            "\n",
            "Interval 2819 (28180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2820 (28190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.000 - mae: 7.666 - mean_q: 13.140 - prob: 1.000\n",
            "\n",
            "Interval 2821 (28200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.336 - mean_q: 12.720 - prob: 1.000\n",
            "\n",
            "Interval 2822 (28210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2823 (28220 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.652 - mean_q: 13.143 - prob: 1.000\n",
            "\n",
            "Interval 2824 (28230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.449 - mean_q: 12.751 - prob: 1.000\n",
            "\n",
            "Interval 2825 (28240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2826 (28250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.614 - mean_q: 12.995 - prob: 1.000\n",
            "\n",
            "Interval 2827 (28260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.687 - mean_q: 13.049 - prob: 1.000\n",
            "\n",
            "Interval 2828 (28270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2829 (28280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.436 - mean_q: 12.835 - prob: 1.000\n",
            "\n",
            "Interval 2830 (28290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.643 - mean_q: 13.130 - prob: 1.000\n",
            "\n",
            "Interval 2831 (28300 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 2832 (28310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.309 - mean_q: 12.654 - prob: 1.000\n",
            "\n",
            "Interval 2833 (28320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.398 - mean_q: 12.754 - prob: 1.000\n",
            "\n",
            "Interval 2834 (28330 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 2835 (28340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.235 - mean_q: 12.587 - prob: 1.000\n",
            "\n",
            "Interval 2836 (28350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.130 - mean_q: 12.242 - prob: 1.000\n",
            "\n",
            "Interval 2837 (28360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2838 (28370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.255 - mean_q: 12.458 - prob: 1.000\n",
            "\n",
            "Interval 2839 (28380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.000 - mae: 7.391 - mean_q: 12.809 - prob: 1.000\n",
            "\n",
            "Interval 2840 (28390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2841 (28400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.566 - mean_q: 12.988 - prob: 1.000\n",
            "\n",
            "Interval 2842 (28410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.001 - mae: 7.434 - mean_q: 12.748 - prob: 1.000\n",
            "\n",
            "Interval 2843 (28420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2844 (28430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.000 - mae: 7.551 - mean_q: 13.122 - prob: 1.000\n",
            "\n",
            "Interval 2845 (28440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2846 (28450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.319 - mean_q: 12.557 - prob: 1.000\n",
            "\n",
            "Interval 2847 (28460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.515 - mean_q: 12.910 - prob: 1.000\n",
            "\n",
            "Interval 2848 (28470 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 2849 (28480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.172 - mean_q: 12.588 - prob: 1.000\n",
            "\n",
            "Interval 2850 (28490 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.391 - mean_q: 12.858 - prob: 1.000\n",
            "\n",
            "Interval 2851 (28500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2852 (28510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.001 - mae: 7.380 - mean_q: 12.828 - prob: 1.000\n",
            "\n",
            "Interval 2853 (28520 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.335 - mean_q: 12.597 - prob: 1.000\n",
            "\n",
            "Interval 2854 (28530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.142 - mean_q: 12.341 - prob: 1.000\n",
            "\n",
            "Interval 2855 (28540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2856 (28550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.000 - mae: 7.373 - mean_q: 12.691 - prob: 1.000\n",
            "\n",
            "Interval 2857 (28560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2858 (28570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.289 - mean_q: 12.684 - prob: 1.000\n",
            "\n",
            "Interval 2859 (28580 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.311 - mean_q: 12.665 - prob: 1.000\n",
            "\n",
            "Interval 2860 (28590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2861 (28600 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.677 - mean_q: 13.191 - prob: 1.000\n",
            "\n",
            "Interval 2862 (28610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2863 (28620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.000 - mae: 7.267 - mean_q: 12.550 - prob: 1.000\n",
            "\n",
            "Interval 2864 (28630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.154 - mean_q: 12.441 - prob: 1.000\n",
            "\n",
            "Interval 2865 (28640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.505 - mean_q: 12.932 - prob: 1.000\n",
            "\n",
            "Interval 2866 (28650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.368 - mean_q: 12.767 - prob: 1.000\n",
            "\n",
            "Interval 2867 (28660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2868 (28670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.331 - mean_q: 12.639 - prob: 1.000\n",
            "\n",
            "Interval 2869 (28680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.335 - mean_q: 12.737 - prob: 1.000\n",
            "\n",
            "Interval 2870 (28690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2871 (28700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.383 - mean_q: 12.715 - prob: 1.000\n",
            "\n",
            "Interval 2872 (28710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.760 - mean_q: 13.154 - prob: 1.000\n",
            "\n",
            "Interval 2873 (28720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.338 - mean_q: 12.718 - prob: 1.000\n",
            "\n",
            "Interval 2874 (28730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.264 - mean_q: 12.610 - prob: 1.000\n",
            "\n",
            "Interval 2875 (28740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2876 (28750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.278 - mean_q: 12.647 - prob: 1.000\n",
            "\n",
            "Interval 2877 (28760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.178 - mean_q: 12.489 - prob: 1.000\n",
            "\n",
            "Interval 2878 (28770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.376 - mean_q: 12.729 - prob: 1.000\n",
            "\n",
            "Interval 2879 (28780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.382 - mean_q: 12.682 - prob: 1.000\n",
            "\n",
            "Interval 2880 (28790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2881 (28800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.261 - mean_q: 12.529 - prob: 1.000\n",
            "\n",
            "Interval 2882 (28810 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.242 - mean_q: 12.501 - prob: 1.000\n",
            "\n",
            "Interval 2883 (28820 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 2884 (28830 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.404 - mean_q: 12.715 - prob: 1.000\n",
            "\n",
            "Interval 2885 (28840 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.384 - mean_q: 12.759 - prob: 1.000\n",
            "\n",
            "Interval 2886 (28850 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 2887 (28860 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.387 - mean_q: 12.743 - prob: 1.000\n",
            "\n",
            "Interval 2888 (28870 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.435 - mean_q: 13.003 - prob: 1.000\n",
            "\n",
            "Interval 2889 (28880 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 2890 (28890 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.437 - mean_q: 12.725 - prob: 1.000\n",
            "\n",
            "Interval 2891 (28900 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.209 - mean_q: 12.456 - prob: 1.000\n",
            "\n",
            "Interval 2892 (28910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2893 (28920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.001 - mae: 7.320 - mean_q: 12.610 - prob: 1.000\n",
            "\n",
            "Interval 2894 (28930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.410 - mean_q: 12.817 - prob: 1.000\n",
            "\n",
            "Interval 2895 (28940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2896 (28950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.207 - mean_q: 12.437 - prob: 1.000\n",
            "\n",
            "Interval 2897 (28960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.283 - mean_q: 12.610 - prob: 1.000\n",
            "\n",
            "Interval 2898 (28970 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 2899 (28980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -18.000 [-18.000, -18.000] - loss: 0.001 - mae: 7.298 - mean_q: 12.760 - prob: 1.000\n",
            "\n",
            "Interval 2900 (28990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2901 (29000 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.174 - mean_q: 12.471 - prob: 1.000\n",
            "\n",
            "Interval 2902 (29010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.561 - mean_q: 12.898 - prob: 1.000\n",
            "\n",
            "Interval 2903 (29020 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.333 - mean_q: 12.820 - prob: 1.000\n",
            "\n",
            "Interval 2904 (29030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2905 (29040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.407 - mean_q: 12.695 - prob: 1.000\n",
            "\n",
            "Interval 2906 (29050 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 2907 (29060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.447 - mean_q: 12.775 - prob: 1.000\n",
            "\n",
            "Interval 2908 (29070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.098 - mean_q: 12.304 - prob: 1.000\n",
            "\n",
            "Interval 2909 (29080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2910 (29090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.492 - mean_q: 12.851 - prob: 1.000\n",
            "\n",
            "Interval 2911 (29100 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 2912 (29110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.184 - mean_q: 12.429 - prob: 1.000\n",
            "\n",
            "Interval 2913 (29120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.270 - mean_q: 12.600 - prob: 1.000\n",
            "\n",
            "Interval 2914 (29130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2915 (29140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.250 - mean_q: 12.531 - prob: 1.000\n",
            "\n",
            "Interval 2916 (29150 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.475 - mean_q: 12.898 - prob: 1.000\n",
            "\n",
            "Interval 2917 (29160 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.175 - mean_q: 12.478 - prob: 1.000\n",
            "\n",
            "Interval 2918 (29170 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.001 - mae: 7.393 - mean_q: 12.661 - prob: 1.000\n",
            "\n",
            "Interval 2919 (29180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.001 - mae: 7.379 - mean_q: 12.875 - prob: 1.000\n",
            "\n",
            "Interval 2920 (29190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 2921 (29200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -24.000 [-24.000, -24.000] - loss: 0.001 - mae: 7.260 - mean_q: 12.418 - prob: 1.000\n",
            "\n",
            "Interval 2922 (29210 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 2923 (29220 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.354 - mean_q: 12.551 - prob: 1.000\n",
            "\n",
            "Interval 2924 (29230 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.218 - mean_q: 12.510 - prob: 1.000\n",
            "\n",
            "Interval 2925 (29240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2926 (29250 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.395 - mean_q: 12.954 - prob: 1.000\n",
            "\n",
            "Interval 2927 (29260 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.308 - mean_q: 12.761 - prob: 1.000\n",
            "\n",
            "Interval 2928 (29270 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.381 - mean_q: 12.612 - prob: 1.000\n",
            "\n",
            "Interval 2929 (29280 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.347 - mean_q: 12.697 - prob: 1.000\n",
            "\n",
            "Interval 2930 (29290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2931 (29300 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.525 - mean_q: 12.889 - prob: 1.000\n",
            "\n",
            "Interval 2932 (29310 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.666 - mean_q: 13.083 - prob: 1.000\n",
            "\n",
            "Interval 2933 (29320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 2934 (29330 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.255 - mean_q: 12.500 - prob: 1.000\n",
            "\n",
            "Interval 2935 (29340 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.402 - mean_q: 12.791 - prob: 1.000\n",
            "\n",
            "Interval 2936 (29350 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.277 - mean_q: 12.555 - prob: 1.000\n",
            "\n",
            "Interval 2937 (29360 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.187 - mean_q: 12.535 - prob: 1.000\n",
            "\n",
            "Interval 2938 (29370 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 2939 (29380 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.314 - mean_q: 12.650 - prob: 1.000\n",
            "\n",
            "Interval 2940 (29390 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.657 - mean_q: 13.156 - prob: 1.000\n",
            "\n",
            "Interval 2941 (29400 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.462 - mean_q: 12.841 - prob: 1.000\n",
            "\n",
            "Interval 2942 (29410 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 2943 (29420 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.188 - mean_q: 12.475 - prob: 1.000\n",
            "\n",
            "Interval 2944 (29430 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.353 - mean_q: 12.780 - prob: 1.000\n",
            "\n",
            "Interval 2945 (29440 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.327 - mean_q: 12.699 - prob: 1.000\n",
            "\n",
            "Interval 2946 (29450 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.200 - mean_q: 12.507 - prob: 1.000\n",
            "\n",
            "Interval 2947 (29460 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.229 - mean_q: 12.524 - prob: 1.000\n",
            "\n",
            "Interval 2948 (29470 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.454 - mean_q: 12.786 - prob: 1.000\n",
            "\n",
            "Interval 2949 (29480 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -2.8000\n",
            "Interval 2950 (29490 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.001 - mae: 7.589 - mean_q: 13.102 - prob: 1.000\n",
            "\n",
            "Interval 2951 (29500 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 2952 (29510 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.001 - mae: 7.374 - mean_q: 12.705 - prob: 1.000\n",
            "\n",
            "Interval 2953 (29520 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.001 - mae: 7.146 - mean_q: 12.476 - prob: 1.000\n",
            "\n",
            "Interval 2954 (29530 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.277 - mean_q: 12.531 - prob: 1.000\n",
            "\n",
            "Interval 2955 (29540 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -2.8000\n",
            "Interval 2956 (29550 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.001 - mae: 7.366 - mean_q: 12.768 - prob: 1.000\n",
            "\n",
            "Interval 2957 (29560 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 2958 (29570 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.001 - mae: 7.397 - mean_q: 12.721 - prob: 1.000\n",
            "\n",
            "Interval 2959 (29580 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 2960 (29590 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.396 - mean_q: 12.854 - prob: 1.000\n",
            "\n",
            "Interval 2961 (29600 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.595 - mean_q: 12.980 - prob: 1.000\n",
            "\n",
            "Interval 2962 (29610 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.325 - mean_q: 12.683 - prob: 1.000\n",
            "\n",
            "Interval 2963 (29620 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 2964 (29630 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.389 - mean_q: 12.862 - prob: 1.000\n",
            "\n",
            "Interval 2965 (29640 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.411 - mean_q: 12.810 - prob: 1.000\n",
            "\n",
            "Interval 2966 (29650 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 2967 (29660 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.6000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.302 - mean_q: 12.410 - prob: 1.000\n",
            "\n",
            "Interval 2968 (29670 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.000 - mae: 7.348 - mean_q: 12.752 - prob: 1.000\n",
            "\n",
            "Interval 2969 (29680 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.9000\n",
            "Interval 2970 (29690 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 2971 (29700 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.001 - mae: 7.723 - mean_q: 13.179 - prob: 1.000\n",
            "\n",
            "Interval 2972 (29710 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.223 - mean_q: 12.495 - prob: 1.000\n",
            "\n",
            "Interval 2973 (29720 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.001 - mae: 7.462 - mean_q: 12.787 - prob: 1.000\n",
            "\n",
            "Interval 2974 (29730 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.098 - mean_q: 12.264 - prob: 1.000\n",
            "\n",
            "Interval 2975 (29740 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.313 - mean_q: 12.638 - prob: 1.000\n",
            "\n",
            "Interval 2976 (29750 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.227 - mean_q: 12.460 - prob: 1.000\n",
            "\n",
            "Interval 2977 (29760 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.000 - mae: 7.416 - mean_q: 12.856 - prob: 1.000\n",
            "\n",
            "Interval 2978 (29770 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 2979 (29780 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.105 - mean_q: 12.431 - prob: 1.000\n",
            "\n",
            "Interval 2980 (29790 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.539 - mean_q: 13.039 - prob: 1.000\n",
            "\n",
            "Interval 2981 (29800 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.540 - mean_q: 12.869 - prob: 1.000\n",
            "\n",
            "Interval 2982 (29810 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 2983 (29820 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 5.000 [-3.000, 13.000] - loss: 0.001 - mae: 7.657 - mean_q: 13.079 - prob: 1.000\n",
            "\n",
            "Interval 2984 (29830 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.001 - mae: 7.363 - mean_q: 12.810 - prob: 1.000\n",
            "\n",
            "Interval 2985 (29840 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 2986 (29850 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.483 - mean_q: 12.831 - prob: 1.000\n",
            "\n",
            "Interval 2987 (29860 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.505 - mean_q: 12.834 - prob: 1.000\n",
            "\n",
            "Interval 2988 (29870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2989 (29880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.780 - mean_q: 13.318 - prob: 1.000\n",
            "\n",
            "Interval 2990 (29890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2991 (29900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 6.744 - mean_q: 11.842 - prob: 1.000\n",
            "\n",
            "Interval 2992 (29910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 2993 (29920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.296 - mean_q: 12.542 - prob: 1.000\n",
            "\n",
            "Interval 2994 (29930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.479 - mean_q: 12.747 - prob: 1.000\n",
            "\n",
            "Interval 2995 (29940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.637 - mean_q: 13.056 - prob: 1.000\n",
            "\n",
            "Interval 2996 (29950 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 2997 (29960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.434 - mean_q: 12.914 - prob: 1.000\n",
            "\n",
            "Interval 2998 (29970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.459 - mean_q: 12.793 - prob: 1.000\n",
            "\n",
            "Interval 2999 (29980 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 3000 (29990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.492 - mean_q: 12.899 - prob: 1.000\n",
            "\n",
            "Interval 3001 (30000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.293 - mean_q: 12.579 - prob: 1.000\n",
            "\n",
            "Interval 3002 (30010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.255 - mean_q: 12.495 - prob: 1.000\n",
            "\n",
            "Interval 3003 (30020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3004 (30030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.199 - mean_q: 12.500 - prob: 1.000\n",
            "\n",
            "Interval 3005 (30040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3006 (30050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.253 - mean_q: 12.462 - prob: 1.000\n",
            "\n",
            "Interval 3007 (30060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3008 (30070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.000 - mae: 7.610 - mean_q: 12.933 - prob: 1.000\n",
            "\n",
            "Interval 3009 (30080 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 3010 (30090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.588 - mean_q: 12.923 - prob: 1.000\n",
            "\n",
            "Interval 3011 (30100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.374 - mean_q: 12.635 - prob: 1.000\n",
            "\n",
            "Interval 3012 (30110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3013 (30120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -47.000 [-47.000, -47.000] - loss: 0.001 - mae: 7.653 - mean_q: 13.055 - prob: 1.000\n",
            "\n",
            "Interval 3014 (30130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3015 (30140 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.223 - mean_q: 12.652 - prob: 1.000\n",
            "\n",
            "Interval 3016 (30150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.327 - mean_q: 12.620 - prob: 1.000\n",
            "\n",
            "Interval 3017 (30160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3018 (30170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.437 - mean_q: 12.888 - prob: 1.000\n",
            "\n",
            "Interval 3019 (30180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.236 - mean_q: 12.563 - prob: 1.000\n",
            "\n",
            "Interval 3020 (30190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3021 (30200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.463 - mean_q: 12.925 - prob: 1.000\n",
            "\n",
            "Interval 3022 (30210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3023 (30220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.553 - mean_q: 12.850 - prob: 1.000\n",
            "\n",
            "Interval 3024 (30230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.001 - mae: 6.953 - mean_q: 12.167 - prob: 1.000\n",
            "\n",
            "Interval 3025 (30240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.421 - mean_q: 12.783 - prob: 1.000\n",
            "\n",
            "Interval 3026 (30250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3027 (30260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.376 - mean_q: 12.762 - prob: 1.000\n",
            "\n",
            "Interval 3028 (30270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3029 (30280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.668 - mean_q: 13.038 - prob: 1.000\n",
            "\n",
            "Interval 3030 (30290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.219 - mean_q: 12.547 - prob: 1.000\n",
            "\n",
            "Interval 3031 (30300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3032 (30310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.143 - mean_q: 12.419 - prob: 1.000\n",
            "\n",
            "Interval 3033 (30320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.291 - mean_q: 12.574 - prob: 1.000\n",
            "\n",
            "Interval 3034 (30330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3035 (30340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.001 - mae: 7.234 - mean_q: 12.491 - prob: 1.000\n",
            "\n",
            "Interval 3036 (30350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.376 - mean_q: 12.715 - prob: 1.000\n",
            "\n",
            "Interval 3037 (30360 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.424 - mean_q: 12.820 - prob: 1.000\n",
            "\n",
            "Interval 3038 (30370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3039 (30380 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 2.3000\n",
            "2 episodes - episode_reward: 7.500 [5.000, 10.000] - loss: 0.001 - mae: 7.612 - mean_q: 12.982 - prob: 1.000\n",
            "\n",
            "Interval 3040 (30390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3041 (30400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.378 - mean_q: 12.663 - prob: 1.000\n",
            "\n",
            "Interval 3042 (30410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.258 - mean_q: 12.541 - prob: 1.000\n",
            "\n",
            "Interval 3043 (30420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.199 - mean_q: 12.564 - prob: 1.000\n",
            "\n",
            "Interval 3044 (30430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.001 - mae: 7.678 - mean_q: 13.046 - prob: 1.000\n",
            "\n",
            "Interval 3045 (30440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3046 (30450 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.647 - mean_q: 12.940 - prob: 1.000\n",
            "\n",
            "Interval 3047 (30460 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.061 - mean_q: 12.161 - prob: 1.000\n",
            "\n",
            "Interval 3048 (30470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.277 - mean_q: 12.520 - prob: 1.000\n",
            "\n",
            "Interval 3049 (30480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.354 - mean_q: 12.569 - prob: 1.000\n",
            "\n",
            "Interval 3050 (30490 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 3051 (30500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.444 - mean_q: 12.608 - prob: 1.000\n",
            "\n",
            "Interval 3052 (30510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.315 - mean_q: 12.778 - prob: 1.000\n",
            "\n",
            "Interval 3053 (30520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3054 (30530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.432 - mean_q: 12.882 - prob: 1.000\n",
            "\n",
            "Interval 3055 (30540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3056 (30550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.557 - mean_q: 13.013 - prob: 1.000\n",
            "\n",
            "Interval 3057 (30560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 3058 (30570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.001 - mae: 7.501 - mean_q: 12.921 - prob: 1.000\n",
            "\n",
            "Interval 3059 (30580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.659 - mean_q: 12.989 - prob: 1.000\n",
            "\n",
            "Interval 3060 (30590 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 3061 (30600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -19.000 [-19.000, -19.000] - loss: 0.001 - mae: 7.505 - mean_q: 12.933 - prob: 1.000\n",
            "\n",
            "Interval 3062 (30610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3063 (30620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.179 - mean_q: 12.494 - prob: 1.000\n",
            "\n",
            "Interval 3064 (30630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.702 - mean_q: 13.204 - prob: 1.000\n",
            "\n",
            "Interval 3065 (30640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.572 - mean_q: 13.136 - prob: 1.000\n",
            "\n",
            "Interval 3066 (30650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.212 - mean_q: 12.433 - prob: 1.000\n",
            "\n",
            "Interval 3067 (30660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3068 (30670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.274 - mean_q: 12.456 - prob: 1.000\n",
            "\n",
            "Interval 3069 (30680 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.613 - mean_q: 13.050 - prob: 1.000\n",
            "\n",
            "Interval 3070 (30690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3071 (30700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.508 - mean_q: 12.994 - prob: 1.000\n",
            "\n",
            "Interval 3072 (30710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3073 (30720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.368 - mean_q: 12.802 - prob: 1.000\n",
            "\n",
            "Interval 3074 (30730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.479 - mean_q: 12.940 - prob: 1.000\n",
            "\n",
            "Interval 3075 (30740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.316 - mean_q: 12.638 - prob: 1.000\n",
            "\n",
            "Interval 3076 (30750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3077 (30760 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 6.925 - mean_q: 12.183 - prob: 1.000\n",
            "\n",
            "Interval 3078 (30770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.335 - mean_q: 12.502 - prob: 1.000\n",
            "\n",
            "Interval 3079 (30780 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.426 - mean_q: 12.870 - prob: 1.000\n",
            "\n",
            "Interval 3080 (30790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3081 (30800 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.000 - mae: 7.391 - mean_q: 12.788 - prob: 1.000\n",
            "\n",
            "Interval 3082 (30810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3083 (30820 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.445 - mean_q: 12.943 - prob: 1.000\n",
            "\n",
            "Interval 3084 (30830 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.115 - mean_q: 12.492 - prob: 1.000\n",
            "\n",
            "Interval 3085 (30840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3086 (30850 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.001 - mae: 7.235 - mean_q: 12.522 - prob: 1.000\n",
            "\n",
            "Interval 3087 (30860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3088 (30870 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.140 - mean_q: 12.404 - prob: 1.000\n",
            "\n",
            "Interval 3089 (30880 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.523 - mean_q: 13.085 - prob: 1.000\n",
            "\n",
            "Interval 3090 (30890 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 3091 (30900 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.110 - mean_q: 12.321 - prob: 1.000\n",
            "\n",
            "Interval 3092 (30910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3093 (30920 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.311 - mean_q: 12.624 - prob: 1.000\n",
            "\n",
            "Interval 3094 (30930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.283 - mean_q: 12.581 - prob: 1.000\n",
            "\n",
            "Interval 3095 (30940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3096 (30950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.523 - mean_q: 13.036 - prob: 1.000\n",
            "\n",
            "Interval 3097 (30960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.001 - mae: 7.186 - mean_q: 12.463 - prob: 1.000\n",
            "\n",
            "Interval 3098 (30970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.467 - mean_q: 12.792 - prob: 1.000\n",
            "\n",
            "Interval 3099 (30980 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.417 - mean_q: 12.711 - prob: 1.000\n",
            "\n",
            "Interval 3100 (30990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3101 (31000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -18.000 [-18.000, -18.000] - loss: 0.001 - mae: 7.392 - mean_q: 12.778 - prob: 1.000\n",
            "\n",
            "Interval 3102 (31010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3103 (31020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.559 - mean_q: 13.076 - prob: 1.000\n",
            "\n",
            "Interval 3104 (31030 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.460 - mean_q: 12.892 - prob: 1.000\n",
            "\n",
            "Interval 3105 (31040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.086 - mean_q: 12.365 - prob: 1.000\n",
            "\n",
            "Interval 3106 (31050 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.369 - mean_q: 12.811 - prob: 1.000\n",
            "\n",
            "Interval 3107 (31060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3108 (31070 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.593 - mean_q: 13.055 - prob: 1.000\n",
            "\n",
            "Interval 3109 (31080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3110 (31090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.615 - mean_q: 13.126 - prob: 1.000\n",
            "\n",
            "Interval 3111 (31100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.517 - mean_q: 12.905 - prob: 1.000\n",
            "\n",
            "Interval 3112 (31110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3113 (31120 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.356 - mean_q: 12.761 - prob: 1.000\n",
            "\n",
            "Interval 3114 (31130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.787 - mean_q: 13.237 - prob: 1.000\n",
            "\n",
            "Interval 3115 (31140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.252 - mean_q: 12.672 - prob: 1.000\n",
            "\n",
            "Interval 3116 (31150 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.441 - mean_q: 12.750 - prob: 1.000\n",
            "\n",
            "Interval 3117 (31160 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 3118 (31170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.535 - mean_q: 12.810 - prob: 1.000\n",
            "\n",
            "Interval 3119 (31180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.032 - mean_q: 12.266 - prob: 1.000\n",
            "\n",
            "Interval 3120 (31190 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.131 - mean_q: 12.438 - prob: 1.000\n",
            "\n",
            "Interval 3121 (31200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3122 (31210 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.561 - mean_q: 13.062 - prob: 1.000\n",
            "\n",
            "Interval 3123 (31220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.247 - mean_q: 12.625 - prob: 1.000\n",
            "\n",
            "Interval 3124 (31230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3125 (31240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.360 - mean_q: 12.744 - prob: 1.000\n",
            "\n",
            "Interval 3126 (31250 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 3127 (31260 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.466 - mean_q: 12.870 - prob: 1.000\n",
            "\n",
            "Interval 3128 (31270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.284 - mean_q: 12.581 - prob: 1.000\n",
            "\n",
            "Interval 3129 (31280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3130 (31290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.001 - mae: 7.492 - mean_q: 13.042 - prob: 1.000\n",
            "\n",
            "Interval 3131 (31300 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.530 - mean_q: 12.890 - prob: 1.000\n",
            "\n",
            "Interval 3132 (31310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3133 (31320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.339 - mean_q: 12.556 - prob: 1.000\n",
            "\n",
            "Interval 3134 (31330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.476 - mean_q: 12.885 - prob: 1.000\n",
            "\n",
            "Interval 3135 (31340 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 3136 (31350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.677 - mean_q: 13.189 - prob: 1.000\n",
            "\n",
            "Interval 3137 (31360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.692 - mean_q: 13.088 - prob: 1.000\n",
            "\n",
            "Interval 3138 (31370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3139 (31380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.456 - mean_q: 12.950 - prob: 1.000\n",
            "\n",
            "Interval 3140 (31390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.404 - mean_q: 12.826 - prob: 1.000\n",
            "\n",
            "Interval 3141 (31400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3142 (31410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.521 - mean_q: 12.911 - prob: 1.000\n",
            "\n",
            "Interval 3143 (31420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.188 - mean_q: 12.443 - prob: 1.000\n",
            "\n",
            "Interval 3144 (31430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3145 (31440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.206 - mean_q: 12.471 - prob: 1.000\n",
            "\n",
            "Interval 3146 (31450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3147 (31460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.660 - mean_q: 13.198 - prob: 1.000\n",
            "\n",
            "Interval 3148 (31470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.001 - mae: 7.084 - mean_q: 12.438 - prob: 1.000\n",
            "\n",
            "Interval 3149 (31480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3150 (31490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.271 - mean_q: 12.654 - prob: 1.000\n",
            "\n",
            "Interval 3151 (31500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.378 - mean_q: 12.718 - prob: 1.000\n",
            "\n",
            "Interval 3152 (31510 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 3153 (31520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.399 - mean_q: 12.819 - prob: 1.000\n",
            "\n",
            "Interval 3154 (31530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.680 - mean_q: 13.133 - prob: 1.000\n",
            "\n",
            "Interval 3155 (31540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.323 - mean_q: 12.834 - prob: 1.000\n",
            "\n",
            "Interval 3156 (31550 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 3157 (31560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.586 - mean_q: 13.013 - prob: 1.000\n",
            "\n",
            "Interval 3158 (31570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.567 - mean_q: 12.971 - prob: 1.000\n",
            "\n",
            "Interval 3159 (31580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3160 (31590 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 3.000 [-6.000, 12.000] - loss: 0.001 - mae: 7.466 - mean_q: 12.887 - prob: 1.000\n",
            "\n",
            "Interval 3161 (31600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3162 (31610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.613 - mean_q: 13.013 - prob: 1.000\n",
            "\n",
            "Interval 3163 (31620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.266 - mean_q: 12.583 - prob: 1.000\n",
            "\n",
            "Interval 3164 (31630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3165 (31640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.342 - mean_q: 12.645 - prob: 1.000\n",
            "\n",
            "Interval 3166 (31650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.249 - mean_q: 12.408 - prob: 1.000\n",
            "\n",
            "Interval 3167 (31660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.297 - mean_q: 12.496 - prob: 1.000\n",
            "\n",
            "Interval 3168 (31670 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 3169 (31680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.048 - mean_q: 12.157 - prob: 1.000\n",
            "\n",
            "Interval 3170 (31690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.151 - mean_q: 12.397 - prob: 1.000\n",
            "\n",
            "Interval 3171 (31700 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.616 - mean_q: 13.068 - prob: 1.000\n",
            "\n",
            "Interval 3172 (31710 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.330 - mean_q: 12.712 - prob: 1.000\n",
            "\n",
            "Interval 3173 (31720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3174 (31730 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 6.961 - mean_q: 12.116 - prob: 1.000\n",
            "\n",
            "Interval 3175 (31740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.226 - mean_q: 12.474 - prob: 1.000\n",
            "\n",
            "Interval 3176 (31750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.000 - mae: 7.473 - mean_q: 12.787 - prob: 1.000\n",
            "\n",
            "Interval 3177 (31760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.065 - mean_q: 12.396 - prob: 1.000\n",
            "\n",
            "Interval 3178 (31770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.605 - mean_q: 13.075 - prob: 1.000\n",
            "\n",
            "Interval 3179 (31780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3180 (31790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.455 - mean_q: 12.676 - prob: 1.000\n",
            "\n",
            "Interval 3181 (31800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.510 - mean_q: 12.981 - prob: 1.000\n",
            "\n",
            "Interval 3182 (31810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3183 (31820 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.001 - mae: 6.784 - mean_q: 11.811 - prob: 1.000\n",
            "\n",
            "Interval 3184 (31830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3185 (31840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.221 - mean_q: 12.554 - prob: 1.000\n",
            "\n",
            "Interval 3186 (31850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3187 (31860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -28.000 [-28.000, -28.000] - loss: 0.000 - mae: 7.438 - mean_q: 12.839 - prob: 1.000\n",
            "\n",
            "Interval 3188 (31870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3189 (31880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.204 - mean_q: 12.486 - prob: 1.000\n",
            "\n",
            "Interval 3190 (31890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.514 - mean_q: 12.863 - prob: 1.000\n",
            "\n",
            "Interval 3191 (31900 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.385 - mean_q: 12.712 - prob: 1.000\n",
            "\n",
            "Interval 3192 (31910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.121 - mean_q: 12.368 - prob: 1.000\n",
            "\n",
            "Interval 3193 (31920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 3194 (31930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.000 - mae: 7.263 - mean_q: 12.558 - prob: 1.000\n",
            "\n",
            "Interval 3195 (31940 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 3196 (31950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -18.000 [-18.000, -18.000] - loss: 0.001 - mae: 7.402 - mean_q: 12.692 - prob: 1.000\n",
            "\n",
            "Interval 3197 (31960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.574 - mean_q: 13.040 - prob: 1.000\n",
            "\n",
            "Interval 3198 (31970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3199 (31980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.155 - mean_q: 12.406 - prob: 1.000\n",
            "\n",
            "Interval 3200 (31990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 6.958 - mean_q: 12.064 - prob: 1.000\n",
            "\n",
            "Interval 3201 (32000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 6.744 - mean_q: 11.926 - prob: 1.000\n",
            "\n",
            "Interval 3202 (32010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.402 - mean_q: 12.785 - prob: 1.000\n",
            "\n",
            "Interval 3203 (32020 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 3204 (32030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.409 - mean_q: 12.823 - prob: 1.000\n",
            "\n",
            "Interval 3205 (32040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3206 (32050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.398 - mean_q: 12.807 - prob: 1.000\n",
            "\n",
            "Interval 3207 (32060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3208 (32070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.169 - mean_q: 12.422 - prob: 1.000\n",
            "\n",
            "Interval 3209 (32080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3210 (32090 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 4.500 [-4.000, 13.000] - loss: 0.000 - mae: 7.046 - mean_q: 12.127 - prob: 1.000\n",
            "\n",
            "Interval 3211 (32100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3212 (32110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.484 - mean_q: 12.916 - prob: 1.000\n",
            "\n",
            "Interval 3213 (32120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3214 (32130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.181 - mean_q: 12.516 - prob: 1.000\n",
            "\n",
            "Interval 3215 (32140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.235 - mean_q: 12.604 - prob: 1.000\n",
            "\n",
            "Interval 3216 (32150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3217 (32160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.238 - mean_q: 12.400 - prob: 1.000\n",
            "\n",
            "Interval 3218 (32170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.133 - mean_q: 12.426 - prob: 1.000\n",
            "\n",
            "Interval 3219 (32180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3220 (32190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.532 - mean_q: 13.000 - prob: 1.000\n",
            "\n",
            "Interval 3221 (32200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.311 - mean_q: 12.633 - prob: 1.000\n",
            "\n",
            "Interval 3222 (32210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.434 - mean_q: 12.780 - prob: 1.000\n",
            "\n",
            "Interval 3223 (32220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.638 - mean_q: 12.962 - prob: 1.000\n",
            "\n",
            "Interval 3224 (32230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.089 - mean_q: 12.403 - prob: 1.000\n",
            "\n",
            "Interval 3225 (32240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3226 (32250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3227 (32260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.205 - mean_q: 12.610 - prob: 1.000\n",
            "\n",
            "Interval 3228 (32270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.415 - mean_q: 12.679 - prob: 1.000\n",
            "\n",
            "Interval 3229 (32280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.279 - mean_q: 12.572 - prob: 1.000\n",
            "\n",
            "Interval 3230 (32290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.001 - mae: 7.122 - mean_q: 12.412 - prob: 1.000\n",
            "\n",
            "Interval 3231 (32300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 6.783 - mean_q: 11.846 - prob: 1.000\n",
            "\n",
            "Interval 3232 (32310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.395 - mean_q: 12.845 - prob: 1.000\n",
            "\n",
            "Interval 3233 (32320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3234 (32330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.377 - mean_q: 12.631 - prob: 1.000\n",
            "\n",
            "Interval 3235 (32340 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.536 - mean_q: 12.990 - prob: 1.000\n",
            "\n",
            "Interval 3236 (32350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3237 (32360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.616 - mean_q: 13.043 - prob: 1.000\n",
            "\n",
            "Interval 3238 (32370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3239 (32380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.001 - mae: 7.463 - mean_q: 12.893 - prob: 1.000\n",
            "\n",
            "Interval 3240 (32390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 6.980 - mean_q: 12.174 - prob: 1.000\n",
            "\n",
            "Interval 3241 (32400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.623 - mean_q: 13.028 - prob: 1.000\n",
            "\n",
            "Interval 3242 (32410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 3243 (32420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.000 - mae: 7.282 - mean_q: 12.576 - prob: 1.000\n",
            "\n",
            "Interval 3244 (32430 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.353 - mean_q: 12.864 - prob: 1.000\n",
            "\n",
            "Interval 3245 (32440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -20.000 [-20.000, -20.000] - loss: 0.001 - mae: 7.319 - mean_q: 12.691 - prob: 1.000\n",
            "\n",
            "Interval 3246 (32450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3247 (32460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.679 - mean_q: 13.101 - prob: 1.000\n",
            "\n",
            "Interval 3248 (32470 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 3249 (32480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.448 - mean_q: 12.831 - prob: 1.000\n",
            "\n",
            "Interval 3250 (32490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 6.920 - mean_q: 12.168 - prob: 1.000\n",
            "\n",
            "Interval 3251 (32500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3252 (32510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.001 - mae: 7.531 - mean_q: 13.011 - prob: 1.000\n",
            "\n",
            "Interval 3253 (32520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.839 - mean_q: 13.395 - prob: 1.000\n",
            "\n",
            "Interval 3254 (32530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.094 - mean_q: 12.243 - prob: 1.000\n",
            "\n",
            "Interval 3255 (32540 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 3256 (32550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.000 - mae: 6.966 - mean_q: 12.116 - prob: 1.000\n",
            "\n",
            "Interval 3257 (32560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.001 - mae: 7.606 - mean_q: 13.071 - prob: 1.000\n",
            "\n",
            "Interval 3258 (32570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.408 - mean_q: 12.741 - prob: 1.000\n",
            "\n",
            "Interval 3259 (32580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3260 (32590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.337 - mean_q: 12.740 - prob: 1.000\n",
            "\n",
            "Interval 3261 (32600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.116 - mean_q: 12.295 - prob: 1.000\n",
            "\n",
            "Interval 3262 (32610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.206 - mean_q: 12.454 - prob: 1.000\n",
            "\n",
            "Interval 3263 (32620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3264 (32630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.168 - mean_q: 12.469 - prob: 1.000\n",
            "\n",
            "Interval 3265 (32640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.473 - mean_q: 12.875 - prob: 1.000\n",
            "\n",
            "Interval 3266 (32650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.357 - mean_q: 12.696 - prob: 1.000\n",
            "\n",
            "Interval 3267 (32660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.262 - mean_q: 12.548 - prob: 1.000\n",
            "\n",
            "Interval 3268 (32670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.284 - mean_q: 12.685 - prob: 1.000\n",
            "\n",
            "Interval 3269 (32680 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 3270 (32690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.687 - mean_q: 13.088 - prob: 1.000\n",
            "\n",
            "Interval 3271 (32700 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.401 - mean_q: 12.721 - prob: 1.000\n",
            "\n",
            "Interval 3272 (32710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3273 (32720 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.427 - mean_q: 12.817 - prob: 1.000\n",
            "\n",
            "Interval 3274 (32730 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.681 - mean_q: 13.116 - prob: 1.000\n",
            "\n",
            "Interval 3275 (32740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3276 (32750 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.503 - mean_q: 12.978 - prob: 1.000\n",
            "\n",
            "Interval 3277 (32760 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.229 - mean_q: 12.506 - prob: 1.000\n",
            "\n",
            "Interval 3278 (32770 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 3279 (32780 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.651 - mean_q: 13.081 - prob: 1.000\n",
            "\n",
            "Interval 3280 (32790 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 3281 (32800 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.000 - mae: 7.189 - mean_q: 12.634 - prob: 1.000\n",
            "\n",
            "Interval 3282 (32810 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.001 - mae: 7.536 - mean_q: 12.858 - prob: 1.000\n",
            "\n",
            "Interval 3283 (32820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.446 - mean_q: 12.804 - prob: 1.000\n",
            "\n",
            "Interval 3284 (32830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3285 (32840 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.409 - mean_q: 12.784 - prob: 1.000\n",
            "\n",
            "Interval 3286 (32850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3287 (32860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.367 - mean_q: 12.689 - prob: 1.000\n",
            "\n",
            "Interval 3288 (32870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3289 (32880 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.483 - mean_q: 12.819 - prob: 1.000\n",
            "\n",
            "Interval 3290 (32890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.493 - mean_q: 12.835 - prob: 1.000\n",
            "\n",
            "Interval 3291 (32900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3292 (32910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.207 - mean_q: 12.622 - prob: 1.000\n",
            "\n",
            "Interval 3293 (32920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.408 - mean_q: 12.706 - prob: 1.000\n",
            "\n",
            "Interval 3294 (32930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.555 - mean_q: 12.936 - prob: 1.000\n",
            "\n",
            "Interval 3295 (32940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.421 - mean_q: 12.725 - prob: 1.000\n",
            "\n",
            "Interval 3296 (32950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3297 (32960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.330 - mean_q: 12.804 - prob: 1.000\n",
            "\n",
            "Interval 3298 (32970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3299 (32980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.001 - mae: 7.495 - mean_q: 12.910 - prob: 1.000\n",
            "\n",
            "Interval 3300 (32990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.361 - mean_q: 12.713 - prob: 1.000\n",
            "\n",
            "Interval 3301 (33000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.841 - mean_q: 13.358 - prob: 1.000\n",
            "\n",
            "Interval 3302 (33010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.559 - mean_q: 12.909 - prob: 1.000\n",
            "\n",
            "Interval 3303 (33020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3304 (33030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.480 - mean_q: 12.871 - prob: 1.000\n",
            "\n",
            "Interval 3305 (33040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.361 - mean_q: 12.781 - prob: 1.000\n",
            "\n",
            "Interval 3306 (33050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -3.7000\n",
            "Interval 3307 (33060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -23.000 [-23.000, -23.000] - loss: 0.001 - mae: 7.311 - mean_q: 12.657 - prob: 1.000\n",
            "\n",
            "Interval 3308 (33070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3309 (33080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.000 - mae: 7.297 - mean_q: 12.587 - prob: 1.000\n",
            "\n",
            "Interval 3310 (33090 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 3311 (33100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.376 - mean_q: 12.794 - prob: 1.000\n",
            "\n",
            "Interval 3312 (33110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.197 - mean_q: 12.608 - prob: 1.000\n",
            "\n",
            "Interval 3313 (33120 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 3314 (33130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.235 - mean_q: 12.547 - prob: 1.000\n",
            "\n",
            "Interval 3315 (33140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.000 - mae: 7.334 - mean_q: 12.664 - prob: 1.000\n",
            "\n",
            "Interval 3316 (33150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3317 (33160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.432 - mean_q: 12.677 - prob: 1.000\n",
            "\n",
            "Interval 3318 (33170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.365 - mean_q: 12.697 - prob: 1.000\n",
            "\n",
            "Interval 3319 (33180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.541 - mean_q: 12.883 - prob: 1.000\n",
            "\n",
            "Interval 3320 (33190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.286 - mean_q: 12.577 - prob: 1.000\n",
            "\n",
            "Interval 3321 (33200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3322 (33210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.275 - mean_q: 12.604 - prob: 1.000\n",
            "\n",
            "Interval 3323 (33220 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 3324 (33230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.001 - mae: 7.153 - mean_q: 12.502 - prob: 1.000\n",
            "\n",
            "Interval 3325 (33240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.683 - mean_q: 13.003 - prob: 1.000\n",
            "\n",
            "Interval 3326 (33250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3327 (33260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.434 - mean_q: 12.843 - prob: 1.000\n",
            "\n",
            "Interval 3328 (33270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3329 (33280 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.358 - mean_q: 12.703 - prob: 1.000\n",
            "\n",
            "Interval 3330 (33290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.422 - mean_q: 12.759 - prob: 1.000\n",
            "\n",
            "Interval 3331 (33300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3332 (33310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -17.000 [-17.000, -17.000] - loss: 0.001 - mae: 7.373 - mean_q: 12.793 - prob: 1.000\n",
            "\n",
            "Interval 3333 (33320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3334 (33330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.784 - mean_q: 13.162 - prob: 1.000\n",
            "\n",
            "Interval 3335 (33340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.416 - mean_q: 12.837 - prob: 1.000\n",
            "\n",
            "Interval 3336 (33350 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.383 - mean_q: 12.818 - prob: 1.000\n",
            "\n",
            "Interval 3337 (33360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3338 (33370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.349 - mean_q: 12.773 - prob: 1.000\n",
            "\n",
            "Interval 3339 (33380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.444 - mean_q: 12.806 - prob: 1.000\n",
            "\n",
            "Interval 3340 (33390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.410 - mean_q: 12.948 - prob: 1.000\n",
            "\n",
            "Interval 3341 (33400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3342 (33410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.236 - mean_q: 12.496 - prob: 1.000\n",
            "\n",
            "Interval 3343 (33420 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.274 - mean_q: 12.489 - prob: 1.000\n",
            "\n",
            "Interval 3344 (33430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3345 (33440 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.543 - mean_q: 13.053 - prob: 1.000\n",
            "\n",
            "Interval 3346 (33450 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.325 - mean_q: 12.679 - prob: 1.000\n",
            "\n",
            "Interval 3347 (33460 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.300 - mean_q: 12.664 - prob: 1.000\n",
            "\n",
            "Interval 3348 (33470 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.377 - mean_q: 12.783 - prob: 1.000\n",
            "\n",
            "Interval 3349 (33480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3350 (33490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.284 - mean_q: 12.606 - prob: 1.000\n",
            "\n",
            "Interval 3351 (33500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3352 (33510 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.376 - mean_q: 12.546 - prob: 1.000\n",
            "\n",
            "Interval 3353 (33520 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.341 - mean_q: 12.727 - prob: 1.000\n",
            "\n",
            "Interval 3354 (33530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.729 - mean_q: 13.261 - prob: 1.000\n",
            "\n",
            "Interval 3355 (33540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3356 (33550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.407 - mean_q: 12.782 - prob: 1.000\n",
            "\n",
            "Interval 3357 (33560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.387 - mean_q: 12.782 - prob: 1.000\n",
            "\n",
            "Interval 3358 (33570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3359 (33580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.378 - mean_q: 12.675 - prob: 1.000\n",
            "\n",
            "Interval 3360 (33590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.455 - mean_q: 12.963 - prob: 1.000\n",
            "\n",
            "Interval 3361 (33600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3362 (33610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.298 - mean_q: 12.724 - prob: 1.000\n",
            "\n",
            "Interval 3363 (33620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.479 - mean_q: 12.948 - prob: 1.000\n",
            "\n",
            "Interval 3364 (33630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3365 (33640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.586 - mean_q: 12.995 - prob: 1.000\n",
            "\n",
            "Interval 3366 (33650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3367 (33660 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -21.000 [-21.000, -21.000] - loss: 0.001 - mae: 7.258 - mean_q: 12.490 - prob: 1.000\n",
            "\n",
            "Interval 3368 (33670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.391 - mean_q: 12.785 - prob: 1.000\n",
            "\n",
            "Interval 3369 (33680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.000 - mae: 7.400 - mean_q: 12.802 - prob: 1.000\n",
            "\n",
            "Interval 3370 (33690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3371 (33700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.408 - mean_q: 12.819 - prob: 1.000\n",
            "\n",
            "Interval 3372 (33710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3373 (33720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.417 - mean_q: 12.757 - prob: 1.000\n",
            "\n",
            "Interval 3374 (33730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.281 - mean_q: 12.532 - prob: 1.000\n",
            "\n",
            "Interval 3375 (33740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3376 (33750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.214 - mean_q: 12.452 - prob: 1.000\n",
            "\n",
            "Interval 3377 (33760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.455 - mean_q: 12.925 - prob: 1.000\n",
            "\n",
            "Interval 3378 (33770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.351 - mean_q: 12.694 - prob: 1.000\n",
            "\n",
            "Interval 3379 (33780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3380 (33790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.353 - mean_q: 12.927 - prob: 1.000\n",
            "\n",
            "Interval 3381 (33800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 6.960 - mean_q: 12.182 - prob: 1.000\n",
            "\n",
            "Interval 3382 (33810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.563 - mean_q: 12.973 - prob: 1.000\n",
            "\n",
            "Interval 3383 (33820 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 3384 (33830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.329 - mean_q: 12.563 - prob: 1.000\n",
            "\n",
            "Interval 3385 (33840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3386 (33850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.001 - mae: 7.251 - mean_q: 12.615 - prob: 1.000\n",
            "\n",
            "Interval 3387 (33860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3388 (33870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -18.000 [-18.000, -18.000] - loss: 0.001 - mae: 7.406 - mean_q: 12.666 - prob: 1.000\n",
            "\n",
            "Interval 3389 (33880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.192 - mean_q: 12.540 - prob: 1.000\n",
            "\n",
            "Interval 3390 (33890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3391 (33900 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.558 - mean_q: 12.997 - prob: 1.000\n",
            "\n",
            "Interval 3392 (33910 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.422 - mean_q: 12.896 - prob: 1.000\n",
            "\n",
            "Interval 3393 (33920 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.217 - mean_q: 12.588 - prob: 1.000\n",
            "\n",
            "Interval 3394 (33930 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.125 - mean_q: 12.375 - prob: 1.000\n",
            "\n",
            "Interval 3395 (33940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3396 (33950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3397 (33960 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.001 - mae: 7.464 - mean_q: 12.860 - prob: 1.000\n",
            "\n",
            "Interval 3398 (33970 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.164 - mean_q: 12.456 - prob: 1.000\n",
            "\n",
            "Interval 3399 (33980 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.450 - mean_q: 12.858 - prob: 1.000\n",
            "\n",
            "Interval 3400 (33990 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.000 - mae: 7.327 - mean_q: 12.726 - prob: 1.000\n",
            "\n",
            "Interval 3401 (34000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3402 (34010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.512 - mean_q: 13.016 - prob: 1.000\n",
            "\n",
            "Interval 3403 (34020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3404 (34030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 6.873 - mean_q: 11.957 - prob: 1.000\n",
            "\n",
            "Interval 3405 (34040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.275 - mean_q: 12.605 - prob: 1.000\n",
            "\n",
            "Interval 3406 (34050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 3407 (34060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3408 (34070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -18.000 [-18.000, -18.000] - loss: 0.001 - mae: 7.086 - mean_q: 12.178 - prob: 1.000\n",
            "\n",
            "Interval 3409 (34080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.618 - mean_q: 12.906 - prob: 1.000\n",
            "\n",
            "Interval 3410 (34090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3411 (34100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.553 - mean_q: 12.977 - prob: 1.000\n",
            "\n",
            "Interval 3412 (34110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3413 (34120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.559 - mean_q: 13.025 - prob: 1.000\n",
            "\n",
            "Interval 3414 (34130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.636 - mean_q: 13.136 - prob: 1.000\n",
            "\n",
            "Interval 3415 (34140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.685 - mean_q: 13.252 - prob: 1.000\n",
            "\n",
            "Interval 3416 (34150 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 3417 (34160 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.454 - mean_q: 12.673 - prob: 1.000\n",
            "\n",
            "Interval 3418 (34170 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.002 - mae: 7.241 - mean_q: 12.745 - prob: 1.000\n",
            "\n",
            "Interval 3419 (34180 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 3420 (34190 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.002 - mae: 7.322 - mean_q: 12.613 - prob: 1.000\n",
            "\n",
            "Interval 3421 (34200 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: -1.9000\n",
            "Interval 3422 (34210 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.002 - mae: 7.415 - mean_q: 12.895 - prob: 1.000\n",
            "\n",
            "Interval 3423 (34220 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.367 - mean_q: 12.670 - prob: 1.000\n",
            "\n",
            "Interval 3424 (34230 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.0000\n",
            "Interval 3425 (34240 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.416 - mean_q: 12.815 - prob: 1.000\n",
            "\n",
            "Interval 3426 (34250 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 3427 (34260 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.745 - mean_q: 13.228 - prob: 1.000\n",
            "\n",
            "Interval 3428 (34270 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.386 - mean_q: 12.759 - prob: 1.000\n",
            "\n",
            "Interval 3429 (34280 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.497 - mean_q: 12.801 - prob: 1.000\n",
            "\n",
            "Interval 3430 (34290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3431 (34300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.394 - mean_q: 12.669 - prob: 1.000\n",
            "\n",
            "Interval 3432 (34310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 6.936 - mean_q: 12.209 - prob: 1.000\n",
            "\n",
            "Interval 3433 (34320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3434 (34330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.294 - mean_q: 12.533 - prob: 1.000\n",
            "\n",
            "Interval 3435 (34340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.178 - mean_q: 12.466 - prob: 1.000\n",
            "\n",
            "Interval 3436 (34350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.303 - mean_q: 12.634 - prob: 1.000\n",
            "\n",
            "Interval 3437 (34360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3438 (34370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.214 - mean_q: 12.394 - prob: 1.000\n",
            "\n",
            "Interval 3439 (34380 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 3440 (34390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.492 - mean_q: 12.987 - prob: 1.000\n",
            "\n",
            "Interval 3441 (34400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3442 (34410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.421 - mean_q: 12.720 - prob: 1.000\n",
            "\n",
            "Interval 3443 (34420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.169 - mean_q: 12.477 - prob: 1.000\n",
            "\n",
            "Interval 3444 (34430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.289 - mean_q: 12.653 - prob: 1.000\n",
            "\n",
            "Interval 3445 (34440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3446 (34450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.001 - mae: 7.047 - mean_q: 12.230 - prob: 1.000\n",
            "\n",
            "Interval 3447 (34460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3448 (34470 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.576 - mean_q: 13.154 - prob: 1.000\n",
            "\n",
            "Interval 3449 (34480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3450 (34490 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.357 - mean_q: 12.692 - prob: 1.000\n",
            "\n",
            "Interval 3451 (34500 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.303 - mean_q: 12.722 - prob: 1.000\n",
            "\n",
            "Interval 3452 (34510 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -2.8000\n",
            "Interval 3453 (34520 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -19.000 [-19.000, -19.000] - loss: 0.001 - mae: 7.186 - mean_q: 12.317 - prob: 1.000\n",
            "\n",
            "Interval 3454 (34530 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.494 - mean_q: 12.722 - prob: 1.000\n",
            "\n",
            "Interval 3455 (34540 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.344 - mean_q: 12.676 - prob: 1.000\n",
            "\n",
            "Interval 3456 (34550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3457 (34560 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.000 - mae: 7.231 - mean_q: 12.676 - prob: 1.000\n",
            "\n",
            "Interval 3458 (34570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3459 (34580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.200 - mean_q: 12.475 - prob: 1.000\n",
            "\n",
            "Interval 3460 (34590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.286 - mean_q: 12.564 - prob: 1.000\n",
            "\n",
            "Interval 3461 (34600 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 3462 (34610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.674 - mean_q: 13.209 - prob: 1.000\n",
            "\n",
            "Interval 3463 (34620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3464 (34630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.351 - mean_q: 12.516 - prob: 1.000\n",
            "\n",
            "Interval 3465 (34640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.001 - mae: 7.302 - mean_q: 12.602 - prob: 1.000\n",
            "\n",
            "Interval 3466 (34650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3467 (34660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.786 - mean_q: 13.272 - prob: 1.000\n",
            "\n",
            "Interval 3468 (34670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.343 - mean_q: 12.845 - prob: 1.000\n",
            "\n",
            "Interval 3469 (34680 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 3470 (34690 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.297 - mean_q: 12.538 - prob: 1.000\n",
            "\n",
            "Interval 3471 (34700 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.592 - mean_q: 13.000 - prob: 1.000\n",
            "\n",
            "Interval 3472 (34710 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.452 - mean_q: 12.825 - prob: 1.000\n",
            "\n",
            "Interval 3473 (34720 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 3474 (34730 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.362 - mean_q: 12.588 - prob: 1.000\n",
            "\n",
            "Interval 3475 (34740 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.449 - mean_q: 12.835 - prob: 1.000\n",
            "\n",
            "Interval 3476 (34750 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.463 - mean_q: 12.890 - prob: 1.000\n",
            "\n",
            "Interval 3477 (34760 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.170 - mean_q: 12.500 - prob: 1.000\n",
            "\n",
            "Interval 3478 (34770 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.539 - mean_q: 12.976 - prob: 1.000\n",
            "\n",
            "Interval 3479 (34780 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 3480 (34790 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.520 - mean_q: 13.055 - prob: 1.000\n",
            "\n",
            "Interval 3481 (34800 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.035 - mean_q: 12.257 - prob: 1.000\n",
            "\n",
            "Interval 3482 (34810 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.448 - mean_q: 12.831 - prob: 1.000\n",
            "\n",
            "Interval 3483 (34820 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.296 - mean_q: 12.621 - prob: 1.000\n",
            "\n",
            "Interval 3484 (34830 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 3485 (34840 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 6.894 - mean_q: 12.107 - prob: 1.000\n",
            "\n",
            "Interval 3486 (34850 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 3487 (34860 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.085 - mean_q: 12.362 - prob: 1.000\n",
            "\n",
            "Interval 3488 (34870 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.617 - mean_q: 13.174 - prob: 1.000\n",
            "\n",
            "Interval 3489 (34880 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 3490 (34890 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.517 - mean_q: 12.799 - prob: 1.000\n",
            "\n",
            "Interval 3491 (34900 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 3492 (34910 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.298 - mean_q: 12.598 - prob: 1.000\n",
            "\n",
            "Interval 3493 (34920 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.145 - mean_q: 12.262 - prob: 1.000\n",
            "\n",
            "Interval 3494 (34930 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.102 - mean_q: 12.305 - prob: 1.000\n",
            "\n",
            "Interval 3495 (34940 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.842 - mean_q: 13.291 - prob: 1.000\n",
            "\n",
            "Interval 3496 (34950 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 3497 (34960 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -2.8000\n",
            "Interval 3498 (34970 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -19.000 [-19.000, -19.000] - loss: 0.001 - mae: 7.335 - mean_q: 12.654 - prob: 1.000\n",
            "\n",
            "Interval 3499 (34980 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.911 - mean_q: 13.503 - prob: 1.000\n",
            "\n",
            "Interval 3500 (34990 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.435 - mean_q: 12.884 - prob: 1.000\n",
            "\n",
            "Interval 3501 (35000 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.9000\n",
            "Interval 3502 (35010 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.696 - mean_q: 13.297 - prob: 1.000\n",
            "\n",
            "Interval 3503 (35020 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.581 - mean_q: 12.979 - prob: 1.000\n",
            "\n",
            "Interval 3504 (35030 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.9000\n",
            "Interval 3505 (35040 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.207 - mean_q: 12.477 - prob: 1.000\n",
            "\n",
            "Interval 3506 (35050 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.243 - mean_q: 12.480 - prob: 1.000\n",
            "\n",
            "Interval 3507 (35060 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.323 - mean_q: 12.745 - prob: 1.000\n",
            "\n",
            "Interval 3508 (35070 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: -1.9000\n",
            "Interval 3509 (35080 steps performed)\n",
            "10/10 [==============================] - 0s 15ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.001 - mae: 7.153 - mean_q: 12.385 - prob: 1.000\n",
            "\n",
            "Interval 3510 (35090 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 3511 (35100 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.485 - mean_q: 13.031 - prob: 1.000\n",
            "\n",
            "Interval 3512 (35110 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: -1.0000\n",
            "Interval 3513 (35120 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.441 - mean_q: 12.723 - prob: 1.000\n",
            "\n",
            "Interval 3514 (35130 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.492 - mean_q: 12.815 - prob: 1.000\n",
            "\n",
            "Interval 3515 (35140 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 3516 (35150 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.265 - mean_q: 12.648 - prob: 1.000\n",
            "\n",
            "Interval 3517 (35160 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.233 - mean_q: 12.528 - prob: 1.000\n",
            "\n",
            "Interval 3518 (35170 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: -2.8000\n",
            "Interval 3519 (35180 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.001 - mae: 7.253 - mean_q: 12.584 - prob: 1.000\n",
            "\n",
            "Interval 3520 (35190 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.594 - mean_q: 12.966 - prob: 1.000\n",
            "\n",
            "Interval 3521 (35200 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 3522 (35210 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.224 - mean_q: 12.482 - prob: 1.000\n",
            "\n",
            "Interval 3523 (35220 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.189 - mean_q: 12.551 - prob: 1.000\n",
            "\n",
            "Interval 3524 (35230 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: -1.0000\n",
            "Interval 3525 (35240 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.471 - mean_q: 12.858 - prob: 1.000\n",
            "\n",
            "Interval 3526 (35250 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.9000\n",
            "Interval 3527 (35260 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.001 - mae: 7.402 - mean_q: 12.709 - prob: 1.000\n",
            "\n",
            "Interval 3528 (35270 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: -2.8000\n",
            "Interval 3529 (35280 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -17.000 [-17.000, -17.000] - loss: 0.001 - mae: 7.307 - mean_q: 12.595 - prob: 1.000\n",
            "\n",
            "Interval 3530 (35290 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.413 - mean_q: 12.792 - prob: 1.000\n",
            "\n",
            "Interval 3531 (35300 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.523 - mean_q: 12.983 - prob: 1.000\n",
            "\n",
            "Interval 3532 (35310 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.425 - mean_q: 12.810 - prob: 1.000\n",
            "\n",
            "Interval 3533 (35320 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.000 - mae: 7.663 - mean_q: 13.084 - prob: 1.000\n",
            "\n",
            "Interval 3534 (35330 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.426 - mean_q: 12.694 - prob: 1.000\n",
            "\n",
            "Interval 3535 (35340 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: -1.9000\n",
            "Interval 3536 (35350 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.9000\n",
            "Interval 3537 (35360 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -19.000 [-19.000, -19.000] - loss: 0.001 - mae: 7.285 - mean_q: 12.696 - prob: 1.000\n",
            "\n",
            "Interval 3538 (35370 steps performed)\n",
            "10/10 [==============================] - 0s 14ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.105 - mean_q: 12.426 - prob: 1.000\n",
            "\n",
            "Interval 3539 (35380 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: -1.9000\n",
            "Interval 3540 (35390 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.002 - mae: 7.344 - mean_q: 12.664 - prob: 1.000\n",
            "\n",
            "Interval 3541 (35400 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.984 - mean_q: 13.463 - prob: 1.000\n",
            "\n",
            "Interval 3542 (35410 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: -1.0000\n",
            "Interval 3543 (35420 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.249 - mean_q: 12.524 - prob: 1.000\n",
            "\n",
            "Interval 3544 (35430 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.175 - mean_q: 12.526 - prob: 1.000\n",
            "\n",
            "Interval 3545 (35440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3546 (35450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.435 - mean_q: 12.822 - prob: 1.000\n",
            "\n",
            "Interval 3547 (35460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3548 (35470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.272 - mean_q: 12.545 - prob: 1.000\n",
            "\n",
            "Interval 3549 (35480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.468 - mean_q: 12.796 - prob: 1.000\n",
            "\n",
            "Interval 3550 (35490 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 6.945 - mean_q: 11.927 - prob: 1.000\n",
            "\n",
            "Interval 3551 (35500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3552 (35510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.086 - mean_q: 12.433 - prob: 1.000\n",
            "\n",
            "Interval 3553 (35520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3554 (35530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.397 - mean_q: 12.585 - prob: 1.000\n",
            "\n",
            "Interval 3555 (35540 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.262 - mean_q: 12.523 - prob: 1.000\n",
            "\n",
            "Interval 3556 (35550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3557 (35560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.204 - mean_q: 12.538 - prob: 1.000\n",
            "\n",
            "Interval 3558 (35570 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 3559 (35580 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.001 - mae: 7.781 - mean_q: 13.392 - prob: 1.000\n",
            "\n",
            "Interval 3560 (35590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.000 - mae: 7.227 - mean_q: 12.471 - prob: 1.000\n",
            "\n",
            "Interval 3561 (35600 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.249 - mean_q: 12.608 - prob: 1.000\n",
            "\n",
            "Interval 3562 (35610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3563 (35620 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.263 - mean_q: 12.561 - prob: 1.000\n",
            "\n",
            "Interval 3564 (35630 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.001 - mae: 7.366 - mean_q: 12.830 - prob: 1.000\n",
            "\n",
            "Interval 3565 (35640 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.294 - mean_q: 12.514 - prob: 1.000\n",
            "\n",
            "Interval 3566 (35650 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.266 - mean_q: 12.777 - prob: 1.000\n",
            "\n",
            "Interval 3567 (35660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3568 (35670 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.315 - mean_q: 12.560 - prob: 1.000\n",
            "\n",
            "Interval 3569 (35680 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 3570 (35690 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.540 - mean_q: 12.843 - prob: 1.000\n",
            "\n",
            "Interval 3571 (35700 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.001 - mae: 7.731 - mean_q: 13.276 - prob: 1.000\n",
            "\n",
            "Interval 3572 (35710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3573 (35720 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.000 - mae: 7.283 - mean_q: 12.567 - prob: 1.000\n",
            "\n",
            "Interval 3574 (35730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3575 (35740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.533 - mean_q: 12.929 - prob: 1.000\n",
            "\n",
            "Interval 3576 (35750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.540 - mean_q: 12.940 - prob: 1.000\n",
            "\n",
            "Interval 3577 (35760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3578 (35770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 6.913 - mean_q: 12.145 - prob: 1.000\n",
            "\n",
            "Interval 3579 (35780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.683 - mean_q: 13.187 - prob: 1.000\n",
            "\n",
            "Interval 3580 (35790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3581 (35800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.000 - mae: 7.498 - mean_q: 12.912 - prob: 1.000\n",
            "\n",
            "Interval 3582 (35810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3583 (35820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -17.000 [-17.000, -17.000] - loss: 0.001 - mae: 7.386 - mean_q: 12.824 - prob: 1.000\n",
            "\n",
            "Interval 3584 (35830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.344 - mean_q: 12.676 - prob: 1.000\n",
            "\n",
            "Interval 3585 (35840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3586 (35850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.590 - mean_q: 13.017 - prob: 1.000\n",
            "\n",
            "Interval 3587 (35860 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.473 - mean_q: 12.877 - prob: 1.000\n",
            "\n",
            "Interval 3588 (35870 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.001 - mae: 7.136 - mean_q: 12.397 - prob: 1.000\n",
            "\n",
            "Interval 3589 (35880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3590 (35890 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.595 - mean_q: 12.990 - prob: 1.000\n",
            "\n",
            "Interval 3591 (35900 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.640 - mean_q: 13.062 - prob: 1.000\n",
            "\n",
            "Interval 3592 (35910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3593 (35920 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.267 - mean_q: 12.564 - prob: 1.000\n",
            "\n",
            "Interval 3594 (35930 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.561 - mean_q: 12.915 - prob: 1.000\n",
            "\n",
            "Interval 3595 (35940 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 3596 (35950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.263 - mean_q: 12.577 - prob: 1.000\n",
            "\n",
            "Interval 3597 (35960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.001 - mae: 7.153 - mean_q: 12.422 - prob: 1.000\n",
            "\n",
            "Interval 3598 (35970 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.404 - mean_q: 12.813 - prob: 1.000\n",
            "\n",
            "Interval 3599 (35980 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.318 - mean_q: 12.715 - prob: 1.000\n",
            "\n",
            "Interval 3600 (35990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.749 - mean_q: 13.381 - prob: 1.000\n",
            "\n",
            "Interval 3601 (36000 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 3602 (36010 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.393 - mean_q: 12.847 - prob: 1.000\n",
            "\n",
            "Interval 3603 (36020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.475 - mean_q: 12.853 - prob: 1.000\n",
            "\n",
            "Interval 3604 (36030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3605 (36040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.486 - mean_q: 12.893 - prob: 1.000\n",
            "\n",
            "Interval 3606 (36050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3607 (36060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.278 - mean_q: 12.570 - prob: 1.000\n",
            "\n",
            "Interval 3608 (36070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.172 - mean_q: 12.417 - prob: 1.000\n",
            "\n",
            "Interval 3609 (36080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.651 - mean_q: 13.151 - prob: 1.000\n",
            "\n",
            "Interval 3610 (36090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3611 (36100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.362 - mean_q: 12.662 - prob: 1.000\n",
            "\n",
            "Interval 3612 (36110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.000 - mae: 7.455 - mean_q: 12.837 - prob: 1.000\n",
            "\n",
            "Interval 3613 (36120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3614 (36130 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.255 - mean_q: 12.573 - prob: 1.000\n",
            "\n",
            "Interval 3615 (36140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.729 - mean_q: 13.088 - prob: 1.000\n",
            "\n",
            "Interval 3616 (36150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 3617 (36160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: -3.500 [-22.000, 15.000] - loss: 0.001 - mae: 7.584 - mean_q: 12.936 - prob: 1.000\n",
            "\n",
            "Interval 3618 (36170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.142 - mean_q: 12.477 - prob: 1.000\n",
            "\n",
            "Interval 3619 (36180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3620 (36190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.281 - mean_q: 12.598 - prob: 1.000\n",
            "\n",
            "Interval 3621 (36200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.061 - mean_q: 12.311 - prob: 1.000\n",
            "\n",
            "Interval 3622 (36210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3623 (36220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.564 - mean_q: 12.952 - prob: 1.000\n",
            "\n",
            "Interval 3624 (36230 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.573 - mean_q: 12.942 - prob: 1.000\n",
            "\n",
            "Interval 3625 (36240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3626 (36250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.462 - mean_q: 12.832 - prob: 1.000\n",
            "\n",
            "Interval 3627 (36260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.400 - mean_q: 12.749 - prob: 1.000\n",
            "\n",
            "Interval 3628 (36270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.491 - mean_q: 12.910 - prob: 1.000\n",
            "\n",
            "Interval 3629 (36280 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 3630 (36290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.146 - mean_q: 12.336 - prob: 1.000\n",
            "\n",
            "Interval 3631 (36300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.458 - mean_q: 12.806 - prob: 1.000\n",
            "\n",
            "Interval 3632 (36310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.125 - mean_q: 12.346 - prob: 1.000\n",
            "\n",
            "Interval 3633 (36320 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.140 - mean_q: 12.313 - prob: 1.000\n",
            "\n",
            "Interval 3634 (36330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3635 (36340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.359 - mean_q: 12.768 - prob: 1.000\n",
            "\n",
            "Interval 3636 (36350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.547 - mean_q: 12.985 - prob: 1.000\n",
            "\n",
            "Interval 3637 (36360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.148 - mean_q: 12.323 - prob: 1.000\n",
            "\n",
            "Interval 3638 (36370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.485 - mean_q: 12.824 - prob: 1.000\n",
            "\n",
            "Interval 3639 (36380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.241 - mean_q: 12.607 - prob: 1.000\n",
            "\n",
            "Interval 3640 (36390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3641 (36400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.561 - mean_q: 13.067 - prob: 1.000\n",
            "\n",
            "Interval 3642 (36410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.637 - mean_q: 13.026 - prob: 1.000\n",
            "\n",
            "Interval 3643 (36420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3644 (36430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.186 - mean_q: 12.506 - prob: 1.000\n",
            "\n",
            "Interval 3645 (36440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3646 (36450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 6.990 - mean_q: 12.192 - prob: 1.000\n",
            "\n",
            "Interval 3647 (36460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.176 - mean_q: 12.518 - prob: 1.000\n",
            "\n",
            "Interval 3648 (36470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3649 (36480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.001 - mae: 7.430 - mean_q: 12.707 - prob: 1.000\n",
            "\n",
            "Interval 3650 (36490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.251 - mean_q: 12.513 - prob: 1.000\n",
            "\n",
            "Interval 3651 (36500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3652 (36510 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.072 - mean_q: 12.333 - prob: 1.000\n",
            "\n",
            "Interval 3653 (36520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.444 - mean_q: 12.928 - prob: 1.000\n",
            "\n",
            "Interval 3654 (36530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3655 (36540 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.224 - mean_q: 12.527 - prob: 1.000\n",
            "\n",
            "Interval 3656 (36550 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 6.868 - mean_q: 12.040 - prob: 1.000\n",
            "\n",
            "Interval 3657 (36560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3658 (36570 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.467 - mean_q: 12.832 - prob: 1.000\n",
            "\n",
            "Interval 3659 (36580 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.371 - mean_q: 12.595 - prob: 1.000\n",
            "\n",
            "Interval 3660 (36590 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 3661 (36600 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.312 - mean_q: 12.533 - prob: 1.000\n",
            "\n",
            "Interval 3662 (36610 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.523 - mean_q: 12.866 - prob: 1.000\n",
            "\n",
            "Interval 3663 (36620 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.477 - mean_q: 12.837 - prob: 1.000\n",
            "\n",
            "Interval 3664 (36630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3665 (36640 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.318 - mean_q: 12.732 - prob: 1.000\n",
            "\n",
            "Interval 3666 (36650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3667 (36660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.114 - mean_q: 12.392 - prob: 1.000\n",
            "\n",
            "Interval 3668 (36670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.238 - mean_q: 12.615 - prob: 1.000\n",
            "\n",
            "Interval 3669 (36680 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.543 - mean_q: 12.893 - prob: 1.000\n",
            "\n",
            "Interval 3670 (36690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3671 (36700 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.159 - mean_q: 12.293 - prob: 1.000\n",
            "\n",
            "Interval 3672 (36710 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.193 - mean_q: 12.402 - prob: 1.000\n",
            "\n",
            "Interval 3673 (36720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3674 (36730 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.488 - mean_q: 12.994 - prob: 1.000\n",
            "\n",
            "Interval 3675 (36740 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.303 - mean_q: 12.583 - prob: 1.000\n",
            "\n",
            "Interval 3676 (36750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3677 (36760 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.000 - mae: 7.518 - mean_q: 12.958 - prob: 1.000\n",
            "\n",
            "Interval 3678 (36770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3679 (36780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.519 - mean_q: 12.908 - prob: 1.000\n",
            "\n",
            "Interval 3680 (36790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.576 - mean_q: 13.050 - prob: 1.000\n",
            "\n",
            "Interval 3681 (36800 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.157 - mean_q: 12.340 - prob: 1.000\n",
            "\n",
            "Interval 3682 (36810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3683 (36820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.293 - mean_q: 12.526 - prob: 1.000\n",
            "\n",
            "Interval 3684 (36830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.401 - mean_q: 12.650 - prob: 1.000\n",
            "\n",
            "Interval 3685 (36840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3686 (36850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.033 - mean_q: 12.321 - prob: 1.000\n",
            "\n",
            "Interval 3687 (36860 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.399 - mean_q: 12.822 - prob: 1.000\n",
            "\n",
            "Interval 3688 (36870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3689 (36880 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.000 - mae: 7.471 - mean_q: 12.706 - prob: 1.000\n",
            "\n",
            "Interval 3690 (36890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3691 (36900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.552 - mean_q: 12.920 - prob: 1.000\n",
            "\n",
            "Interval 3692 (36910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3693 (36920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.543 - mean_q: 12.980 - prob: 1.000\n",
            "\n",
            "Interval 3694 (36930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3695 (36940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 2.500 [-9.000, 14.000] - loss: 0.000 - mae: 7.321 - mean_q: 12.685 - prob: 1.000\n",
            "\n",
            "Interval 3696 (36950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.000 - mae: 7.357 - mean_q: 12.589 - prob: 1.000\n",
            "\n",
            "Interval 3697 (36960 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.360 - mean_q: 12.640 - prob: 1.000\n",
            "\n",
            "Interval 3698 (36970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3699 (36980 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.001 - mae: 7.692 - mean_q: 13.131 - prob: 1.000\n",
            "\n",
            "Interval 3700 (36990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3701 (37000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.399 - mean_q: 12.729 - prob: 1.000\n",
            "\n",
            "Interval 3702 (37010 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.358 - mean_q: 12.622 - prob: 1.000\n",
            "\n",
            "Interval 3703 (37020 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.000 - mae: 7.169 - mean_q: 12.371 - prob: 1.000\n",
            "\n",
            "Interval 3704 (37030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.293 - mean_q: 12.538 - prob: 1.000\n",
            "\n",
            "Interval 3705 (37040 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.000 - mae: 7.434 - mean_q: 12.822 - prob: 1.000\n",
            "\n",
            "Interval 3706 (37050 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 3707 (37060 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.000 - mae: 7.479 - mean_q: 12.846 - prob: 1.000\n",
            "\n",
            "Interval 3708 (37070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.6000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.368 - mean_q: 12.717 - prob: 1.000\n",
            "\n",
            "Interval 3709 (37080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3710 (37090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.000 - mae: 7.548 - mean_q: 13.015 - prob: 1.000\n",
            "\n",
            "Interval 3711 (37100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.245 - mean_q: 12.544 - prob: 1.000\n",
            "\n",
            "Interval 3712 (37110 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 3713 (37120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.511 - mean_q: 12.839 - prob: 1.000\n",
            "\n",
            "Interval 3714 (37130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.078 - mean_q: 12.337 - prob: 1.000\n",
            "\n",
            "Interval 3715 (37140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.397 - mean_q: 12.704 - prob: 1.000\n",
            "\n",
            "Interval 3716 (37150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3717 (37160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.381 - mean_q: 12.871 - prob: 1.000\n",
            "\n",
            "Interval 3718 (37170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3719 (37180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.001 - mae: 7.438 - mean_q: 12.821 - prob: 1.000\n",
            "\n",
            "Interval 3720 (37190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3721 (37200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.321 - mean_q: 12.654 - prob: 1.000\n",
            "\n",
            "Interval 3722 (37210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.167 - mean_q: 12.462 - prob: 1.000\n",
            "\n",
            "Interval 3723 (37220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.167 - mean_q: 12.524 - prob: 1.000\n",
            "\n",
            "Interval 3724 (37230 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 3725 (37240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.344 - mean_q: 12.660 - prob: 1.000\n",
            "\n",
            "Interval 3726 (37250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3727 (37260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.570 - mean_q: 12.957 - prob: 1.000\n",
            "\n",
            "Interval 3728 (37270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.460 - mean_q: 12.921 - prob: 1.000\n",
            "\n",
            "Interval 3729 (37280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3730 (37290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.278 - mean_q: 12.509 - prob: 1.000\n",
            "\n",
            "Interval 3731 (37300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.537 - mean_q: 13.016 - prob: 1.000\n",
            "\n",
            "Interval 3732 (37310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.562 - mean_q: 13.197 - prob: 1.000\n",
            "\n",
            "Interval 3733 (37320 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 3734 (37330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.574 - mean_q: 12.944 - prob: 1.000\n",
            "\n",
            "Interval 3735 (37340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.063 - mean_q: 12.136 - prob: 1.000\n",
            "\n",
            "Interval 3736 (37350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3737 (37360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.578 - mean_q: 13.119 - prob: 1.000\n",
            "\n",
            "Interval 3738 (37370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.346 - mean_q: 12.570 - prob: 1.000\n",
            "\n",
            "Interval 3739 (37380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3740 (37390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 3741 (37400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -19.000 [-19.000, -19.000] - loss: 0.000 - mae: 7.484 - mean_q: 12.893 - prob: 1.000\n",
            "\n",
            "Interval 3742 (37410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.331 - mean_q: 12.562 - prob: 1.000\n",
            "\n",
            "Interval 3743 (37420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3744 (37430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.439 - mean_q: 12.929 - prob: 1.000\n",
            "\n",
            "Interval 3745 (37440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.728 - mean_q: 13.311 - prob: 1.000\n",
            "\n",
            "Interval 3746 (37450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3747 (37460 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.414 - mean_q: 12.771 - prob: 1.000\n",
            "\n",
            "Interval 3748 (37470 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.551 - mean_q: 12.953 - prob: 1.000\n",
            "\n",
            "Interval 3749 (37480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3750 (37490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.535 - mean_q: 13.018 - prob: 1.000\n",
            "\n",
            "Interval 3751 (37500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.217 - mean_q: 12.570 - prob: 1.000\n",
            "\n",
            "Interval 3752 (37510 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 3753 (37520 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.436 - mean_q: 12.852 - prob: 1.000\n",
            "\n",
            "Interval 3754 (37530 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.400 - mean_q: 12.766 - prob: 1.000\n",
            "\n",
            "Interval 3755 (37540 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.171 - mean_q: 12.408 - prob: 1.000\n",
            "\n",
            "Interval 3756 (37550 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 3757 (37560 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.072 - mean_q: 12.293 - prob: 1.000\n",
            "\n",
            "Interval 3758 (37570 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.104 - mean_q: 12.247 - prob: 1.000\n",
            "\n",
            "Interval 3759 (37580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3760 (37590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.135 - mean_q: 12.357 - prob: 1.000\n",
            "\n",
            "Interval 3761 (37600 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.288 - mean_q: 12.602 - prob: 1.000\n",
            "\n",
            "Interval 3762 (37610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3763 (37620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.435 - mean_q: 12.748 - prob: 1.000\n",
            "\n",
            "Interval 3764 (37630 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.001 - mae: 7.129 - mean_q: 12.299 - prob: 1.000\n",
            "\n",
            "Interval 3765 (37640 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.648 - mean_q: 12.967 - prob: 1.000\n",
            "\n",
            "Interval 3766 (37650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.355 - mean_q: 12.736 - prob: 1.000\n",
            "\n",
            "Interval 3767 (37660 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.000 - mae: 7.304 - mean_q: 12.622 - prob: 1.000\n",
            "\n",
            "Interval 3768 (37670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.572 - mean_q: 13.035 - prob: 1.000\n",
            "\n",
            "Interval 3769 (37680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3770 (37690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.516 - mean_q: 12.764 - prob: 1.000\n",
            "\n",
            "Interval 3771 (37700 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.453 - mean_q: 12.792 - prob: 1.000\n",
            "\n",
            "Interval 3772 (37710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3773 (37720 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.332 - mean_q: 12.645 - prob: 1.000\n",
            "\n",
            "Interval 3774 (37730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.309 - mean_q: 12.583 - prob: 1.000\n",
            "\n",
            "Interval 3775 (37740 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 3776 (37750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.221 - mean_q: 12.478 - prob: 1.000\n",
            "\n",
            "Interval 3777 (37760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3778 (37770 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.201 - mean_q: 12.468 - prob: 1.000\n",
            "\n",
            "Interval 3779 (37780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.274 - mean_q: 12.598 - prob: 1.000\n",
            "\n",
            "Interval 3780 (37790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.632 - mean_q: 13.098 - prob: 1.000\n",
            "\n",
            "Interval 3781 (37800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3782 (37810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.477 - mean_q: 12.873 - prob: 1.000\n",
            "\n",
            "Interval 3783 (37820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.001 - mae: 7.495 - mean_q: 12.906 - prob: 1.000\n",
            "\n",
            "Interval 3784 (37830 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.266 - mean_q: 12.709 - prob: 1.000\n",
            "\n",
            "Interval 3785 (37840 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 3786 (37850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.431 - mean_q: 12.872 - prob: 1.000\n",
            "\n",
            "Interval 3787 (37860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.327 - mean_q: 12.634 - prob: 1.000\n",
            "\n",
            "Interval 3788 (37870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3789 (37880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.285 - mean_q: 12.534 - prob: 1.000\n",
            "\n",
            "Interval 3790 (37890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.211 - mean_q: 12.414 - prob: 1.000\n",
            "\n",
            "Interval 3791 (37900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3792 (37910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.336 - mean_q: 12.712 - prob: 1.000\n",
            "\n",
            "Interval 3793 (37920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3794 (37930 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.331 - mean_q: 12.671 - prob: 1.000\n",
            "\n",
            "Interval 3795 (37940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.521 - mean_q: 12.964 - prob: 1.000\n",
            "\n",
            "Interval 3796 (37950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3797 (37960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.001 - mae: 7.164 - mean_q: 12.484 - prob: 1.000\n",
            "\n",
            "Interval 3798 (37970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.541 - mean_q: 13.025 - prob: 1.000\n",
            "\n",
            "Interval 3799 (37980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3800 (37990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.451 - mean_q: 12.957 - prob: 1.000\n",
            "\n",
            "Interval 3801 (38000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.001 - mae: 7.460 - mean_q: 12.825 - prob: 1.000\n",
            "\n",
            "Interval 3802 (38010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3803 (38020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.294 - mean_q: 12.772 - prob: 1.000\n",
            "\n",
            "Interval 3804 (38030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3805 (38040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.482 - mean_q: 12.810 - prob: 1.000\n",
            "\n",
            "Interval 3806 (38050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.025 - mean_q: 12.306 - prob: 1.000\n",
            "\n",
            "Interval 3807 (38060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3808 (38070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.600 - mean_q: 13.078 - prob: 1.000\n",
            "\n",
            "Interval 3809 (38080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.001 - mae: 7.259 - mean_q: 12.661 - prob: 1.000\n",
            "\n",
            "Interval 3810 (38090 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 3811 (38100 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.465 - mean_q: 12.749 - prob: 1.000\n",
            "\n",
            "Interval 3812 (38110 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.354 - mean_q: 12.745 - prob: 1.000\n",
            "\n",
            "Interval 3813 (38120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.308 - mean_q: 12.584 - prob: 1.000\n",
            "\n",
            "Interval 3814 (38130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3815 (38140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.461 - mean_q: 12.818 - prob: 1.000\n",
            "\n",
            "Interval 3816 (38150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.076 - mean_q: 12.242 - prob: 1.000\n",
            "\n",
            "Interval 3817 (38160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3818 (38170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.421 - mean_q: 12.866 - prob: 1.000\n",
            "\n",
            "Interval 3819 (38180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3820 (38190 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.374 - mean_q: 12.732 - prob: 1.000\n",
            "\n",
            "Interval 3821 (38200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.001 - mae: 7.473 - mean_q: 12.910 - prob: 1.000\n",
            "\n",
            "Interval 3822 (38210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.002 - mae: 7.440 - mean_q: 12.876 - prob: 1.000\n",
            "\n",
            "Interval 3823 (38220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3824 (38230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.001 - mae: 7.905 - mean_q: 13.477 - prob: 1.000\n",
            "\n",
            "Interval 3825 (38240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.492 - mean_q: 12.881 - prob: 1.000\n",
            "\n",
            "Interval 3826 (38250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3827 (38260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.234 - mean_q: 12.562 - prob: 1.000\n",
            "\n",
            "Interval 3828 (38270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.465 - mean_q: 12.850 - prob: 1.000\n",
            "\n",
            "Interval 3829 (38280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3830 (38290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 6.984 - mean_q: 12.121 - prob: 1.000\n",
            "\n",
            "Interval 3831 (38300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.535 - mean_q: 12.920 - prob: 1.000\n",
            "\n",
            "Interval 3832 (38310 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.340 - mean_q: 12.752 - prob: 1.000\n",
            "\n",
            "Interval 3833 (38320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3834 (38330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.854 - mean_q: 13.414 - prob: 1.000\n",
            "\n",
            "Interval 3835 (38340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.509 - mean_q: 12.889 - prob: 1.000\n",
            "\n",
            "Interval 3836 (38350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.226 - mean_q: 12.520 - prob: 1.000\n",
            "\n",
            "Interval 3837 (38360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.418 - mean_q: 12.728 - prob: 1.000\n",
            "\n",
            "Interval 3838 (38370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3839 (38380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.316 - mean_q: 12.640 - prob: 1.000\n",
            "\n",
            "Interval 3840 (38390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.244 - mean_q: 12.379 - prob: 1.000\n",
            "\n",
            "Interval 3841 (38400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3842 (38410 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.366 - mean_q: 12.750 - prob: 1.000\n",
            "\n",
            "Interval 3843 (38420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.599 - mean_q: 13.038 - prob: 1.000\n",
            "\n",
            "Interval 3844 (38430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3845 (38440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.164 - mean_q: 12.355 - prob: 1.000\n",
            "\n",
            "Interval 3846 (38450 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 3847 (38460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.353 - mean_q: 12.700 - prob: 1.000\n",
            "\n",
            "Interval 3848 (38470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.717 - mean_q: 13.207 - prob: 1.000\n",
            "\n",
            "Interval 3849 (38480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3850 (38490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 6.916 - mean_q: 12.085 - prob: 1.000\n",
            "\n",
            "Interval 3851 (38500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.170 - mean_q: 12.338 - prob: 1.000\n",
            "\n",
            "Interval 3852 (38510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.250 - mean_q: 12.606 - prob: 1.000\n",
            "\n",
            "Interval 3853 (38520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3854 (38530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.643 - mean_q: 13.145 - prob: 1.000\n",
            "\n",
            "Interval 3855 (38540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3856 (38550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.534 - mean_q: 12.984 - prob: 1.000\n",
            "\n",
            "Interval 3857 (38560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.444 - mean_q: 12.812 - prob: 1.000\n",
            "\n",
            "Interval 3858 (38570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3859 (38580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.505 - mean_q: 12.854 - prob: 1.000\n",
            "\n",
            "Interval 3860 (38590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.228 - mean_q: 12.512 - prob: 1.000\n",
            "\n",
            "Interval 3861 (38600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3862 (38610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.343 - mean_q: 12.692 - prob: 1.000\n",
            "\n",
            "Interval 3863 (38620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3864 (38630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.281 - mean_q: 12.439 - prob: 1.000\n",
            "\n",
            "Interval 3865 (38640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.270 - mean_q: 12.732 - prob: 1.000\n",
            "\n",
            "Interval 3866 (38650 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 3867 (38660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.168 - mean_q: 12.524 - prob: 1.000\n",
            "\n",
            "Interval 3868 (38670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.305 - mean_q: 12.705 - prob: 1.000\n",
            "\n",
            "Interval 3869 (38680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 13.000 [12.000, 14.000] - loss: 0.001 - mae: 7.416 - mean_q: 12.791 - prob: 1.000\n",
            "\n",
            "Interval 3870 (38690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3871 (38700 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.500 - mean_q: 12.878 - prob: 1.000\n",
            "\n",
            "Interval 3872 (38710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.562 - mean_q: 12.924 - prob: 1.000\n",
            "\n",
            "Interval 3873 (38720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.467 - mean_q: 12.833 - prob: 1.000\n",
            "\n",
            "Interval 3874 (38730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3875 (38740 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.446 - mean_q: 12.781 - prob: 1.000\n",
            "\n",
            "Interval 3876 (38750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.133 - mean_q: 12.421 - prob: 1.000\n",
            "\n",
            "Interval 3877 (38760 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 3878 (38770 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.553 - mean_q: 13.088 - prob: 1.000\n",
            "\n",
            "Interval 3879 (38780 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.143 - mean_q: 12.526 - prob: 1.000\n",
            "\n",
            "Interval 3880 (38790 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 3881 (38800 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.362 - mean_q: 12.670 - prob: 1.000\n",
            "\n",
            "Interval 3882 (38810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.385 - mean_q: 12.780 - prob: 1.000\n",
            "\n",
            "Interval 3883 (38820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3884 (38830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.132 - mean_q: 12.430 - prob: 1.000\n",
            "\n",
            "Interval 3885 (38840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3886 (38850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.001 - mae: 7.231 - mean_q: 12.483 - prob: 1.000\n",
            "\n",
            "Interval 3887 (38860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.340 - mean_q: 12.704 - prob: 1.000\n",
            "\n",
            "Interval 3888 (38870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3889 (38880 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.275 - mean_q: 12.661 - prob: 1.000\n",
            "\n",
            "Interval 3890 (38890 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.392 - mean_q: 12.827 - prob: 1.000\n",
            "\n",
            "Interval 3891 (38900 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 3892 (38910 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.309 - mean_q: 12.641 - prob: 1.000\n",
            "\n",
            "Interval 3893 (38920 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.467 - mean_q: 12.747 - prob: 1.000\n",
            "\n",
            "Interval 3894 (38930 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 3895 (38940 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.001 - mae: 7.463 - mean_q: 12.717 - prob: 1.000\n",
            "\n",
            "Interval 3896 (38950 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.396 - mean_q: 12.861 - prob: 1.000\n",
            "\n",
            "Interval 3897 (38960 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 3898 (38970 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.178 - mean_q: 12.439 - prob: 1.000\n",
            "\n",
            "Interval 3899 (38980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.162 - mean_q: 12.441 - prob: 1.000\n",
            "\n",
            "Interval 3900 (38990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.003 - mae: 7.525 - mean_q: 12.787 - prob: 1.000\n",
            "\n",
            "Interval 3901 (39000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3902 (39010 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.391 - mean_q: 12.719 - prob: 1.000\n",
            "\n",
            "Interval 3903 (39020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.442 - mean_q: 12.999 - prob: 1.000\n",
            "\n",
            "Interval 3904 (39030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3905 (39040 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.193 - mean_q: 12.507 - prob: 1.000\n",
            "\n",
            "Interval 3906 (39050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.306 - mean_q: 12.655 - prob: 1.000\n",
            "\n",
            "Interval 3907 (39060 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 3908 (39070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.528 - mean_q: 12.903 - prob: 1.000\n",
            "\n",
            "Interval 3909 (39080 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 3910 (39090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.000 - mae: 7.312 - mean_q: 12.646 - prob: 1.000\n",
            "\n",
            "Interval 3911 (39100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.332 - mean_q: 12.685 - prob: 1.000\n",
            "\n",
            "Interval 3912 (39110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.328 - mean_q: 12.749 - prob: 1.000\n",
            "\n",
            "Interval 3913 (39120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3914 (39130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.397 - mean_q: 12.751 - prob: 1.000\n",
            "\n",
            "Interval 3915 (39140 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.001 - mae: 7.343 - mean_q: 12.625 - prob: 1.000\n",
            "\n",
            "Interval 3916 (39150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3917 (39160 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.000 - mae: 7.302 - mean_q: 12.569 - prob: 1.000\n",
            "\n",
            "Interval 3918 (39170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3919 (39180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.409 - mean_q: 12.937 - prob: 1.000\n",
            "\n",
            "Interval 3920 (39190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3921 (39200 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.266 - mean_q: 12.481 - prob: 1.000\n",
            "\n",
            "Interval 3922 (39210 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.355 - mean_q: 12.728 - prob: 1.000\n",
            "\n",
            "Interval 3923 (39220 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.643 - mean_q: 13.044 - prob: 1.000\n",
            "\n",
            "Interval 3924 (39230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3925 (39240 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.139 - mean_q: 12.389 - prob: 1.000\n",
            "\n",
            "Interval 3926 (39250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3927 (39260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.000 - mae: 6.997 - mean_q: 12.248 - prob: 1.000\n",
            "\n",
            "Interval 3928 (39270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3929 (39280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 7.000 [2.000, 12.000] - loss: 0.000 - mae: 7.132 - mean_q: 12.398 - prob: 1.000\n",
            "\n",
            "Interval 3930 (39290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3931 (39300 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.632 - mean_q: 13.077 - prob: 1.000\n",
            "\n",
            "Interval 3932 (39310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3933 (39320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.559 - mean_q: 13.065 - prob: 1.000\n",
            "\n",
            "Interval 3934 (39330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 3935 (39340 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.001 - mae: 7.138 - mean_q: 12.446 - prob: 1.000\n",
            "\n",
            "Interval 3936 (39350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.640 - mean_q: 13.264 - prob: 1.000\n",
            "\n",
            "Interval 3937 (39360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 3938 (39370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.499 - mean_q: 12.784 - prob: 1.000\n",
            "\n",
            "Interval 3939 (39380 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.589 - mean_q: 13.027 - prob: 1.000\n",
            "\n",
            "Interval 3940 (39390 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 3941 (39400 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -18.000 [-18.000, -18.000] - loss: 0.000 - mae: 7.390 - mean_q: 12.818 - prob: 1.000\n",
            "\n",
            "Interval 3942 (39410 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 3943 (39420 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -2.8000\n",
            "Interval 3944 (39430 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -19.000 [-19.000, -19.000] - loss: 0.000 - mae: 7.089 - mean_q: 12.497 - prob: 1.000\n",
            "\n",
            "Interval 3945 (39440 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 7.000 [2.000, 12.000] - loss: 0.001 - mae: 7.455 - mean_q: 12.824 - prob: 1.000\n",
            "\n",
            "Interval 3946 (39450 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 3947 (39460 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.534 - mean_q: 12.925 - prob: 1.000\n",
            "\n",
            "Interval 3948 (39470 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.002 - mae: 7.448 - mean_q: 12.863 - prob: 1.000\n",
            "\n",
            "Interval 3949 (39480 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.462 - mean_q: 12.799 - prob: 1.000\n",
            "\n",
            "Interval 3950 (39490 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 3951 (39500 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.586 - mean_q: 13.041 - prob: 1.000\n",
            "\n",
            "Interval 3952 (39510 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.305 - mean_q: 12.737 - prob: 1.000\n",
            "\n",
            "Interval 3953 (39520 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.523 - mean_q: 12.992 - prob: 1.000\n",
            "\n",
            "Interval 3954 (39530 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.592 - mean_q: 13.022 - prob: 1.000\n",
            "\n",
            "Interval 3955 (39540 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 3956 (39550 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.603 - mean_q: 12.982 - prob: 1.000\n",
            "\n",
            "Interval 3957 (39560 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 3958 (39570 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.581 - mean_q: 13.028 - prob: 1.000\n",
            "\n",
            "Interval 3959 (39580 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.523 - mean_q: 13.039 - prob: 1.000\n",
            "\n",
            "Interval 3960 (39590 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.454 - mean_q: 12.808 - prob: 1.000\n",
            "\n",
            "Interval 3961 (39600 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 3962 (39610 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.079 - mean_q: 12.510 - prob: 1.000\n",
            "\n",
            "Interval 3963 (39620 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.429 - mean_q: 12.868 - prob: 1.000\n",
            "\n",
            "Interval 3964 (39630 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 3965 (39640 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.184 - mean_q: 12.494 - prob: 1.000\n",
            "\n",
            "Interval 3966 (39650 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.099 - mean_q: 12.340 - prob: 1.000\n",
            "\n",
            "Interval 3967 (39660 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 3968 (39670 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.052 - mean_q: 12.249 - prob: 1.000\n",
            "\n",
            "Interval 3969 (39680 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.321 - mean_q: 12.814 - prob: 1.000\n",
            "\n",
            "Interval 3970 (39690 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.352 - mean_q: 12.686 - prob: 1.000\n",
            "\n",
            "Interval 3971 (39700 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.562 - mean_q: 13.095 - prob: 1.000\n",
            "\n",
            "Interval 3972 (39710 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -2.8000\n",
            "Interval 3973 (39720 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.000 - mae: 7.276 - mean_q: 12.613 - prob: 1.000\n",
            "\n",
            "Interval 3974 (39730 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.000 - mae: 7.321 - mean_q: 12.494 - prob: 1.000\n",
            "\n",
            "Interval 3975 (39740 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.405 - mean_q: 12.648 - prob: 1.000\n",
            "\n",
            "Interval 3976 (39750 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 3977 (39760 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.267 - mean_q: 12.557 - prob: 1.000\n",
            "\n",
            "Interval 3978 (39770 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.574 - mean_q: 12.953 - prob: 1.000\n",
            "\n",
            "Interval 3979 (39780 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.444 - mean_q: 12.775 - prob: 1.000\n",
            "\n",
            "Interval 3980 (39790 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 3981 (39800 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.002 - mae: 7.304 - mean_q: 12.648 - prob: 1.000\n",
            "\n",
            "Interval 3982 (39810 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.620 - mean_q: 13.069 - prob: 1.000\n",
            "\n",
            "Interval 3983 (39820 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 3984 (39830 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.222 - mean_q: 12.365 - prob: 1.000\n",
            "\n",
            "Interval 3985 (39840 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.082 - mean_q: 12.347 - prob: 1.000\n",
            "\n",
            "Interval 3986 (39850 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 3987 (39860 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.470 - mean_q: 12.788 - prob: 1.000\n",
            "\n",
            "Interval 3988 (39870 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.263 - mean_q: 12.398 - prob: 1.000\n",
            "\n",
            "Interval 3989 (39880 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.464 - mean_q: 12.747 - prob: 1.000\n",
            "\n",
            "Interval 3990 (39890 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 3991 (39900 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.520 - mean_q: 12.899 - prob: 1.000\n",
            "\n",
            "Interval 3992 (39910 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.529 - mean_q: 13.085 - prob: 1.000\n",
            "\n",
            "Interval 3993 (39920 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 3994 (39930 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.619 - mean_q: 13.011 - prob: 1.000\n",
            "\n",
            "Interval 3995 (39940 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.448 - mean_q: 12.859 - prob: 1.000\n",
            "\n",
            "Interval 3996 (39950 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 3997 (39960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.644 - mean_q: 13.127 - prob: 1.000\n",
            "\n",
            "Interval 3998 (39970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.260 - mean_q: 12.489 - prob: 1.000\n",
            "\n",
            "Interval 3999 (39980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.284 - mean_q: 12.647 - prob: 1.000\n",
            "\n",
            "Interval 4000 (39990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.287 - mean_q: 12.479 - prob: 1.000\n",
            "\n",
            "Interval 4001 (40000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.294 - mean_q: 12.605 - prob: 1.000\n",
            "\n",
            "Interval 4002 (40010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4003 (40020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.561 - mean_q: 13.013 - prob: 1.000\n",
            "\n",
            "Interval 4004 (40030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4005 (40040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.581 - mean_q: 13.135 - prob: 1.000\n",
            "\n",
            "Interval 4006 (40050 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 4007 (40060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -17.000 [-17.000, -17.000] - loss: 0.000 - mae: 7.307 - mean_q: 12.679 - prob: 1.000\n",
            "\n",
            "Interval 4008 (40070 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 6.914 - mean_q: 12.124 - prob: 1.000\n",
            "\n",
            "Interval 4009 (40080 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.084 - mean_q: 12.405 - prob: 1.000\n",
            "\n",
            "Interval 4010 (40090 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 4011 (40100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.553 - mean_q: 12.985 - prob: 1.000\n",
            "\n",
            "Interval 4012 (40110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 6.986 - mean_q: 12.338 - prob: 1.000\n",
            "\n",
            "Interval 4013 (40120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4014 (40130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.514 - mean_q: 12.861 - prob: 1.000\n",
            "\n",
            "Interval 4015 (40140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4016 (40150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -20.000 [-20.000, -20.000] - loss: 0.001 - mae: 7.597 - mean_q: 13.032 - prob: 1.000\n",
            "\n",
            "Interval 4017 (40160 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 4018 (40170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.437 - mean_q: 12.704 - prob: 1.000\n",
            "\n",
            "Interval 4019 (40180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -3.7000\n",
            "Interval 4020 (40190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -27.000 [-27.000, -27.000] - loss: 0.001 - mae: 7.521 - mean_q: 12.874 - prob: 1.000\n",
            "\n",
            "Interval 4021 (40200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4022 (40210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.229 - mean_q: 12.399 - prob: 1.000\n",
            "\n",
            "Interval 4023 (40220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 4024 (40230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -27.000 [-27.000, -27.000] - loss: 0.001 - mae: 6.996 - mean_q: 12.231 - prob: 1.000\n",
            "\n",
            "Interval 4025 (40240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.610 - mean_q: 13.145 - prob: 1.000\n",
            "\n",
            "Interval 4026 (40250 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 4027 (40260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.001 - mae: 7.377 - mean_q: 12.718 - prob: 1.000\n",
            "\n",
            "Interval 4028 (40270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.351 - mean_q: 12.514 - prob: 1.000\n",
            "\n",
            "Interval 4029 (40280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.443 - mean_q: 12.873 - prob: 1.000\n",
            "\n",
            "Interval 4030 (40290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.579 - mean_q: 12.812 - prob: 1.000\n",
            "\n",
            "Interval 4031 (40300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4032 (40310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.313 - mean_q: 12.636 - prob: 1.000\n",
            "\n",
            "Interval 4033 (40320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4034 (40330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.125 - mean_q: 12.475 - prob: 1.000\n",
            "\n",
            "Interval 4035 (40340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.526 - mean_q: 12.826 - prob: 1.000\n",
            "\n",
            "Interval 4036 (40350 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 4037 (40360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.452 - mean_q: 12.874 - prob: 1.000\n",
            "\n",
            "Interval 4038 (40370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.684 - mean_q: 13.111 - prob: 1.000\n",
            "\n",
            "Interval 4039 (40380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4040 (40390 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.422 - mean_q: 12.803 - prob: 1.000\n",
            "\n",
            "Interval 4041 (40400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.152 - mean_q: 12.323 - prob: 1.000\n",
            "\n",
            "Interval 4042 (40410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4043 (40420 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.577 - mean_q: 12.967 - prob: 1.000\n",
            "\n",
            "Interval 4044 (40430 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.375 - mean_q: 12.784 - prob: 1.000\n",
            "\n",
            "Interval 4045 (40440 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.379 - mean_q: 12.642 - prob: 1.000\n",
            "\n",
            "Interval 4046 (40450 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.222 - mean_q: 12.437 - prob: 1.000\n",
            "\n",
            "Interval 4047 (40460 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.524 - mean_q: 12.830 - prob: 1.000\n",
            "\n",
            "Interval 4048 (40470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4049 (40480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4050 (40490 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.147 - mean_q: 12.480 - prob: 1.000\n",
            "\n",
            "Interval 4051 (40500 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.203 - mean_q: 12.606 - prob: 1.000\n",
            "\n",
            "Interval 4052 (40510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.267 - mean_q: 12.467 - prob: 1.000\n",
            "\n",
            "Interval 4053 (40520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4054 (40530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.693 - mean_q: 13.140 - prob: 1.000\n",
            "\n",
            "Interval 4055 (40540 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 4056 (40550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.361 - mean_q: 12.832 - prob: 1.000\n",
            "\n",
            "Interval 4057 (40560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4058 (40570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.236 - mean_q: 12.541 - prob: 1.000\n",
            "\n",
            "Interval 4059 (40580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.383 - mean_q: 12.794 - prob: 1.000\n",
            "\n",
            "Interval 4060 (40590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4061 (40600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.276 - mean_q: 12.610 - prob: 1.000\n",
            "\n",
            "Interval 4062 (40610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.288 - mean_q: 12.529 - prob: 1.000\n",
            "\n",
            "Interval 4063 (40620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.282 - mean_q: 12.564 - prob: 1.000\n",
            "\n",
            "Interval 4064 (40630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4065 (40640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.354 - mean_q: 12.582 - prob: 1.000\n",
            "\n",
            "Interval 4066 (40650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.393 - mean_q: 12.783 - prob: 1.000\n",
            "\n",
            "Interval 4067 (40660 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 4068 (40670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.592 - mean_q: 13.093 - prob: 1.000\n",
            "\n",
            "Interval 4069 (40680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.455 - mean_q: 12.875 - prob: 1.000\n",
            "\n",
            "Interval 4070 (40690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4071 (40700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 6.966 - mean_q: 12.129 - prob: 1.000\n",
            "\n",
            "Interval 4072 (40710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.382 - mean_q: 12.848 - prob: 1.000\n",
            "\n",
            "Interval 4073 (40720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4074 (40730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.494 - mean_q: 12.811 - prob: 1.000\n",
            "\n",
            "Interval 4075 (40740 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 4076 (40750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.724 - mean_q: 13.202 - prob: 1.000\n",
            "\n",
            "Interval 4077 (40760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4078 (40770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 6.997 - mean_q: 12.138 - prob: 1.000\n",
            "\n",
            "Interval 4079 (40780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.101 - mean_q: 12.180 - prob: 1.000\n",
            "\n",
            "Interval 4080 (40790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.386 - mean_q: 12.712 - prob: 1.000\n",
            "\n",
            "Interval 4081 (40800 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 4082 (40810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 6.967 - mean_q: 12.296 - prob: 1.000\n",
            "\n",
            "Interval 4083 (40820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.345 - mean_q: 12.749 - prob: 1.000\n",
            "\n",
            "Interval 4084 (40830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4085 (40840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.164 - mean_q: 12.373 - prob: 1.000\n",
            "\n",
            "Interval 4086 (40850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.430 - mean_q: 12.731 - prob: 1.000\n",
            "\n",
            "Interval 4087 (40860 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 4088 (40870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.312 - mean_q: 12.524 - prob: 1.000\n",
            "\n",
            "Interval 4089 (40880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.361 - mean_q: 12.729 - prob: 1.000\n",
            "\n",
            "Interval 4090 (40890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.227 - mean_q: 12.652 - prob: 1.000\n",
            "\n",
            "Interval 4091 (40900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4092 (40910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.391 - mean_q: 12.724 - prob: 1.000\n",
            "\n",
            "Interval 4093 (40920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.771 - mean_q: 13.298 - prob: 1.000\n",
            "\n",
            "Interval 4094 (40930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4095 (40940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.096 - mean_q: 12.322 - prob: 1.000\n",
            "\n",
            "Interval 4096 (40950 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.418 - mean_q: 12.822 - prob: 1.000\n",
            "\n",
            "Interval 4097 (40960 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 4098 (40970 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.108 - mean_q: 12.339 - prob: 1.000\n",
            "\n",
            "Interval 4099 (40980 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.185 - mean_q: 12.481 - prob: 1.000\n",
            "\n",
            "Interval 4100 (40990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4101 (41000 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.450 - mean_q: 12.735 - prob: 1.000\n",
            "\n",
            "Interval 4102 (41010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4103 (41020 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.252 - mean_q: 12.578 - prob: 1.000\n",
            "\n",
            "Interval 4104 (41030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.072 - mean_q: 12.349 - prob: 1.000\n",
            "\n",
            "Interval 4105 (41040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.297 - mean_q: 12.559 - prob: 1.000\n",
            "\n",
            "Interval 4106 (41050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4107 (41060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.000 - mae: 7.670 - mean_q: 13.228 - prob: 1.000\n",
            "\n",
            "Interval 4108 (41070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4109 (41080 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.000 - mae: 7.429 - mean_q: 12.799 - prob: 1.000\n",
            "\n",
            "Interval 4110 (41090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4111 (41100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.025 - mean_q: 12.251 - prob: 1.000\n",
            "\n",
            "Interval 4112 (41110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.000 - mae: 7.364 - mean_q: 12.719 - prob: 1.000\n",
            "\n",
            "Interval 4113 (41120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4114 (41130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.288 - mean_q: 12.711 - prob: 1.000\n",
            "\n",
            "Interval 4115 (41140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.143 - mean_q: 12.277 - prob: 1.000\n",
            "\n",
            "Interval 4116 (41150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4117 (41160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.361 - mean_q: 12.732 - prob: 1.000\n",
            "\n",
            "Interval 4118 (41170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.620 - mean_q: 13.105 - prob: 1.000\n",
            "\n",
            "Interval 4119 (41180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.311 - mean_q: 12.579 - prob: 1.000\n",
            "\n",
            "Interval 4120 (41190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.374 - mean_q: 12.700 - prob: 1.000\n",
            "\n",
            "Interval 4121 (41200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4122 (41210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.001 - mae: 7.271 - mean_q: 12.642 - prob: 1.000\n",
            "\n",
            "Interval 4123 (41220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4124 (41230 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.000 - mae: 7.352 - mean_q: 12.782 - prob: 1.000\n",
            "\n",
            "Interval 4125 (41240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4126 (41250 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.479 - mean_q: 12.944 - prob: 1.000\n",
            "\n",
            "Interval 4127 (41260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.545 - mean_q: 12.924 - prob: 1.000\n",
            "\n",
            "Interval 4128 (41270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 4129 (41280 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -22.000 [-22.000, -22.000] - loss: 0.001 - mae: 7.109 - mean_q: 12.304 - prob: 1.000\n",
            "\n",
            "Interval 4130 (41290 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.594 - mean_q: 13.032 - prob: 1.000\n",
            "\n",
            "Interval 4131 (41300 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 4132 (41310 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.368 - mean_q: 12.589 - prob: 1.000\n",
            "\n",
            "Interval 4133 (41320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.203 - mean_q: 12.620 - prob: 1.000\n",
            "\n",
            "Interval 4134 (41330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4135 (41340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.207 - mean_q: 12.459 - prob: 1.000\n",
            "\n",
            "Interval 4136 (41350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4137 (41360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.468 - mean_q: 12.891 - prob: 1.000\n",
            "\n",
            "Interval 4138 (41370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.540 - mean_q: 12.943 - prob: 1.000\n",
            "\n",
            "Interval 4139 (41380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4140 (41390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 6.857 - mean_q: 11.941 - prob: 1.000\n",
            "\n",
            "Interval 4141 (41400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.106 - mean_q: 12.442 - prob: 1.000\n",
            "\n",
            "Interval 4142 (41410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4143 (41420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.477 - mean_q: 12.724 - prob: 1.000\n",
            "\n",
            "Interval 4144 (41430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4145 (41440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.469 - mean_q: 12.828 - prob: 1.000\n",
            "\n",
            "Interval 4146 (41450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.289 - mean_q: 12.419 - prob: 1.000\n",
            "\n",
            "Interval 4147 (41460 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.305 - mean_q: 12.645 - prob: 1.000\n",
            "\n",
            "Interval 4148 (41470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4149 (41480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.267 - mean_q: 12.632 - prob: 1.000\n",
            "\n",
            "Interval 4150 (41490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.384 - mean_q: 12.746 - prob: 1.000\n",
            "\n",
            "Interval 4151 (41500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.217 - mean_q: 12.469 - prob: 1.000\n",
            "\n",
            "Interval 4152 (41510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.226 - mean_q: 12.544 - prob: 1.000\n",
            "\n",
            "Interval 4153 (41520 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.585 - mean_q: 12.937 - prob: 1.000\n",
            "\n",
            "Interval 4154 (41530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 4155 (41540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -17.000 [-17.000, -17.000] - loss: 0.000 - mae: 7.551 - mean_q: 12.957 - prob: 1.000\n",
            "\n",
            "Interval 4156 (41550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4157 (41560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.485 - mean_q: 12.889 - prob: 1.000\n",
            "\n",
            "Interval 4158 (41570 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 4159 (41580 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.001 - mae: 7.465 - mean_q: 12.934 - prob: 1.000\n",
            "\n",
            "Interval 4160 (41590 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.093 - mean_q: 12.371 - prob: 1.000\n",
            "\n",
            "Interval 4161 (41600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.594 - mean_q: 12.922 - prob: 1.000\n",
            "\n",
            "Interval 4162 (41610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4163 (41620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.179 - mean_q: 12.555 - prob: 1.000\n",
            "\n",
            "Interval 4164 (41630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.206 - mean_q: 12.426 - prob: 1.000\n",
            "\n",
            "Interval 4165 (41640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.408 - mean_q: 12.769 - prob: 1.000\n",
            "\n",
            "Interval 4166 (41650 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 4167 (41660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.370 - mean_q: 12.717 - prob: 1.000\n",
            "\n",
            "Interval 4168 (41670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 11.500 [11.000, 12.000] - loss: 0.000 - mae: 7.429 - mean_q: 12.887 - prob: 1.000\n",
            "\n",
            "Interval 4169 (41680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4170 (41690 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 4171 (41700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -19.000 [-19.000, -19.000] - loss: 0.001 - mae: 7.399 - mean_q: 12.750 - prob: 1.000\n",
            "\n",
            "Interval 4172 (41710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.337 - mean_q: 12.751 - prob: 1.000\n",
            "\n",
            "Interval 4173 (41720 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 4174 (41730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.234 - mean_q: 12.493 - prob: 1.000\n",
            "\n",
            "Interval 4175 (41740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.437 - mean_q: 12.843 - prob: 1.000\n",
            "\n",
            "Interval 4176 (41750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4177 (41760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.370 - mean_q: 12.752 - prob: 1.000\n",
            "\n",
            "Interval 4178 (41770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.399 - mean_q: 12.807 - prob: 1.000\n",
            "\n",
            "Interval 4179 (41780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.536 - mean_q: 12.877 - prob: 1.000\n",
            "\n",
            "Interval 4180 (41790 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 6.957 - mean_q: 12.078 - prob: 1.000\n",
            "\n",
            "Interval 4181 (41800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4182 (41810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.590 - mean_q: 13.003 - prob: 1.000\n",
            "\n",
            "Interval 4183 (41820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4184 (41830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.393 - mean_q: 12.692 - prob: 1.000\n",
            "\n",
            "Interval 4185 (41840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.301 - mean_q: 12.655 - prob: 1.000\n",
            "\n",
            "Interval 4186 (41850 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 4187 (41860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.615 - mean_q: 13.137 - prob: 1.000\n",
            "\n",
            "Interval 4188 (41870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.722 - mean_q: 13.181 - prob: 1.000\n",
            "\n",
            "Interval 4189 (41880 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 4190 (41890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.364 - mean_q: 12.689 - prob: 1.000\n",
            "\n",
            "Interval 4191 (41900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4192 (41910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.275 - mean_q: 12.704 - prob: 1.000\n",
            "\n",
            "Interval 4193 (41920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.331 - mean_q: 12.679 - prob: 1.000\n",
            "\n",
            "Interval 4194 (41930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.000 - mae: 7.592 - mean_q: 12.938 - prob: 1.000\n",
            "\n",
            "Interval 4195 (41940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4196 (41950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.193 - mean_q: 12.407 - prob: 1.000\n",
            "\n",
            "Interval 4197 (41960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.483 - mean_q: 12.901 - prob: 1.000\n",
            "\n",
            "Interval 4198 (41970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4199 (41980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.000 - mae: 7.594 - mean_q: 12.965 - prob: 1.000\n",
            "\n",
            "Interval 4200 (41990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.344 - mean_q: 12.638 - prob: 1.000\n",
            "\n",
            "Interval 4201 (42000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4202 (42010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.559 - mean_q: 12.980 - prob: 1.000\n",
            "\n",
            "Interval 4203 (42020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.322 - mean_q: 12.708 - prob: 1.000\n",
            "\n",
            "Interval 4204 (42030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4205 (42040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.454 - mean_q: 12.913 - prob: 1.000\n",
            "\n",
            "Interval 4206 (42050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.052 - mean_q: 12.212 - prob: 1.000\n",
            "\n",
            "Interval 4207 (42060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4208 (42070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.276 - mean_q: 12.508 - prob: 1.000\n",
            "\n",
            "Interval 4209 (42080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.120 - mean_q: 12.318 - prob: 1.000\n",
            "\n",
            "Interval 4210 (42090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.473 - mean_q: 12.817 - prob: 1.000\n",
            "\n",
            "Interval 4211 (42100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.678 - mean_q: 13.066 - prob: 1.000\n",
            "\n",
            "Interval 4212 (42110 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 4213 (42120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.426 - mean_q: 12.834 - prob: 1.000\n",
            "\n",
            "Interval 4214 (42130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.001 - mae: 7.688 - mean_q: 13.110 - prob: 1.000\n",
            "\n",
            "Interval 4215 (42140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4216 (42150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.322 - mean_q: 12.702 - prob: 1.000\n",
            "\n",
            "Interval 4217 (42160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4218 (42170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.641 - mean_q: 12.935 - prob: 1.000\n",
            "\n",
            "Interval 4219 (42180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4220 (42190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.280 - mean_q: 12.566 - prob: 1.000\n",
            "\n",
            "Interval 4221 (42200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.302 - mean_q: 12.426 - prob: 1.000\n",
            "\n",
            "Interval 4222 (42210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.514 - mean_q: 12.908 - prob: 1.000\n",
            "\n",
            "Interval 4223 (42220 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 4224 (42230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.000 - mae: 7.421 - mean_q: 12.812 - prob: 1.000\n",
            "\n",
            "Interval 4225 (42240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4226 (42250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.246 - mean_q: 12.668 - prob: 1.000\n",
            "\n",
            "Interval 4227 (42260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.036 - mean_q: 12.354 - prob: 1.000\n",
            "\n",
            "Interval 4228 (42270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4229 (42280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.363 - mean_q: 12.703 - prob: 1.000\n",
            "\n",
            "Interval 4230 (42290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.002 - mean_q: 12.080 - prob: 1.000\n",
            "\n",
            "Interval 4231 (42300 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.000 - mae: 7.211 - mean_q: 12.502 - prob: 1.000\n",
            "\n",
            "Interval 4232 (42310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.500 - mean_q: 13.113 - prob: 1.000\n",
            "\n",
            "Interval 4233 (42320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4234 (42330 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.106 - mean_q: 12.383 - prob: 1.000\n",
            "\n",
            "Interval 4235 (42340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.628 - mean_q: 13.110 - prob: 1.000\n",
            "\n",
            "Interval 4236 (42350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4237 (42360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.452 - mean_q: 12.906 - prob: 1.000\n",
            "\n",
            "Interval 4238 (42370 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 4239 (42380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.130 - mean_q: 12.333 - prob: 1.000\n",
            "\n",
            "Interval 4240 (42390 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.327 - mean_q: 12.613 - prob: 1.000\n",
            "\n",
            "Interval 4241 (42400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4242 (42410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.578 - mean_q: 13.015 - prob: 1.000\n",
            "\n",
            "Interval 4243 (42420 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.285 - mean_q: 12.600 - prob: 1.000\n",
            "\n",
            "Interval 4244 (42430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4245 (42440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.001 - mae: 7.035 - mean_q: 12.215 - prob: 1.000\n",
            "\n",
            "Interval 4246 (42450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4247 (42460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 6.959 - mean_q: 12.266 - prob: 1.000\n",
            "\n",
            "Interval 4248 (42470 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.567 - mean_q: 13.025 - prob: 1.000\n",
            "\n",
            "Interval 4249 (42480 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.499 - mean_q: 12.918 - prob: 1.000\n",
            "\n",
            "Interval 4250 (42490 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 4251 (42500 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.098 - mean_q: 12.361 - prob: 1.000\n",
            "\n",
            "Interval 4252 (42510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.244 - mean_q: 12.648 - prob: 1.000\n",
            "\n",
            "Interval 4253 (42520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4254 (42530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 6.936 - mean_q: 12.200 - prob: 1.000\n",
            "\n",
            "Interval 4255 (42540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.596 - mean_q: 13.083 - prob: 1.000\n",
            "\n",
            "Interval 4256 (42550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4257 (42560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.458 - mean_q: 12.823 - prob: 1.000\n",
            "\n",
            "Interval 4258 (42570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4259 (42580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.517 - mean_q: 12.992 - prob: 1.000\n",
            "\n",
            "Interval 4260 (42590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4261 (42600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.468 - mean_q: 12.787 - prob: 1.000\n",
            "\n",
            "Interval 4262 (42610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4263 (42620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.546 - mean_q: 13.082 - prob: 1.000\n",
            "\n",
            "Interval 4264 (42630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4265 (42640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.292 - mean_q: 12.527 - prob: 1.000\n",
            "\n",
            "Interval 4266 (42650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.157 - mean_q: 12.459 - prob: 1.000\n",
            "\n",
            "Interval 4267 (42660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.154 - mean_q: 12.485 - prob: 1.000\n",
            "\n",
            "Interval 4268 (42670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.434 - mean_q: 12.768 - prob: 1.000\n",
            "\n",
            "Interval 4269 (42680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4270 (42690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.477 - mean_q: 12.977 - prob: 1.000\n",
            "\n",
            "Interval 4271 (42700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.574 - mean_q: 13.033 - prob: 1.000\n",
            "\n",
            "Interval 4272 (42710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.191 - mean_q: 12.452 - prob: 1.000\n",
            "\n",
            "Interval 4273 (42720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4274 (42730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.175 - mean_q: 12.380 - prob: 1.000\n",
            "\n",
            "Interval 4275 (42740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.018 - mean_q: 12.170 - prob: 1.000\n",
            "\n",
            "Interval 4276 (42750 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.230 - mean_q: 12.402 - prob: 1.000\n",
            "\n",
            "Interval 4277 (42760 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 4278 (42770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.496 - mean_q: 12.903 - prob: 1.000\n",
            "\n",
            "Interval 4279 (42780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.506 - mean_q: 12.856 - prob: 1.000\n",
            "\n",
            "Interval 4280 (42790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4281 (42800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.000 - mae: 7.141 - mean_q: 12.483 - prob: 1.000\n",
            "\n",
            "Interval 4282 (42810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4283 (42820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.637 - mean_q: 13.205 - prob: 1.000\n",
            "\n",
            "Interval 4284 (42830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4285 (42840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.500 - mean_q: 12.871 - prob: 1.000\n",
            "\n",
            "Interval 4286 (42850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.089 - mean_q: 12.323 - prob: 1.000\n",
            "\n",
            "Interval 4287 (42860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.115 - mean_q: 12.354 - prob: 1.000\n",
            "\n",
            "Interval 4288 (42870 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.001 - mae: 7.320 - mean_q: 12.557 - prob: 1.000\n",
            "\n",
            "Interval 4289 (42880 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.468 - mean_q: 12.901 - prob: 1.000\n",
            "\n",
            "Interval 4290 (42890 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.218 - mean_q: 12.586 - prob: 1.000\n",
            "\n",
            "Interval 4291 (42900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 6.852 - mean_q: 11.935 - prob: 1.000\n",
            "\n",
            "Interval 4292 (42910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4293 (42920 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.537 - mean_q: 12.789 - prob: 1.000\n",
            "\n",
            "Interval 4294 (42930 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 4295 (42940 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.438 - mean_q: 12.804 - prob: 1.000\n",
            "\n",
            "Interval 4296 (42950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.106 - mean_q: 12.372 - prob: 1.000\n",
            "\n",
            "Interval 4297 (42960 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 4298 (42970 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.270 - mean_q: 12.641 - prob: 1.000\n",
            "\n",
            "Interval 4299 (42980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4300 (42990 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.661 - mean_q: 13.157 - prob: 1.000\n",
            "\n",
            "Interval 4301 (43000 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.494 - mean_q: 12.742 - prob: 1.000\n",
            "\n",
            "Interval 4302 (43010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4303 (43020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.230 - mean_q: 12.484 - prob: 1.000\n",
            "\n",
            "Interval 4304 (43030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.000 - mae: 7.216 - mean_q: 12.606 - prob: 1.000\n",
            "\n",
            "Interval 4305 (43040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4306 (43050 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.158 - mean_q: 12.366 - prob: 1.000\n",
            "\n",
            "Interval 4307 (43060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.380 - mean_q: 12.690 - prob: 1.000\n",
            "\n",
            "Interval 4308 (43070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.001 - mae: 7.208 - mean_q: 12.375 - prob: 1.000\n",
            "\n",
            "Interval 4309 (43080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4310 (43090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.534 - mean_q: 12.946 - prob: 1.000\n",
            "\n",
            "Interval 4311 (43100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.595 - mean_q: 12.938 - prob: 1.000\n",
            "\n",
            "Interval 4312 (43110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.539 - mean_q: 12.851 - prob: 1.000\n",
            "\n",
            "Interval 4313 (43120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4314 (43130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.000 - mae: 7.018 - mean_q: 12.307 - prob: 1.000\n",
            "\n",
            "Interval 4315 (43140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4316 (43150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.000 - mae: 7.444 - mean_q: 12.783 - prob: 1.000\n",
            "\n",
            "Interval 4317 (43160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 4318 (43170 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.000 - mae: 7.214 - mean_q: 12.314 - prob: 1.000\n",
            "\n",
            "Interval 4319 (43180 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.306 - mean_q: 12.639 - prob: 1.000\n",
            "\n",
            "Interval 4320 (43190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4321 (43200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.632 - mean_q: 12.997 - prob: 1.000\n",
            "\n",
            "Interval 4322 (43210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.552 - mean_q: 13.026 - prob: 1.000\n",
            "\n",
            "Interval 4323 (43220 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 4324 (43230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.204 - mean_q: 12.440 - prob: 1.000\n",
            "\n",
            "Interval 4325 (43240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.534 - mean_q: 12.976 - prob: 1.000\n",
            "\n",
            "Interval 4326 (43250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4327 (43260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.334 - mean_q: 12.686 - prob: 1.000\n",
            "\n",
            "Interval 4328 (43270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.433 - mean_q: 12.854 - prob: 1.000\n",
            "\n",
            "Interval 4329 (43280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4330 (43290 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.347 - mean_q: 12.717 - prob: 1.000\n",
            "\n",
            "Interval 4331 (43300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.002 - mae: 7.460 - mean_q: 12.917 - prob: 1.000\n",
            "\n",
            "Interval 4332 (43310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.411 - mean_q: 12.743 - prob: 1.000\n",
            "\n",
            "Interval 4333 (43320 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 4334 (43330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.001 - mae: 7.039 - mean_q: 12.225 - prob: 1.000\n",
            "\n",
            "Interval 4335 (43340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4336 (43350 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.409 - mean_q: 12.838 - prob: 1.000\n",
            "\n",
            "Interval 4337 (43360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.433 - mean_q: 12.723 - prob: 1.000\n",
            "\n",
            "Interval 4338 (43370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4339 (43380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.168 - mean_q: 12.423 - prob: 1.000\n",
            "\n",
            "Interval 4340 (43390 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.282 - mean_q: 12.617 - prob: 1.000\n",
            "\n",
            "Interval 4341 (43400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4342 (43410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.000 - mae: 7.530 - mean_q: 13.144 - prob: 1.000\n",
            "\n",
            "Interval 4343 (43420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.000 - mae: 6.945 - mean_q: 12.292 - prob: 1.000\n",
            "\n",
            "Interval 4344 (43430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.640 - mean_q: 12.967 - prob: 1.000\n",
            "\n",
            "Interval 4345 (43440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4346 (43450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.520 - mean_q: 12.988 - prob: 1.000\n",
            "\n",
            "Interval 4347 (43460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4348 (43470 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.399 - mean_q: 12.898 - prob: 1.000\n",
            "\n",
            "Interval 4349 (43480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.056 - mean_q: 12.120 - prob: 1.000\n",
            "\n",
            "Interval 4350 (43490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.001 - mae: 7.426 - mean_q: 12.734 - prob: 1.000\n",
            "\n",
            "Interval 4351 (43500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.313 - mean_q: 12.595 - prob: 1.000\n",
            "\n",
            "Interval 4352 (43510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4353 (43520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.398 - mean_q: 12.793 - prob: 1.000\n",
            "\n",
            "Interval 4354 (43530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4355 (43540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.220 - mean_q: 12.539 - prob: 1.000\n",
            "\n",
            "Interval 4356 (43550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4357 (43560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.433 - mean_q: 12.863 - prob: 1.000\n",
            "\n",
            "Interval 4358 (43570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4359 (43580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.163 - mean_q: 12.496 - prob: 1.000\n",
            "\n",
            "Interval 4360 (43590 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.754 - mean_q: 13.245 - prob: 1.000\n",
            "\n",
            "Interval 4361 (43600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4362 (43610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.235 - mean_q: 12.526 - prob: 1.000\n",
            "\n",
            "Interval 4363 (43620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.512 - mean_q: 12.952 - prob: 1.000\n",
            "\n",
            "Interval 4364 (43630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4365 (43640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.001 - mae: 7.168 - mean_q: 12.554 - prob: 1.000\n",
            "\n",
            "Interval 4366 (43650 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 4367 (43660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.347 - mean_q: 12.498 - prob: 1.000\n",
            "\n",
            "Interval 4368 (43670 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.423 - mean_q: 12.880 - prob: 1.000\n",
            "\n",
            "Interval 4369 (43680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.001 - mae: 7.253 - mean_q: 12.454 - prob: 1.000\n",
            "\n",
            "Interval 4370 (43690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4371 (43700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.181 - mean_q: 12.648 - prob: 1.000\n",
            "\n",
            "Interval 4372 (43710 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.078 - mean_q: 12.249 - prob: 1.000\n",
            "\n",
            "Interval 4373 (43720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.152 - mean_q: 12.496 - prob: 1.000\n",
            "\n",
            "Interval 4374 (43730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4375 (43740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.300 - mean_q: 12.641 - prob: 1.000\n",
            "\n",
            "Interval 4376 (43750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.262 - mean_q: 12.545 - prob: 1.000\n",
            "\n",
            "Interval 4377 (43760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4378 (43770 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.378 - mean_q: 12.667 - prob: 1.000\n",
            "\n",
            "Interval 4379 (43780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.177 - mean_q: 12.413 - prob: 1.000\n",
            "\n",
            "Interval 4380 (43790 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 4381 (43800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.298 - mean_q: 12.672 - prob: 1.000\n",
            "\n",
            "Interval 4382 (43810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.258 - mean_q: 12.633 - prob: 1.000\n",
            "\n",
            "Interval 4383 (43820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 4384 (43830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.001 - mae: 7.780 - mean_q: 13.173 - prob: 1.000\n",
            "\n",
            "Interval 4385 (43840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.192 - mean_q: 12.442 - prob: 1.000\n",
            "\n",
            "Interval 4386 (43850 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.799 - mean_q: 13.232 - prob: 1.000\n",
            "\n",
            "Interval 4387 (43860 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.654 - mean_q: 13.188 - prob: 1.000\n",
            "\n",
            "Interval 4388 (43870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4389 (43880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.693 - mean_q: 13.224 - prob: 1.000\n",
            "\n",
            "Interval 4390 (43890 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.789 - mean_q: 13.270 - prob: 1.000\n",
            "\n",
            "Interval 4391 (43900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4392 (43910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.001 - mae: 7.163 - mean_q: 12.361 - prob: 1.000\n",
            "\n",
            "Interval 4393 (43920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.361 - mean_q: 12.593 - prob: 1.000\n",
            "\n",
            "Interval 4394 (43930 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 4395 (43940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.390 - mean_q: 12.900 - prob: 1.000\n",
            "\n",
            "Interval 4396 (43950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.610 - mean_q: 13.153 - prob: 1.000\n",
            "\n",
            "Interval 4397 (43960 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.557 - mean_q: 13.142 - prob: 1.000\n",
            "\n",
            "Interval 4398 (43970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4399 (43980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.215 - mean_q: 12.515 - prob: 1.000\n",
            "\n",
            "Interval 4400 (43990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4401 (44000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.389 - mean_q: 12.633 - prob: 1.000\n",
            "\n",
            "Interval 4402 (44010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4403 (44020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.201 - mean_q: 12.624 - prob: 1.000\n",
            "\n",
            "Interval 4404 (44030 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 4405 (44040 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.298 - mean_q: 12.622 - prob: 1.000\n",
            "\n",
            "Interval 4406 (44050 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.241 - mean_q: 12.527 - prob: 1.000\n",
            "\n",
            "Interval 4407 (44060 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 4408 (44070 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.280 - mean_q: 12.533 - prob: 1.000\n",
            "\n",
            "Interval 4409 (44080 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.288 - mean_q: 12.579 - prob: 1.000\n",
            "\n",
            "Interval 4410 (44090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4411 (44100 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.282 - mean_q: 12.667 - prob: 1.000\n",
            "\n",
            "Interval 4412 (44110 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.194 - mean_q: 12.424 - prob: 1.000\n",
            "\n",
            "Interval 4413 (44120 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.348 - mean_q: 12.569 - prob: 1.000\n",
            "\n",
            "Interval 4414 (44130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.573 - mean_q: 13.011 - prob: 1.000\n",
            "\n",
            "Interval 4415 (44140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4416 (44150 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.671 - mean_q: 13.170 - prob: 1.000\n",
            "\n",
            "Interval 4417 (44160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.443 - mean_q: 12.783 - prob: 1.000\n",
            "\n",
            "Interval 4418 (44170 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 4419 (44180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.318 - mean_q: 12.730 - prob: 1.000\n",
            "\n",
            "Interval 4420 (44190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4421 (44200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.002 - mae: 7.514 - mean_q: 12.994 - prob: 1.000\n",
            "\n",
            "Interval 4422 (44210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.291 - mean_q: 12.619 - prob: 1.000\n",
            "\n",
            "Interval 4423 (44220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.705 - mean_q: 13.194 - prob: 1.000\n",
            "\n",
            "Interval 4424 (44230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.001 - mae: 7.685 - mean_q: 13.229 - prob: 1.000\n",
            "\n",
            "Interval 4425 (44240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4426 (44250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.337 - mean_q: 12.690 - prob: 1.000\n",
            "\n",
            "Interval 4427 (44260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.414 - mean_q: 12.744 - prob: 1.000\n",
            "\n",
            "Interval 4428 (44270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4429 (44280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.296 - mean_q: 12.756 - prob: 1.000\n",
            "\n",
            "Interval 4430 (44290 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.166 - mean_q: 12.536 - prob: 1.000\n",
            "\n",
            "Interval 4431 (44300 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.0000\n",
            "Interval 4432 (44310 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.374 - mean_q: 12.715 - prob: 1.000\n",
            "\n",
            "Interval 4433 (44320 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 4434 (44330 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 6.916 - mean_q: 12.223 - prob: 1.000\n",
            "\n",
            "Interval 4435 (44340 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.107 - mean_q: 12.448 - prob: 1.000\n",
            "\n",
            "Interval 4436 (44350 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 4437 (44360 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.189 - mean_q: 12.522 - prob: 1.000\n",
            "\n",
            "Interval 4438 (44370 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.297 - mean_q: 12.464 - prob: 1.000\n",
            "\n",
            "Interval 4439 (44380 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 4440 (44390 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.342 - mean_q: 12.562 - prob: 1.000\n",
            "\n",
            "Interval 4441 (44400 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.200 - mean_q: 12.567 - prob: 1.000\n",
            "\n",
            "Interval 4442 (44410 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.198 - mean_q: 12.506 - prob: 1.000\n",
            "\n",
            "Interval 4443 (44420 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 4444 (44430 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.512 - mean_q: 13.028 - prob: 1.000\n",
            "\n",
            "Interval 4445 (44440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4446 (44450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.522 - mean_q: 12.933 - prob: 1.000\n",
            "\n",
            "Interval 4447 (44460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.637 - mean_q: 13.166 - prob: 1.000\n",
            "\n",
            "Interval 4448 (44470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.614 - mean_q: 13.008 - prob: 1.000\n",
            "\n",
            "Interval 4449 (44480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4450 (44490 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.466 - mean_q: 13.051 - prob: 1.000\n",
            "\n",
            "Interval 4451 (44500 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.303 - mean_q: 12.615 - prob: 1.000\n",
            "\n",
            "Interval 4452 (44510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4453 (44520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.208 - mean_q: 12.501 - prob: 1.000\n",
            "\n",
            "Interval 4454 (44530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.662 - mean_q: 13.046 - prob: 1.000\n",
            "\n",
            "Interval 4455 (44540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4456 (44550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -29.000 [-29.000, -29.000] - loss: 0.001 - mae: 7.536 - mean_q: 12.886 - prob: 1.000\n",
            "\n",
            "Interval 4457 (44560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4458 (44570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.387 - mean_q: 12.855 - prob: 1.000\n",
            "\n",
            "Interval 4459 (44580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.740 - mean_q: 13.215 - prob: 1.000\n",
            "\n",
            "Interval 4460 (44590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4461 (44600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.352 - mean_q: 12.544 - prob: 1.000\n",
            "\n",
            "Interval 4462 (44610 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.203 - mean_q: 12.443 - prob: 1.000\n",
            "\n",
            "Interval 4463 (44620 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.489 - mean_q: 12.920 - prob: 1.000\n",
            "\n",
            "Interval 4464 (44630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4465 (44640 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.465 - mean_q: 12.842 - prob: 1.000\n",
            "\n",
            "Interval 4466 (44650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4467 (44660 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.436 - mean_q: 12.720 - prob: 1.000\n",
            "\n",
            "Interval 4468 (44670 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.238 - mean_q: 12.649 - prob: 1.000\n",
            "\n",
            "Interval 4469 (44680 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 4470 (44690 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.513 - mean_q: 12.959 - prob: 1.000\n",
            "\n",
            "Interval 4471 (44700 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.489 - mean_q: 12.990 - prob: 1.000\n",
            "\n",
            "Interval 4472 (44710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4473 (44720 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 4474 (44730 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.000 - mae: 7.102 - mean_q: 12.263 - prob: 1.000\n",
            "\n",
            "Interval 4475 (44740 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -24.000 [-24.000, -24.000] - loss: 0.000 - mae: 7.340 - mean_q: 12.573 - prob: 1.000\n",
            "\n",
            "Interval 4476 (44750 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 4477 (44760 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.317 - mean_q: 12.713 - prob: 1.000\n",
            "\n",
            "Interval 4478 (44770 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 4479 (44780 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.043 - mean_q: 12.278 - prob: 1.000\n",
            "\n",
            "Interval 4480 (44790 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.9000\n",
            "Interval 4481 (44800 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.813 - mean_q: 13.291 - prob: 1.000\n",
            "\n",
            "Interval 4482 (44810 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.382 - mean_q: 12.739 - prob: 1.000\n",
            "\n",
            "Interval 4483 (44820 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 4484 (44830 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.377 - mean_q: 12.671 - prob: 1.000\n",
            "\n",
            "Interval 4485 (44840 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 4486 (44850 steps performed)\n",
            "10/10 [==============================] - 0s 14ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.162 - mean_q: 12.402 - prob: 1.000\n",
            "\n",
            "Interval 4487 (44860 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.613 - mean_q: 13.077 - prob: 1.000\n",
            "\n",
            "Interval 4488 (44870 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 4489 (44880 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.264 - mean_q: 12.489 - prob: 1.000\n",
            "\n",
            "Interval 4490 (44890 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.610 - mean_q: 13.010 - prob: 1.000\n",
            "\n",
            "Interval 4491 (44900 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.001 - mae: 7.247 - mean_q: 12.572 - prob: 1.000\n",
            "\n",
            "Interval 4492 (44910 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 4493 (44920 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 1.4000\n",
            "2 episodes - episode_reward: 2.000 [-5.000, 9.000] - loss: 0.001 - mae: 7.339 - mean_q: 12.504 - prob: 1.000\n",
            "\n",
            "Interval 4494 (44930 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 4495 (44940 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.587 - mean_q: 12.985 - prob: 1.000\n",
            "\n",
            "Interval 4496 (44950 steps performed)\n",
            "10/10 [==============================] - 0s 15ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.459 - mean_q: 12.892 - prob: 1.000\n",
            "\n",
            "Interval 4497 (44960 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 4498 (44970 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.393 - mean_q: 12.778 - prob: 1.000\n",
            "\n",
            "Interval 4499 (44980 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.211 - mean_q: 12.392 - prob: 1.000\n",
            "\n",
            "Interval 4500 (44990 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.000 - mae: 7.303 - mean_q: 12.741 - prob: 1.000\n",
            "\n",
            "Interval 4501 (45000 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 4502 (45010 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.000 - mae: 7.386 - mean_q: 12.790 - prob: 1.000\n",
            "\n",
            "Interval 4503 (45020 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 4504 (45030 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.621 - mean_q: 13.172 - prob: 1.000\n",
            "\n",
            "Interval 4505 (45040 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.553 - mean_q: 13.094 - prob: 1.000\n",
            "\n",
            "Interval 4506 (45050 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.248 - mean_q: 12.454 - prob: 1.000\n",
            "\n",
            "Interval 4507 (45060 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 4508 (45070 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.650 - mean_q: 13.075 - prob: 1.000\n",
            "\n",
            "Interval 4509 (45080 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.497 - mean_q: 13.020 - prob: 1.000\n",
            "\n",
            "Interval 4510 (45090 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 4511 (45100 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.285 - mean_q: 12.699 - prob: 1.000\n",
            "\n",
            "Interval 4512 (45110 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 4513 (45120 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.394 - mean_q: 12.690 - prob: 1.000\n",
            "\n",
            "Interval 4514 (45130 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.025 - mean_q: 12.192 - prob: 1.000\n",
            "\n",
            "Interval 4515 (45140 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 4516 (45150 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.640 - mean_q: 13.091 - prob: 1.000\n",
            "\n",
            "Interval 4517 (45160 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.336 - mean_q: 12.668 - prob: 1.000\n",
            "\n",
            "Interval 4518 (45170 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 4519 (45180 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.356 - mean_q: 12.759 - prob: 1.000\n",
            "\n",
            "Interval 4520 (45190 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 6.912 - mean_q: 12.010 - prob: 1.000\n",
            "\n",
            "Interval 4521 (45200 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.474 - mean_q: 12.932 - prob: 1.000\n",
            "\n",
            "Interval 4522 (45210 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.395 - mean_q: 12.602 - prob: 1.000\n",
            "\n",
            "Interval 4523 (45220 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: -1.9000\n",
            "Interval 4524 (45230 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.431 - mean_q: 12.807 - prob: 1.000\n",
            "\n",
            "Interval 4525 (45240 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 4526 (45250 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.461 - mean_q: 12.746 - prob: 1.000\n",
            "\n",
            "Interval 4527 (45260 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.436 - mean_q: 12.836 - prob: 1.000\n",
            "\n",
            "Interval 4528 (45270 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.771 - mean_q: 13.326 - prob: 1.000\n",
            "\n",
            "Interval 4529 (45280 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.9000\n",
            "Interval 4530 (45290 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.382 - mean_q: 12.714 - prob: 1.000\n",
            "\n",
            "Interval 4531 (45300 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 4532 (45310 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.500 - mean_q: 12.854 - prob: 1.000\n",
            "\n",
            "Interval 4533 (45320 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.354 - mean_q: 12.720 - prob: 1.000\n",
            "\n",
            "Interval 4534 (45330 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.9000\n",
            "Interval 4535 (45340 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.620 - mean_q: 13.076 - prob: 1.000\n",
            "\n",
            "Interval 4536 (45350 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.336 - mean_q: 12.780 - prob: 1.000\n",
            "\n",
            "Interval 4537 (45360 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.425 - mean_q: 12.931 - prob: 1.000\n",
            "\n",
            "Interval 4538 (45370 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.557 - mean_q: 12.936 - prob: 1.000\n",
            "\n",
            "Interval 4539 (45380 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 4540 (45390 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.425 - mean_q: 12.726 - prob: 1.000\n",
            "\n",
            "Interval 4541 (45400 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 4542 (45410 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.412 - mean_q: 12.766 - prob: 1.000\n",
            "\n",
            "Interval 4543 (45420 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.197 - mean_q: 12.463 - prob: 1.000\n",
            "\n",
            "Interval 4544 (45430 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 4545 (45440 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.321 - mean_q: 12.578 - prob: 1.000\n",
            "\n",
            "Interval 4546 (45450 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 4547 (45460 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.462 - mean_q: 12.871 - prob: 1.000\n",
            "\n",
            "Interval 4548 (45470 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.135 - mean_q: 12.285 - prob: 1.000\n",
            "\n",
            "Interval 4549 (45480 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 4550 (45490 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.547 - mean_q: 12.900 - prob: 1.000\n",
            "\n",
            "Interval 4551 (45500 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.200 - mean_q: 12.638 - prob: 1.000\n",
            "\n",
            "Interval 4552 (45510 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.238 - mean_q: 12.651 - prob: 1.000\n",
            "\n",
            "Interval 4553 (45520 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 4554 (45530 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.4000\n",
            "2 episodes - episode_reward: 6.000 [4.000, 8.000] - loss: 0.001 - mae: 7.221 - mean_q: 12.583 - prob: 1.000\n",
            "\n",
            "Interval 4555 (45540 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 4556 (45550 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -17.000 [-17.000, -17.000] - loss: 0.001 - mae: 7.559 - mean_q: 12.990 - prob: 1.000\n",
            "\n",
            "Interval 4557 (45560 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 4558 (45570 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.001 - mae: 7.446 - mean_q: 12.691 - prob: 1.000\n",
            "\n",
            "Interval 4559 (45580 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 4560 (45590 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.000 - mae: 7.233 - mean_q: 12.570 - prob: 1.000\n",
            "\n",
            "Interval 4561 (45600 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.534 - mean_q: 12.905 - prob: 1.000\n",
            "\n",
            "Interval 4562 (45610 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 4563 (45620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.663 - mean_q: 13.087 - prob: 1.000\n",
            "\n",
            "Interval 4564 (45630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4565 (45640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.306 - mean_q: 12.668 - prob: 1.000\n",
            "\n",
            "Interval 4566 (45650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.084 - mean_q: 12.294 - prob: 1.000\n",
            "\n",
            "Interval 4567 (45660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.277 - mean_q: 12.651 - prob: 1.000\n",
            "\n",
            "Interval 4568 (45670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4569 (45680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.496 - mean_q: 13.028 - prob: 1.000\n",
            "\n",
            "Interval 4570 (45690 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.367 - mean_q: 12.818 - prob: 1.000\n",
            "\n",
            "Interval 4571 (45700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4572 (45710 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.338 - mean_q: 12.602 - prob: 1.000\n",
            "\n",
            "Interval 4573 (45720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4574 (45730 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.203 - mean_q: 12.489 - prob: 1.000\n",
            "\n",
            "Interval 4575 (45740 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.314 - mean_q: 12.671 - prob: 1.000\n",
            "\n",
            "Interval 4576 (45750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4577 (45760 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.312 - mean_q: 12.710 - prob: 1.000\n",
            "\n",
            "Interval 4578 (45770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.567 - mean_q: 12.928 - prob: 1.000\n",
            "\n",
            "Interval 4579 (45780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4580 (45790 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.001 - mae: 7.172 - mean_q: 12.506 - prob: 1.000\n",
            "\n",
            "Interval 4581 (45800 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.822 - mean_q: 13.272 - prob: 1.000\n",
            "\n",
            "Interval 4582 (45810 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 4583 (45820 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 6.990 - mean_q: 12.243 - prob: 1.000\n",
            "\n",
            "Interval 4584 (45830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.486 - mean_q: 12.938 - prob: 1.000\n",
            "\n",
            "Interval 4585 (45840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 4586 (45850 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.000 - mae: 7.175 - mean_q: 12.378 - prob: 1.000\n",
            "\n",
            "Interval 4587 (45860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4588 (45870 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.212 - mean_q: 12.378 - prob: 1.000\n",
            "\n",
            "Interval 4589 (45880 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.503 - mean_q: 12.844 - prob: 1.000\n",
            "\n",
            "Interval 4590 (45890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.091 - mean_q: 12.321 - prob: 1.000\n",
            "\n",
            "Interval 4591 (45900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4592 (45910 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.609 - mean_q: 13.024 - prob: 1.000\n",
            "\n",
            "Interval 4593 (45920 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.435 - mean_q: 12.844 - prob: 1.000\n",
            "\n",
            "Interval 4594 (45930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.759 - mean_q: 13.218 - prob: 1.000\n",
            "\n",
            "Interval 4595 (45940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4596 (45950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4597 (45960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.001 - mae: 7.167 - mean_q: 12.408 - prob: 1.000\n",
            "\n",
            "Interval 4598 (45970 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.406 - mean_q: 12.770 - prob: 1.000\n",
            "\n",
            "Interval 4599 (45980 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.861 - mean_q: 13.497 - prob: 1.000\n",
            "\n",
            "Interval 4600 (45990 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.654 - mean_q: 13.174 - prob: 1.000\n",
            "\n",
            "Interval 4601 (46000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4602 (46010 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.261 - mean_q: 12.519 - prob: 1.000\n",
            "\n",
            "Interval 4603 (46020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4604 (46030 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.493 - mean_q: 12.944 - prob: 1.000\n",
            "\n",
            "Interval 4605 (46040 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.533 - mean_q: 12.952 - prob: 1.000\n",
            "\n",
            "Interval 4606 (46050 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.094 - mean_q: 12.336 - prob: 1.000\n",
            "\n",
            "Interval 4607 (46060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4608 (46070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.123 - mean_q: 12.378 - prob: 1.000\n",
            "\n",
            "Interval 4609 (46080 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.462 - mean_q: 12.951 - prob: 1.000\n",
            "\n",
            "Interval 4610 (46090 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.393 - mean_q: 12.732 - prob: 1.000\n",
            "\n",
            "Interval 4611 (46100 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 4612 (46110 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.095 - mean_q: 12.251 - prob: 1.000\n",
            "\n",
            "Interval 4613 (46120 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.836 - mean_q: 13.489 - prob: 1.000\n",
            "\n",
            "Interval 4614 (46130 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.001 - mae: 7.002 - mean_q: 12.290 - prob: 1.000\n",
            "\n",
            "Interval 4615 (46140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4616 (46150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.000 - mae: 7.049 - mean_q: 12.228 - prob: 1.000\n",
            "\n",
            "Interval 4617 (46160 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.596 - mean_q: 13.074 - prob: 1.000\n",
            "\n",
            "Interval 4618 (46170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4619 (46180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.277 - mean_q: 12.495 - prob: 1.000\n",
            "\n",
            "Interval 4620 (46190 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.350 - mean_q: 12.650 - prob: 1.000\n",
            "\n",
            "Interval 4621 (46200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4622 (46210 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.456 - mean_q: 12.836 - prob: 1.000\n",
            "\n",
            "Interval 4623 (46220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4624 (46230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.117 - mean_q: 12.376 - prob: 1.000\n",
            "\n",
            "Interval 4625 (46240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4626 (46250 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 8.000 [4.000, 12.000] - loss: 0.000 - mae: 7.470 - mean_q: 12.926 - prob: 1.000\n",
            "\n",
            "Interval 4627 (46260 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 4628 (46270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.431 - mean_q: 12.772 - prob: 1.000\n",
            "\n",
            "Interval 4629 (46280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.316 - mean_q: 12.751 - prob: 1.000\n",
            "\n",
            "Interval 4630 (46290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.546 - mean_q: 12.972 - prob: 1.000\n",
            "\n",
            "Interval 4631 (46300 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.465 - mean_q: 12.935 - prob: 1.000\n",
            "\n",
            "Interval 4632 (46310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4633 (46320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.194 - mean_q: 12.526 - prob: 1.000\n",
            "\n",
            "Interval 4634 (46330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4635 (46340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.216 - mean_q: 12.540 - prob: 1.000\n",
            "\n",
            "Interval 4636 (46350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.001 - mae: 7.219 - mean_q: 12.499 - prob: 1.000\n",
            "\n",
            "Interval 4637 (46360 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 4638 (46370 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.628 - mean_q: 13.167 - prob: 1.000\n",
            "\n",
            "Interval 4639 (46380 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 4640 (46390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.568 - mean_q: 12.952 - prob: 1.000\n",
            "\n",
            "Interval 4641 (46400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.217 - mean_q: 12.548 - prob: 1.000\n",
            "\n",
            "Interval 4642 (46410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4643 (46420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.175 - mean_q: 12.418 - prob: 1.000\n",
            "\n",
            "Interval 4644 (46430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4645 (46440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.001 - mae: 7.161 - mean_q: 12.573 - prob: 1.000\n",
            "\n",
            "Interval 4646 (46450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.482 - mean_q: 12.975 - prob: 1.000\n",
            "\n",
            "Interval 4647 (46460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.124 - mean_q: 12.394 - prob: 1.000\n",
            "\n",
            "Interval 4648 (46470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4649 (46480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.267 - mean_q: 12.690 - prob: 1.000\n",
            "\n",
            "Interval 4650 (46490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.521 - mean_q: 12.899 - prob: 1.000\n",
            "\n",
            "Interval 4651 (46500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4652 (46510 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.644 - mean_q: 13.046 - prob: 1.000\n",
            "\n",
            "Interval 4653 (46520 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.331 - mean_q: 12.665 - prob: 1.000\n",
            "\n",
            "Interval 4654 (46530 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.192 - mean_q: 12.460 - prob: 1.000\n",
            "\n",
            "Interval 4655 (46540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4656 (46550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.6000\n",
            "1 episodes - episode_reward: -17.000 [-17.000, -17.000] - loss: 0.000 - mae: 7.214 - mean_q: 12.571 - prob: 1.000\n",
            "\n",
            "Interval 4657 (46560 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 4658 (46570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.001 - mae: 7.650 - mean_q: 13.223 - prob: 1.000\n",
            "\n",
            "Interval 4659 (46580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4660 (46590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.211 - mean_q: 12.409 - prob: 1.000\n",
            "\n",
            "Interval 4661 (46600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.366 - mean_q: 12.537 - prob: 1.000\n",
            "\n",
            "Interval 4662 (46610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.400 - mean_q: 12.675 - prob: 1.000\n",
            "\n",
            "Interval 4663 (46620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4664 (46630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.357 - mean_q: 12.709 - prob: 1.000\n",
            "\n",
            "Interval 4665 (46640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 11.500 [9.000, 14.000] - loss: 0.000 - mae: 7.115 - mean_q: 12.466 - prob: 1.000\n",
            "\n",
            "Interval 4666 (46650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4667 (46660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.693 - mean_q: 13.136 - prob: 1.000\n",
            "\n",
            "Interval 4668 (46670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.155 - mean_q: 12.454 - prob: 1.000\n",
            "\n",
            "Interval 4669 (46680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4670 (46690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.251 - mean_q: 12.493 - prob: 1.000\n",
            "\n",
            "Interval 4671 (46700 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.403 - mean_q: 12.777 - prob: 1.000\n",
            "\n",
            "Interval 4672 (46710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.328 - mean_q: 12.737 - prob: 1.000\n",
            "\n",
            "Interval 4673 (46720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.253 - mean_q: 12.693 - prob: 1.000\n",
            "\n",
            "Interval 4674 (46730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4675 (46740 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.014 - mean_q: 12.076 - prob: 1.000\n",
            "\n",
            "Interval 4676 (46750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.278 - mean_q: 12.643 - prob: 1.000\n",
            "\n",
            "Interval 4677 (46760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.657 - mean_q: 13.114 - prob: 1.000\n",
            "\n",
            "Interval 4678 (46770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.521 - mean_q: 12.810 - prob: 1.000\n",
            "\n",
            "Interval 4679 (46780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4680 (46790 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.308 - mean_q: 12.751 - prob: 1.000\n",
            "\n",
            "Interval 4681 (46800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4682 (46810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.001 - mae: 7.860 - mean_q: 13.371 - prob: 1.000\n",
            "\n",
            "Interval 4683 (46820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4684 (46830 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -19.000 [-19.000, -19.000] - loss: 0.001 - mae: 7.244 - mean_q: 12.706 - prob: 1.000\n",
            "\n",
            "Interval 4685 (46840 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.297 - mean_q: 12.632 - prob: 1.000\n",
            "\n",
            "Interval 4686 (46850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4687 (46860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.610 - mean_q: 13.017 - prob: 1.000\n",
            "\n",
            "Interval 4688 (46870 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.490 - mean_q: 12.881 - prob: 1.000\n",
            "\n",
            "Interval 4689 (46880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4690 (46890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.054 - mean_q: 12.345 - prob: 1.000\n",
            "\n",
            "Interval 4691 (46900 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.423 - mean_q: 12.899 - prob: 1.000\n",
            "\n",
            "Interval 4692 (46910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4693 (46920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.584 - mean_q: 13.060 - prob: 1.000\n",
            "\n",
            "Interval 4694 (46930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.215 - mean_q: 12.532 - prob: 1.000\n",
            "\n",
            "Interval 4695 (46940 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 4696 (46950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.000 - mae: 6.934 - mean_q: 12.080 - prob: 1.000\n",
            "\n",
            "Interval 4697 (46960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4698 (46970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.418 - mean_q: 12.696 - prob: 1.000\n",
            "\n",
            "Interval 4699 (46980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4700 (46990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.001 - mae: 7.556 - mean_q: 12.891 - prob: 1.000\n",
            "\n",
            "Interval 4701 (47000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.275 - mean_q: 12.536 - prob: 1.000\n",
            "\n",
            "Interval 4702 (47010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4703 (47020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.408 - mean_q: 12.843 - prob: 1.000\n",
            "\n",
            "Interval 4704 (47030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.310 - mean_q: 12.562 - prob: 1.000\n",
            "\n",
            "Interval 4705 (47040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4706 (47050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.365 - mean_q: 12.845 - prob: 1.000\n",
            "\n",
            "Interval 4707 (47060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4708 (47070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.149 - mean_q: 12.524 - prob: 1.000\n",
            "\n",
            "Interval 4709 (47080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.342 - mean_q: 12.677 - prob: 1.000\n",
            "\n",
            "Interval 4710 (47090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4711 (47100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.468 - mean_q: 13.020 - prob: 1.000\n",
            "\n",
            "Interval 4712 (47110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.489 - mean_q: 12.919 - prob: 1.000\n",
            "\n",
            "Interval 4713 (47120 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.272 - mean_q: 12.492 - prob: 1.000\n",
            "\n",
            "Interval 4714 (47130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4715 (47140 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.440 - mean_q: 12.833 - prob: 1.000\n",
            "\n",
            "Interval 4716 (47150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.458 - mean_q: 12.787 - prob: 1.000\n",
            "\n",
            "Interval 4717 (47160 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.435 - mean_q: 12.785 - prob: 1.000\n",
            "\n",
            "Interval 4718 (47170 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.591 - mean_q: 13.095 - prob: 1.000\n",
            "\n",
            "Interval 4719 (47180 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.138 - mean_q: 12.381 - prob: 1.000\n",
            "\n",
            "Interval 4720 (47190 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 4721 (47200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.157 - mean_q: 12.550 - prob: 1.000\n",
            "\n",
            "Interval 4722 (47210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.373 - mean_q: 12.726 - prob: 1.000\n",
            "\n",
            "Interval 4723 (47220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4724 (47230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.375 - mean_q: 12.712 - prob: 1.000\n",
            "\n",
            "Interval 4725 (47240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.274 - mean_q: 12.592 - prob: 1.000\n",
            "\n",
            "Interval 4726 (47250 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.090 - mean_q: 12.366 - prob: 1.000\n",
            "\n",
            "Interval 4727 (47260 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 4728 (47270 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.360 - mean_q: 12.760 - prob: 1.000\n",
            "\n",
            "Interval 4729 (47280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4730 (47290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.290 - mean_q: 12.651 - prob: 1.000\n",
            "\n",
            "Interval 4731 (47300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.148 - mean_q: 12.336 - prob: 1.000\n",
            "\n",
            "Interval 4732 (47310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.023 - mean_q: 12.309 - prob: 1.000\n",
            "\n",
            "Interval 4733 (47320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.213 - mean_q: 12.555 - prob: 1.000\n",
            "\n",
            "Interval 4734 (47330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 6.937 - mean_q: 12.169 - prob: 1.000\n",
            "\n",
            "Interval 4735 (47340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.452 - mean_q: 12.873 - prob: 1.000\n",
            "\n",
            "Interval 4736 (47350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4737 (47360 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.383 - mean_q: 12.829 - prob: 1.000\n",
            "\n",
            "Interval 4738 (47370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4739 (47380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.256 - mean_q: 12.554 - prob: 1.000\n",
            "\n",
            "Interval 4740 (47390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.067 - mean_q: 12.373 - prob: 1.000\n",
            "\n",
            "Interval 4741 (47400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4742 (47410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.145 - mean_q: 12.531 - prob: 1.000\n",
            "\n",
            "Interval 4743 (47420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.007 - mean_q: 12.281 - prob: 1.000\n",
            "\n",
            "Interval 4744 (47430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.002 - mae: 7.228 - mean_q: 12.583 - prob: 1.000\n",
            "\n",
            "Interval 4745 (47440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4746 (47450 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.454 - mean_q: 12.898 - prob: 1.000\n",
            "\n",
            "Interval 4747 (47460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4748 (47470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.114 - mean_q: 12.197 - prob: 1.000\n",
            "\n",
            "Interval 4749 (47480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 6.719 - mean_q: 11.683 - prob: 1.000\n",
            "\n",
            "Interval 4750 (47490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.693 - mean_q: 13.314 - prob: 1.000\n",
            "\n",
            "Interval 4751 (47500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.133 - mean_q: 12.471 - prob: 1.000\n",
            "\n",
            "Interval 4752 (47510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4753 (47520 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.083 - mean_q: 12.317 - prob: 1.000\n",
            "\n",
            "Interval 4754 (47530 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.647 - mean_q: 13.021 - prob: 1.000\n",
            "\n",
            "Interval 4755 (47540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4756 (47550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.386 - mean_q: 12.718 - prob: 1.000\n",
            "\n",
            "Interval 4757 (47560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4758 (47570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 6.955 - mean_q: 12.089 - prob: 1.000\n",
            "\n",
            "Interval 4759 (47580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.065 - mean_q: 12.368 - prob: 1.000\n",
            "\n",
            "Interval 4760 (47590 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 4761 (47600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -18.000 [-18.000, -18.000] - loss: 0.001 - mae: 7.468 - mean_q: 12.729 - prob: 1.000\n",
            "\n",
            "Interval 4762 (47610 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.597 - mean_q: 13.059 - prob: 1.000\n",
            "\n",
            "Interval 4763 (47620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4764 (47630 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.346 - mean_q: 12.648 - prob: 1.000\n",
            "\n",
            "Interval 4765 (47640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.335 - mean_q: 12.653 - prob: 1.000\n",
            "\n",
            "Interval 4766 (47650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4767 (47660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4768 (47670 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.001 - mae: 7.401 - mean_q: 12.715 - prob: 1.000\n",
            "\n",
            "Interval 4769 (47680 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.451 - mean_q: 12.917 - prob: 1.000\n",
            "\n",
            "Interval 4770 (47690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.001 - mae: 7.507 - mean_q: 12.862 - prob: 1.000\n",
            "\n",
            "Interval 4771 (47700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.510 - mean_q: 12.914 - prob: 1.000\n",
            "\n",
            "Interval 4772 (47710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.293 - mean_q: 12.690 - prob: 1.000\n",
            "\n",
            "Interval 4773 (47720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4774 (47730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4775 (47740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -20.000 [-20.000, -20.000] - loss: 0.001 - mae: 7.585 - mean_q: 13.067 - prob: 1.000\n",
            "\n",
            "Interval 4776 (47750 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 4777 (47760 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.000 - mae: 7.147 - mean_q: 12.517 - prob: 1.000\n",
            "\n",
            "Interval 4778 (47770 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.517 - mean_q: 12.838 - prob: 1.000\n",
            "\n",
            "Interval 4779 (47780 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 4780 (47790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -22.000 [-22.000, -22.000] - loss: 0.000 - mae: 7.281 - mean_q: 12.656 - prob: 1.000\n",
            "\n",
            "Interval 4781 (47800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.646 - mean_q: 13.184 - prob: 1.000\n",
            "\n",
            "Interval 4782 (47810 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.343 - mean_q: 12.613 - prob: 1.000\n",
            "\n",
            "Interval 4783 (47820 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 4784 (47830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.523 - mean_q: 12.955 - prob: 1.000\n",
            "\n",
            "Interval 4785 (47840 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.322 - mean_q: 12.679 - prob: 1.000\n",
            "\n",
            "Interval 4786 (47850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4787 (47860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.509 - mean_q: 12.885 - prob: 1.000\n",
            "\n",
            "Interval 4788 (47870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.315 - mean_q: 12.665 - prob: 1.000\n",
            "\n",
            "Interval 4789 (47880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4790 (47890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.551 - mean_q: 12.952 - prob: 1.000\n",
            "\n",
            "Interval 4791 (47900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.267 - mean_q: 12.690 - prob: 1.000\n",
            "\n",
            "Interval 4792 (47910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4793 (47920 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.256 - mean_q: 12.591 - prob: 1.000\n",
            "\n",
            "Interval 4794 (47930 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.486 - mean_q: 12.825 - prob: 1.000\n",
            "\n",
            "Interval 4795 (47940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.178 - mean_q: 12.437 - prob: 1.000\n",
            "\n",
            "Interval 4796 (47950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4797 (47960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.190 - mean_q: 12.471 - prob: 1.000\n",
            "\n",
            "Interval 4798 (47970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.426 - mean_q: 12.794 - prob: 1.000\n",
            "\n",
            "Interval 4799 (47980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4800 (47990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.198 - mean_q: 12.478 - prob: 1.000\n",
            "\n",
            "Interval 4801 (48000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4802 (48010 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.679 - mean_q: 13.156 - prob: 1.000\n",
            "\n",
            "Interval 4803 (48020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.357 - mean_q: 12.785 - prob: 1.000\n",
            "\n",
            "Interval 4804 (48030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.505 - mean_q: 12.922 - prob: 1.000\n",
            "\n",
            "Interval 4805 (48040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4806 (48050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.482 - mean_q: 12.894 - prob: 1.000\n",
            "\n",
            "Interval 4807 (48060 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 13.000 [11.000, 15.000] - loss: 0.001 - mae: 7.520 - mean_q: 12.955 - prob: 1.000\n",
            "\n",
            "Interval 4808 (48070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4809 (48080 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.521 - mean_q: 12.815 - prob: 1.000\n",
            "\n",
            "Interval 4810 (48090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4811 (48100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.515 - mean_q: 12.877 - prob: 1.000\n",
            "\n",
            "Interval 4812 (48110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.381 - mean_q: 12.718 - prob: 1.000\n",
            "\n",
            "Interval 4813 (48120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4814 (48130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.048 - mean_q: 12.221 - prob: 1.000\n",
            "\n",
            "Interval 4815 (48140 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.512 - mean_q: 12.976 - prob: 1.000\n",
            "\n",
            "Interval 4816 (48150 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.394 - mean_q: 12.712 - prob: 1.000\n",
            "\n",
            "Interval 4817 (48160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4818 (48170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.211 - mean_q: 12.383 - prob: 1.000\n",
            "\n",
            "Interval 4819 (48180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4820 (48190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 10.000 [7.000, 13.000] - loss: 0.000 - mae: 7.322 - mean_q: 12.831 - prob: 1.000\n",
            "\n",
            "Interval 4821 (48200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4822 (48210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.299 - mean_q: 12.708 - prob: 1.000\n",
            "\n",
            "Interval 4823 (48220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4824 (48230 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.383 - mean_q: 12.664 - prob: 1.000\n",
            "\n",
            "Interval 4825 (48240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.522 - mean_q: 12.885 - prob: 1.000\n",
            "\n",
            "Interval 4826 (48250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.352 - mean_q: 12.580 - prob: 1.000\n",
            "\n",
            "Interval 4827 (48260 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 4828 (48270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.323 - mean_q: 12.488 - prob: 1.000\n",
            "\n",
            "Interval 4829 (48280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.377 - mean_q: 12.564 - prob: 1.000\n",
            "\n",
            "Interval 4830 (48290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.300 - mean_q: 12.719 - prob: 1.000\n",
            "\n",
            "Interval 4831 (48300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.893 - mean_q: 13.423 - prob: 1.000\n",
            "\n",
            "Interval 4832 (48310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4833 (48320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.349 - mean_q: 12.598 - prob: 1.000\n",
            "\n",
            "Interval 4834 (48330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.634 - mean_q: 13.091 - prob: 1.000\n",
            "\n",
            "Interval 4835 (48340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4836 (48350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.625 - mean_q: 13.137 - prob: 1.000\n",
            "\n",
            "Interval 4837 (48360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.001 - mae: 7.393 - mean_q: 12.868 - prob: 1.000\n",
            "\n",
            "Interval 4838 (48370 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.321 - mean_q: 12.653 - prob: 1.000\n",
            "\n",
            "Interval 4839 (48380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.365 - mean_q: 12.633 - prob: 1.000\n",
            "\n",
            "Interval 4840 (48390 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 4841 (48400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.361 - mean_q: 12.681 - prob: 1.000\n",
            "\n",
            "Interval 4842 (48410 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.233 - mean_q: 12.425 - prob: 1.000\n",
            "\n",
            "Interval 4843 (48420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.263 - mean_q: 12.543 - prob: 1.000\n",
            "\n",
            "Interval 4844 (48430 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 4845 (48440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.456 - mean_q: 12.851 - prob: 1.000\n",
            "\n",
            "Interval 4846 (48450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.369 - mean_q: 12.762 - prob: 1.000\n",
            "\n",
            "Interval 4847 (48460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.348 - mean_q: 12.784 - prob: 1.000\n",
            "\n",
            "Interval 4848 (48470 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.466 - mean_q: 12.845 - prob: 1.000\n",
            "\n",
            "Interval 4849 (48480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4850 (48490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.391 - mean_q: 12.701 - prob: 1.000\n",
            "\n",
            "Interval 4851 (48500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.333 - mean_q: 12.706 - prob: 1.000\n",
            "\n",
            "Interval 4852 (48510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.251 - mean_q: 12.602 - prob: 1.000\n",
            "\n",
            "Interval 4853 (48520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4854 (48530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.506 - mean_q: 12.910 - prob: 1.000\n",
            "\n",
            "Interval 4855 (48540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.188 - mean_q: 12.576 - prob: 1.000\n",
            "\n",
            "Interval 4856 (48550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4857 (48560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.229 - mean_q: 12.493 - prob: 1.000\n",
            "\n",
            "Interval 4858 (48570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.525 - mean_q: 13.007 - prob: 1.000\n",
            "\n",
            "Interval 4859 (48580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.336 - mean_q: 12.706 - prob: 1.000\n",
            "\n",
            "Interval 4860 (48590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4861 (48600 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.385 - mean_q: 12.759 - prob: 1.000\n",
            "\n",
            "Interval 4862 (48610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.795 - mean_q: 13.236 - prob: 1.000\n",
            "\n",
            "Interval 4863 (48620 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 4864 (48630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.583 - mean_q: 13.005 - prob: 1.000\n",
            "\n",
            "Interval 4865 (48640 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.404 - mean_q: 12.767 - prob: 1.000\n",
            "\n",
            "Interval 4866 (48650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4867 (48660 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.387 - mean_q: 12.816 - prob: 1.000\n",
            "\n",
            "Interval 4868 (48670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.487 - mean_q: 12.787 - prob: 1.000\n",
            "\n",
            "Interval 4869 (48680 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 4870 (48690 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.390 - mean_q: 12.766 - prob: 1.000\n",
            "\n",
            "Interval 4871 (48700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4872 (48710 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.145 - mean_q: 12.376 - prob: 1.000\n",
            "\n",
            "Interval 4873 (48720 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.033 - mean_q: 12.199 - prob: 1.000\n",
            "\n",
            "Interval 4874 (48730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.000 - mae: 7.334 - mean_q: 12.750 - prob: 1.000\n",
            "\n",
            "Interval 4875 (48740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4876 (48750 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.562 - mean_q: 13.136 - prob: 1.000\n",
            "\n",
            "Interval 4877 (48760 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.639 - mean_q: 13.206 - prob: 1.000\n",
            "\n",
            "Interval 4878 (48770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4879 (48780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.753 - mean_q: 13.262 - prob: 1.000\n",
            "\n",
            "Interval 4880 (48790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4881 (48800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.494 - mean_q: 12.922 - prob: 1.000\n",
            "\n",
            "Interval 4882 (48810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.305 - mean_q: 12.771 - prob: 1.000\n",
            "\n",
            "Interval 4883 (48820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4884 (48830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.400 - mean_q: 12.867 - prob: 1.000\n",
            "\n",
            "Interval 4885 (48840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.619 - mean_q: 13.185 - prob: 1.000\n",
            "\n",
            "Interval 4886 (48850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4887 (48860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.268 - mean_q: 12.600 - prob: 1.000\n",
            "\n",
            "Interval 4888 (48870 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.000 - mae: 7.369 - mean_q: 12.667 - prob: 1.000\n",
            "\n",
            "Interval 4889 (48880 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.000 - mae: 7.779 - mean_q: 13.397 - prob: 1.000\n",
            "\n",
            "Interval 4890 (48890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.406 - mean_q: 12.909 - prob: 1.000\n",
            "\n",
            "Interval 4891 (48900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.492 - mean_q: 13.000 - prob: 1.000\n",
            "\n",
            "Interval 4892 (48910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4893 (48920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.431 - mean_q: 12.775 - prob: 1.000\n",
            "\n",
            "Interval 4894 (48930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.000 - mae: 7.525 - mean_q: 13.142 - prob: 1.000\n",
            "\n",
            "Interval 4895 (48940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4896 (48950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.452 - mean_q: 12.808 - prob: 1.000\n",
            "\n",
            "Interval 4897 (48960 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 4898 (48970 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.001 - mae: 7.715 - mean_q: 13.158 - prob: 1.000\n",
            "\n",
            "Interval 4899 (48980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.134 - mean_q: 12.498 - prob: 1.000\n",
            "\n",
            "Interval 4900 (48990 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 4901 (49000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.103 - mean_q: 12.342 - prob: 1.000\n",
            "\n",
            "Interval 4902 (49010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.075 - mean_q: 12.454 - prob: 1.000\n",
            "\n",
            "Interval 4903 (49020 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 4904 (49030 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.220 - mean_q: 12.612 - prob: 1.000\n",
            "\n",
            "Interval 4905 (49040 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 4906 (49050 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.001 - mae: 7.310 - mean_q: 12.680 - prob: 1.000\n",
            "\n",
            "Interval 4907 (49060 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.421 - mean_q: 12.756 - prob: 1.000\n",
            "\n",
            "Interval 4908 (49070 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.554 - mean_q: 12.904 - prob: 1.000\n",
            "\n",
            "Interval 4909 (49080 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 4910 (49090 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.002 - mae: 7.083 - mean_q: 12.289 - prob: 1.000\n",
            "\n",
            "Interval 4911 (49100 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.200 - mean_q: 12.567 - prob: 1.000\n",
            "\n",
            "Interval 4912 (49110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4913 (49120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.644 - mean_q: 13.024 - prob: 1.000\n",
            "\n",
            "Interval 4914 (49130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4915 (49140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.245 - mean_q: 12.451 - prob: 1.000\n",
            "\n",
            "Interval 4916 (49150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.376 - mean_q: 12.742 - prob: 1.000\n",
            "\n",
            "Interval 4917 (49160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.161 - mean_q: 12.415 - prob: 1.000\n",
            "\n",
            "Interval 4918 (49170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 4919 (49180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.001 - mae: 7.382 - mean_q: 12.748 - prob: 1.000\n",
            "\n",
            "Interval 4920 (49190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.437 - mean_q: 12.908 - prob: 1.000\n",
            "\n",
            "Interval 4921 (49200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4922 (49210 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.319 - mean_q: 12.549 - prob: 1.000\n",
            "\n",
            "Interval 4923 (49220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4924 (49230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.129 - mean_q: 12.377 - prob: 1.000\n",
            "\n",
            "Interval 4925 (49240 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.526 - mean_q: 12.978 - prob: 1.000\n",
            "\n",
            "Interval 4926 (49250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.439 - mean_q: 12.649 - prob: 1.000\n",
            "\n",
            "Interval 4927 (49260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4928 (49270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.455 - mean_q: 12.916 - prob: 1.000\n",
            "\n",
            "Interval 4929 (49280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.156 - mean_q: 12.306 - prob: 1.000\n",
            "\n",
            "Interval 4930 (49290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4931 (49300 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.596 - mean_q: 12.854 - prob: 1.000\n",
            "\n",
            "Interval 4932 (49310 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 4933 (49320 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.113 - mean_q: 12.292 - prob: 1.000\n",
            "\n",
            "Interval 4934 (49330 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 4935 (49340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.020 - mean_q: 12.282 - prob: 1.000\n",
            "\n",
            "Interval 4936 (49350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4937 (49360 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.422 - mean_q: 12.838 - prob: 1.000\n",
            "\n",
            "Interval 4938 (49370 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.158 - mean_q: 12.582 - prob: 1.000\n",
            "\n",
            "Interval 4939 (49380 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.495 - mean_q: 12.824 - prob: 1.000\n",
            "\n",
            "Interval 4940 (49390 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.628 - mean_q: 13.102 - prob: 1.000\n",
            "\n",
            "Interval 4941 (49400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4942 (49410 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.306 - mean_q: 12.741 - prob: 1.000\n",
            "\n",
            "Interval 4943 (49420 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.313 - mean_q: 12.614 - prob: 1.000\n",
            "\n",
            "Interval 4944 (49430 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.334 - mean_q: 12.866 - prob: 1.000\n",
            "\n",
            "Interval 4945 (49440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4946 (49450 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.452 - mean_q: 12.889 - prob: 1.000\n",
            "\n",
            "Interval 4947 (49460 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 4948 (49470 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.279 - mean_q: 12.674 - prob: 1.000\n",
            "\n",
            "Interval 4949 (49480 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.201 - mean_q: 12.415 - prob: 1.000\n",
            "\n",
            "Interval 4950 (49490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 4951 (49500 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.001 - mae: 7.374 - mean_q: 12.753 - prob: 1.000\n",
            "\n",
            "Interval 4952 (49510 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.134 - mean_q: 12.532 - prob: 1.000\n",
            "\n",
            "Interval 4953 (49520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 4954 (49530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.111 - mean_q: 12.374 - prob: 1.000\n",
            "\n",
            "Interval 4955 (49540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 4956 (49550 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.001 - mae: 7.126 - mean_q: 12.427 - prob: 1.000\n",
            "\n",
            "Interval 4957 (49560 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 4958 (49570 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.019 - mean_q: 12.242 - prob: 1.000\n",
            "\n",
            "Interval 4959 (49580 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.461 - mean_q: 12.962 - prob: 1.000\n",
            "\n",
            "Interval 4960 (49590 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.374 - mean_q: 12.597 - prob: 1.000\n",
            "\n",
            "Interval 4961 (49600 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 4962 (49610 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.723 - mean_q: 13.200 - prob: 1.000\n",
            "\n",
            "Interval 4963 (49620 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.401 - mean_q: 12.921 - prob: 1.000\n",
            "\n",
            "Interval 4964 (49630 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 4965 (49640 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 4966 (49650 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: -2.000 [-19.000, 15.000] - loss: 0.001 - mae: 7.243 - mean_q: 12.451 - prob: 1.000\n",
            "\n",
            "Interval 4967 (49660 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 4968 (49670 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.420 - mean_q: 12.738 - prob: 1.000\n",
            "\n",
            "Interval 4969 (49680 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 6.947 - mean_q: 12.167 - prob: 1.000\n",
            "\n",
            "Interval 4970 (49690 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 4971 (49700 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.371 - mean_q: 12.620 - prob: 1.000\n",
            "\n",
            "Interval 4972 (49710 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -2.8000\n",
            "Interval 4973 (49720 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.000 - mae: 7.742 - mean_q: 13.153 - prob: 1.000\n",
            "\n",
            "Interval 4974 (49730 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.598 - mean_q: 13.090 - prob: 1.000\n",
            "\n",
            "Interval 4975 (49740 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 4976 (49750 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.449 - mean_q: 12.738 - prob: 1.000\n",
            "\n",
            "Interval 4977 (49760 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.030 - mean_q: 12.302 - prob: 1.000\n",
            "\n",
            "Interval 4978 (49770 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.454 - mean_q: 12.892 - prob: 1.000\n",
            "\n",
            "Interval 4979 (49780 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.442 - mean_q: 12.772 - prob: 1.000\n",
            "\n",
            "Interval 4980 (49790 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.378 - mean_q: 12.723 - prob: 1.000\n",
            "\n",
            "Interval 4981 (49800 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 4982 (49810 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.494 - mean_q: 13.035 - prob: 1.000\n",
            "\n",
            "Interval 4983 (49820 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.628 - mean_q: 13.105 - prob: 1.000\n",
            "\n",
            "Interval 4984 (49830 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 4985 (49840 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.430 - mean_q: 12.867 - prob: 1.000\n",
            "\n",
            "Interval 4986 (49850 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.469 - mean_q: 12.859 - prob: 1.000\n",
            "\n",
            "Interval 4987 (49860 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.485 - mean_q: 12.857 - prob: 1.000\n",
            "\n",
            "Interval 4988 (49870 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.436 - mean_q: 12.954 - prob: 1.000\n",
            "\n",
            "Interval 4989 (49880 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 4990 (49890 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.286 - mean_q: 12.501 - prob: 1.000\n",
            "\n",
            "Interval 4991 (49900 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.338 - mean_q: 12.680 - prob: 1.000\n",
            "\n",
            "Interval 4992 (49910 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 4993 (49920 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 8.500 [5.000, 12.000] - loss: 0.001 - mae: 7.538 - mean_q: 12.943 - prob: 1.000\n",
            "\n",
            "Interval 4994 (49930 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 4995 (49940 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.155 - mean_q: 12.455 - prob: 1.000\n",
            "\n",
            "Interval 4996 (49950 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.652 - mean_q: 13.011 - prob: 1.000\n",
            "\n",
            "Interval 4997 (49960 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 4998 (49970 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 6.500 [0.000, 13.000] - loss: 0.000 - mae: 7.205 - mean_q: 12.514 - prob: 1.000\n",
            "\n",
            "Interval 4999 (49980 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.9000\n",
            "Interval 5000 (49990 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.000 - mae: 7.321 - mean_q: 12.567 - prob: 1.000\n",
            "\n",
            "Interval 5001 (50000 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.465 - mean_q: 12.824 - prob: 1.000\n",
            "\n",
            "Interval 5002 (50010 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 5003 (50020 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.234 - mean_q: 12.520 - prob: 1.000\n",
            "\n",
            "Interval 5004 (50030 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.650 - mean_q: 13.238 - prob: 1.000\n",
            "\n",
            "Interval 5005 (50040 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 5006 (50050 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.002 - mae: 7.130 - mean_q: 12.447 - prob: 1.000\n",
            "\n",
            "Interval 5007 (50060 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.462 - mean_q: 12.853 - prob: 1.000\n",
            "\n",
            "Interval 5008 (50070 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 5009 (50080 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.547 - mean_q: 12.941 - prob: 1.000\n",
            "\n",
            "Interval 5010 (50090 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.279 - mean_q: 12.525 - prob: 1.000\n",
            "\n",
            "Interval 5011 (50100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5012 (50110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.518 - mean_q: 12.830 - prob: 1.000\n",
            "\n",
            "Interval 5013 (50120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.221 - mean_q: 12.582 - prob: 1.000\n",
            "\n",
            "Interval 5014 (50130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5015 (50140 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.173 - mean_q: 12.576 - prob: 1.000\n",
            "\n",
            "Interval 5016 (50150 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.408 - mean_q: 12.820 - prob: 1.000\n",
            "\n",
            "Interval 5017 (50160 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.525 - mean_q: 12.951 - prob: 1.000\n",
            "\n",
            "Interval 5018 (50170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5019 (50180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.535 - mean_q: 13.067 - prob: 1.000\n",
            "\n",
            "Interval 5020 (50190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5021 (50200 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.163 - mean_q: 12.388 - prob: 1.000\n",
            "\n",
            "Interval 5022 (50210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.001 - mae: 7.543 - mean_q: 13.030 - prob: 1.000\n",
            "\n",
            "Interval 5023 (50220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 5024 (50230 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.303 - mean_q: 12.706 - prob: 1.000\n",
            "\n",
            "Interval 5025 (50240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 5026 (50250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.001 - mae: 7.574 - mean_q: 13.050 - prob: 1.000\n",
            "\n",
            "Interval 5027 (50260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.046 - mean_q: 12.183 - prob: 1.000\n",
            "\n",
            "Interval 5028 (50270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5029 (50280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.000 - mae: 7.191 - mean_q: 12.563 - prob: 1.000\n",
            "\n",
            "Interval 5030 (50290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.052 - mean_q: 12.411 - prob: 1.000\n",
            "\n",
            "Interval 5031 (50300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5032 (50310 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.002 - mae: 7.454 - mean_q: 12.818 - prob: 1.000\n",
            "\n",
            "Interval 5033 (50320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 6.965 - mean_q: 12.147 - prob: 1.000\n",
            "\n",
            "Interval 5034 (50330 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.545 - mean_q: 12.995 - prob: 1.000\n",
            "\n",
            "Interval 5035 (50340 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.419 - mean_q: 12.762 - prob: 1.000\n",
            "\n",
            "Interval 5036 (50350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5037 (50360 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.447 - mean_q: 12.905 - prob: 1.000\n",
            "\n",
            "Interval 5038 (50370 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.409 - mean_q: 12.758 - prob: 1.000\n",
            "\n",
            "Interval 5039 (50380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.163 - mean_q: 12.390 - prob: 1.000\n",
            "\n",
            "Interval 5040 (50390 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 5041 (50400 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.578 - mean_q: 13.078 - prob: 1.000\n",
            "\n",
            "Interval 5042 (50410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.074 - mean_q: 12.225 - prob: 1.000\n",
            "\n",
            "Interval 5043 (50420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5044 (50430 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.647 - mean_q: 13.170 - prob: 1.000\n",
            "\n",
            "Interval 5045 (50440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.234 - mean_q: 12.589 - prob: 1.000\n",
            "\n",
            "Interval 5046 (50450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5047 (50460 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.001 - mae: 7.244 - mean_q: 12.613 - prob: 1.000\n",
            "\n",
            "Interval 5048 (50470 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.605 - mean_q: 13.074 - prob: 1.000\n",
            "\n",
            "Interval 5049 (50480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.309 - mean_q: 12.608 - prob: 1.000\n",
            "\n",
            "Interval 5050 (50490 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 5051 (50500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.056 - mean_q: 12.219 - prob: 1.000\n",
            "\n",
            "Interval 5052 (50510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.244 - mean_q: 12.526 - prob: 1.000\n",
            "\n",
            "Interval 5053 (50520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.033 - mean_q: 12.331 - prob: 1.000\n",
            "\n",
            "Interval 5054 (50530 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 5055 (50540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.360 - mean_q: 12.752 - prob: 1.000\n",
            "\n",
            "Interval 5056 (50550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.240 - mean_q: 12.535 - prob: 1.000\n",
            "\n",
            "Interval 5057 (50560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 5058 (50570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.430 - mean_q: 12.942 - prob: 1.000\n",
            "\n",
            "Interval 5059 (50580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.530 - mean_q: 12.825 - prob: 1.000\n",
            "\n",
            "Interval 5060 (50590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5061 (50600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.486 - mean_q: 13.002 - prob: 1.000\n",
            "\n",
            "Interval 5062 (50610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.281 - mean_q: 12.625 - prob: 1.000\n",
            "\n",
            "Interval 5063 (50620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.704 - mean_q: 13.217 - prob: 1.000\n",
            "\n",
            "Interval 5064 (50630 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 5065 (50640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.247 - mean_q: 12.477 - prob: 1.000\n",
            "\n",
            "Interval 5066 (50650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 5067 (50660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.526 - mean_q: 12.816 - prob: 1.000\n",
            "\n",
            "Interval 5068 (50670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.165 - mean_q: 12.240 - prob: 1.000\n",
            "\n",
            "Interval 5069 (50680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 5070 (50690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.001 - mae: 7.358 - mean_q: 12.744 - prob: 1.000\n",
            "\n",
            "Interval 5071 (50700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.001 - mae: 7.665 - mean_q: 13.171 - prob: 1.000\n",
            "\n",
            "Interval 5072 (50710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5073 (50720 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.424 - mean_q: 12.803 - prob: 1.000\n",
            "\n",
            "Interval 5074 (50730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.404 - mean_q: 12.623 - prob: 1.000\n",
            "\n",
            "Interval 5075 (50740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 5076 (50750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5077 (50760 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.001 - mae: 7.437 - mean_q: 12.819 - prob: 1.000\n",
            "\n",
            "Interval 5078 (50770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 5079 (50780 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.001 - mae: 7.368 - mean_q: 12.781 - prob: 1.000\n",
            "\n",
            "Interval 5080 (50790 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.567 - mean_q: 13.080 - prob: 1.000\n",
            "\n",
            "Interval 5081 (50800 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.001 - mean_q: 12.206 - prob: 1.000\n",
            "\n",
            "Interval 5082 (50810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.152 - mean_q: 12.336 - prob: 1.000\n",
            "\n",
            "Interval 5083 (50820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5084 (50830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.290 - mean_q: 12.580 - prob: 1.000\n",
            "\n",
            "Interval 5085 (50840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.562 - mean_q: 12.917 - prob: 1.000\n",
            "\n",
            "Interval 5086 (50850 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.437 - mean_q: 12.816 - prob: 1.000\n",
            "\n",
            "Interval 5087 (50860 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.443 - mean_q: 12.816 - prob: 1.000\n",
            "\n",
            "Interval 5088 (50870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5089 (50880 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.609 - mean_q: 13.081 - prob: 1.000\n",
            "\n",
            "Interval 5090 (50890 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.399 - mean_q: 12.843 - prob: 1.000\n",
            "\n",
            "Interval 5091 (50900 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.519 - mean_q: 12.822 - prob: 1.000\n",
            "\n",
            "Interval 5092 (50910 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.279 - mean_q: 12.628 - prob: 1.000\n",
            "\n",
            "Interval 5093 (50920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 5094 (50930 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.306 - mean_q: 12.560 - prob: 1.000\n",
            "\n",
            "Interval 5095 (50940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5096 (50950 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.447 - mean_q: 12.942 - prob: 1.000\n",
            "\n",
            "Interval 5097 (50960 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.407 - mean_q: 12.826 - prob: 1.000\n",
            "\n",
            "Interval 5098 (50970 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.527 - mean_q: 12.961 - prob: 1.000\n",
            "\n",
            "Interval 5099 (50980 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.389 - mean_q: 12.734 - prob: 1.000\n",
            "\n",
            "Interval 5100 (50990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5101 (51000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.099 - mean_q: 12.252 - prob: 1.000\n",
            "\n",
            "Interval 5102 (51010 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.365 - mean_q: 12.883 - prob: 1.000\n",
            "\n",
            "Interval 5103 (51020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5104 (51030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.220 - mean_q: 12.480 - prob: 1.000\n",
            "\n",
            "Interval 5105 (51040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.136 - mean_q: 12.450 - prob: 1.000\n",
            "\n",
            "Interval 5106 (51050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5107 (51060 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -22.000 [-22.000, -22.000] - loss: 0.001 - mae: 6.940 - mean_q: 12.164 - prob: 1.000\n",
            "\n",
            "Interval 5108 (51070 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 5109 (51080 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.503 - mean_q: 12.922 - prob: 1.000\n",
            "\n",
            "Interval 5110 (51090 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.382 - mean_q: 12.821 - prob: 1.000\n",
            "\n",
            "Interval 5111 (51100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.301 - mean_q: 12.644 - prob: 1.000\n",
            "\n",
            "Interval 5112 (51110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5113 (51120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.747 - mean_q: 13.273 - prob: 1.000\n",
            "\n",
            "Interval 5114 (51130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5115 (51140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.561 - mean_q: 12.934 - prob: 1.000\n",
            "\n",
            "Interval 5116 (51150 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.399 - mean_q: 12.812 - prob: 1.000\n",
            "\n",
            "Interval 5117 (51160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -3.7000\n",
            "Interval 5118 (51170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -26.000 [-26.000, -26.000] - loss: 0.001 - mae: 7.411 - mean_q: 12.667 - prob: 1.000\n",
            "\n",
            "Interval 5119 (51180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5120 (51190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.266 - mean_q: 12.714 - prob: 1.000\n",
            "\n",
            "Interval 5121 (51200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.211 - mean_q: 12.525 - prob: 1.000\n",
            "\n",
            "Interval 5122 (51210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5123 (51220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.016 - mean_q: 12.215 - prob: 1.000\n",
            "\n",
            "Interval 5124 (51230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.456 - mean_q: 12.899 - prob: 1.000\n",
            "\n",
            "Interval 5125 (51240 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 5126 (51250 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 6.941 - mean_q: 12.254 - prob: 1.000\n",
            "\n",
            "Interval 5127 (51260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5128 (51270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.238 - mean_q: 12.542 - prob: 1.000\n",
            "\n",
            "Interval 5129 (51280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 5130 (51290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -17.000 [-17.000, -17.000] - loss: 0.001 - mae: 7.069 - mean_q: 12.316 - prob: 1.000\n",
            "\n",
            "Interval 5131 (51300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 5132 (51310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.000 - mae: 7.605 - mean_q: 13.061 - prob: 1.000\n",
            "\n",
            "Interval 5133 (51320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5134 (51330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.328 - mean_q: 12.668 - prob: 1.000\n",
            "\n",
            "Interval 5135 (51340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.560 - mean_q: 12.901 - prob: 1.000\n",
            "\n",
            "Interval 5136 (51350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 5137 (51360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.001 - mae: 7.133 - mean_q: 12.458 - prob: 1.000\n",
            "\n",
            "Interval 5138 (51370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.133 - mean_q: 12.386 - prob: 1.000\n",
            "\n",
            "Interval 5139 (51380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.417 - mean_q: 12.698 - prob: 1.000\n",
            "\n",
            "Interval 5140 (51390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.006 - mean_q: 12.163 - prob: 1.000\n",
            "\n",
            "Interval 5141 (51400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.474 - mean_q: 12.950 - prob: 1.000\n",
            "\n",
            "Interval 5142 (51410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.437 - mean_q: 12.904 - prob: 1.000\n",
            "\n",
            "Interval 5143 (51420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.306 - mean_q: 12.472 - prob: 1.000\n",
            "\n",
            "Interval 5144 (51430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5145 (51440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 6.991 - mean_q: 12.039 - prob: 1.000\n",
            "\n",
            "Interval 5146 (51450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5147 (51460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.508 - mean_q: 12.821 - prob: 1.000\n",
            "\n",
            "Interval 5148 (51470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.003 - mae: 7.169 - mean_q: 12.413 - prob: 1.000\n",
            "\n",
            "Interval 5149 (51480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5150 (51490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.540 - mean_q: 12.923 - prob: 1.000\n",
            "\n",
            "Interval 5151 (51500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.002 - mae: 7.558 - mean_q: 13.025 - prob: 1.000\n",
            "\n",
            "Interval 5152 (51510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5153 (51520 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.500 - mean_q: 12.845 - prob: 1.000\n",
            "\n",
            "Interval 5154 (51530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.111 - mean_q: 12.300 - prob: 1.000\n",
            "\n",
            "Interval 5155 (51540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.161 - mean_q: 12.556 - prob: 1.000\n",
            "\n",
            "Interval 5156 (51550 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.259 - mean_q: 12.712 - prob: 1.000\n",
            "\n",
            "Interval 5157 (51560 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.9000\n",
            "Interval 5158 (51570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.314 - mean_q: 12.629 - prob: 1.000\n",
            "\n",
            "Interval 5159 (51580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.037 - mean_q: 12.286 - prob: 1.000\n",
            "\n",
            "Interval 5160 (51590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.155 - mean_q: 12.480 - prob: 1.000\n",
            "\n",
            "Interval 5161 (51600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5162 (51610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.447 - mean_q: 12.789 - prob: 1.000\n",
            "\n",
            "Interval 5163 (51620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.384 - mean_q: 12.844 - prob: 1.000\n",
            "\n",
            "Interval 5164 (51630 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 5165 (51640 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.003 - mae: 7.282 - mean_q: 12.575 - prob: 1.000\n",
            "\n",
            "Interval 5166 (51650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.001 - mae: 7.351 - mean_q: 12.596 - prob: 1.000\n",
            "\n",
            "Interval 5167 (51660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.648 - mean_q: 13.112 - prob: 1.000\n",
            "\n",
            "Interval 5168 (51670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5169 (51680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.438 - mean_q: 12.897 - prob: 1.000\n",
            "\n",
            "Interval 5170 (51690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.454 - mean_q: 12.760 - prob: 1.000\n",
            "\n",
            "Interval 5171 (51700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5172 (51710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.418 - mean_q: 12.664 - prob: 1.000\n",
            "\n",
            "Interval 5173 (51720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.658 - mean_q: 13.118 - prob: 1.000\n",
            "\n",
            "Interval 5174 (51730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.251 - mean_q: 12.432 - prob: 1.000\n",
            "\n",
            "Interval 5175 (51740 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 5176 (51750 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.139 - mean_q: 12.460 - prob: 1.000\n",
            "\n",
            "Interval 5177 (51760 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.341 - mean_q: 12.767 - prob: 1.000\n",
            "\n",
            "Interval 5178 (51770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5179 (51780 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.236 - mean_q: 12.412 - prob: 1.000\n",
            "\n",
            "Interval 5180 (51790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5181 (51800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.488 - mean_q: 12.857 - prob: 1.000\n",
            "\n",
            "Interval 5182 (51810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 5183 (51820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -37.000 [-37.000, -37.000] - loss: 0.000 - mae: 7.318 - mean_q: 12.732 - prob: 1.000\n",
            "\n",
            "Interval 5184 (51830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5185 (51840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.001 - mae: 7.431 - mean_q: 12.801 - prob: 1.000\n",
            "\n",
            "Interval 5186 (51850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5187 (51860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.518 - mean_q: 13.007 - prob: 1.000\n",
            "\n",
            "Interval 5188 (51870 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.541 - mean_q: 12.996 - prob: 1.000\n",
            "\n",
            "Interval 5189 (51880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.156 - mean_q: 12.442 - prob: 1.000\n",
            "\n",
            "Interval 5190 (51890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.000 - mae: 7.112 - mean_q: 12.373 - prob: 1.000\n",
            "\n",
            "Interval 5191 (51900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 5192 (51910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.001 - mae: 7.779 - mean_q: 13.169 - prob: 1.000\n",
            "\n",
            "Interval 5193 (51920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.548 - mean_q: 12.902 - prob: 1.000\n",
            "\n",
            "Interval 5194 (51930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5195 (51940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.262 - mean_q: 12.715 - prob: 1.000\n",
            "\n",
            "Interval 5196 (51950 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 5197 (51960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.456 - mean_q: 12.782 - prob: 1.000\n",
            "\n",
            "Interval 5198 (51970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.183 - mean_q: 12.445 - prob: 1.000\n",
            "\n",
            "Interval 5199 (51980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.565 - mean_q: 12.891 - prob: 1.000\n",
            "\n",
            "Interval 5200 (51990 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 5201 (52000 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 5202 (52010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.310 - mean_q: 12.559 - prob: 1.000\n",
            "\n",
            "Interval 5203 (52020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.749 - mean_q: 13.325 - prob: 1.000\n",
            "\n",
            "Interval 5204 (52030 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.336 - mean_q: 12.722 - prob: 1.000\n",
            "\n",
            "Interval 5205 (52040 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.415 - mean_q: 12.831 - prob: 1.000\n",
            "\n",
            "Interval 5206 (52050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5207 (52060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.007 - mean_q: 12.185 - prob: 1.000\n",
            "\n",
            "Interval 5208 (52070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.180 - mean_q: 12.543 - prob: 1.000\n",
            "\n",
            "Interval 5209 (52080 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.433 - mean_q: 12.719 - prob: 1.000\n",
            "\n",
            "Interval 5210 (52090 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.001 - mae: 7.469 - mean_q: 12.791 - prob: 1.000\n",
            "\n",
            "Interval 5211 (52100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 5212 (52110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.000 - mae: 7.556 - mean_q: 12.995 - prob: 1.000\n",
            "\n",
            "Interval 5213 (52120 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 6.950 - mean_q: 12.035 - prob: 1.000\n",
            "\n",
            "Interval 5214 (52130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5215 (52140 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.177 - mean_q: 12.479 - prob: 1.000\n",
            "\n",
            "Interval 5216 (52150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.752 - mean_q: 13.179 - prob: 1.000\n",
            "\n",
            "Interval 5217 (52160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.550 - mean_q: 12.988 - prob: 1.000\n",
            "\n",
            "Interval 5218 (52170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5219 (52180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.784 - mean_q: 13.141 - prob: 1.000\n",
            "\n",
            "Interval 5220 (52190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.210 - mean_q: 12.477 - prob: 1.000\n",
            "\n",
            "Interval 5221 (52200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5222 (52210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.290 - mean_q: 12.726 - prob: 1.000\n",
            "\n",
            "Interval 5223 (52220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 5224 (52230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -17.000 [-17.000, -17.000] - loss: 0.001 - mae: 7.469 - mean_q: 12.882 - prob: 1.000\n",
            "\n",
            "Interval 5225 (52240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5226 (52250 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.499 - mean_q: 12.865 - prob: 1.000\n",
            "\n",
            "Interval 5227 (52260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.709 - mean_q: 13.200 - prob: 1.000\n",
            "\n",
            "Interval 5228 (52270 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.358 - mean_q: 12.610 - prob: 1.000\n",
            "\n",
            "Interval 5229 (52280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5230 (52290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.406 - mean_q: 12.842 - prob: 1.000\n",
            "\n",
            "Interval 5231 (52300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.486 - mean_q: 12.837 - prob: 1.000\n",
            "\n",
            "Interval 5232 (52310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.299 - mean_q: 12.564 - prob: 1.000\n",
            "\n",
            "Interval 5233 (52320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 5234 (52330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.550 - mean_q: 12.898 - prob: 1.000\n",
            "\n",
            "Interval 5235 (52340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.493 - mean_q: 12.871 - prob: 1.000\n",
            "\n",
            "Interval 5236 (52350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 5237 (52360 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -19.000 [-19.000, -19.000] - loss: 0.001 - mae: 7.247 - mean_q: 12.737 - prob: 1.000\n",
            "\n",
            "Interval 5238 (52370 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 5239 (52380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.000 - mae: 7.495 - mean_q: 12.808 - prob: 1.000\n",
            "\n",
            "Interval 5240 (52390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.000 - mae: 7.347 - mean_q: 12.572 - prob: 1.000\n",
            "\n",
            "Interval 5241 (52400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.555 - mean_q: 12.934 - prob: 1.000\n",
            "\n",
            "Interval 5242 (52410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 5243 (52420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.002 - mae: 7.287 - mean_q: 12.511 - prob: 1.000\n",
            "\n",
            "Interval 5244 (52430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.414 - mean_q: 12.857 - prob: 1.000\n",
            "\n",
            "Interval 5245 (52440 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.174 - mean_q: 12.256 - prob: 1.000\n",
            "\n",
            "Interval 5246 (52450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.340 - mean_q: 12.610 - prob: 1.000\n",
            "\n",
            "Interval 5247 (52460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.001 - mae: 6.980 - mean_q: 12.269 - prob: 1.000\n",
            "\n",
            "Interval 5248 (52470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.001 - mae: 7.443 - mean_q: 12.840 - prob: 1.000\n",
            "\n",
            "Interval 5249 (52480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.001 - mae: 7.369 - mean_q: 12.507 - prob: 1.000\n",
            "\n",
            "Interval 5250 (52490 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.264 - mean_q: 12.547 - prob: 1.000\n",
            "\n",
            "Interval 5251 (52500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.724 - mean_q: 13.243 - prob: 1.000\n",
            "\n",
            "Interval 5252 (52510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 5253 (52520 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.618 - mean_q: 12.915 - prob: 1.000\n",
            "\n",
            "Interval 5254 (52530 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.308 - mean_q: 12.646 - prob: 1.000\n",
            "\n",
            "Interval 5255 (52540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5256 (52550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 6.965 - mean_q: 12.073 - prob: 1.000\n",
            "\n",
            "Interval 5257 (52560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 5258 (52570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -18.000 [-18.000, -18.000] - loss: 0.001 - mae: 7.244 - mean_q: 12.501 - prob: 1.000\n",
            "\n",
            "Interval 5259 (52580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.690 - mean_q: 13.110 - prob: 1.000\n",
            "\n",
            "Interval 5260 (52590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5261 (52600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.163 - mean_q: 12.442 - prob: 1.000\n",
            "\n",
            "Interval 5262 (52610 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.278 - mean_q: 12.600 - prob: 1.000\n",
            "\n",
            "Interval 5263 (52620 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 5264 (52630 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.432 - mean_q: 12.769 - prob: 1.000\n",
            "\n",
            "Interval 5265 (52640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.169 - mean_q: 12.450 - prob: 1.000\n",
            "\n",
            "Interval 5266 (52650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5267 (52660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.516 - mean_q: 12.927 - prob: 1.000\n",
            "\n",
            "Interval 5268 (52670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.002 - mae: 7.307 - mean_q: 12.584 - prob: 1.000\n",
            "\n",
            "Interval 5269 (52680 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.098 - mean_q: 12.349 - prob: 1.000\n",
            "\n",
            "Interval 5270 (52690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5271 (52700 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.325 - mean_q: 12.677 - prob: 1.000\n",
            "\n",
            "Interval 5272 (52710 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.469 - mean_q: 12.916 - prob: 1.000\n",
            "\n",
            "Interval 5273 (52720 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.001 - mae: 7.115 - mean_q: 12.433 - prob: 1.000\n",
            "\n",
            "Interval 5274 (52730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.200 - mean_q: 12.465 - prob: 1.000\n",
            "\n",
            "Interval 5275 (52740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 5276 (52750 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.001 - mae: 7.086 - mean_q: 12.374 - prob: 1.000\n",
            "\n",
            "Interval 5277 (52760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5278 (52770 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.308 - mean_q: 12.709 - prob: 1.000\n",
            "\n",
            "Interval 5279 (52780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5280 (52790 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.276 - mean_q: 12.635 - prob: 1.000\n",
            "\n",
            "Interval 5281 (52800 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.568 - mean_q: 12.867 - prob: 1.000\n",
            "\n",
            "Interval 5282 (52810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5283 (52820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.305 - mean_q: 12.542 - prob: 1.000\n",
            "\n",
            "Interval 5284 (52830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5285 (52840 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.361 - mean_q: 12.778 - prob: 1.000\n",
            "\n",
            "Interval 5286 (52850 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.240 - mean_q: 12.617 - prob: 1.000\n",
            "\n",
            "Interval 5287 (52860 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 5288 (52870 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.001 - mae: 7.489 - mean_q: 12.885 - prob: 1.000\n",
            "\n",
            "Interval 5289 (52880 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.391 - mean_q: 12.603 - prob: 1.000\n",
            "\n",
            "Interval 5290 (52890 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.452 - mean_q: 12.777 - prob: 1.000\n",
            "\n",
            "Interval 5291 (52900 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.281 - mean_q: 12.588 - prob: 1.000\n",
            "\n",
            "Interval 5292 (52910 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.290 - mean_q: 12.699 - prob: 1.000\n",
            "\n",
            "Interval 5293 (52920 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 5294 (52930 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.405 - mean_q: 12.867 - prob: 1.000\n",
            "\n",
            "Interval 5295 (52940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 5296 (52950 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -21.000 [-21.000, -21.000] - loss: 0.000 - mae: 7.294 - mean_q: 12.670 - prob: 1.000\n",
            "\n",
            "Interval 5297 (52960 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.275 - mean_q: 12.616 - prob: 1.000\n",
            "\n",
            "Interval 5298 (52970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 5299 (52980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.001 - mae: 7.305 - mean_q: 12.669 - prob: 1.000\n",
            "\n",
            "Interval 5300 (52990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 5301 (53000 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.138 - mean_q: 12.495 - prob: 1.000\n",
            "\n",
            "Interval 5302 (53010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.122 - mean_q: 12.417 - prob: 1.000\n",
            "\n",
            "Interval 5303 (53020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5304 (53030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.251 - mean_q: 12.499 - prob: 1.000\n",
            "\n",
            "Interval 5305 (53040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 5306 (53050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.001 - mae: 7.355 - mean_q: 12.801 - prob: 1.000\n",
            "\n",
            "Interval 5307 (53060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.316 - mean_q: 12.728 - prob: 1.000\n",
            "\n",
            "Interval 5308 (53070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5309 (53080 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.357 - mean_q: 12.779 - prob: 1.000\n",
            "\n",
            "Interval 5310 (53090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 5311 (53100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.001 - mae: 7.025 - mean_q: 12.291 - prob: 1.000\n",
            "\n",
            "Interval 5312 (53110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5313 (53120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.600 - mean_q: 13.222 - prob: 1.000\n",
            "\n",
            "Interval 5314 (53130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.263 - mean_q: 12.450 - prob: 1.000\n",
            "\n",
            "Interval 5315 (53140 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.191 - mean_q: 12.432 - prob: 1.000\n",
            "\n",
            "Interval 5316 (53150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5317 (53160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.577 - mean_q: 12.903 - prob: 1.000\n",
            "\n",
            "Interval 5318 (53170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.773 - mean_q: 13.200 - prob: 1.000\n",
            "\n",
            "Interval 5319 (53180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 5320 (53190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.000 - mae: 7.350 - mean_q: 12.710 - prob: 1.000\n",
            "\n",
            "Interval 5321 (53200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 6.993 - mean_q: 12.114 - prob: 1.000\n",
            "\n",
            "Interval 5322 (53210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5323 (53220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.000 - mae: 7.450 - mean_q: 12.812 - prob: 1.000\n",
            "\n",
            "Interval 5324 (53230 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.001 - mae: 7.470 - mean_q: 12.797 - prob: 1.000\n",
            "\n",
            "Interval 5325 (53240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5326 (53250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.395 - mean_q: 12.729 - prob: 1.000\n",
            "\n",
            "Interval 5327 (53260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 5328 (53270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -28.000 [-28.000, -28.000] - loss: 0.001 - mae: 7.139 - mean_q: 12.418 - prob: 1.000\n",
            "\n",
            "Interval 5329 (53280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.913 - mean_q: 13.367 - prob: 1.000\n",
            "\n",
            "Interval 5330 (53290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5331 (53300 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.550 - mean_q: 12.991 - prob: 1.000\n",
            "\n",
            "Interval 5332 (53310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.273 - mean_q: 12.615 - prob: 1.000\n",
            "\n",
            "Interval 5333 (53320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.001 - mae: 7.402 - mean_q: 12.837 - prob: 1.000\n",
            "\n",
            "Interval 5334 (53330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.464 - mean_q: 12.784 - prob: 1.000\n",
            "\n",
            "Interval 5335 (53340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5336 (53350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.447 - mean_q: 12.860 - prob: 1.000\n",
            "\n",
            "Interval 5337 (53360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.405 - mean_q: 12.692 - prob: 1.000\n",
            "\n",
            "Interval 5338 (53370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.606 - mean_q: 13.057 - prob: 1.000\n",
            "\n",
            "Interval 5339 (53380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5340 (53390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.350 - mean_q: 12.758 - prob: 1.000\n",
            "\n",
            "Interval 5341 (53400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5342 (53410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5343 (53420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.001 - mae: 7.383 - mean_q: 12.704 - prob: 1.000\n",
            "\n",
            "Interval 5344 (53430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.270 - mean_q: 12.714 - prob: 1.000\n",
            "\n",
            "Interval 5345 (53440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.225 - mean_q: 12.543 - prob: 1.000\n",
            "\n",
            "Interval 5346 (53450 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 5347 (53460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.516 - mean_q: 13.014 - prob: 1.000\n",
            "\n",
            "Interval 5348 (53470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.525 - mean_q: 13.009 - prob: 1.000\n",
            "\n",
            "Interval 5349 (53480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.322 - mean_q: 12.652 - prob: 1.000\n",
            "\n",
            "Interval 5350 (53490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 5351 (53500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -22.000 [-22.000, -22.000] - loss: 0.001 - mae: 7.220 - mean_q: 12.497 - prob: 1.000\n",
            "\n",
            "Interval 5352 (53510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.001 - mae: 7.841 - mean_q: 13.342 - prob: 1.000\n",
            "\n",
            "Interval 5353 (53520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5354 (53530 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 11.000 [8.000, 14.000] - loss: 0.001 - mae: 7.193 - mean_q: 12.460 - prob: 1.000\n",
            "\n",
            "Interval 5355 (53540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 5356 (53550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.161 - mean_q: 12.506 - prob: 1.000\n",
            "\n",
            "Interval 5357 (53560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.336 - mean_q: 12.700 - prob: 1.000\n",
            "\n",
            "Interval 5358 (53570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.435 - mean_q: 12.749 - prob: 1.000\n",
            "\n",
            "Interval 5359 (53580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5360 (53590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.450 - mean_q: 12.752 - prob: 1.000\n",
            "\n",
            "Interval 5361 (53600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.666 - mean_q: 13.086 - prob: 1.000\n",
            "\n",
            "Interval 5362 (53610 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 5363 (53620 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.054 - mean_q: 12.239 - prob: 1.000\n",
            "\n",
            "Interval 5364 (53630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.266 - mean_q: 12.604 - prob: 1.000\n",
            "\n",
            "Interval 5365 (53640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5366 (53650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.460 - mean_q: 12.801 - prob: 1.000\n",
            "\n",
            "Interval 5367 (53660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5368 (53670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.367 - mean_q: 12.665 - prob: 1.000\n",
            "\n",
            "Interval 5369 (53680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5370 (53690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.293 - mean_q: 12.567 - prob: 1.000\n",
            "\n",
            "Interval 5371 (53700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.255 - mean_q: 12.580 - prob: 1.000\n",
            "\n",
            "Interval 5372 (53710 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 5373 (53720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.206 - mean_q: 12.543 - prob: 1.000\n",
            "\n",
            "Interval 5374 (53730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 5375 (53740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 6.000 [-2.000, 14.000] - loss: 0.001 - mae: 7.252 - mean_q: 12.561 - prob: 1.000\n",
            "\n",
            "Interval 5376 (53750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5377 (53760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.315 - mean_q: 12.491 - prob: 1.000\n",
            "\n",
            "Interval 5378 (53770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5379 (53780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.300 - mean_q: 12.593 - prob: 1.000\n",
            "\n",
            "Interval 5380 (53790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.324 - mean_q: 12.617 - prob: 1.000\n",
            "\n",
            "Interval 5381 (53800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5382 (53810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.366 - mean_q: 12.603 - prob: 1.000\n",
            "\n",
            "Interval 5383 (53820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.157 - mean_q: 12.393 - prob: 1.000\n",
            "\n",
            "Interval 5384 (53830 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.431 - mean_q: 12.897 - prob: 1.000\n",
            "\n",
            "Interval 5385 (53840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.070 - mean_q: 12.268 - prob: 1.000\n",
            "\n",
            "Interval 5386 (53850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 5387 (53860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.252 - mean_q: 12.486 - prob: 1.000\n",
            "\n",
            "Interval 5388 (53870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.319 - mean_q: 12.698 - prob: 1.000\n",
            "\n",
            "Interval 5389 (53880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.141 - mean_q: 12.495 - prob: 1.000\n",
            "\n",
            "Interval 5390 (53890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5391 (53900 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.302 - mean_q: 12.755 - prob: 1.000\n",
            "\n",
            "Interval 5392 (53910 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.310 - mean_q: 12.702 - prob: 1.000\n",
            "\n",
            "Interval 5393 (53920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 5394 (53930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 5395 (53940 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -32.000 [-32.000, -32.000] - loss: 0.001 - mae: 7.436 - mean_q: 12.765 - prob: 1.000\n",
            "\n",
            "Interval 5396 (53950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.049 - mean_q: 12.288 - prob: 1.000\n",
            "\n",
            "Interval 5397 (53960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5398 (53970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.536 - mean_q: 12.723 - prob: 1.000\n",
            "\n",
            "Interval 5399 (53980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.264 - mean_q: 12.657 - prob: 1.000\n",
            "\n",
            "Interval 5400 (53990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5401 (54000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.299 - mean_q: 12.845 - prob: 1.000\n",
            "\n",
            "Interval 5402 (54010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.295 - mean_q: 12.562 - prob: 1.000\n",
            "\n",
            "Interval 5403 (54020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 5404 (54030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.563 - mean_q: 12.989 - prob: 1.000\n",
            "\n",
            "Interval 5405 (54040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.053 - mean_q: 12.226 - prob: 1.000\n",
            "\n",
            "Interval 5406 (54050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.337 - mean_q: 12.611 - prob: 1.000\n",
            "\n",
            "Interval 5407 (54060 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 5408 (54070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.496 - mean_q: 12.981 - prob: 1.000\n",
            "\n",
            "Interval 5409 (54080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.206 - mean_q: 12.614 - prob: 1.000\n",
            "\n",
            "Interval 5410 (54090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.367 - mean_q: 12.757 - prob: 1.000\n",
            "\n",
            "Interval 5411 (54100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 5412 (54110 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.545 - mean_q: 13.028 - prob: 1.000\n",
            "\n",
            "Interval 5413 (54120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.260 - mean_q: 12.579 - prob: 1.000\n",
            "\n",
            "Interval 5414 (54130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5415 (54140 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.298 - mean_q: 12.554 - prob: 1.000\n",
            "\n",
            "Interval 5416 (54150 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.051 - mean_q: 12.383 - prob: 1.000\n",
            "\n",
            "Interval 5417 (54160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.512 - mean_q: 12.899 - prob: 1.000\n",
            "\n",
            "Interval 5418 (54170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5419 (54180 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.365 - mean_q: 12.835 - prob: 1.000\n",
            "\n",
            "Interval 5420 (54190 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.0000\n",
            "Interval 5421 (54200 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.297 - mean_q: 12.474 - prob: 1.000\n",
            "\n",
            "Interval 5422 (54210 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.487 - mean_q: 12.860 - prob: 1.000\n",
            "\n",
            "Interval 5423 (54220 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.366 - mean_q: 12.715 - prob: 1.000\n",
            "\n",
            "Interval 5424 (54230 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.000 - mae: 7.613 - mean_q: 13.142 - prob: 1.000\n",
            "\n",
            "Interval 5425 (54240 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -2.8000\n",
            "Interval 5426 (54250 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.000 - mae: 7.205 - mean_q: 12.417 - prob: 1.000\n",
            "\n",
            "Interval 5427 (54260 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.0000\n",
            "Interval 5428 (54270 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.002 - mae: 7.518 - mean_q: 12.998 - prob: 1.000\n",
            "\n",
            "Interval 5429 (54280 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.249 - mean_q: 12.483 - prob: 1.000\n",
            "\n",
            "Interval 5430 (54290 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 5431 (54300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.264 - mean_q: 12.640 - prob: 1.000\n",
            "\n",
            "Interval 5432 (54310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.363 - mean_q: 12.651 - prob: 1.000\n",
            "\n",
            "Interval 5433 (54320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5434 (54330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.137 - mean_q: 12.318 - prob: 1.000\n",
            "\n",
            "Interval 5435 (54340 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.145 - mean_q: 12.352 - prob: 1.000\n",
            "\n",
            "Interval 5436 (54350 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 5437 (54360 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.344 - mean_q: 12.702 - prob: 1.000\n",
            "\n",
            "Interval 5438 (54370 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.207 - mean_q: 12.419 - prob: 1.000\n",
            "\n",
            "Interval 5439 (54380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 5440 (54390 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.002 - mae: 7.096 - mean_q: 12.410 - prob: 1.000\n",
            "\n",
            "Interval 5441 (54400 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.272 - mean_q: 12.484 - prob: 1.000\n",
            "\n",
            "Interval 5442 (54410 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.002 - mae: 7.460 - mean_q: 12.766 - prob: 1.000\n",
            "\n",
            "Interval 5443 (54420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 5444 (54430 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.449 - mean_q: 12.873 - prob: 1.000\n",
            "\n",
            "Interval 5445 (54440 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 5446 (54450 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.209 - mean_q: 12.421 - prob: 1.000\n",
            "\n",
            "Interval 5447 (54460 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.486 - mean_q: 12.855 - prob: 1.000\n",
            "\n",
            "Interval 5448 (54470 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.467 - mean_q: 12.952 - prob: 1.000\n",
            "\n",
            "Interval 5449 (54480 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 5450 (54490 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.651 - mean_q: 13.096 - prob: 1.000\n",
            "\n",
            "Interval 5451 (54500 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.185 - mean_q: 12.640 - prob: 1.000\n",
            "\n",
            "Interval 5452 (54510 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 5453 (54520 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.376 - mean_q: 12.653 - prob: 1.000\n",
            "\n",
            "Interval 5454 (54530 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.294 - mean_q: 12.439 - prob: 1.000\n",
            "\n",
            "Interval 5455 (54540 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.640 - mean_q: 13.099 - prob: 1.000\n",
            "\n",
            "Interval 5456 (54550 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 5457 (54560 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.177 - mean_q: 12.425 - prob: 1.000\n",
            "\n",
            "Interval 5458 (54570 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.445 - mean_q: 12.931 - prob: 1.000\n",
            "\n",
            "Interval 5459 (54580 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 5460 (54590 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.161 - mean_q: 12.455 - prob: 1.000\n",
            "\n",
            "Interval 5461 (54600 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.243 - mean_q: 12.559 - prob: 1.000\n",
            "\n",
            "Interval 5462 (54610 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.344 - mean_q: 12.624 - prob: 1.000\n",
            "\n",
            "Interval 5463 (54620 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 5464 (54630 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.148 - mean_q: 12.515 - prob: 1.000\n",
            "\n",
            "Interval 5465 (54640 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.253 - mean_q: 12.517 - prob: 1.000\n",
            "\n",
            "Interval 5466 (54650 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.585 - mean_q: 12.906 - prob: 1.000\n",
            "\n",
            "Interval 5467 (54660 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 5468 (54670 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.474 - mean_q: 12.905 - prob: 1.000\n",
            "\n",
            "Interval 5469 (54680 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.204 - mean_q: 12.432 - prob: 1.000\n",
            "\n",
            "Interval 5470 (54690 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.9000\n",
            "Interval 5471 (54700 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.354 - mean_q: 12.806 - prob: 1.000\n",
            "\n",
            "Interval 5472 (54710 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 5473 (54720 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.648 - mean_q: 13.145 - prob: 1.000\n",
            "\n",
            "Interval 5474 (54730 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 5475 (54740 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.001 - mae: 7.587 - mean_q: 13.029 - prob: 1.000\n",
            "\n",
            "Interval 5476 (54750 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.0000\n",
            "Interval 5477 (54760 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -19.000 [-19.000, -19.000] - loss: 0.002 - mae: 7.250 - mean_q: 12.492 - prob: 1.000\n",
            "\n",
            "Interval 5478 (54770 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.9000\n",
            "Interval 5479 (54780 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.343 - mean_q: 12.681 - prob: 1.000\n",
            "\n",
            "Interval 5480 (54790 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.267 - mean_q: 12.686 - prob: 1.000\n",
            "\n",
            "Interval 5481 (54800 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.441 - mean_q: 12.803 - prob: 1.000\n",
            "\n",
            "Interval 5482 (54810 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.001 - mae: 7.105 - mean_q: 12.285 - prob: 1.000\n",
            "\n",
            "Interval 5483 (54820 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.424 - mean_q: 12.750 - prob: 1.000\n",
            "\n",
            "Interval 5484 (54830 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 5485 (54840 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.526 - mean_q: 12.870 - prob: 1.000\n",
            "\n",
            "Interval 5486 (54850 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.187 - mean_q: 12.457 - prob: 1.000\n",
            "\n",
            "Interval 5487 (54860 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.455 - mean_q: 12.860 - prob: 1.000\n",
            "\n",
            "Interval 5488 (54870 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 5489 (54880 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.306 - mean_q: 12.704 - prob: 1.000\n",
            "\n",
            "Interval 5490 (54890 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 5491 (54900 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.524 - mean_q: 13.013 - prob: 1.000\n",
            "\n",
            "Interval 5492 (54910 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 5493 (54920 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.286 - mean_q: 12.543 - prob: 1.000\n",
            "\n",
            "Interval 5494 (54930 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.002 - mae: 7.106 - mean_q: 12.276 - prob: 1.000\n",
            "\n",
            "Interval 5495 (54940 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.9000\n",
            "Interval 5496 (54950 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.580 - mean_q: 13.028 - prob: 1.000\n",
            "\n",
            "Interval 5497 (54960 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.618 - mean_q: 13.060 - prob: 1.000\n",
            "\n",
            "Interval 5498 (54970 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.124 - mean_q: 12.230 - prob: 1.000\n",
            "\n",
            "Interval 5499 (54980 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -2.8000\n",
            "Interval 5500 (54990 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.001 - mae: 7.640 - mean_q: 13.175 - prob: 1.000\n",
            "\n",
            "Interval 5501 (55000 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.418 - mean_q: 12.782 - prob: 1.000\n",
            "\n",
            "Interval 5502 (55010 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -2.8000\n",
            "Interval 5503 (55020 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -23.000 [-23.000, -23.000] - loss: 0.001 - mae: 7.576 - mean_q: 12.966 - prob: 1.000\n",
            "\n",
            "Interval 5504 (55030 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.199 - mean_q: 12.574 - prob: 1.000\n",
            "\n",
            "Interval 5505 (55040 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.797 - mean_q: 13.231 - prob: 1.000\n",
            "\n",
            "Interval 5506 (55050 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 5507 (55060 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.000 - mae: 6.938 - mean_q: 12.070 - prob: 1.000\n",
            "\n",
            "Interval 5508 (55070 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 5509 (55080 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.569 - mean_q: 13.087 - prob: 1.000\n",
            "\n",
            "Interval 5510 (55090 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.202 - mean_q: 12.460 - prob: 1.000\n",
            "\n",
            "Interval 5511 (55100 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.371 - mean_q: 12.651 - prob: 1.000\n",
            "\n",
            "Interval 5512 (55110 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.584 - mean_q: 12.973 - prob: 1.000\n",
            "\n",
            "Interval 5513 (55120 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 5514 (55130 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.000 - mae: 7.289 - mean_q: 12.680 - prob: 1.000\n",
            "\n",
            "Interval 5515 (55140 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.269 - mean_q: 12.627 - prob: 1.000\n",
            "\n",
            "Interval 5516 (55150 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.000 - mae: 7.418 - mean_q: 12.780 - prob: 1.000\n",
            "\n",
            "Interval 5517 (55160 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.028 - mean_q: 12.242 - prob: 1.000\n",
            "\n",
            "Interval 5518 (55170 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.9000\n",
            "Interval 5519 (55180 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.239 - mean_q: 12.415 - prob: 1.000\n",
            "\n",
            "Interval 5520 (55190 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.472 - mean_q: 12.824 - prob: 1.000\n",
            "\n",
            "Interval 5521 (55200 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: -1.9000\n",
            "Interval 5522 (55210 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.573 - mean_q: 13.093 - prob: 1.000\n",
            "\n",
            "Interval 5523 (55220 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 5524 (55230 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.548 - mean_q: 13.083 - prob: 1.000\n",
            "\n",
            "Interval 5525 (55240 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.161 - mean_q: 12.460 - prob: 1.000\n",
            "\n",
            "Interval 5526 (55250 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.0000\n",
            "Interval 5527 (55260 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.702 - mean_q: 13.066 - prob: 1.000\n",
            "\n",
            "Interval 5528 (55270 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 5529 (55280 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.001 - mae: 7.526 - mean_q: 12.968 - prob: 1.000\n",
            "\n",
            "Interval 5530 (55290 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 5531 (55300 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.233 - mean_q: 12.537 - prob: 1.000\n",
            "\n",
            "Interval 5532 (55310 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.492 - mean_q: 12.932 - prob: 1.000\n",
            "\n",
            "Interval 5533 (55320 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 5534 (55330 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -18.000 [-18.000, -18.000] - loss: 0.001 - mae: 7.251 - mean_q: 12.552 - prob: 1.000\n",
            "\n",
            "Interval 5535 (55340 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 5536 (55350 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.163 - mean_q: 12.307 - prob: 1.000\n",
            "\n",
            "Interval 5537 (55360 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.050 - mean_q: 12.368 - prob: 1.000\n",
            "\n",
            "Interval 5538 (55370 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.571 - mean_q: 13.087 - prob: 1.000\n",
            "\n",
            "Interval 5539 (55380 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.000 - mae: 7.387 - mean_q: 12.821 - prob: 1.000\n",
            "\n",
            "Interval 5540 (55390 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 5541 (55400 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.341 - mean_q: 12.601 - prob: 1.000\n",
            "\n",
            "Interval 5542 (55410 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.420 - mean_q: 12.778 - prob: 1.000\n",
            "\n",
            "Interval 5543 (55420 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.174 - mean_q: 12.455 - prob: 1.000\n",
            "\n",
            "Interval 5544 (55430 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 5545 (55440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.556 - mean_q: 13.022 - prob: 1.000\n",
            "\n",
            "Interval 5546 (55450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5547 (55460 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -20.000 [-20.000, -20.000] - loss: 0.001 - mae: 7.222 - mean_q: 12.447 - prob: 1.000\n",
            "\n",
            "Interval 5548 (55470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.573 - mean_q: 13.034 - prob: 1.000\n",
            "\n",
            "Interval 5549 (55480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 5550 (55490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.001 - mae: 7.135 - mean_q: 12.477 - prob: 1.000\n",
            "\n",
            "Interval 5551 (55500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.391 - mean_q: 12.446 - prob: 1.000\n",
            "\n",
            "Interval 5552 (55510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5553 (55520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.397 - mean_q: 12.737 - prob: 1.000\n",
            "\n",
            "Interval 5554 (55530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 5555 (55540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.306 - mean_q: 12.615 - prob: 1.000\n",
            "\n",
            "Interval 5556 (55550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.192 - mean_q: 12.452 - prob: 1.000\n",
            "\n",
            "Interval 5557 (55560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5558 (55570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.655 - mean_q: 13.186 - prob: 1.000\n",
            "\n",
            "Interval 5559 (55580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.285 - mean_q: 12.647 - prob: 1.000\n",
            "\n",
            "Interval 5560 (55590 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 5561 (55600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.353 - mean_q: 12.667 - prob: 1.000\n",
            "\n",
            "Interval 5562 (55610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.256 - mean_q: 12.541 - prob: 1.000\n",
            "\n",
            "Interval 5563 (55620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5564 (55630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.309 - mean_q: 12.631 - prob: 1.000\n",
            "\n",
            "Interval 5565 (55640 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.237 - mean_q: 12.539 - prob: 1.000\n",
            "\n",
            "Interval 5566 (55650 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.001 - mae: 7.342 - mean_q: 12.732 - prob: 1.000\n",
            "\n",
            "Interval 5567 (55660 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.080 - mean_q: 12.300 - prob: 1.000\n",
            "\n",
            "Interval 5568 (55670 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.377 - mean_q: 12.757 - prob: 1.000\n",
            "\n",
            "Interval 5569 (55680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.220 - mean_q: 12.471 - prob: 1.000\n",
            "\n",
            "Interval 5570 (55690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5571 (55700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.337 - mean_q: 12.641 - prob: 1.000\n",
            "\n",
            "Interval 5572 (55710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.398 - mean_q: 12.707 - prob: 1.000\n",
            "\n",
            "Interval 5573 (55720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.527 - mean_q: 12.986 - prob: 1.000\n",
            "\n",
            "Interval 5574 (55730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5575 (55740 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.374 - mean_q: 12.641 - prob: 1.000\n",
            "\n",
            "Interval 5576 (55750 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.265 - mean_q: 12.533 - prob: 1.000\n",
            "\n",
            "Interval 5577 (55760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.270 - mean_q: 12.473 - prob: 1.000\n",
            "\n",
            "Interval 5578 (55770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5579 (55780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.604 - mean_q: 13.185 - prob: 1.000\n",
            "\n",
            "Interval 5580 (55790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.500 - mean_q: 12.919 - prob: 1.000\n",
            "\n",
            "Interval 5581 (55800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 5582 (55810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.000 - mae: 7.097 - mean_q: 12.249 - prob: 1.000\n",
            "\n",
            "Interval 5583 (55820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.404 - mean_q: 12.819 - prob: 1.000\n",
            "\n",
            "Interval 5584 (55830 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 5585 (55840 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.555 - mean_q: 12.919 - prob: 1.000\n",
            "\n",
            "Interval 5586 (55850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5587 (55860 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.564 - mean_q: 13.051 - prob: 1.000\n",
            "\n",
            "Interval 5588 (55870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5589 (55880 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.324 - mean_q: 12.754 - prob: 1.000\n",
            "\n",
            "Interval 5590 (55890 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 5591 (55900 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.233 - mean_q: 12.494 - prob: 1.000\n",
            "\n",
            "Interval 5592 (55910 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 5593 (55920 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.000 - mae: 7.164 - mean_q: 12.419 - prob: 1.000\n",
            "\n",
            "Interval 5594 (55930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5595 (55940 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 10.500 [8.000, 13.000] - loss: 0.000 - mae: 7.335 - mean_q: 12.695 - prob: 1.000\n",
            "\n",
            "Interval 5596 (55950 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 5597 (55960 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.519 - mean_q: 12.951 - prob: 1.000\n",
            "\n",
            "Interval 5598 (55970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5599 (55980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.248 - mean_q: 12.731 - prob: 1.000\n",
            "\n",
            "Interval 5600 (55990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.183 - mean_q: 12.514 - prob: 1.000\n",
            "\n",
            "Interval 5601 (56000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 5602 (56010 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.388 - mean_q: 12.898 - prob: 1.000\n",
            "\n",
            "Interval 5603 (56020 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.047 - mean_q: 12.380 - prob: 1.000\n",
            "\n",
            "Interval 5604 (56030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.416 - mean_q: 12.747 - prob: 1.000\n",
            "\n",
            "Interval 5605 (56040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.001 - mae: 7.331 - mean_q: 12.494 - prob: 1.000\n",
            "\n",
            "Interval 5606 (56050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5607 (56060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 10.000 [5.000, 15.000] - loss: 0.001 - mae: 7.406 - mean_q: 12.994 - prob: 1.000\n",
            "\n",
            "Interval 5608 (56070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 5609 (56080 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.331 - mean_q: 12.755 - prob: 1.000\n",
            "\n",
            "Interval 5610 (56090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.400 - mean_q: 12.745 - prob: 1.000\n",
            "\n",
            "Interval 5611 (56100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 5612 (56110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.003 - mae: 7.191 - mean_q: 12.355 - prob: 1.000\n",
            "\n",
            "Interval 5613 (56120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.563 - mean_q: 12.955 - prob: 1.000\n",
            "\n",
            "Interval 5614 (56130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5615 (56140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.175 - mean_q: 12.275 - prob: 1.000\n",
            "\n",
            "Interval 5616 (56150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.591 - mean_q: 13.108 - prob: 1.000\n",
            "\n",
            "Interval 5617 (56160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5618 (56170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.121 - mean_q: 12.443 - prob: 1.000\n",
            "\n",
            "Interval 5619 (56180 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.433 - mean_q: 12.735 - prob: 1.000\n",
            "\n",
            "Interval 5620 (56190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5621 (56200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.356 - mean_q: 12.614 - prob: 1.000\n",
            "\n",
            "Interval 5622 (56210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.591 - mean_q: 12.945 - prob: 1.000\n",
            "\n",
            "Interval 5623 (56220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 5624 (56230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.354 - mean_q: 12.593 - prob: 1.000\n",
            "\n",
            "Interval 5625 (56240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 5626 (56250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.002 - mae: 7.496 - mean_q: 13.009 - prob: 1.000\n",
            "\n",
            "Interval 5627 (56260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.258 - mean_q: 12.423 - prob: 1.000\n",
            "\n",
            "Interval 5628 (56270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.385 - mean_q: 12.715 - prob: 1.000\n",
            "\n",
            "Interval 5629 (56280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5630 (56290 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.140 - mean_q: 12.539 - prob: 1.000\n",
            "\n",
            "Interval 5631 (56300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5632 (56310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.235 - mean_q: 12.514 - prob: 1.000\n",
            "\n",
            "Interval 5633 (56320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.501 - mean_q: 12.877 - prob: 1.000\n",
            "\n",
            "Interval 5634 (56330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.321 - mean_q: 12.582 - prob: 1.000\n",
            "\n",
            "Interval 5635 (56340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 5636 (56350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.001 - mae: 7.478 - mean_q: 12.849 - prob: 1.000\n",
            "\n",
            "Interval 5637 (56360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.210 - mean_q: 12.452 - prob: 1.000\n",
            "\n",
            "Interval 5638 (56370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5639 (56380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.494 - mean_q: 12.958 - prob: 1.000\n",
            "\n",
            "Interval 5640 (56390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5641 (56400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.227 - mean_q: 12.463 - prob: 1.000\n",
            "\n",
            "Interval 5642 (56410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.414 - mean_q: 12.797 - prob: 1.000\n",
            "\n",
            "Interval 5643 (56420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.001 - mae: 7.336 - mean_q: 12.657 - prob: 1.000\n",
            "\n",
            "Interval 5644 (56430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 5645 (56440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -32.000 [-32.000, -32.000] - loss: 0.001 - mae: 7.274 - mean_q: 12.465 - prob: 1.000\n",
            "\n",
            "Interval 5646 (56450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -4.6000\n",
            "Interval 5647 (56460 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -30.000 [-30.000, -30.000] - loss: 0.001 - mae: 7.259 - mean_q: 12.736 - prob: 1.000\n",
            "\n",
            "Interval 5648 (56470 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.276 - mean_q: 12.385 - prob: 1.000\n",
            "\n",
            "Interval 5649 (56480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.410 - mean_q: 12.646 - prob: 1.000\n",
            "\n",
            "Interval 5650 (56490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 5651 (56500 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.230 - mean_q: 12.581 - prob: 1.000\n",
            "\n",
            "Interval 5652 (56510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.346 - mean_q: 12.639 - prob: 1.000\n",
            "\n",
            "Interval 5653 (56520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5654 (56530 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.001 - mae: 7.554 - mean_q: 13.043 - prob: 1.000\n",
            "\n",
            "Interval 5655 (56540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5656 (56550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.001 - mae: 7.686 - mean_q: 13.095 - prob: 1.000\n",
            "\n",
            "Interval 5657 (56560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.416 - mean_q: 12.765 - prob: 1.000\n",
            "\n",
            "Interval 5658 (56570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.382 - mean_q: 12.771 - prob: 1.000\n",
            "\n",
            "Interval 5659 (56580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.113 - mean_q: 12.440 - prob: 1.000\n",
            "\n",
            "Interval 5660 (56590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 5661 (56600 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.319 - mean_q: 12.648 - prob: 1.000\n",
            "\n",
            "Interval 5662 (56610 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 5663 (56620 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.037 - mean_q: 12.204 - prob: 1.000\n",
            "\n",
            "Interval 5664 (56630 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.361 - mean_q: 12.640 - prob: 1.000\n",
            "\n",
            "Interval 5665 (56640 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 5666 (56650 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.612 - mean_q: 12.993 - prob: 1.000\n",
            "\n",
            "Interval 5667 (56660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.431 - mean_q: 12.819 - prob: 1.000\n",
            "\n",
            "Interval 5668 (56670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.540 - mean_q: 12.928 - prob: 1.000\n",
            "\n",
            "Interval 5669 (56680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5670 (56690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.292 - mean_q: 12.644 - prob: 1.000\n",
            "\n",
            "Interval 5671 (56700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.001 - mae: 7.187 - mean_q: 12.499 - prob: 1.000\n",
            "\n",
            "Interval 5672 (56710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.003 - mae: 7.198 - mean_q: 12.511 - prob: 1.000\n",
            "\n",
            "Interval 5673 (56720 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 5674 (56730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.139 - mean_q: 12.447 - prob: 1.000\n",
            "\n",
            "Interval 5675 (56740 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.310 - mean_q: 12.595 - prob: 1.000\n",
            "\n",
            "Interval 5676 (56750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5677 (56760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.396 - mean_q: 12.857 - prob: 1.000\n",
            "\n",
            "Interval 5678 (56770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.630 - mean_q: 13.053 - prob: 1.000\n",
            "\n",
            "Interval 5679 (56780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.367 - mean_q: 12.656 - prob: 1.000\n",
            "\n",
            "Interval 5680 (56790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 5681 (56800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.266 - mean_q: 12.455 - prob: 1.000\n",
            "\n",
            "Interval 5682 (56810 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.429 - mean_q: 12.900 - prob: 1.000\n",
            "\n",
            "Interval 5683 (56820 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.220 - mean_q: 12.509 - prob: 1.000\n",
            "\n",
            "Interval 5684 (56830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5685 (56840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.365 - mean_q: 12.794 - prob: 1.000\n",
            "\n",
            "Interval 5686 (56850 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.420 - mean_q: 12.712 - prob: 1.000\n",
            "\n",
            "Interval 5687 (56860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5688 (56870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.343 - mean_q: 12.738 - prob: 1.000\n",
            "\n",
            "Interval 5689 (56880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5690 (56890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.271 - mean_q: 12.730 - prob: 1.000\n",
            "\n",
            "Interval 5691 (56900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.632 - mean_q: 13.073 - prob: 1.000\n",
            "\n",
            "Interval 5692 (56910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5693 (56920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.668 - mean_q: 13.053 - prob: 1.000\n",
            "\n",
            "Interval 5694 (56930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.252 - mean_q: 12.491 - prob: 1.000\n",
            "\n",
            "Interval 5695 (56940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5696 (56950 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.741 - mean_q: 13.202 - prob: 1.000\n",
            "\n",
            "Interval 5697 (56960 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.496 - mean_q: 12.741 - prob: 1.000\n",
            "\n",
            "Interval 5698 (56970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 5699 (56980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.378 - mean_q: 12.666 - prob: 1.000\n",
            "\n",
            "Interval 5700 (56990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.305 - mean_q: 12.583 - prob: 1.000\n",
            "\n",
            "Interval 5701 (57000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.528 - mean_q: 12.867 - prob: 1.000\n",
            "\n",
            "Interval 5702 (57010 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.509 - mean_q: 12.805 - prob: 1.000\n",
            "\n",
            "Interval 5703 (57020 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.092 - mean_q: 12.258 - prob: 1.000\n",
            "\n",
            "Interval 5704 (57030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 5705 (57040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.355 - mean_q: 12.727 - prob: 1.000\n",
            "\n",
            "Interval 5706 (57050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5707 (57060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.587 - mean_q: 13.156 - prob: 1.000\n",
            "\n",
            "Interval 5708 (57070 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.263 - mean_q: 12.478 - prob: 1.000\n",
            "\n",
            "Interval 5709 (57080 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 5710 (57090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.153 - mean_q: 12.492 - prob: 1.000\n",
            "\n",
            "Interval 5711 (57100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.449 - mean_q: 12.717 - prob: 1.000\n",
            "\n",
            "Interval 5712 (57110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5713 (57120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.002 - mae: 7.433 - mean_q: 12.621 - prob: 1.000\n",
            "\n",
            "Interval 5714 (57130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5715 (57140 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.745 - mean_q: 13.258 - prob: 1.000\n",
            "\n",
            "Interval 5716 (57150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.857 - mean_q: 13.430 - prob: 1.000\n",
            "\n",
            "Interval 5717 (57160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.634 - mean_q: 13.225 - prob: 1.000\n",
            "\n",
            "Interval 5718 (57170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5719 (57180 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.598 - mean_q: 13.024 - prob: 1.000\n",
            "\n",
            "Interval 5720 (57190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.001 - mae: 7.152 - mean_q: 12.475 - prob: 1.000\n",
            "\n",
            "Interval 5721 (57200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.685 - mean_q: 13.118 - prob: 1.000\n",
            "\n",
            "Interval 5722 (57210 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 5723 (57220 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 2.3000\n",
            "2 episodes - episode_reward: 6.500 [3.000, 10.000] - loss: 0.001 - mae: 7.131 - mean_q: 12.272 - prob: 1.000\n",
            "\n",
            "Interval 5724 (57230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5725 (57240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.648 - mean_q: 13.162 - prob: 1.000\n",
            "\n",
            "Interval 5726 (57250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.446 - mean_q: 12.967 - prob: 1.000\n",
            "\n",
            "Interval 5727 (57260 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.402 - mean_q: 12.584 - prob: 1.000\n",
            "\n",
            "Interval 5728 (57270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 5729 (57280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.000 - mae: 7.602 - mean_q: 13.018 - prob: 1.000\n",
            "\n",
            "Interval 5730 (57290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5731 (57300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.233 - mean_q: 12.731 - prob: 1.000\n",
            "\n",
            "Interval 5732 (57310 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.514 - mean_q: 13.014 - prob: 1.000\n",
            "\n",
            "Interval 5733 (57320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5734 (57330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.204 - mean_q: 12.641 - prob: 1.000\n",
            "\n",
            "Interval 5735 (57340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.462 - mean_q: 12.819 - prob: 1.000\n",
            "\n",
            "Interval 5736 (57350 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 5737 (57360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.433 - mean_q: 12.747 - prob: 1.000\n",
            "\n",
            "Interval 5738 (57370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.633 - mean_q: 13.101 - prob: 1.000\n",
            "\n",
            "Interval 5739 (57380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 5740 (57390 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.608 - mean_q: 13.085 - prob: 1.000\n",
            "\n",
            "Interval 5741 (57400 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.827 - mean_q: 13.443 - prob: 1.000\n",
            "\n",
            "Interval 5742 (57410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5743 (57420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.480 - mean_q: 12.838 - prob: 1.000\n",
            "\n",
            "Interval 5744 (57430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 5745 (57440 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.523 - mean_q: 12.957 - prob: 1.000\n",
            "\n",
            "Interval 5746 (57450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.557 - mean_q: 13.089 - prob: 1.000\n",
            "\n",
            "Interval 5747 (57460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.249 - mean_q: 12.478 - prob: 1.000\n",
            "\n",
            "Interval 5748 (57470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5749 (57480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.166 - mean_q: 12.600 - prob: 1.000\n",
            "\n",
            "Interval 5750 (57490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.248 - mean_q: 12.598 - prob: 1.000\n",
            "\n",
            "Interval 5751 (57500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 5752 (57510 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.354 - mean_q: 12.858 - prob: 1.000\n",
            "\n",
            "Interval 5753 (57520 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.211 - mean_q: 12.510 - prob: 1.000\n",
            "\n",
            "Interval 5754 (57530 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.515 - mean_q: 12.852 - prob: 1.000\n",
            "\n",
            "Interval 5755 (57540 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 5756 (57550 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.486 - mean_q: 12.830 - prob: 1.000\n",
            "\n",
            "Interval 5757 (57560 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.462 - mean_q: 12.832 - prob: 1.000\n",
            "\n",
            "Interval 5758 (57570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5759 (57580 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.673 - mean_q: 13.181 - prob: 1.000\n",
            "\n",
            "Interval 5760 (57590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5761 (57600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -17.000 [-17.000, -17.000] - loss: 0.001 - mae: 7.504 - mean_q: 12.949 - prob: 1.000\n",
            "\n",
            "Interval 5762 (57610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.449 - mean_q: 12.931 - prob: 1.000\n",
            "\n",
            "Interval 5763 (57620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.404 - mean_q: 12.806 - prob: 1.000\n",
            "\n",
            "Interval 5764 (57630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5765 (57640 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.012 - mean_q: 12.289 - prob: 1.000\n",
            "\n",
            "Interval 5766 (57650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.334 - mean_q: 12.622 - prob: 1.000\n",
            "\n",
            "Interval 5767 (57660 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.175 - mean_q: 12.452 - prob: 1.000\n",
            "\n",
            "Interval 5768 (57670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5769 (57680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.471 - mean_q: 12.658 - prob: 1.000\n",
            "\n",
            "Interval 5770 (57690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5771 (57700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.363 - mean_q: 12.752 - prob: 1.000\n",
            "\n",
            "Interval 5772 (57710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.537 - mean_q: 12.879 - prob: 1.000\n",
            "\n",
            "Interval 5773 (57720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.180 - mean_q: 12.364 - prob: 1.000\n",
            "\n",
            "Interval 5774 (57730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5775 (57740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.378 - mean_q: 12.706 - prob: 1.000\n",
            "\n",
            "Interval 5776 (57750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.273 - mean_q: 12.547 - prob: 1.000\n",
            "\n",
            "Interval 5777 (57760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5778 (57770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.000 - mae: 7.103 - mean_q: 12.303 - prob: 1.000\n",
            "\n",
            "Interval 5779 (57780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 5780 (57790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.000 - mae: 7.269 - mean_q: 12.584 - prob: 1.000\n",
            "\n",
            "Interval 5781 (57800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5782 (57810 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.160 - mean_q: 12.496 - prob: 1.000\n",
            "\n",
            "Interval 5783 (57820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.282 - mean_q: 12.637 - prob: 1.000\n",
            "\n",
            "Interval 5784 (57830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5785 (57840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.176 - mean_q: 12.522 - prob: 1.000\n",
            "\n",
            "Interval 5786 (57850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5787 (57860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.377 - mean_q: 12.788 - prob: 1.000\n",
            "\n",
            "Interval 5788 (57870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 5789 (57880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.453 - mean_q: 12.903 - prob: 1.000\n",
            "\n",
            "Interval 5790 (57890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.574 - mean_q: 12.961 - prob: 1.000\n",
            "\n",
            "Interval 5791 (57900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5792 (57910 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.257 - mean_q: 12.479 - prob: 1.000\n",
            "\n",
            "Interval 5793 (57920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.6000\n",
            "1 episodes - episode_reward: -19.000 [-19.000, -19.000] - loss: 0.001 - mae: 7.469 - mean_q: 12.993 - prob: 1.000\n",
            "\n",
            "Interval 5794 (57930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 5795 (57940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.000 - mae: 7.574 - mean_q: 13.078 - prob: 1.000\n",
            "\n",
            "Interval 5796 (57950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5797 (57960 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.6000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.508 - mean_q: 12.914 - prob: 1.000\n",
            "\n",
            "Interval 5798 (57970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5799 (57980 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -27.000 [-27.000, -27.000] - loss: 0.000 - mae: 7.352 - mean_q: 12.711 - prob: 1.000\n",
            "\n",
            "Interval 5800 (57990 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.314 - mean_q: 12.541 - prob: 1.000\n",
            "\n",
            "Interval 5801 (58000 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.422 - mean_q: 12.815 - prob: 1.000\n",
            "\n",
            "Interval 5802 (58010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.103 - mean_q: 12.351 - prob: 1.000\n",
            "\n",
            "Interval 5803 (58020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5804 (58030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.801 - mean_q: 13.543 - prob: 1.000\n",
            "\n",
            "Interval 5805 (58040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.317 - mean_q: 12.613 - prob: 1.000\n",
            "\n",
            "Interval 5806 (58050 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 5807 (58060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.459 - mean_q: 12.859 - prob: 1.000\n",
            "\n",
            "Interval 5808 (58070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5809 (58080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.411 - mean_q: 12.871 - prob: 1.000\n",
            "\n",
            "Interval 5810 (58090 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.123 - mean_q: 12.365 - prob: 1.000\n",
            "\n",
            "Interval 5811 (58100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5812 (58110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.273 - mean_q: 12.742 - prob: 1.000\n",
            "\n",
            "Interval 5813 (58120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.529 - mean_q: 12.807 - prob: 1.000\n",
            "\n",
            "Interval 5814 (58130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5815 (58140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.000 - mae: 7.177 - mean_q: 12.498 - prob: 1.000\n",
            "\n",
            "Interval 5816 (58150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5817 (58160 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.513 - mean_q: 13.028 - prob: 1.000\n",
            "\n",
            "Interval 5818 (58170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5819 (58180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.359 - mean_q: 12.828 - prob: 1.000\n",
            "\n",
            "Interval 5820 (58190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.535 - mean_q: 12.989 - prob: 1.000\n",
            "\n",
            "Interval 5821 (58200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 5822 (58210 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.001 - mae: 7.287 - mean_q: 12.670 - prob: 1.000\n",
            "\n",
            "Interval 5823 (58220 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.517 - mean_q: 12.833 - prob: 1.000\n",
            "\n",
            "Interval 5824 (58230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.600 - mean_q: 13.071 - prob: 1.000\n",
            "\n",
            "Interval 5825 (58240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.541 - mean_q: 12.910 - prob: 1.000\n",
            "\n",
            "Interval 5826 (58250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5827 (58260 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.274 - mean_q: 12.626 - prob: 1.000\n",
            "\n",
            "Interval 5828 (58270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5829 (58280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 6.917 - mean_q: 12.139 - prob: 1.000\n",
            "\n",
            "Interval 5830 (58290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.455 - mean_q: 12.893 - prob: 1.000\n",
            "\n",
            "Interval 5831 (58300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.001 - mae: 7.365 - mean_q: 12.697 - prob: 1.000\n",
            "\n",
            "Interval 5832 (58310 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.304 - mean_q: 12.695 - prob: 1.000\n",
            "\n",
            "Interval 5833 (58320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5834 (58330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.149 - mean_q: 12.430 - prob: 1.000\n",
            "\n",
            "Interval 5835 (58340 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.123 - mean_q: 12.330 - prob: 1.000\n",
            "\n",
            "Interval 5836 (58350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5837 (58360 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.002 - mae: 7.145 - mean_q: 12.286 - prob: 1.000\n",
            "\n",
            "Interval 5838 (58370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.108 - mean_q: 12.230 - prob: 1.000\n",
            "\n",
            "Interval 5839 (58380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 5840 (58390 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.001 - mae: 7.226 - mean_q: 12.501 - prob: 1.000\n",
            "\n",
            "Interval 5841 (58400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.727 - mean_q: 13.173 - prob: 1.000\n",
            "\n",
            "Interval 5842 (58410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.222 - mean_q: 12.477 - prob: 1.000\n",
            "\n",
            "Interval 5843 (58420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.601 - mean_q: 12.948 - prob: 1.000\n",
            "\n",
            "Interval 5844 (58430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5845 (58440 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.044 - mean_q: 12.279 - prob: 1.000\n",
            "\n",
            "Interval 5846 (58450 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.456 - mean_q: 12.810 - prob: 1.000\n",
            "\n",
            "Interval 5847 (58460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 5848 (58470 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.222 - mean_q: 12.598 - prob: 1.000\n",
            "\n",
            "Interval 5849 (58480 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.001 - mae: 7.287 - mean_q: 12.608 - prob: 1.000\n",
            "\n",
            "Interval 5850 (58490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5851 (58500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.675 - mean_q: 13.246 - prob: 1.000\n",
            "\n",
            "Interval 5852 (58510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5853 (58520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.248 - mean_q: 12.453 - prob: 1.000\n",
            "\n",
            "Interval 5854 (58530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.612 - mean_q: 12.854 - prob: 1.000\n",
            "\n",
            "Interval 5855 (58540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.407 - mean_q: 12.880 - prob: 1.000\n",
            "\n",
            "Interval 5856 (58550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5857 (58560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.153 - mean_q: 12.477 - prob: 1.000\n",
            "\n",
            "Interval 5858 (58570 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.686 - mean_q: 13.136 - prob: 1.000\n",
            "\n",
            "Interval 5859 (58580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5860 (58590 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.000 - mae: 7.255 - mean_q: 12.584 - prob: 1.000\n",
            "\n",
            "Interval 5861 (58600 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.331 - mean_q: 12.745 - prob: 1.000\n",
            "\n",
            "Interval 5862 (58610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5863 (58620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.548 - mean_q: 13.015 - prob: 1.000\n",
            "\n",
            "Interval 5864 (58630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.481 - mean_q: 12.951 - prob: 1.000\n",
            "\n",
            "Interval 5865 (58640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.296 - mean_q: 12.565 - prob: 1.000\n",
            "\n",
            "Interval 5866 (58650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.320 - mean_q: 12.758 - prob: 1.000\n",
            "\n",
            "Interval 5867 (58660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 5868 (58670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.474 - mean_q: 12.817 - prob: 1.000\n",
            "\n",
            "Interval 5869 (58680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 5870 (58690 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.362 - mean_q: 12.498 - prob: 1.000\n",
            "\n",
            "Interval 5871 (58700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.259 - mean_q: 12.560 - prob: 1.000\n",
            "\n",
            "Interval 5872 (58710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5873 (58720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.532 - mean_q: 13.026 - prob: 1.000\n",
            "\n",
            "Interval 5874 (58730 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.412 - mean_q: 12.785 - prob: 1.000\n",
            "\n",
            "Interval 5875 (58740 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 5876 (58750 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.543 - mean_q: 12.987 - prob: 1.000\n",
            "\n",
            "Interval 5877 (58760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.001 - mae: 7.574 - mean_q: 12.946 - prob: 1.000\n",
            "\n",
            "Interval 5878 (58770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.349 - mean_q: 12.702 - prob: 1.000\n",
            "\n",
            "Interval 5879 (58780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5880 (58790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.310 - mean_q: 12.483 - prob: 1.000\n",
            "\n",
            "Interval 5881 (58800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5882 (58810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.001 - mae: 7.280 - mean_q: 12.496 - prob: 1.000\n",
            "\n",
            "Interval 5883 (58820 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 5884 (58830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -17.000 [-17.000, -17.000] - loss: 0.001 - mae: 7.310 - mean_q: 12.683 - prob: 1.000\n",
            "\n",
            "Interval 5885 (58840 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.922 - mean_q: 13.545 - prob: 1.000\n",
            "\n",
            "Interval 5886 (58850 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.753 - mean_q: 13.306 - prob: 1.000\n",
            "\n",
            "Interval 5887 (58860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.002 - mae: 7.390 - mean_q: 12.710 - prob: 1.000\n",
            "\n",
            "Interval 5888 (58870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.403 - mean_q: 12.734 - prob: 1.000\n",
            "\n",
            "Interval 5889 (58880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5890 (58890 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.156 - mean_q: 12.549 - prob: 1.000\n",
            "\n",
            "Interval 5891 (58900 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 5892 (58910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.299 - mean_q: 12.565 - prob: 1.000\n",
            "\n",
            "Interval 5893 (58920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 6.970 - mean_q: 12.182 - prob: 1.000\n",
            "\n",
            "Interval 5894 (58930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.332 - mean_q: 12.691 - prob: 1.000\n",
            "\n",
            "Interval 5895 (58940 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 5896 (58950 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.192 - mean_q: 12.562 - prob: 1.000\n",
            "\n",
            "Interval 5897 (58960 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 5898 (58970 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.001 - mae: 7.357 - mean_q: 12.614 - prob: 1.000\n",
            "\n",
            "Interval 5899 (58980 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.568 - mean_q: 13.040 - prob: 1.000\n",
            "\n",
            "Interval 5900 (58990 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.219 - mean_q: 12.581 - prob: 1.000\n",
            "\n",
            "Interval 5901 (59000 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 5902 (59010 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.176 - mean_q: 12.355 - prob: 1.000\n",
            "\n",
            "Interval 5903 (59020 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.001 - mae: 7.315 - mean_q: 12.651 - prob: 1.000\n",
            "\n",
            "Interval 5904 (59030 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.327 - mean_q: 12.583 - prob: 1.000\n",
            "\n",
            "Interval 5905 (59040 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 5906 (59050 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.172 - mean_q: 12.365 - prob: 1.000\n",
            "\n",
            "Interval 5907 (59060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5908 (59070 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.294 - mean_q: 12.617 - prob: 1.000\n",
            "\n",
            "Interval 5909 (59080 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.355 - mean_q: 12.633 - prob: 1.000\n",
            "\n",
            "Interval 5910 (59090 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.219 - mean_q: 12.498 - prob: 1.000\n",
            "\n",
            "Interval 5911 (59100 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.218 - mean_q: 12.518 - prob: 1.000\n",
            "\n",
            "Interval 5912 (59110 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.398 - mean_q: 12.778 - prob: 1.000\n",
            "\n",
            "Interval 5913 (59120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5914 (59130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.235 - mean_q: 12.582 - prob: 1.000\n",
            "\n",
            "Interval 5915 (59140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.001 - mae: 7.391 - mean_q: 12.719 - prob: 1.000\n",
            "\n",
            "Interval 5916 (59150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5917 (59160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.152 - mean_q: 12.415 - prob: 1.000\n",
            "\n",
            "Interval 5918 (59170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 5919 (59180 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.622 - mean_q: 13.002 - prob: 1.000\n",
            "\n",
            "Interval 5920 (59190 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.388 - mean_q: 12.659 - prob: 1.000\n",
            "\n",
            "Interval 5921 (59200 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.205 - mean_q: 12.554 - prob: 1.000\n",
            "\n",
            "Interval 5922 (59210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5923 (59220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.757 - mean_q: 13.282 - prob: 1.000\n",
            "\n",
            "Interval 5924 (59230 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.340 - mean_q: 12.816 - prob: 1.000\n",
            "\n",
            "Interval 5925 (59240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5926 (59250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.474 - mean_q: 12.794 - prob: 1.000\n",
            "\n",
            "Interval 5927 (59260 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.200 - mean_q: 12.425 - prob: 1.000\n",
            "\n",
            "Interval 5928 (59270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.330 - mean_q: 12.796 - prob: 1.000\n",
            "\n",
            "Interval 5929 (59280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5930 (59290 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.223 - mean_q: 12.532 - prob: 1.000\n",
            "\n",
            "Interval 5931 (59300 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.355 - mean_q: 12.655 - prob: 1.000\n",
            "\n",
            "Interval 5932 (59310 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.000 - mae: 7.413 - mean_q: 12.819 - prob: 1.000\n",
            "\n",
            "Interval 5933 (59320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5934 (59330 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.000 - mae: 7.455 - mean_q: 12.817 - prob: 1.000\n",
            "\n",
            "Interval 5935 (59340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5936 (59350 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 6.977 - mean_q: 12.114 - prob: 1.000\n",
            "\n",
            "Interval 5937 (59360 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.446 - mean_q: 12.796 - prob: 1.000\n",
            "\n",
            "Interval 5938 (59370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.443 - mean_q: 12.803 - prob: 1.000\n",
            "\n",
            "Interval 5939 (59380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5940 (59390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.000 - mae: 7.197 - mean_q: 12.525 - prob: 1.000\n",
            "\n",
            "Interval 5941 (59400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.197 - mean_q: 12.493 - prob: 1.000\n",
            "\n",
            "Interval 5942 (59410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 5943 (59420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.001 - mae: 7.395 - mean_q: 12.816 - prob: 1.000\n",
            "\n",
            "Interval 5944 (59430 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 6.948 - mean_q: 12.238 - prob: 1.000\n",
            "\n",
            "Interval 5945 (59440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 5946 (59450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.502 - mean_q: 13.123 - prob: 1.000\n",
            "\n",
            "Interval 5947 (59460 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 5948 (59470 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.601 - mean_q: 13.082 - prob: 1.000\n",
            "\n",
            "Interval 5949 (59480 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.132 - mean_q: 12.451 - prob: 1.000\n",
            "\n",
            "Interval 5950 (59490 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 5951 (59500 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.830 - mean_q: 13.397 - prob: 1.000\n",
            "\n",
            "Interval 5952 (59510 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.454 - mean_q: 12.912 - prob: 1.000\n",
            "\n",
            "Interval 5953 (59520 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 5954 (59530 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.408 - mean_q: 12.740 - prob: 1.000\n",
            "\n",
            "Interval 5955 (59540 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.042 - mean_q: 12.335 - prob: 1.000\n",
            "\n",
            "Interval 5956 (59550 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.413 - mean_q: 12.759 - prob: 1.000\n",
            "\n",
            "Interval 5957 (59560 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 5958 (59570 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.308 - mean_q: 12.680 - prob: 1.000\n",
            "\n",
            "Interval 5959 (59580 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.670 - mean_q: 13.124 - prob: 1.000\n",
            "\n",
            "Interval 5960 (59590 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.112 - mean_q: 12.379 - prob: 1.000\n",
            "\n",
            "Interval 5961 (59600 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.052 - mean_q: 12.207 - prob: 1.000\n",
            "\n",
            "Interval 5962 (59610 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.290 - mean_q: 12.522 - prob: 1.000\n",
            "\n",
            "Interval 5963 (59620 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.488 - mean_q: 12.879 - prob: 1.000\n",
            "\n",
            "Interval 5964 (59630 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 5965 (59640 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.433 - mean_q: 12.727 - prob: 1.000\n",
            "\n",
            "Interval 5966 (59650 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.217 - mean_q: 12.445 - prob: 1.000\n",
            "\n",
            "Interval 5967 (59660 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.388 - mean_q: 12.801 - prob: 1.000\n",
            "\n",
            "Interval 5968 (59670 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 5969 (59680 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.000 - mae: 7.206 - mean_q: 12.374 - prob: 1.000\n",
            "\n",
            "Interval 5970 (59690 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.159 - mean_q: 12.458 - prob: 1.000\n",
            "\n",
            "Interval 5971 (59700 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 5972 (59710 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.318 - mean_q: 12.691 - prob: 1.000\n",
            "\n",
            "Interval 5973 (59720 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.539 - mean_q: 12.916 - prob: 1.000\n",
            "\n",
            "Interval 5974 (59730 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 5975 (59740 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.580 - mean_q: 13.060 - prob: 1.000\n",
            "\n",
            "Interval 5976 (59750 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.002 - mae: 7.470 - mean_q: 12.965 - prob: 1.000\n",
            "\n",
            "Interval 5977 (59760 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.002 - mae: 7.252 - mean_q: 12.421 - prob: 1.000\n",
            "\n",
            "Interval 5978 (59770 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.002 - mae: 7.438 - mean_q: 12.604 - prob: 1.000\n",
            "\n",
            "Interval 5979 (59780 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 5980 (59790 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.608 - mean_q: 13.059 - prob: 1.000\n",
            "\n",
            "Interval 5981 (59800 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 5982 (59810 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.502 - mean_q: 13.073 - prob: 1.000\n",
            "\n",
            "Interval 5983 (59820 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 5984 (59830 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.532 - mean_q: 12.954 - prob: 1.000\n",
            "\n",
            "Interval 5985 (59840 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.303 - mean_q: 12.723 - prob: 1.000\n",
            "\n",
            "Interval 5986 (59850 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 5987 (59860 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.599 - mean_q: 13.142 - prob: 1.000\n",
            "\n",
            "Interval 5988 (59870 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.538 - mean_q: 13.038 - prob: 1.000\n",
            "\n",
            "Interval 5989 (59880 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 5990 (59890 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.321 - mean_q: 12.732 - prob: 1.000\n",
            "\n",
            "Interval 5991 (59900 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 5992 (59910 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.285 - mean_q: 12.665 - prob: 1.000\n",
            "\n",
            "Interval 5993 (59920 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.001 - mae: 7.768 - mean_q: 13.230 - prob: 1.000\n",
            "\n",
            "Interval 5994 (59930 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.000 - mae: 7.035 - mean_q: 12.059 - prob: 1.000\n",
            "\n",
            "Interval 5995 (59940 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.174 - mean_q: 12.423 - prob: 1.000\n",
            "\n",
            "Interval 5996 (59950 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 5997 (59960 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.504 - mean_q: 12.860 - prob: 1.000\n",
            "\n",
            "Interval 5998 (59970 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 5999 (59980 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.489 - mean_q: 12.966 - prob: 1.000\n",
            "\n",
            "Interval 6000 (59990 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -2.8000\n",
            "Interval 6001 (60000 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.001 - mae: 7.215 - mean_q: 12.507 - prob: 1.000\n",
            "\n",
            "Interval 6002 (60010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.242 - mean_q: 12.443 - prob: 1.000\n",
            "\n",
            "Interval 6003 (60020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.193 - mean_q: 12.563 - prob: 1.000\n",
            "\n",
            "Interval 6004 (60030 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 6005 (60040 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 11.500 [8.000, 15.000] - loss: 0.000 - mae: 7.272 - mean_q: 12.645 - prob: 1.000\n",
            "\n",
            "Interval 6006 (60050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.393 - mean_q: 12.763 - prob: 1.000\n",
            "\n",
            "Interval 6007 (60060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6008 (60070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.000 - mae: 7.607 - mean_q: 13.104 - prob: 1.000\n",
            "\n",
            "Interval 6009 (60080 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 6010 (60090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 6011 (60100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 1.000 [-10.000, 12.000] - loss: 0.000 - mae: 7.552 - mean_q: 12.954 - prob: 1.000\n",
            "\n",
            "Interval 6012 (60110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6013 (60120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.376 - mean_q: 12.639 - prob: 1.000\n",
            "\n",
            "Interval 6014 (60130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.000 - mae: 7.409 - mean_q: 12.786 - prob: 1.000\n",
            "\n",
            "Interval 6015 (60140 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 6016 (60150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.322 - mean_q: 12.496 - prob: 1.000\n",
            "\n",
            "Interval 6017 (60160 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 6018 (60170 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.697 - mean_q: 13.221 - prob: 1.000\n",
            "\n",
            "Interval 6019 (60180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.491 - mean_q: 12.975 - prob: 1.000\n",
            "\n",
            "Interval 6020 (60190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6021 (60200 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.906 - mean_q: 13.543 - prob: 1.000\n",
            "\n",
            "Interval 6022 (60210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.429 - mean_q: 12.805 - prob: 1.000\n",
            "\n",
            "Interval 6023 (60220 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 6024 (60230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.360 - mean_q: 12.783 - prob: 1.000\n",
            "\n",
            "Interval 6025 (60240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6026 (60250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.449 - mean_q: 12.859 - prob: 1.000\n",
            "\n",
            "Interval 6027 (60260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.443 - mean_q: 12.903 - prob: 1.000\n",
            "\n",
            "Interval 6028 (60270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 6029 (60280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.243 - mean_q: 12.543 - prob: 1.000\n",
            "\n",
            "Interval 6030 (60290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6031 (60300 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.002 - mae: 7.372 - mean_q: 12.606 - prob: 1.000\n",
            "\n",
            "Interval 6032 (60310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.257 - mean_q: 12.573 - prob: 1.000\n",
            "\n",
            "Interval 6033 (60320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6034 (60330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.612 - mean_q: 13.138 - prob: 1.000\n",
            "\n",
            "Interval 6035 (60340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.472 - mean_q: 12.769 - prob: 1.000\n",
            "\n",
            "Interval 6036 (60350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 6037 (60360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.299 - mean_q: 12.672 - prob: 1.000\n",
            "\n",
            "Interval 6038 (60370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.483 - mean_q: 12.856 - prob: 1.000\n",
            "\n",
            "Interval 6039 (60380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6040 (60390 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.351 - mean_q: 12.704 - prob: 1.000\n",
            "\n",
            "Interval 6041 (60400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.000 - mae: 7.325 - mean_q: 12.603 - prob: 1.000\n",
            "\n",
            "Interval 6042 (60410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.407 - mean_q: 12.814 - prob: 1.000\n",
            "\n",
            "Interval 6043 (60420 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 6044 (60430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.000 - mae: 7.147 - mean_q: 12.454 - prob: 1.000\n",
            "\n",
            "Interval 6045 (60440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.482 - mean_q: 12.926 - prob: 1.000\n",
            "\n",
            "Interval 6046 (60450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 6047 (60460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.014 - mean_q: 12.168 - prob: 1.000\n",
            "\n",
            "Interval 6048 (60470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.000 - mae: 7.581 - mean_q: 13.065 - prob: 1.000\n",
            "\n",
            "Interval 6049 (60480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 6050 (60490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.001 - mae: 7.323 - mean_q: 12.741 - prob: 1.000\n",
            "\n",
            "Interval 6051 (60500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.370 - mean_q: 12.666 - prob: 1.000\n",
            "\n",
            "Interval 6052 (60510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6053 (60520 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.552 - mean_q: 13.047 - prob: 1.000\n",
            "\n",
            "Interval 6054 (60530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.413 - mean_q: 12.827 - prob: 1.000\n",
            "\n",
            "Interval 6055 (60540 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.299 - mean_q: 12.574 - prob: 1.000\n",
            "\n",
            "Interval 6056 (60550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.345 - mean_q: 12.618 - prob: 1.000\n",
            "\n",
            "Interval 6057 (60560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6058 (60570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.260 - mean_q: 12.679 - prob: 1.000\n",
            "\n",
            "Interval 6059 (60580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.264 - mean_q: 12.730 - prob: 1.000\n",
            "\n",
            "Interval 6060 (60590 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.538 - mean_q: 13.171 - prob: 1.000\n",
            "\n",
            "Interval 6061 (60600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.639 - mean_q: 13.181 - prob: 1.000\n",
            "\n",
            "Interval 6062 (60610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6063 (60620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.562 - mean_q: 13.033 - prob: 1.000\n",
            "\n",
            "Interval 6064 (60630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6065 (60640 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.000 - mae: 7.293 - mean_q: 12.564 - prob: 1.000\n",
            "\n",
            "Interval 6066 (60650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 6067 (60660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.497 - mean_q: 12.919 - prob: 1.000\n",
            "\n",
            "Interval 6068 (60670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.001 - mae: 7.449 - mean_q: 12.811 - prob: 1.000\n",
            "\n",
            "Interval 6069 (60680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6070 (60690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.486 - mean_q: 12.881 - prob: 1.000\n",
            "\n",
            "Interval 6071 (60700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.521 - mean_q: 12.856 - prob: 1.000\n",
            "\n",
            "Interval 6072 (60710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6073 (60720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.218 - mean_q: 12.567 - prob: 1.000\n",
            "\n",
            "Interval 6074 (60730 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.148 - mean_q: 12.391 - prob: 1.000\n",
            "\n",
            "Interval 6075 (60740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6076 (60750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.000 - mae: 7.395 - mean_q: 12.844 - prob: 1.000\n",
            "\n",
            "Interval 6077 (60760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6078 (60770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 6.996 - mean_q: 12.204 - prob: 1.000\n",
            "\n",
            "Interval 6079 (60780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.405 - mean_q: 12.913 - prob: 1.000\n",
            "\n",
            "Interval 6080 (60790 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.053 - mean_q: 12.184 - prob: 1.000\n",
            "\n",
            "Interval 6081 (60800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 6082 (60810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.001 - mae: 7.318 - mean_q: 12.627 - prob: 1.000\n",
            "\n",
            "Interval 6083 (60820 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.325 - mean_q: 12.542 - prob: 1.000\n",
            "\n",
            "Interval 6084 (60830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6085 (60840 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.001 - mae: 7.890 - mean_q: 13.361 - prob: 1.000\n",
            "\n",
            "Interval 6086 (60850 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.432 - mean_q: 12.852 - prob: 1.000\n",
            "\n",
            "Interval 6087 (60860 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.126 - mean_q: 12.375 - prob: 1.000\n",
            "\n",
            "Interval 6088 (60870 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 6089 (60880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.000 - mae: 7.384 - mean_q: 12.675 - prob: 1.000\n",
            "\n",
            "Interval 6090 (60890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 6091 (60900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.001 - mae: 7.622 - mean_q: 13.042 - prob: 1.000\n",
            "\n",
            "Interval 6092 (60910 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.834 - mean_q: 13.422 - prob: 1.000\n",
            "\n",
            "Interval 6093 (60920 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.225 - mean_q: 12.613 - prob: 1.000\n",
            "\n",
            "Interval 6094 (60930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.502 - mean_q: 12.871 - prob: 1.000\n",
            "\n",
            "Interval 6095 (60940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6096 (60950 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -19.000 [-19.000, -19.000] - loss: 0.001 - mae: 7.133 - mean_q: 12.532 - prob: 1.000\n",
            "\n",
            "Interval 6097 (60960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6098 (60970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.199 - mean_q: 12.605 - prob: 1.000\n",
            "\n",
            "Interval 6099 (60980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.246 - mean_q: 12.633 - prob: 1.000\n",
            "\n",
            "Interval 6100 (60990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6101 (61000 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.478 - mean_q: 12.896 - prob: 1.000\n",
            "\n",
            "Interval 6102 (61010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6103 (61020 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 2.3000\n",
            "2 episodes - episode_reward: 3.000 [2.000, 4.000] - loss: 0.001 - mae: 7.261 - mean_q: 12.660 - prob: 1.000\n",
            "\n",
            "Interval 6104 (61030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 6105 (61040 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.001 - mae: 7.355 - mean_q: 12.738 - prob: 1.000\n",
            "\n",
            "Interval 6106 (61050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6107 (61060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.002 - mae: 7.301 - mean_q: 12.738 - prob: 1.000\n",
            "\n",
            "Interval 6108 (61070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.076 - mean_q: 12.361 - prob: 1.000\n",
            "\n",
            "Interval 6109 (61080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.528 - mean_q: 13.027 - prob: 1.000\n",
            "\n",
            "Interval 6110 (61090 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.323 - mean_q: 12.629 - prob: 1.000\n",
            "\n",
            "Interval 6111 (61100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6112 (61110 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.002 - mae: 7.131 - mean_q: 12.238 - prob: 1.000\n",
            "\n",
            "Interval 6113 (61120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6114 (61130 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.356 - mean_q: 12.740 - prob: 1.000\n",
            "\n",
            "Interval 6115 (61140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -25.000 [-25.000, -25.000] - loss: 0.001 - mae: 7.715 - mean_q: 13.193 - prob: 1.000\n",
            "\n",
            "Interval 6116 (61150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6117 (61160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.466 - mean_q: 12.879 - prob: 1.000\n",
            "\n",
            "Interval 6118 (61170 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 6119 (61180 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.298 - mean_q: 12.680 - prob: 1.000\n",
            "\n",
            "Interval 6120 (61190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6121 (61200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.214 - mean_q: 12.583 - prob: 1.000\n",
            "\n",
            "Interval 6122 (61210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.147 - mean_q: 12.480 - prob: 1.000\n",
            "\n",
            "Interval 6123 (61220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.595 - mean_q: 12.863 - prob: 1.000\n",
            "\n",
            "Interval 6124 (61230 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 6125 (61240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 6126 (61250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.001 - mae: 7.450 - mean_q: 12.889 - prob: 1.000\n",
            "\n",
            "Interval 6127 (61260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6128 (61270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.277 - mean_q: 12.632 - prob: 1.000\n",
            "\n",
            "Interval 6129 (61280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.742 - mean_q: 13.269 - prob: 1.000\n",
            "\n",
            "Interval 6130 (61290 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 6131 (61300 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -21.000 [-21.000, -21.000] - loss: 0.000 - mae: 7.359 - mean_q: 12.768 - prob: 1.000\n",
            "\n",
            "Interval 6132 (61310 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 6133 (61320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.000 - mae: 7.197 - mean_q: 12.613 - prob: 1.000\n",
            "\n",
            "Interval 6134 (61330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.278 - mean_q: 12.636 - prob: 1.000\n",
            "\n",
            "Interval 6135 (61340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.487 - mean_q: 13.026 - prob: 1.000\n",
            "\n",
            "Interval 6136 (61350 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 6137 (61360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.001 - mae: 6.961 - mean_q: 12.138 - prob: 1.000\n",
            "\n",
            "Interval 6138 (61370 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 6139 (61380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.000 - mae: 7.655 - mean_q: 13.232 - prob: 1.000\n",
            "\n",
            "Interval 6140 (61390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -24.000 [-24.000, -24.000] - loss: 0.001 - mae: 7.596 - mean_q: 13.026 - prob: 1.000\n",
            "\n",
            "Interval 6141 (61400 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 6142 (61410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.362 - mean_q: 12.610 - prob: 1.000\n",
            "\n",
            "Interval 6143 (61420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.190 - mean_q: 12.356 - prob: 1.000\n",
            "\n",
            "Interval 6144 (61430 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.098 - mean_q: 12.225 - prob: 1.000\n",
            "\n",
            "Interval 6145 (61440 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 6146 (61450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.159 - mean_q: 12.525 - prob: 1.000\n",
            "\n",
            "Interval 6147 (61460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.686 - mean_q: 13.304 - prob: 1.000\n",
            "\n",
            "Interval 6148 (61470 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 6149 (61480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.361 - mean_q: 12.821 - prob: 1.000\n",
            "\n",
            "Interval 6150 (61490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6151 (61500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.593 - mean_q: 12.993 - prob: 1.000\n",
            "\n",
            "Interval 6152 (61510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.114 - mean_q: 12.413 - prob: 1.000\n",
            "\n",
            "Interval 6153 (61520 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 6154 (61530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 11.000 [8.000, 14.000] - loss: 0.001 - mae: 7.492 - mean_q: 12.827 - prob: 1.000\n",
            "\n",
            "Interval 6155 (61540 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 6156 (61550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -18.000 [-18.000, -18.000] - loss: 0.001 - mae: 7.186 - mean_q: 12.514 - prob: 1.000\n",
            "\n",
            "Interval 6157 (61560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6158 (61570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.433 - mean_q: 12.873 - prob: 1.000\n",
            "\n",
            "Interval 6159 (61580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 6160 (61590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.125 - mean_q: 12.400 - prob: 1.000\n",
            "\n",
            "Interval 6161 (61600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6162 (61610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.001 - mae: 7.082 - mean_q: 12.469 - prob: 1.000\n",
            "\n",
            "Interval 6163 (61620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.348 - mean_q: 12.886 - prob: 1.000\n",
            "\n",
            "Interval 6164 (61630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 6165 (61640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.000 - mae: 7.651 - mean_q: 13.073 - prob: 1.000\n",
            "\n",
            "Interval 6166 (61650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.279 - mean_q: 12.644 - prob: 1.000\n",
            "\n",
            "Interval 6167 (61660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.390 - mean_q: 12.786 - prob: 1.000\n",
            "\n",
            "Interval 6168 (61670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6169 (61680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.676 - mean_q: 13.049 - prob: 1.000\n",
            "\n",
            "Interval 6170 (61690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.423 - mean_q: 12.848 - prob: 1.000\n",
            "\n",
            "Interval 6171 (61700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 6172 (61710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -26.000 [-26.000, -26.000] - loss: 0.001 - mae: 7.041 - mean_q: 12.295 - prob: 1.000\n",
            "\n",
            "Interval 6173 (61720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.328 - mean_q: 12.625 - prob: 1.000\n",
            "\n",
            "Interval 6174 (61730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.546 - mean_q: 13.062 - prob: 1.000\n",
            "\n",
            "Interval 6175 (61740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.002 - mae: 7.080 - mean_q: 12.441 - prob: 1.000\n",
            "\n",
            "Interval 6176 (61750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.293 - mean_q: 12.576 - prob: 1.000\n",
            "\n",
            "Interval 6177 (61760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.411 - mean_q: 12.896 - prob: 1.000\n",
            "\n",
            "Interval 6178 (61770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.627 - mean_q: 13.006 - prob: 1.000\n",
            "\n",
            "Interval 6179 (61780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.359 - mean_q: 12.738 - prob: 1.000\n",
            "\n",
            "Interval 6180 (61790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.442 - mean_q: 12.831 - prob: 1.000\n",
            "\n",
            "Interval 6181 (61800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6182 (61810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.246 - mean_q: 12.647 - prob: 1.000\n",
            "\n",
            "Interval 6183 (61820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6184 (61830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.287 - mean_q: 12.467 - prob: 1.000\n",
            "\n",
            "Interval 6185 (61840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6186 (61850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 8.000 [3.000, 13.000] - loss: 0.001 - mae: 7.117 - mean_q: 12.459 - prob: 1.000\n",
            "\n",
            "Interval 6187 (61860 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.001 - mae: 7.381 - mean_q: 12.893 - prob: 1.000\n",
            "\n",
            "Interval 6188 (61870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.001 - mae: 7.209 - mean_q: 12.627 - prob: 1.000\n",
            "\n",
            "Interval 6189 (61880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.409 - mean_q: 12.831 - prob: 1.000\n",
            "\n",
            "Interval 6190 (61890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6191 (61900 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.384 - mean_q: 12.669 - prob: 1.000\n",
            "\n",
            "Interval 6192 (61910 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 6193 (61920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.263 - mean_q: 12.681 - prob: 1.000\n",
            "\n",
            "Interval 6194 (61930 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.488 - mean_q: 12.883 - prob: 1.000\n",
            "\n",
            "Interval 6195 (61940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6196 (61950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.017 - mean_q: 12.252 - prob: 1.000\n",
            "\n",
            "Interval 6197 (61960 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.470 - mean_q: 12.902 - prob: 1.000\n",
            "\n",
            "Interval 6198 (61970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.378 - mean_q: 12.617 - prob: 1.000\n",
            "\n",
            "Interval 6199 (61980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6200 (61990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.269 - mean_q: 12.642 - prob: 1.000\n",
            "\n",
            "Interval 6201 (62000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.526 - mean_q: 12.969 - prob: 1.000\n",
            "\n",
            "Interval 6202 (62010 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.001 - mae: 7.027 - mean_q: 12.289 - prob: 1.000\n",
            "\n",
            "Interval 6203 (62020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.581 - mean_q: 13.081 - prob: 1.000\n",
            "\n",
            "Interval 6204 (62030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.542 - mean_q: 13.089 - prob: 1.000\n",
            "\n",
            "Interval 6205 (62040 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.218 - mean_q: 12.508 - prob: 1.000\n",
            "\n",
            "Interval 6206 (62050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.122 - mean_q: 12.401 - prob: 1.000\n",
            "\n",
            "Interval 6207 (62060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 6208 (62070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.409 - mean_q: 12.813 - prob: 1.000\n",
            "\n",
            "Interval 6209 (62080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6210 (62090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.405 - mean_q: 12.832 - prob: 1.000\n",
            "\n",
            "Interval 6211 (62100 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.289 - mean_q: 12.625 - prob: 1.000\n",
            "\n",
            "Interval 6212 (62110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.294 - mean_q: 12.670 - prob: 1.000\n",
            "\n",
            "Interval 6213 (62120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6214 (62130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.550 - mean_q: 12.813 - prob: 1.000\n",
            "\n",
            "Interval 6215 (62140 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 6216 (62150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.569 - mean_q: 13.052 - prob: 1.000\n",
            "\n",
            "Interval 6217 (62160 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.489 - mean_q: 12.905 - prob: 1.000\n",
            "\n",
            "Interval 6218 (62170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6219 (62180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.520 - mean_q: 12.872 - prob: 1.000\n",
            "\n",
            "Interval 6220 (62190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6221 (62200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.361 - mean_q: 12.561 - prob: 1.000\n",
            "\n",
            "Interval 6222 (62210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6223 (62220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.397 - mean_q: 12.706 - prob: 1.000\n",
            "\n",
            "Interval 6224 (62230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.392 - mean_q: 12.698 - prob: 1.000\n",
            "\n",
            "Interval 6225 (62240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6226 (62250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.446 - mean_q: 12.894 - prob: 1.000\n",
            "\n",
            "Interval 6227 (62260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6228 (62270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.175 - mean_q: 12.322 - prob: 1.000\n",
            "\n",
            "Interval 6229 (62280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.435 - mean_q: 12.896 - prob: 1.000\n",
            "\n",
            "Interval 6230 (62290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.6000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.499 - mean_q: 12.942 - prob: 1.000\n",
            "\n",
            "Interval 6231 (62300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6232 (62310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -36.000 [-36.000, -36.000] - loss: 0.001 - mae: 7.396 - mean_q: 12.707 - prob: 1.000\n",
            "\n",
            "Interval 6233 (62320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.358 - mean_q: 12.637 - prob: 1.000\n",
            "\n",
            "Interval 6234 (62330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.092 - mean_q: 12.359 - prob: 1.000\n",
            "\n",
            "Interval 6235 (62340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6236 (62350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.357 - mean_q: 12.679 - prob: 1.000\n",
            "\n",
            "Interval 6237 (62360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.003 - mae: 7.022 - mean_q: 12.291 - prob: 1.000\n",
            "\n",
            "Interval 6238 (62370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6239 (62380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.206 - mean_q: 12.433 - prob: 1.000\n",
            "\n",
            "Interval 6240 (62390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.728 - mean_q: 13.353 - prob: 1.000\n",
            "\n",
            "Interval 6241 (62400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.612 - mean_q: 12.918 - prob: 1.000\n",
            "\n",
            "Interval 6242 (62410 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.001 - mae: 7.431 - mean_q: 12.848 - prob: 1.000\n",
            "\n",
            "Interval 6243 (62420 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.310 - mean_q: 12.644 - prob: 1.000\n",
            "\n",
            "Interval 6244 (62430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.137 - mean_q: 12.343 - prob: 1.000\n",
            "\n",
            "Interval 6245 (62440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6246 (62450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.488 - mean_q: 12.876 - prob: 1.000\n",
            "\n",
            "Interval 6247 (62460 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 6.904 - mean_q: 12.190 - prob: 1.000\n",
            "\n",
            "Interval 6248 (62470 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.793 - mean_q: 13.174 - prob: 1.000\n",
            "\n",
            "Interval 6249 (62480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6250 (62490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.343 - mean_q: 12.694 - prob: 1.000\n",
            "\n",
            "Interval 6251 (62500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.415 - mean_q: 12.589 - prob: 1.000\n",
            "\n",
            "Interval 6252 (62510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 6253 (62520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.002 - mae: 7.144 - mean_q: 12.404 - prob: 1.000\n",
            "\n",
            "Interval 6254 (62530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6255 (62540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.320 - mean_q: 12.727 - prob: 1.000\n",
            "\n",
            "Interval 6256 (62550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6257 (62560 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.474 - mean_q: 12.783 - prob: 1.000\n",
            "\n",
            "Interval 6258 (62570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.300 - mean_q: 12.614 - prob: 1.000\n",
            "\n",
            "Interval 6259 (62580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.518 - mean_q: 12.976 - prob: 1.000\n",
            "\n",
            "Interval 6260 (62590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6261 (62600 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.434 - mean_q: 12.869 - prob: 1.000\n",
            "\n",
            "Interval 6262 (62610 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 6263 (62620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.001 - mae: 7.634 - mean_q: 13.023 - prob: 1.000\n",
            "\n",
            "Interval 6264 (62630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6265 (62640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -17.000 [-17.000, -17.000] - loss: 0.000 - mae: 7.727 - mean_q: 13.142 - prob: 1.000\n",
            "\n",
            "Interval 6266 (62650 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 12.000 [10.000, 14.000] - loss: 0.001 - mae: 7.205 - mean_q: 12.383 - prob: 1.000\n",
            "\n",
            "Interval 6267 (62660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6268 (62670 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.063 - mean_q: 12.202 - prob: 1.000\n",
            "\n",
            "Interval 6269 (62680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6270 (62690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.533 - mean_q: 12.797 - prob: 1.000\n",
            "\n",
            "Interval 6271 (62700 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.393 - mean_q: 12.744 - prob: 1.000\n",
            "\n",
            "Interval 6272 (62710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.373 - mean_q: 12.844 - prob: 1.000\n",
            "\n",
            "Interval 6273 (62720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 6274 (62730 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.750 - mean_q: 13.307 - prob: 1.000\n",
            "\n",
            "Interval 6275 (62740 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.460 - mean_q: 12.829 - prob: 1.000\n",
            "\n",
            "Interval 6276 (62750 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 6277 (62760 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.271 - mean_q: 12.645 - prob: 1.000\n",
            "\n",
            "Interval 6278 (62770 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.639 - mean_q: 13.040 - prob: 1.000\n",
            "\n",
            "Interval 6279 (62780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6280 (62790 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.403 - mean_q: 12.863 - prob: 1.000\n",
            "\n",
            "Interval 6281 (62800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 6282 (62810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.147 - mean_q: 12.399 - prob: 1.000\n",
            "\n",
            "Interval 6283 (62820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.873 - mean_q: 13.380 - prob: 1.000\n",
            "\n",
            "Interval 6284 (62830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6285 (62840 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.216 - mean_q: 12.397 - prob: 1.000\n",
            "\n",
            "Interval 6286 (62850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 6287 (62860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.267 - mean_q: 12.660 - prob: 1.000\n",
            "\n",
            "Interval 6288 (62870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.001 - mae: 7.194 - mean_q: 12.524 - prob: 1.000\n",
            "\n",
            "Interval 6289 (62880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.385 - mean_q: 12.640 - prob: 1.000\n",
            "\n",
            "Interval 6290 (62890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.064 - mean_q: 12.234 - prob: 1.000\n",
            "\n",
            "Interval 6291 (62900 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 6292 (62910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.309 - mean_q: 12.578 - prob: 1.000\n",
            "\n",
            "Interval 6293 (62920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.000 - mae: 7.578 - mean_q: 12.990 - prob: 1.000\n",
            "\n",
            "Interval 6294 (62930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 6295 (62940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.001 - mae: 7.431 - mean_q: 12.806 - prob: 1.000\n",
            "\n",
            "Interval 6296 (62950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 6297 (62960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.000 - mae: 7.666 - mean_q: 13.194 - prob: 1.000\n",
            "\n",
            "Interval 6298 (62970 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.651 - mean_q: 13.120 - prob: 1.000\n",
            "\n",
            "Interval 6299 (62980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6300 (62990 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.372 - mean_q: 12.812 - prob: 1.000\n",
            "\n",
            "Interval 6301 (63000 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.408 - mean_q: 12.754 - prob: 1.000\n",
            "\n",
            "Interval 6302 (63010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6303 (63020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.504 - mean_q: 12.759 - prob: 1.000\n",
            "\n",
            "Interval 6304 (63030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6305 (63040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.303 - mean_q: 12.657 - prob: 1.000\n",
            "\n",
            "Interval 6306 (63050 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.611 - mean_q: 13.161 - prob: 1.000\n",
            "\n",
            "Interval 6307 (63060 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 6308 (63070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.214 - mean_q: 12.533 - prob: 1.000\n",
            "\n",
            "Interval 6309 (63080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6310 (63090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.243 - mean_q: 12.689 - prob: 1.000\n",
            "\n",
            "Interval 6311 (63100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.595 - mean_q: 13.031 - prob: 1.000\n",
            "\n",
            "Interval 6312 (63110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6313 (63120 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.407 - mean_q: 12.915 - prob: 1.000\n",
            "\n",
            "Interval 6314 (63130 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.001 - mae: 7.387 - mean_q: 12.626 - prob: 1.000\n",
            "\n",
            "Interval 6315 (63140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6316 (63150 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.198 - mean_q: 12.437 - prob: 1.000\n",
            "\n",
            "Interval 6317 (63160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.433 - mean_q: 12.840 - prob: 1.000\n",
            "\n",
            "Interval 6318 (63170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.525 - mean_q: 12.930 - prob: 1.000\n",
            "\n",
            "Interval 6319 (63180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6320 (63190 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 6.986 - mean_q: 12.195 - prob: 1.000\n",
            "\n",
            "Interval 6321 (63200 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.179 - mean_q: 12.480 - prob: 1.000\n",
            "\n",
            "Interval 6322 (63210 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.439 - mean_q: 12.739 - prob: 1.000\n",
            "\n",
            "Interval 6323 (63220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 6324 (63230 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -24.000 [-24.000, -24.000] - loss: 0.001 - mae: 7.717 - mean_q: 13.245 - prob: 1.000\n",
            "\n",
            "Interval 6325 (63240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.398 - mean_q: 12.808 - prob: 1.000\n",
            "\n",
            "Interval 6326 (63250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.444 - mean_q: 12.930 - prob: 1.000\n",
            "\n",
            "Interval 6327 (63260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 6328 (63270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.536 - mean_q: 12.939 - prob: 1.000\n",
            "\n",
            "Interval 6329 (63280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.481 - mean_q: 13.081 - prob: 1.000\n",
            "\n",
            "Interval 6330 (63290 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.505 - mean_q: 13.094 - prob: 1.000\n",
            "\n",
            "Interval 6331 (63300 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.017 - mean_q: 12.279 - prob: 1.000\n",
            "\n",
            "Interval 6332 (63310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.617 - mean_q: 13.137 - prob: 1.000\n",
            "\n",
            "Interval 6333 (63320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6334 (63330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.537 - mean_q: 12.990 - prob: 1.000\n",
            "\n",
            "Interval 6335 (63340 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.074 - mean_q: 12.311 - prob: 1.000\n",
            "\n",
            "Interval 6336 (63350 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.542 - mean_q: 13.035 - prob: 1.000\n",
            "\n",
            "Interval 6337 (63360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.183 - mean_q: 12.486 - prob: 1.000\n",
            "\n",
            "Interval 6338 (63370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 6339 (63380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 6340 (63390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -29.000 [-29.000, -29.000] - loss: 0.000 - mae: 7.164 - mean_q: 12.521 - prob: 1.000\n",
            "\n",
            "Interval 6341 (63400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.506 - mean_q: 12.922 - prob: 1.000\n",
            "\n",
            "Interval 6342 (63410 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 6343 (63420 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.001 - mae: 7.545 - mean_q: 12.934 - prob: 1.000\n",
            "\n",
            "Interval 6344 (63430 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.278 - mean_q: 12.674 - prob: 1.000\n",
            "\n",
            "Interval 6345 (63440 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 6346 (63450 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.305 - mean_q: 12.704 - prob: 1.000\n",
            "\n",
            "Interval 6347 (63460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 6348 (63470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.678 - mean_q: 13.243 - prob: 1.000\n",
            "\n",
            "Interval 6349 (63480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.406 - mean_q: 12.838 - prob: 1.000\n",
            "\n",
            "Interval 6350 (63490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 6351 (63500 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.309 - mean_q: 12.730 - prob: 1.000\n",
            "\n",
            "Interval 6352 (63510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.380 - mean_q: 12.737 - prob: 1.000\n",
            "\n",
            "Interval 6353 (63520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 6354 (63530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 41203 (412020 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.283 - mean_q: 12.453 - prob: 1.000\n",
            "\n",
            "Interval 41204 (412030 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.001 - mae: 7.204 - mean_q: 12.534 - prob: 1.000\n",
            "\n",
            "Interval 41205 (412040 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.000 - mae: 7.382 - mean_q: 12.684 - prob: 1.000\n",
            "\n",
            "Interval 41206 (412050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 41207 (412060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.146 - mean_q: 12.487 - prob: 1.000\n",
            "\n",
            "Interval 41208 (412070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.272 - mean_q: 12.710 - prob: 1.000\n",
            "\n",
            "Interval 41209 (412080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 41210 (412090 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.210 - mean_q: 12.526 - prob: 1.000\n",
            "\n",
            "Interval 41211 (412100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 12.500 [12.000, 13.000] - loss: 0.000 - mae: 7.457 - mean_q: 12.831 - prob: 1.000\n",
            "\n",
            "Interval 41212 (412110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 41213 (412120 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.787 - mean_q: 13.174 - prob: 1.000\n",
            "\n",
            "Interval 41214 (412130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.283 - mean_q: 12.550 - prob: 1.000\n",
            "\n",
            "Interval 41215 (412140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.000 - mae: 7.248 - mean_q: 12.559 - prob: 1.000\n",
            "\n",
            "Interval 41216 (412150 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.000 - mae: 7.195 - mean_q: 12.503 - prob: 1.000\n",
            "\n",
            "Interval 41217 (412160 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 41218 (412170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.630 - mean_q: 13.210 - prob: 1.000\n",
            "\n",
            "Interval 41219 (412180 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.500 - mean_q: 12.935 - prob: 1.000\n",
            "\n",
            "Interval 41220 (412190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.001 - mae: 7.295 - mean_q: 12.623 - prob: 1.000\n",
            "\n",
            "Interval 41221 (412200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.335 - mean_q: 12.703 - prob: 1.000\n",
            "\n",
            "Interval 41222 (412210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.316 - mean_q: 12.601 - prob: 1.000\n",
            "\n",
            "Interval 41223 (412220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 41224 (412230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.001 - mae: 7.079 - mean_q: 12.391 - prob: 1.000\n",
            "\n",
            "Interval 41225 (412240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 41226 (412250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.001 - mae: 7.277 - mean_q: 12.789 - prob: 1.000\n",
            "\n",
            "Interval 41227 (412260 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 41228 (412270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.281 - mean_q: 12.541 - prob: 1.000\n",
            "\n",
            "Interval 41229 (412280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 41230 (412290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -35.000 [-35.000, -35.000] - loss: 0.001 - mae: 7.302 - mean_q: 12.645 - prob: 1.000\n",
            "\n",
            "Interval 41231 (412300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.328 - mean_q: 12.600 - prob: 1.000\n",
            "\n",
            "Interval 41232 (412310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 41233 (412320 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.000 - mae: 7.685 - mean_q: 13.222 - prob: 1.000\n",
            "\n",
            "Interval 41234 (412330 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 41235 (412340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.428 - mean_q: 12.911 - prob: 1.000\n",
            "\n",
            "Interval 41236 (412350 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.309 - mean_q: 12.607 - prob: 1.000\n",
            "\n",
            "Interval 41237 (412360 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.400 - mean_q: 12.757 - prob: 1.000\n",
            "\n",
            "Interval 41238 (412370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 41239 (412380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.000 - mae: 7.169 - mean_q: 12.475 - prob: 1.000\n",
            "\n",
            "Interval 41240 (412390 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.403 - mean_q: 12.831 - prob: 1.000\n",
            "\n",
            "Interval 41241 (412400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 41242 (412410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.000 - mae: 7.465 - mean_q: 12.708 - prob: 1.000\n",
            "\n",
            "Interval 41243 (412420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 41244 (412430 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.201 - mean_q: 12.436 - prob: 1.000\n",
            "\n",
            "Interval 41245 (412440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 41246 (412450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 3.500 [-5.000, 12.000] - loss: 0.000 - mae: 7.285 - mean_q: 12.615 - prob: 1.000\n",
            "\n",
            "Interval 41247 (412460 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 41248 (412470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.001 - mae: 7.496 - mean_q: 13.028 - prob: 1.000\n",
            "\n",
            "Interval 41249 (412480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.603 - mean_q: 13.127 - prob: 1.000\n",
            "\n",
            "Interval 41250 (412490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.281 - mean_q: 12.547 - prob: 1.000\n",
            "\n",
            "Interval 41251 (412500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 41252 (412510 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.376 - mean_q: 12.871 - prob: 1.000\n",
            "\n",
            "Interval 41253 (412520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 41254 (412530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 5.500 [-2.000, 13.000] - loss: 0.000 - mae: 7.184 - mean_q: 12.366 - prob: 1.000\n",
            "\n",
            "Interval 41255 (412540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 41256 (412550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.292 - mean_q: 12.707 - prob: 1.000\n",
            "\n",
            "Interval 41257 (412560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.585 - mean_q: 13.054 - prob: 1.000\n",
            "\n",
            "Interval 41258 (412570 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.000 - mae: 7.010 - mean_q: 12.206 - prob: 1.000\n",
            "\n",
            "Interval 41259 (412580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 41260 (412590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.000 - mae: 7.154 - mean_q: 12.328 - prob: 1.000\n",
            "\n",
            "Interval 41261 (412600 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.485 - mean_q: 12.905 - prob: 1.000\n",
            "\n",
            "Interval 41262 (412610 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 41263 (412620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.254 - mean_q: 12.619 - prob: 1.000\n",
            "\n",
            "Interval 41264 (412630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.376 - mean_q: 12.757 - prob: 1.000\n",
            "\n",
            "Interval 41265 (412640 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 41266 (412650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 41267 (412660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -18.000 [-18.000, -18.000] - loss: 0.000 - mae: 7.392 - mean_q: 12.700 - prob: 1.000\n",
            "\n",
            "Interval 41268 (412670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.123 - mean_q: 12.413 - prob: 1.000\n",
            "\n",
            "Interval 41269 (412680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.143 - mean_q: 12.438 - prob: 1.000\n",
            "\n",
            "Interval 41270 (412690 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 41271 (412700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.199 - mean_q: 12.576 - prob: 1.000\n",
            "\n",
            "Interval 41272 (412710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.190 - mean_q: 12.421 - prob: 1.000\n",
            "\n",
            "Interval 41273 (412720 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.000 - mae: 7.327 - mean_q: 12.668 - prob: 1.000\n",
            "\n",
            "Interval 41274 (412730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 41275 (412740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.452 - mean_q: 12.803 - prob: 1.000\n",
            "\n",
            "Interval 41276 (412750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 41277 (412760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.047 - mean_q: 12.203 - prob: 1.000\n",
            "\n",
            "Interval 41278 (412770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.079 - mean_q: 12.401 - prob: 1.000\n",
            "\n",
            "Interval 41279 (412780 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 41280 (412790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.363 - mean_q: 12.735 - prob: 1.000\n",
            "\n",
            "Interval 41281 (412800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.350 - mean_q: 12.721 - prob: 1.000\n",
            "\n",
            "Interval 41282 (412810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 41283 (412820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.491 - mean_q: 12.928 - prob: 1.000\n",
            "\n",
            "Interval 41284 (412830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 41285 (412840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -19.000 [-19.000, -19.000] - loss: 0.000 - mae: 6.996 - mean_q: 12.218 - prob: 1.000\n",
            "\n",
            "Interval 41286 (412850 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 41287 (412860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.423 - mean_q: 12.896 - prob: 1.000\n",
            "\n",
            "Interval 41288 (412870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.204 - mean_q: 12.421 - prob: 1.000\n",
            "\n",
            "Interval 41289 (412880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 41290 (412890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.588 - mean_q: 13.091 - prob: 1.000\n",
            "\n",
            "Interval 41291 (412900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 41292 (412910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.004 - mean_q: 12.216 - prob: 1.000\n",
            "\n",
            "Interval 41293 (412920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.335 - mean_q: 12.675 - prob: 1.000\n",
            "\n",
            "Interval 41294 (412930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.175 - mean_q: 12.582 - prob: 1.000\n",
            "\n",
            "Interval 41295 (412940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.321 - mean_q: 12.741 - prob: 1.000\n",
            "\n",
            "Interval 41296 (412950 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 41297 (412960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.000 - mae: 7.545 - mean_q: 12.916 - prob: 1.000\n",
            "\n",
            "Interval 41298 (412970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 41299 (412980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.420 - mean_q: 12.807 - prob: 1.000\n",
            "\n",
            "Interval 41300 (412990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.417 - mean_q: 12.708 - prob: 1.000\n",
            "\n",
            "Interval 41301 (413000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.322 - mean_q: 12.722 - prob: 1.000\n",
            "\n",
            "Interval 41302 (413010 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 41303 (413020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.040 - mean_q: 12.318 - prob: 1.000\n",
            "\n",
            "Interval 41304 (413030 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.001 - mae: 7.580 - mean_q: 12.945 - prob: 1.000\n",
            "\n",
            "Interval 41305 (413040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.000 - mae: 7.760 - mean_q: 13.070 - prob: 1.000\n",
            "\n",
            "Interval 41306 (413050 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 41307 (413060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 6.970 - mean_q: 12.103 - prob: 1.000\n",
            "\n",
            "Interval 41308 (413070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.652 - mean_q: 13.025 - prob: 1.000\n",
            "\n",
            "Interval 41309 (413080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.482 - mean_q: 13.028 - prob: 1.000\n",
            "\n",
            "Interval 41310 (413090 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 41311 (413100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.316 - mean_q: 12.666 - prob: 1.000\n",
            "\n",
            "Interval 41312 (413110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.164 - mean_q: 12.535 - prob: 1.000\n",
            "\n",
            "Interval 41313 (413120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.326 - mean_q: 12.657 - prob: 1.000\n",
            "\n",
            "Interval 41314 (413130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 41315 (413140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.104 - mean_q: 12.290 - prob: 1.000\n",
            "\n",
            "Interval 41316 (413150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 41317 (413160 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.659 - mean_q: 13.197 - prob: 1.000\n",
            "\n",
            "Interval 41318 (413170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.423 - mean_q: 12.815 - prob: 1.000\n",
            "\n",
            "Interval 41319 (413180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 41320 (413190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.871 - mean_q: 13.360 - prob: 1.000\n",
            "\n",
            "Interval 41321 (413200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.000 - mae: 7.717 - mean_q: 13.170 - prob: 1.000\n",
            "\n",
            "Interval 41322 (413210 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.121 - mean_q: 12.459 - prob: 1.000\n",
            "\n",
            "Interval 41323 (413220 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 41324 (413230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.001 - mae: 7.261 - mean_q: 12.532 - prob: 1.000\n",
            "\n",
            "Interval 41325 (413240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 41326 (413250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.047 - mean_q: 12.126 - prob: 1.000\n",
            "\n",
            "Interval 41327 (413260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.713 - mean_q: 13.273 - prob: 1.000\n",
            "\n",
            "Interval 41328 (413270 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 41329 (413280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.000 - mae: 7.834 - mean_q: 13.326 - prob: 1.000\n",
            "\n",
            "Interval 41330 (413290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.000 - mae: 7.182 - mean_q: 12.497 - prob: 1.000\n",
            "\n",
            "Interval 41331 (413300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.000 - mae: 7.070 - mean_q: 12.349 - prob: 1.000\n",
            "\n",
            "Interval 41332 (413310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 41333 (413320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.239 - mean_q: 12.561 - prob: 1.000\n",
            "\n",
            "Interval 41334 (413330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.380 - mean_q: 12.786 - prob: 1.000\n",
            "\n",
            "Interval 41335 (413340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 41336 (413350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.133 - mean_q: 12.244 - prob: 1.000\n",
            "\n",
            "Interval 41337 (413360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.295 - mean_q: 12.663 - prob: 1.000\n",
            "\n",
            "Interval 41338 (413370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.451 - mean_q: 12.930 - prob: 1.000\n",
            "\n",
            "Interval 41339 (413380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.719 - mean_q: 13.176 - prob: 1.000\n",
            "\n",
            "Interval 41340 (413390 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 41341 (413400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.460 - mean_q: 12.643 - prob: 1.000\n",
            "\n",
            "Interval 41342 (413410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.287 - mean_q: 12.632 - prob: 1.000\n",
            "\n",
            "Interval 41343 (413420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 41344 (413430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.579 - mean_q: 12.828 - prob: 1.000\n",
            "\n",
            "Interval 41345 (413440 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.549 - mean_q: 12.958 - prob: 1.000\n",
            "\n",
            "Interval 41346 (413450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.413 - mean_q: 12.801 - prob: 1.000\n",
            "\n",
            "Interval 41347 (413460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 41348 (413470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.409 - mean_q: 12.712 - prob: 1.000\n",
            "\n",
            "Interval 41349 (413480 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.894 - mean_q: 13.440 - prob: 1.000\n",
            "\n",
            "Interval 41350 (413490 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.391 - mean_q: 12.724 - prob: 1.000\n",
            "\n",
            "Interval 41351 (413500 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 41352 (413510 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.252 - mean_q: 12.494 - prob: 1.000\n",
            "\n",
            "Interval 41353 (413520 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.404 - mean_q: 12.817 - prob: 1.000\n",
            "\n",
            "Interval 41354 (413530 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 41355 (413540 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.027 - mean_q: 12.235 - prob: 1.000\n",
            "\n",
            "Interval 41356 (413550 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.251 - mean_q: 12.633 - prob: 1.000\n",
            "\n",
            "Interval 41357 (413560 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.272 - mean_q: 12.506 - prob: 1.000\n",
            "\n",
            "Interval 41358 (413570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 41359 (413580 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.499 - mean_q: 12.960 - prob: 1.000\n",
            "\n",
            "Interval 41360 (413590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 41361 (413600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.001 - mae: 7.307 - mean_q: 12.595 - prob: 1.000\n",
            "\n",
            "Interval 41362 (413610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.512 - mean_q: 12.860 - prob: 1.000\n",
            "\n",
            "Interval 41363 (413620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.279 - mean_q: 12.555 - prob: 1.000\n",
            "\n",
            "Interval 41364 (413630 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 41365 (413640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.426 - mean_q: 12.751 - prob: 1.000\n",
            "\n",
            "Interval 41366 (413650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 41367 (413660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -19.000 [-19.000, -19.000] - loss: 0.001 - mae: 7.665 - mean_q: 13.100 - prob: 1.000\n",
            "\n",
            "Interval 41368 (413670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.355 - mean_q: 12.624 - prob: 1.000\n",
            "\n",
            "Interval 41369 (413680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.336 - mean_q: 12.533 - prob: 1.000\n",
            "\n",
            "Interval 41370 (413690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.768 - mean_q: 13.356 - prob: 1.000\n",
            "\n",
            "Interval 41371 (413700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 41372 (413710 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.264 - mean_q: 12.547 - prob: 1.000\n",
            "\n",
            "Interval 41373 (413720 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.346 - mean_q: 12.575 - prob: 1.000\n",
            "\n",
            "Interval 41374 (413730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 41375 (413740 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.000 - mae: 7.798 - mean_q: 13.332 - prob: 1.000\n",
            "\n",
            "Interval 41376 (413750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.334 - mean_q: 12.617 - prob: 1.000\n",
            "\n",
            "Interval 41377 (413760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 41378 (413770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.189 - mean_q: 12.408 - prob: 1.000\n",
            "\n",
            "Interval 41379 (413780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 41380 (413790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.675 - mean_q: 12.988 - prob: 1.000\n",
            "\n",
            "Interval 41381 (413800 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.553 - mean_q: 12.944 - prob: 1.000\n",
            "\n",
            "Interval 41382 (413810 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.394 - mean_q: 12.735 - prob: 1.000\n",
            "\n",
            "Interval 41383 (413820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 41384 (413830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.235 - mean_q: 12.496 - prob: 1.000\n",
            "\n",
            "Interval 41385 (413840 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.599 - mean_q: 13.006 - prob: 1.000\n",
            "\n",
            "Interval 41386 (413850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 41387 (413860 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -17.000 [-17.000, -17.000] - loss: 0.001 - mae: 7.531 - mean_q: 12.998 - prob: 1.000\n",
            "\n",
            "Interval 41388 (413870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 41389 (413880 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.164 - mean_q: 12.550 - prob: 1.000\n",
            "\n",
            "Interval 41390 (413890 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.335 - mean_q: 12.734 - prob: 1.000\n",
            "\n",
            "Interval 41391 (413900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.463 - mean_q: 12.775 - prob: 1.000\n",
            "\n",
            "Interval 41392 (413910 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 41393 (413920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.467 - mean_q: 12.877 - prob: 1.000\n",
            "\n",
            "Interval 41394 (413930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.441 - mean_q: 12.746 - prob: 1.000\n",
            "\n",
            "Interval 41395 (413940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 41396 (413950 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.210 - mean_q: 12.560 - prob: 1.000\n",
            "\n",
            "Interval 41397 (413960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 41398 (413970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.000 - mae: 7.332 - mean_q: 12.689 - prob: 1.000\n",
            "\n",
            "Interval 41399 (413980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 41400 (413990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.388 - mean_q: 12.835 - prob: 1.000\n",
            "\n",
            "Interval 41401 (414000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.001 - mae: 6.979 - mean_q: 12.181 - prob: 1.000\n",
            "\n",
            "Interval 41402 (414010 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.000 - mae: 7.232 - mean_q: 12.580 - prob: 1.000\n",
            "\n",
            "Interval 41403 (414020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.446 - mean_q: 12.959 - prob: 1.000\n",
            "\n",
            "Interval 41404 (414030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.345 - mean_q: 12.821 - prob: 1.000\n",
            "\n",
            "Interval 41405 (414040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 41406 (414050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.451 - mean_q: 12.907 - prob: 1.000\n",
            "\n",
            "Interval 41407 (414060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.618 - mean_q: 13.097 - prob: 1.000\n",
            "\n",
            "Interval 41408 (414070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.001 - mae: 7.609 - mean_q: 13.195 - prob: 1.000\n",
            "\n",
            "Interval 41409 (414080 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.141 - mean_q: 12.391 - prob: 1.000\n",
            "\n",
            "Interval 49046 (490450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 6.932 - mean_q: 12.216 - prob: 1.000\n",
            "\n",
            "Interval 49047 (490460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 49048 (490470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.265 - mean_q: 12.584 - prob: 1.000\n",
            "\n",
            "Interval 49049 (490480 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 49050 (490490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.260 - mean_q: 12.506 - prob: 1.000\n",
            "\n",
            "Interval 49051 (490500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.414 - mean_q: 12.841 - prob: 1.000\n",
            "\n",
            "Interval 49052 (490510 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 49053 (490520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.416 - mean_q: 12.903 - prob: 1.000\n",
            "\n",
            "Interval 49054 (490530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.368 - mean_q: 12.806 - prob: 1.000\n",
            "\n",
            "Interval 49055 (490540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 49056 (490550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.383 - mean_q: 12.658 - prob: 1.000\n",
            "\n",
            "Interval 49057 (490560 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.510 - mean_q: 12.900 - prob: 1.000\n",
            "\n",
            "Interval 49058 (490570 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.337 - mean_q: 12.652 - prob: 1.000\n",
            "\n",
            "Interval 49059 (490580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.001 - mae: 7.123 - mean_q: 12.355 - prob: 1.000\n",
            "\n",
            "Interval 49060 (490590 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.099 - mean_q: 12.367 - prob: 1.000\n",
            "\n",
            "Interval 49061 (490600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 49062 (490610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.235 - mean_q: 12.565 - prob: 1.000\n",
            "\n",
            "Interval 49063 (490620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.397 - mean_q: 12.768 - prob: 1.000\n",
            "\n",
            "Interval 49064 (490630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 49065 (490640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.145 - mean_q: 12.443 - prob: 1.000\n",
            "\n",
            "Interval 49066 (490650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.213 - mean_q: 12.493 - prob: 1.000\n",
            "\n",
            "Interval 49067 (490660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.354 - mean_q: 12.776 - prob: 1.000\n",
            "\n",
            "Interval 49068 (490670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 49069 (490680 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -21.000 [-21.000, -21.000] - loss: 0.001 - mae: 7.207 - mean_q: 12.489 - prob: 1.000\n",
            "\n",
            "Interval 49070 (490690 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 49071 (490700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.540 - mean_q: 12.915 - prob: 1.000\n",
            "\n",
            "Interval 49072 (490710 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 49073 (490720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.237 - mean_q: 12.647 - prob: 1.000\n",
            "\n",
            "Interval 49074 (490730 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 49075 (490740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.578 - mean_q: 13.012 - prob: 1.000\n",
            "\n",
            "Interval 49076 (490750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.160 - mean_q: 12.509 - prob: 1.000\n",
            "\n",
            "Interval 49077 (490760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 49078 (490770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.533 - mean_q: 13.006 - prob: 1.000\n",
            "\n",
            "Interval 49079 (490780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 49080 (490790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.283 - mean_q: 12.498 - prob: 1.000\n",
            "\n",
            "Interval 49081 (490800 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.503 - mean_q: 13.039 - prob: 1.000\n",
            "\n",
            "Interval 49082 (490810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 49083 (490820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 49084 (490830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -21.000 [-21.000, -21.000] - loss: 0.000 - mae: 7.386 - mean_q: 12.708 - prob: 1.000\n",
            "\n",
            "Interval 49085 (490840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.066 - mean_q: 12.222 - prob: 1.000\n",
            "\n",
            "Interval 49086 (490850 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.403 - mean_q: 12.818 - prob: 1.000\n",
            "\n",
            "Interval 49087 (490860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.217 - mean_q: 12.414 - prob: 1.000\n",
            "\n",
            "Interval 49088 (490870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 49089 (490880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.625 - mean_q: 13.146 - prob: 1.000\n",
            "\n",
            "Interval 49090 (490890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 49091 (490900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.000 - mae: 7.430 - mean_q: 12.766 - prob: 1.000\n",
            "\n",
            "Interval 49092 (490910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.733 - mean_q: 13.309 - prob: 1.000\n",
            "\n",
            "Interval 49093 (490920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.520 - mean_q: 12.972 - prob: 1.000\n",
            "\n",
            "Interval 49094 (490930 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.000 - mae: 7.296 - mean_q: 12.612 - prob: 1.000\n",
            "\n",
            "Interval 49095 (490940 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 49096 (490950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 49097 (490960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.407 - mean_q: 12.728 - prob: 1.000\n",
            "\n",
            "Interval 49098 (490970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.268 - mean_q: 12.646 - prob: 1.000\n",
            "\n",
            "Interval 49099 (490980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.312 - mean_q: 12.582 - prob: 1.000\n",
            "\n",
            "Interval 49100 (490990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 49101 (491000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.469 - mean_q: 12.944 - prob: 1.000\n",
            "\n",
            "Interval 49102 (491010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.372 - mean_q: 12.812 - prob: 1.000\n",
            "\n",
            "Interval 49103 (491020 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 49104 (491030 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.410 - mean_q: 12.854 - prob: 1.000\n",
            "\n",
            "Interval 49105 (491040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.000 - mae: 7.316 - mean_q: 12.705 - prob: 1.000\n",
            "\n",
            "Interval 49106 (491050 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.168 - mean_q: 12.518 - prob: 1.000\n",
            "\n",
            "Interval 49107 (491060 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 49108 (491070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -18.000 [-18.000, -18.000] - loss: 0.000 - mae: 7.624 - mean_q: 13.179 - prob: 1.000\n",
            "\n",
            "Interval 49109 (491080 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 49110 (491090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.152 - mean_q: 12.440 - prob: 1.000\n",
            "\n",
            "Interval 49111 (491100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 49112 (491110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.029 - mean_q: 12.269 - prob: 1.000\n",
            "\n",
            "Interval 49113 (491120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.121 - mean_q: 12.310 - prob: 1.000\n",
            "\n",
            "Interval 49114 (491130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.799 - mean_q: 13.336 - prob: 1.000\n",
            "\n",
            "Interval 49115 (491140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.578 - mean_q: 12.811 - prob: 1.000\n",
            "\n",
            "Interval 49116 (491150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.862 - mean_q: 13.376 - prob: 1.000\n",
            "\n",
            "Interval 49117 (491160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.396 - mean_q: 12.749 - prob: 1.000\n",
            "\n",
            "Interval 49118 (491170 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 49119 (491180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.424 - mean_q: 12.941 - prob: 1.000\n",
            "\n",
            "Interval 49120 (491190 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.000 - mae: 7.159 - mean_q: 12.463 - prob: 1.000\n",
            "\n",
            "Interval 49121 (491200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.360 - mean_q: 12.659 - prob: 1.000\n",
            "\n",
            "Interval 49122 (491210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 49123 (491220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.000 - mae: 7.397 - mean_q: 12.890 - prob: 1.000\n",
            "\n",
            "Interval 49124 (491230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 49125 (491240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.245 - mean_q: 12.461 - prob: 1.000\n",
            "\n",
            "Interval 49126 (491250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.434 - mean_q: 12.724 - prob: 1.000\n",
            "\n",
            "Interval 49127 (491260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.307 - mean_q: 12.629 - prob: 1.000\n",
            "\n",
            "Interval 49128 (491270 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 49129 (491280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.464 - mean_q: 12.850 - prob: 1.000\n",
            "\n",
            "Interval 49130 (491290 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 49131 (491300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.000 - mae: 7.407 - mean_q: 12.776 - prob: 1.000\n",
            "\n",
            "Interval 49132 (491310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.717 - mean_q: 12.994 - prob: 1.000\n",
            "\n",
            "Interval 49133 (491320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.000 - mae: 7.536 - mean_q: 12.961 - prob: 1.000\n",
            "\n",
            "Interval 49134 (491330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.6000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.000 - mae: 7.448 - mean_q: 12.933 - prob: 1.000\n",
            "\n",
            "Interval 49135 (491340 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 49136 (491350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.000 - mae: 7.395 - mean_q: 12.798 - prob: 1.000\n",
            "\n",
            "Interval 49137 (491360 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 49138 (491370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.052 - mean_q: 12.246 - prob: 1.000\n",
            "\n",
            "Interval 49139 (491380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.245 - mean_q: 12.573 - prob: 1.000\n",
            "\n",
            "Interval 49140 (491390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.327 - mean_q: 12.610 - prob: 1.000\n",
            "\n",
            "Interval 49141 (491400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 49142 (491410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.599 - mean_q: 13.046 - prob: 1.000\n",
            "\n",
            "Interval 49143 (491420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.672 - mean_q: 13.180 - prob: 1.000\n",
            "\n",
            "Interval 49144 (491430 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 49145 (491440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.768 - mean_q: 13.334 - prob: 1.000\n",
            "\n",
            "Interval 49146 (491450 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.000 - mae: 7.450 - mean_q: 12.843 - prob: 1.000\n",
            "\n",
            "Interval 49147 (491460 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 49148 (491470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.521 - mean_q: 12.968 - prob: 1.000\n",
            "\n",
            "Interval 49149 (491480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.192 - mean_q: 12.467 - prob: 1.000\n",
            "\n",
            "Interval 49150 (491490 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 49151 (491500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.163 - mean_q: 12.444 - prob: 1.000\n",
            "\n",
            "Interval 49152 (491510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.484 - mean_q: 12.843 - prob: 1.000\n",
            "\n",
            "Interval 49153 (491520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.562 - mean_q: 13.058 - prob: 1.000\n",
            "\n",
            "Interval 49154 (491530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.148 - mean_q: 12.377 - prob: 1.000\n",
            "\n",
            "Interval 49155 (491540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 49156 (491550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.475 - mean_q: 13.042 - prob: 1.000\n",
            "\n",
            "Interval 49157 (491560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.221 - mean_q: 12.499 - prob: 1.000\n",
            "\n",
            "Interval 49158 (491570 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.261 - mean_q: 12.612 - prob: 1.000\n",
            "\n",
            "Interval 49159 (491580 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 49160 (491590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 49161 (491600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -29.000 [-29.000, -29.000] - loss: 0.000 - mae: 7.325 - mean_q: 12.644 - prob: 1.000\n",
            "\n",
            "Interval 49162 (491610 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.264 - mean_q: 12.530 - prob: 1.000\n",
            "\n",
            "Interval 49163 (491620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.114 - mean_q: 12.374 - prob: 1.000\n",
            "\n",
            "Interval 49164 (491630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 49165 (491640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.325 - mean_q: 12.695 - prob: 1.000\n",
            "\n",
            "Interval 49166 (491650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.185 - mean_q: 12.363 - prob: 1.000\n",
            "\n",
            "Interval 49167 (491660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.359 - mean_q: 12.650 - prob: 1.000\n",
            "\n",
            "Interval 49168 (491670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.341 - mean_q: 12.608 - prob: 1.000\n",
            "\n",
            "Interval 49169 (491680 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.276 - mean_q: 12.730 - prob: 1.000\n",
            "\n",
            "Interval 49170 (491690 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 49171 (491700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.607 - mean_q: 12.922 - prob: 1.000\n",
            "\n",
            "Interval 49172 (491710 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.224 - mean_q: 12.594 - prob: 1.000\n",
            "\n",
            "Interval 49173 (491720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.583 - mean_q: 12.991 - prob: 1.000\n",
            "\n",
            "Interval 49174 (491730 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 49175 (491740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.406 - mean_q: 12.684 - prob: 1.000\n",
            "\n",
            "Interval 49176 (491750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 49177 (491760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.383 - mean_q: 12.845 - prob: 1.000\n",
            "\n",
            "Interval 49178 (491770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.369 - mean_q: 12.815 - prob: 1.000\n",
            "\n",
            "Interval 49179 (491780 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 49180 (491790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.294 - mean_q: 12.457 - prob: 1.000\n",
            "\n",
            "Interval 49181 (491800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 49182 (491810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.808 - mean_q: 13.318 - prob: 1.000\n",
            "\n",
            "Interval 49183 (491820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.296 - mean_q: 12.516 - prob: 1.000\n",
            "\n",
            "Interval 49184 (491830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 49185 (491840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.549 - mean_q: 12.876 - prob: 1.000\n",
            "\n",
            "Interval 49186 (491850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.170 - mean_q: 12.533 - prob: 1.000\n",
            "\n",
            "Interval 49187 (491860 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.345 - mean_q: 12.678 - prob: 1.000\n",
            "\n",
            "Interval 49188 (491870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 49189 (491880 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.585 - mean_q: 13.124 - prob: 1.000\n",
            "\n",
            "Interval 49190 (491890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.249 - mean_q: 12.365 - prob: 1.000\n",
            "\n",
            "Interval 49191 (491900 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.768 - mean_q: 13.130 - prob: 1.000\n",
            "\n",
            "Interval 49192 (491910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.179 - mean_q: 12.479 - prob: 1.000\n",
            "\n",
            "Interval 49193 (491920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 49194 (491930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.617 - mean_q: 13.076 - prob: 1.000\n",
            "\n",
            "Interval 49195 (491940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 49196 (491950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.020 - mean_q: 12.195 - prob: 1.000\n",
            "\n",
            "Interval 49197 (491960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.554 - mean_q: 12.851 - prob: 1.000\n",
            "\n",
            "Interval 49198 (491970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 49199 (491980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.444 - mean_q: 13.059 - prob: 1.000\n",
            "\n",
            "Interval 49200 (491990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 49201 (492000 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.165 - mean_q: 12.400 - prob: 1.000\n",
            "\n",
            "Interval 49202 (492010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -24.000 [-24.000, -24.000] - loss: 0.000 - mae: 7.426 - mean_q: 12.850 - prob: 1.000\n",
            "\n",
            "Interval 49203 (492020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 49204 (492030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 49205 (492040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.291 - mean_q: 12.671 - prob: 1.000\n",
            "\n",
            "Interval 49206 (492050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 49207 (492060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.153 - mean_q: 12.374 - prob: 1.000\n",
            "\n",
            "Interval 49208 (492070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.195 - mean_q: 12.472 - prob: 1.000\n",
            "\n",
            "Interval 49209 (492080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 49210 (492090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.681 - mean_q: 13.254 - prob: 1.000\n",
            "\n",
            "Interval 49211 (492100 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.211 - mean_q: 12.510 - prob: 1.000\n",
            "\n",
            "Interval 49212 (492110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.549 - mean_q: 13.048 - prob: 1.000\n",
            "\n",
            "Interval 49213 (492120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.472 - mean_q: 12.914 - prob: 1.000\n",
            "\n",
            "Interval 49214 (492130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 49215 (492140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.533 - mean_q: 12.952 - prob: 1.000\n",
            "\n",
            "Interval 49216 (492150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.191 - mean_q: 12.424 - prob: 1.000\n",
            "\n",
            "Interval 49217 (492160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 49218 (492170 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.000 - mae: 7.320 - mean_q: 12.726 - prob: 1.000\n",
            "\n",
            "Interval 49219 (492180 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.377 - mean_q: 12.745 - prob: 1.000\n",
            "\n",
            "Interval 49220 (492190 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.288 - mean_q: 12.721 - prob: 1.000\n",
            "\n",
            "Interval 49221 (492200 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.445 - mean_q: 12.777 - prob: 1.000\n",
            "\n",
            "Interval 49222 (492210 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 49223 (492220 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.182 - mean_q: 12.474 - prob: 1.000\n",
            "\n",
            "Interval 49224 (492230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 49225 (492240 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.000 - mae: 7.491 - mean_q: 12.969 - prob: 1.000\n",
            "\n",
            "Interval 49226 (492250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.207 - mean_q: 12.517 - prob: 1.000\n",
            "\n",
            "Interval 49227 (492260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.440 - mean_q: 12.792 - prob: 1.000\n",
            "\n",
            "Interval 49228 (492270 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 49229 (492280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 11.000 [10.000, 12.000] - loss: 0.000 - mae: 7.221 - mean_q: 12.513 - prob: 1.000\n",
            "\n",
            "Interval 49230 (492290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 49231 (492300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.637 - mean_q: 13.113 - prob: 1.000\n",
            "\n",
            "Interval 49232 (492310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.309 - mean_q: 12.709 - prob: 1.000\n",
            "\n",
            "Interval 49233 (492320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 49234 (492330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.296 - mean_q: 12.613 - prob: 1.000\n",
            "\n",
            "Interval 49235 (492340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.169 - mean_q: 12.550 - prob: 1.000\n",
            "\n",
            "Interval 49236 (492350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 49237 (492360 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.336 - mean_q: 12.658 - prob: 1.000\n",
            "\n",
            "Interval 49238 (492370 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 6.999 - mean_q: 12.263 - prob: 1.000\n",
            "\n",
            "Interval 49239 (492380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 49240 (492390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.499 - mean_q: 12.984 - prob: 1.000\n",
            "\n",
            "Interval 49241 (492400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.577 - mean_q: 13.031 - prob: 1.000\n",
            "\n",
            "Interval 49242 (492410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 49243 (492420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.139 - mean_q: 12.333 - prob: 1.000\n",
            "\n",
            "Interval 49244 (492430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 49245 (492440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -36.000 [-36.000, -36.000] - loss: 0.000 - mae: 7.359 - mean_q: 12.772 - prob: 1.000\n",
            "\n",
            "Interval 49246 (492450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 49247 (492460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.407 - mean_q: 12.819 - prob: 1.000\n",
            "\n",
            "Interval 49248 (492470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.429 - mean_q: 12.833 - prob: 1.000\n",
            "\n",
            "Interval 49249 (492480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 49250 (492490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.269 - mean_q: 12.617 - prob: 1.000\n",
            "\n",
            "Interval 49251 (492500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 49252 (492510 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -19.000 [-19.000, -19.000] - loss: 0.000 - mae: 7.544 - mean_q: 12.993 - prob: 1.000\n",
            "\n",
            "Interval 49253 (492520 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.426 - mean_q: 12.821 - prob: 1.000\n",
            "\n",
            "Interval 49254 (492530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.000 - mae: 7.497 - mean_q: 12.962 - prob: 1.000\n",
            "\n",
            "Interval 49255 (492540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.214 - mean_q: 12.458 - prob: 1.000\n",
            "\n",
            "Interval 49256 (492550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 49257 (492560 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.550 - mean_q: 12.849 - prob: 1.000\n",
            "\n",
            "Interval 56825 (568240 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 56826 (568250 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -18.000 [-18.000, -18.000] - loss: 0.000 - mae: 7.210 - mean_q: 12.551 - prob: 1.000\n",
            "\n",
            "Interval 56827 (568260 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.386 - mean_q: 12.920 - prob: 1.000\n",
            "\n",
            "Interval 56828 (568270 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 56829 (568280 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.086 - mean_q: 12.316 - prob: 1.000\n",
            "\n",
            "Interval 56830 (568290 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.348 - mean_q: 12.660 - prob: 1.000\n",
            "\n",
            "Interval 56831 (568300 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 56832 (568310 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.371 - mean_q: 12.667 - prob: 1.000\n",
            "\n",
            "Interval 56833 (568320 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.512 - mean_q: 12.883 - prob: 1.000\n",
            "\n",
            "Interval 56834 (568330 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 56835 (568340 steps performed)\n",
            "10/10 [==============================] - 0s 14ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.435 - mean_q: 12.877 - prob: 1.000\n",
            "\n",
            "Interval 56836 (568350 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 56837 (568360 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.454 - mean_q: 12.901 - prob: 1.000\n",
            "\n",
            "Interval 56838 (568370 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.500 - mean_q: 13.065 - prob: 1.000\n",
            "\n",
            "Interval 56839 (568380 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 56840 (568390 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.299 - mean_q: 12.544 - prob: 1.000\n",
            "\n",
            "Interval 56841 (568400 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 56842 (568410 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.000 - mae: 7.515 - mean_q: 12.904 - prob: 1.000\n",
            "\n",
            "Interval 56843 (568420 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.159 - mean_q: 12.375 - prob: 1.000\n",
            "\n",
            "Interval 56844 (568430 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.545 - mean_q: 12.910 - prob: 1.000\n",
            "\n",
            "Interval 56845 (568440 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 56846 (568450 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.170 - mean_q: 12.325 - prob: 1.000\n",
            "\n",
            "Interval 56847 (568460 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.549 - mean_q: 12.994 - prob: 1.000\n",
            "\n",
            "Interval 56848 (568470 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.473 - mean_q: 12.762 - prob: 1.000\n",
            "\n",
            "Interval 56849 (568480 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 56850 (568490 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.342 - mean_q: 12.789 - prob: 1.000\n",
            "\n",
            "Interval 56851 (568500 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 56852 (568510 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.000 - mae: 7.621 - mean_q: 13.077 - prob: 1.000\n",
            "\n",
            "Interval 56853 (568520 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.000 - mae: 7.510 - mean_q: 12.994 - prob: 1.000\n",
            "\n",
            "Interval 56854 (568530 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.459 - mean_q: 12.936 - prob: 1.000\n",
            "\n",
            "Interval 56855 (568540 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.198 - mean_q: 12.483 - prob: 1.000\n",
            "\n",
            "Interval 56856 (568550 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 56857 (568560 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.247 - mean_q: 12.514 - prob: 1.000\n",
            "\n",
            "Interval 56858 (568570 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 56859 (568580 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.156 - mean_q: 12.369 - prob: 1.000\n",
            "\n",
            "Interval 56860 (568590 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.371 - mean_q: 12.806 - prob: 1.000\n",
            "\n",
            "Interval 56861 (568600 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.001 - mae: 7.215 - mean_q: 12.632 - prob: 1.000\n",
            "\n",
            "Interval 56862 (568610 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.632 - mean_q: 13.016 - prob: 1.000\n",
            "\n",
            "Interval 56863 (568620 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 56864 (568630 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.574 - mean_q: 13.028 - prob: 1.000\n",
            "\n",
            "Interval 56865 (568640 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.180 - mean_q: 12.411 - prob: 1.000\n",
            "\n",
            "Interval 56866 (568650 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 56867 (568660 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.324 - mean_q: 12.684 - prob: 1.000\n",
            "\n",
            "Interval 56868 (568670 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.9000\n",
            "Interval 56869 (568680 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.000 - mae: 7.171 - mean_q: 12.426 - prob: 1.000\n",
            "\n",
            "Interval 56870 (568690 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -3.7000\n",
            "Interval 56871 (568700 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -33.000 [-33.000, -33.000] - loss: 0.001 - mae: 7.055 - mean_q: 12.322 - prob: 1.000\n",
            "\n",
            "Interval 56872 (568710 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.303 - mean_q: 12.664 - prob: 1.000\n",
            "\n",
            "Interval 56873 (568720 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.001 - mae: 7.367 - mean_q: 12.693 - prob: 1.000\n",
            "\n",
            "Interval 56874 (568730 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.215 - mean_q: 12.484 - prob: 1.000\n",
            "\n",
            "Interval 56875 (568740 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.543 - mean_q: 13.118 - prob: 1.000\n",
            "\n",
            "Interval 56876 (568750 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 56877 (568760 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.636 - mean_q: 13.097 - prob: 1.000\n",
            "\n",
            "Interval 56878 (568770 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -2.8000\n",
            "Interval 56879 (568780 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -18.000 [-18.000, -18.000] - loss: 0.001 - mae: 7.351 - mean_q: 12.765 - prob: 1.000\n",
            "\n",
            "Interval 56880 (568790 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.0000\n",
            "Interval 56881 (568800 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.406 - mean_q: 12.684 - prob: 1.000\n",
            "\n",
            "Interval 56882 (568810 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.478 - mean_q: 12.727 - prob: 1.000\n",
            "\n",
            "Interval 56883 (568820 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.327 - mean_q: 12.761 - prob: 1.000\n",
            "\n",
            "Interval 56884 (568830 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 56885 (568840 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.192 - mean_q: 12.462 - prob: 1.000\n",
            "\n",
            "Interval 56886 (568850 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 56887 (568860 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.001 - mae: 7.655 - mean_q: 13.168 - prob: 1.000\n",
            "\n",
            "Interval 56888 (568870 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 56889 (568880 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.808 - mean_q: 13.294 - prob: 1.000\n",
            "\n",
            "Interval 56890 (568890 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.349 - mean_q: 12.711 - prob: 1.000\n",
            "\n",
            "Interval 56891 (568900 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 56892 (568910 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 10.000 [8.000, 12.000] - loss: 0.001 - mae: 7.277 - mean_q: 12.585 - prob: 1.000\n",
            "\n",
            "Interval 56893 (568920 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 56894 (568930 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.032 - mean_q: 12.416 - prob: 1.000\n",
            "\n",
            "Interval 56895 (568940 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.0000\n",
            "Interval 56896 (568950 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.003 - mae: 7.207 - mean_q: 12.574 - prob: 1.000\n",
            "\n",
            "Interval 56897 (568960 steps performed)\n",
            "10/10 [==============================] - 0s 14ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.360 - mean_q: 12.688 - prob: 1.000\n",
            "\n",
            "Interval 56898 (568970 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: -1.0000\n",
            "Interval 56899 (568980 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.351 - mean_q: 12.689 - prob: 1.000\n",
            "\n",
            "Interval 56900 (568990 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.450 - mean_q: 12.910 - prob: 1.000\n",
            "\n",
            "Interval 56901 (569000 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 56902 (569010 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.331 - mean_q: 12.776 - prob: 1.000\n",
            "\n",
            "Interval 56903 (569020 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 56904 (569030 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.475 - mean_q: 13.012 - prob: 1.000\n",
            "\n",
            "Interval 56905 (569040 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.385 - mean_q: 12.711 - prob: 1.000\n",
            "\n",
            "Interval 56906 (569050 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.440 - mean_q: 12.743 - prob: 1.000\n",
            "\n",
            "Interval 56907 (569060 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 56908 (569070 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.738 - mean_q: 13.124 - prob: 1.000\n",
            "\n",
            "Interval 56909 (569080 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.444 - mean_q: 12.811 - prob: 1.000\n",
            "\n",
            "Interval 56910 (569090 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.9000\n",
            "Interval 56911 (569100 steps performed)\n",
            "10/10 [==============================] - 0s 15ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.447 - mean_q: 12.870 - prob: 1.000\n",
            "\n",
            "Interval 56912 (569110 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.324 - mean_q: 12.957 - prob: 1.000\n",
            "\n",
            "Interval 56913 (569120 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 56914 (569130 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.328 - mean_q: 12.553 - prob: 1.000\n",
            "\n",
            "Interval 56915 (569140 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.388 - mean_q: 12.729 - prob: 1.000\n",
            "\n",
            "Interval 56916 (569150 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.001 - mae: 7.518 - mean_q: 13.108 - prob: 1.000\n",
            "\n",
            "Interval 56917 (569160 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -2.8000\n",
            "Interval 56918 (569170 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.001 - mae: 7.323 - mean_q: 12.789 - prob: 1.000\n",
            "\n",
            "Interval 56919 (569180 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.454 - mean_q: 12.843 - prob: 1.000\n",
            "\n",
            "Interval 56920 (569190 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 56921 (569200 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.451 - mean_q: 12.803 - prob: 1.000\n",
            "\n",
            "Interval 56922 (569210 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 56923 (569220 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.520 - mean_q: 13.015 - prob: 1.000\n",
            "\n",
            "Interval 56924 (569230 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.416 - mean_q: 12.854 - prob: 1.000\n",
            "\n",
            "Interval 56925 (569240 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 56926 (569250 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.310 - mean_q: 12.465 - prob: 1.000\n",
            "\n",
            "Interval 56927 (569260 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.297 - mean_q: 12.474 - prob: 1.000\n",
            "\n",
            "Interval 56928 (569270 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.255 - mean_q: 12.450 - prob: 1.000\n",
            "\n",
            "Interval 56929 (569280 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.9000\n",
            "Interval 56930 (569290 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.533 - mean_q: 12.939 - prob: 1.000\n",
            "\n",
            "Interval 56931 (569300 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.883 - mean_q: 13.529 - prob: 1.000\n",
            "\n",
            "Interval 56932 (569310 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 56933 (569320 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.380 - mean_q: 12.751 - prob: 1.000\n",
            "\n",
            "Interval 56934 (569330 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.125 - mean_q: 12.454 - prob: 1.000\n",
            "\n",
            "Interval 56935 (569340 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 56936 (569350 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.648 - mean_q: 13.091 - prob: 1.000\n",
            "\n",
            "Interval 56937 (569360 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 56938 (569370 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.140 - mean_q: 12.370 - prob: 1.000\n",
            "\n",
            "Interval 56939 (569380 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.276 - mean_q: 12.572 - prob: 1.000\n",
            "\n",
            "Interval 56940 (569390 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 56941 (569400 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.463 - mean_q: 12.909 - prob: 1.000\n",
            "\n",
            "Interval 56942 (569410 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.263 - mean_q: 12.554 - prob: 1.000\n",
            "\n",
            "Interval 56943 (569420 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 56944 (569430 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.000 - mae: 7.274 - mean_q: 12.483 - prob: 1.000\n",
            "\n",
            "Interval 56945 (569440 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.000 - mae: 7.712 - mean_q: 13.080 - prob: 1.000\n",
            "\n",
            "Interval 56946 (569450 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 56947 (569460 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.535 - mean_q: 12.977 - prob: 1.000\n",
            "\n",
            "Interval 56948 (569470 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.347 - mean_q: 12.522 - prob: 1.000\n",
            "\n",
            "Interval 56949 (569480 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 56950 (569490 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.449 - mean_q: 12.961 - prob: 1.000\n",
            "\n",
            "Interval 56951 (569500 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.755 - mean_q: 13.176 - prob: 1.000\n",
            "\n",
            "Interval 56952 (569510 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.323 - mean_q: 12.654 - prob: 1.000\n",
            "\n",
            "Interval 56953 (569520 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 56954 (569530 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.501 - mean_q: 13.086 - prob: 1.000\n",
            "\n",
            "Interval 56955 (569540 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 56956 (569550 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.000 - mae: 7.699 - mean_q: 13.226 - prob: 1.000\n",
            "\n",
            "Interval 56957 (569560 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 56958 (569570 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.000 - mae: 7.345 - mean_q: 12.728 - prob: 1.000\n",
            "\n",
            "Interval 56959 (569580 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.001 - mae: 7.530 - mean_q: 12.928 - prob: 1.000\n",
            "\n",
            "Interval 56960 (569590 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 56961 (569600 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.792 - mean_q: 13.394 - prob: 1.000\n",
            "\n",
            "Interval 56962 (569610 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.000 - mae: 7.658 - mean_q: 13.164 - prob: 1.000\n",
            "\n",
            "Interval 56963 (569620 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 56964 (569630 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.230 - mean_q: 12.405 - prob: 1.000\n",
            "\n",
            "Interval 56965 (569640 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: -1.0000\n",
            "Interval 56966 (569650 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.146 - mean_q: 12.554 - prob: 1.000\n",
            "\n",
            "Interval 56967 (569660 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.457 - mean_q: 12.686 - prob: 1.000\n",
            "\n",
            "Interval 56968 (569670 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.683 - mean_q: 13.143 - prob: 1.000\n",
            "\n",
            "Interval 56969 (569680 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.722 - mean_q: 13.174 - prob: 1.000\n",
            "\n",
            "Interval 64391 (643900 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 64392 (643910 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.384 - mean_q: 12.744 - prob: 1.000\n",
            "\n",
            "Interval 64393 (643920 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.157 - mean_q: 12.482 - prob: 1.000\n",
            "\n",
            "Interval 64394 (643930 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.177 - mean_q: 12.660 - prob: 1.000\n",
            "\n",
            "Interval 64395 (643940 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 64396 (643950 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.289 - mean_q: 12.578 - prob: 1.000\n",
            "\n",
            "Interval 64397 (643960 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.212 - mean_q: 12.454 - prob: 1.000\n",
            "\n",
            "Interval 64398 (643970 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 64399 (643980 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.118 - mean_q: 12.357 - prob: 1.000\n",
            "\n",
            "Interval 64400 (643990 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.010 - mean_q: 12.192 - prob: 1.000\n",
            "\n",
            "Interval 64401 (644000 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 64402 (644010 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.400 - mean_q: 12.829 - prob: 1.000\n",
            "\n",
            "Interval 64403 (644020 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.021 - mean_q: 12.146 - prob: 1.000\n",
            "\n",
            "Interval 64404 (644030 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 64405 (644040 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.336 - mean_q: 12.759 - prob: 1.000\n",
            "\n",
            "Interval 64406 (644050 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.315 - mean_q: 12.663 - prob: 1.000\n",
            "\n",
            "Interval 64407 (644060 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -19.000 [-19.000, -19.000] - loss: 0.000 - mae: 7.337 - mean_q: 12.588 - prob: 1.000\n",
            "\n",
            "Interval 64408 (644070 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 64409 (644080 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.185 - mean_q: 12.503 - prob: 1.000\n",
            "\n",
            "Interval 64410 (644090 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 64411 (644100 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.196 - mean_q: 12.331 - prob: 1.000\n",
            "\n",
            "Interval 64412 (644110 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.000 - mae: 7.103 - mean_q: 12.487 - prob: 1.000\n",
            "\n",
            "Interval 64413 (644120 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.306 - mean_q: 12.718 - prob: 1.000\n",
            "\n",
            "Interval 64414 (644130 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.378 - mean_q: 12.708 - prob: 1.000\n",
            "\n",
            "Interval 64415 (644140 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 64416 (644150 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -21.000 [-21.000, -21.000] - loss: 0.001 - mae: 7.389 - mean_q: 12.670 - prob: 1.000\n",
            "\n",
            "Interval 64417 (644160 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 64418 (644170 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.675 - mean_q: 13.112 - prob: 1.000\n",
            "\n",
            "Interval 64419 (644180 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.375 - mean_q: 12.645 - prob: 1.000\n",
            "\n",
            "Interval 64420 (644190 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 64421 (644200 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.325 - mean_q: 12.501 - prob: 1.000\n",
            "\n",
            "Interval 64422 (644210 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.091 - mean_q: 12.386 - prob: 1.000\n",
            "\n",
            "Interval 64423 (644220 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.669 - mean_q: 13.215 - prob: 1.000\n",
            "\n",
            "Interval 64424 (644230 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 64425 (644240 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.629 - mean_q: 13.143 - prob: 1.000\n",
            "\n",
            "Interval 64426 (644250 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.441 - mean_q: 12.906 - prob: 1.000\n",
            "\n",
            "Interval 64427 (644260 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.198 - mean_q: 12.526 - prob: 1.000\n",
            "\n",
            "Interval 64428 (644270 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 64429 (644280 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.000 - mae: 7.294 - mean_q: 12.660 - prob: 1.000\n",
            "\n",
            "Interval 64430 (644290 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 64431 (644300 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.000 - mae: 7.236 - mean_q: 12.595 - prob: 1.000\n",
            "\n",
            "Interval 64432 (644310 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 64433 (644320 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.324 - mean_q: 12.632 - prob: 1.000\n",
            "\n",
            "Interval 64434 (644330 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 64435 (644340 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.542 - mean_q: 12.735 - prob: 1.000\n",
            "\n",
            "Interval 64436 (644350 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 64437 (644360 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.354 - mean_q: 12.709 - prob: 1.000\n",
            "\n",
            "Interval 64438 (644370 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.133 - mean_q: 12.360 - prob: 1.000\n",
            "\n",
            "Interval 64439 (644380 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.143 - mean_q: 12.465 - prob: 1.000\n",
            "\n",
            "Interval 64440 (644390 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 64441 (644400 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.000 - mae: 7.201 - mean_q: 12.514 - prob: 1.000\n",
            "\n",
            "Interval 64442 (644410 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 64443 (644420 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.000 - mae: 7.363 - mean_q: 12.659 - prob: 1.000\n",
            "\n",
            "Interval 64444 (644430 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 64445 (644440 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.226 - mean_q: 12.367 - prob: 1.000\n",
            "\n",
            "Interval 64446 (644450 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -2.8000\n",
            "Interval 64447 (644460 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.000 - mae: 7.451 - mean_q: 12.896 - prob: 1.000\n",
            "\n",
            "Interval 64448 (644470 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.003 - mae: 7.463 - mean_q: 12.810 - prob: 1.000\n",
            "\n",
            "Interval 64449 (644480 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 64450 (644490 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.698 - mean_q: 13.321 - prob: 1.000\n",
            "\n",
            "Interval 64451 (644500 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.345 - mean_q: 12.842 - prob: 1.000\n",
            "\n",
            "Interval 64452 (644510 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.0000\n",
            "Interval 64453 (644520 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.501 - mean_q: 12.922 - prob: 1.000\n",
            "\n",
            "Interval 64454 (644530 steps performed)\n",
            "10/10 [==============================] - 0s 14ms/step - reward: -1.9000\n",
            "Interval 64455 (644540 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.272 - mean_q: 12.783 - prob: 1.000\n",
            "\n",
            "Interval 64456 (644550 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.233 - mean_q: 12.488 - prob: 1.000\n",
            "\n",
            "Interval 64457 (644560 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.028 - mean_q: 12.069 - prob: 1.000\n",
            "\n",
            "Interval 64458 (644570 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.429 - mean_q: 12.835 - prob: 1.000\n",
            "\n",
            "Interval 64459 (644580 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.281 - mean_q: 12.535 - prob: 1.000\n",
            "\n",
            "Interval 64460 (644590 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.0000\n",
            "Interval 64461 (644600 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.641 - mean_q: 13.119 - prob: 1.000\n",
            "\n",
            "Interval 64462 (644610 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.452 - mean_q: 12.755 - prob: 1.000\n",
            "\n",
            "Interval 64463 (644620 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.118 - mean_q: 12.381 - prob: 1.000\n",
            "\n",
            "Interval 64464 (644630 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 64465 (644640 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.311 - mean_q: 12.602 - prob: 1.000\n",
            "\n",
            "Interval 64466 (644650 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 64467 (644660 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.711 - mean_q: 13.145 - prob: 1.000\n",
            "\n",
            "Interval 64468 (644670 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.319 - mean_q: 12.713 - prob: 1.000\n",
            "\n",
            "Interval 64469 (644680 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.360 - mean_q: 12.748 - prob: 1.000\n",
            "\n",
            "Interval 64470 (644690 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.257 - mean_q: 12.613 - prob: 1.000\n",
            "\n",
            "Interval 64471 (644700 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 64472 (644710 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.001 - mae: 7.478 - mean_q: 12.663 - prob: 1.000\n",
            "\n",
            "Interval 64473 (644720 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.307 - mean_q: 12.527 - prob: 1.000\n",
            "\n",
            "Interval 64474 (644730 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 64475 (644740 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 6.928 - mean_q: 12.149 - prob: 1.000\n",
            "\n",
            "Interval 64476 (644750 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.001 - mae: 7.272 - mean_q: 12.455 - prob: 1.000\n",
            "\n",
            "Interval 64477 (644760 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 64478 (644770 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.633 - mean_q: 13.134 - prob: 1.000\n",
            "\n",
            "Interval 64479 (644780 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.636 - mean_q: 13.118 - prob: 1.000\n",
            "\n",
            "Interval 64480 (644790 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 64481 (644800 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.238 - mean_q: 12.397 - prob: 1.000\n",
            "\n",
            "Interval 64482 (644810 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -2.8000\n",
            "Interval 64483 (644820 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.001 - mae: 7.541 - mean_q: 12.850 - prob: 1.000\n",
            "\n",
            "Interval 64484 (644830 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.001 - mae: 7.405 - mean_q: 12.705 - prob: 1.000\n",
            "\n",
            "Interval 64485 (644840 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -2.8000\n",
            "Interval 64486 (644850 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.000 - mae: 7.490 - mean_q: 12.744 - prob: 1.000\n",
            "\n",
            "Interval 64487 (644860 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.520 - mean_q: 12.855 - prob: 1.000\n",
            "\n",
            "Interval 64488 (644870 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.000 - mae: 7.626 - mean_q: 12.989 - prob: 1.000\n",
            "\n",
            "Interval 64489 (644880 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.175 - mean_q: 12.481 - prob: 1.000\n",
            "\n",
            "Interval 64490 (644890 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.585 - mean_q: 13.025 - prob: 1.000\n",
            "\n",
            "Interval 64491 (644900 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 64492 (644910 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.381 - mean_q: 12.656 - prob: 1.000\n",
            "\n",
            "Interval 64493 (644920 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 64494 (644930 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -18.000 [-18.000, -18.000] - loss: 0.000 - mae: 7.404 - mean_q: 12.855 - prob: 1.000\n",
            "\n",
            "Interval 64495 (644940 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.882 - mean_q: 13.558 - prob: 1.000\n",
            "\n",
            "Interval 64496 (644950 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.280 - mean_q: 12.703 - prob: 1.000\n",
            "\n",
            "Interval 64497 (644960 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.434 - mean_q: 12.910 - prob: 1.000\n",
            "\n",
            "Interval 64498 (644970 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -2.8000\n",
            "Interval 64499 (644980 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.000 - mae: 7.654 - mean_q: 13.138 - prob: 1.000\n",
            "\n",
            "Interval 64500 (644990 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.000 - mae: 7.850 - mean_q: 13.370 - prob: 1.000\n",
            "\n",
            "Interval 64501 (645000 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 64502 (645010 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.242 - mean_q: 12.600 - prob: 1.000\n",
            "\n",
            "Interval 64503 (645020 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.9000\n",
            "Interval 64504 (645030 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.001 - mae: 7.240 - mean_q: 12.515 - prob: 1.000\n",
            "\n",
            "Interval 64505 (645040 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.207 - mean_q: 12.553 - prob: 1.000\n",
            "\n",
            "Interval 64506 (645050 steps performed)\n",
            "10/10 [==============================] - 0s 14ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.452 - mean_q: 12.774 - prob: 1.000\n",
            "\n",
            "Interval 64507 (645060 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.9000\n",
            "Interval 64508 (645070 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.314 - mean_q: 12.543 - prob: 1.000\n",
            "\n",
            "Interval 64509 (645080 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.513 - mean_q: 12.951 - prob: 1.000\n",
            "\n",
            "Interval 64510 (645090 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 64511 (645100 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.473 - mean_q: 12.855 - prob: 1.000\n",
            "\n",
            "Interval 64512 (645110 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 64513 (645120 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.532 - mean_q: 13.078 - prob: 1.000\n",
            "\n",
            "Interval 64514 (645130 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.508 - mean_q: 12.944 - prob: 1.000\n",
            "\n",
            "Interval 64515 (645140 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 64516 (645150 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.001 - mae: 7.333 - mean_q: 12.739 - prob: 1.000\n",
            "\n",
            "Interval 64517 (645160 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.477 - mean_q: 12.776 - prob: 1.000\n",
            "\n",
            "Interval 64518 (645170 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 64519 (645180 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.585 - mean_q: 13.150 - prob: 1.000\n",
            "\n",
            "Interval 64520 (645190 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.386 - mean_q: 12.716 - prob: 1.000\n",
            "\n",
            "Interval 64521 (645200 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.000 - mae: 7.308 - mean_q: 12.717 - prob: 1.000\n",
            "\n",
            "Interval 64522 (645210 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.427 - mean_q: 12.919 - prob: 1.000\n",
            "\n",
            "Interval 64523 (645220 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 64524 (645230 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.000 - mae: 7.572 - mean_q: 13.176 - prob: 1.000\n",
            "\n",
            "Interval 64525 (645240 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 64526 (645250 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.000 - mae: 7.152 - mean_q: 12.400 - prob: 1.000\n",
            "\n",
            "Interval 64527 (645260 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.649 - mean_q: 13.054 - prob: 1.000\n",
            "\n",
            "Interval 64528 (645270 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 64529 (645280 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.203 - mean_q: 12.597 - prob: 1.000\n",
            "\n",
            "Interval 64530 (645290 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 64531 (645300 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.271 - mean_q: 12.715 - prob: 1.000\n",
            "\n",
            "Interval 64532 (645310 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.241 - mean_q: 12.656 - prob: 1.000\n",
            "\n",
            "Interval 64533 (645320 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.635 - mean_q: 13.068 - prob: 1.000\n",
            "\n",
            "Interval 64534 (645330 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.0000\n",
            "Interval 64535 (645340 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.002 - mae: 7.251 - mean_q: 12.622 - prob: 1.000\n",
            "\n",
            "Interval 64536 (645350 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.286 - mean_q: 12.514 - prob: 1.000\n",
            "\n",
            "Interval 64537 (645360 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.9000\n",
            "Interval 64538 (645370 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.187 - mean_q: 12.415 - prob: 1.000\n",
            "\n",
            "Interval 64539 (645380 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.777 - mean_q: 13.316 - prob: 1.000\n",
            "\n",
            "Interval 64540 (645390 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.0000\n",
            "Interval 64541 (645400 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.371 - mean_q: 12.685 - prob: 1.000\n",
            "\n",
            "Interval 64542 (645410 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.383 - mean_q: 12.698 - prob: 1.000\n",
            "\n",
            "Interval 64543 (645420 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.316 - mean_q: 12.691 - prob: 1.000\n",
            "\n",
            "Interval 64544 (645430 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.0000\n",
            "Interval 64545 (645440 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.525 - mean_q: 13.049 - prob: 1.000\n",
            "\n",
            "Interval 64546 (645450 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.212 - mean_q: 12.631 - prob: 1.000\n",
            "\n",
            "Interval 71847 (718460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.224 - mean_q: 12.587 - prob: 1.000\n",
            "\n",
            "Interval 71848 (718470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 71849 (718480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.514 - mean_q: 13.026 - prob: 1.000\n",
            "\n",
            "Interval 71850 (718490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 71851 (718500 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.000 - mae: 7.155 - mean_q: 12.432 - prob: 1.000\n",
            "\n",
            "Interval 71852 (718510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.484 - mean_q: 12.888 - prob: 1.000\n",
            "\n",
            "Interval 71853 (718520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.000 - mae: 7.398 - mean_q: 12.783 - prob: 1.000\n",
            "\n",
            "Interval 71854 (718530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 71855 (718540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.377 - mean_q: 12.833 - prob: 1.000\n",
            "\n",
            "Interval 71856 (718550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.407 - mean_q: 12.660 - prob: 1.000\n",
            "\n",
            "Interval 71857 (718560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.495 - mean_q: 13.023 - prob: 1.000\n",
            "\n",
            "Interval 71858 (718570 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.326 - mean_q: 12.587 - prob: 1.000\n",
            "\n",
            "Interval 71859 (718580 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 71860 (718590 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 71861 (718600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.223 - mean_q: 12.485 - prob: 1.000\n",
            "\n",
            "Interval 71862 (718610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.335 - mean_q: 12.565 - prob: 1.000\n",
            "\n",
            "Interval 71863 (718620 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 71864 (718630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.410 - mean_q: 12.711 - prob: 1.000\n",
            "\n",
            "Interval 71865 (718640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.000 - mae: 7.579 - mean_q: 12.932 - prob: 1.000\n",
            "\n",
            "Interval 71866 (718650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 71867 (718660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.000 - mae: 7.260 - mean_q: 12.565 - prob: 1.000\n",
            "\n",
            "Interval 71868 (718670 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.348 - mean_q: 12.722 - prob: 1.000\n",
            "\n",
            "Interval 71869 (718680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 71870 (718690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.582 - mean_q: 13.124 - prob: 1.000\n",
            "\n",
            "Interval 71871 (718700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.483 - mean_q: 12.854 - prob: 1.000\n",
            "\n",
            "Interval 71872 (718710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.002 - mae: 7.080 - mean_q: 12.411 - prob: 1.000\n",
            "\n",
            "Interval 71873 (718720 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 71874 (718730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.098 - mean_q: 12.313 - prob: 1.000\n",
            "\n",
            "Interval 71875 (718740 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.002 - mae: 7.124 - mean_q: 12.315 - prob: 1.000\n",
            "\n",
            "Interval 71876 (718750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 71877 (718760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.795 - mean_q: 13.240 - prob: 1.000\n",
            "\n",
            "Interval 71878 (718770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.001 - mae: 6.973 - mean_q: 12.157 - prob: 1.000\n",
            "\n",
            "Interval 71879 (718780 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.135 - mean_q: 12.441 - prob: 1.000\n",
            "\n",
            "Interval 71880 (718790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 71881 (718800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.361 - mean_q: 12.753 - prob: 1.000\n",
            "\n",
            "Interval 71882 (718810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.001 - mae: 7.380 - mean_q: 12.533 - prob: 1.000\n",
            "\n",
            "Interval 71883 (718820 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 71884 (718830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.000 - mae: 7.378 - mean_q: 12.802 - prob: 1.000\n",
            "\n",
            "Interval 71885 (718840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.568 - mean_q: 12.964 - prob: 1.000\n",
            "\n",
            "Interval 71886 (718850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 71887 (718860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.699 - mean_q: 13.245 - prob: 1.000\n",
            "\n",
            "Interval 71888 (718870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.323 - mean_q: 12.498 - prob: 1.000\n",
            "\n",
            "Interval 71889 (718880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.654 - mean_q: 13.149 - prob: 1.000\n",
            "\n",
            "Interval 71890 (718890 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 71891 (718900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.410 - mean_q: 12.778 - prob: 1.000\n",
            "\n",
            "Interval 71892 (718910 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 71893 (718920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.400 - mean_q: 12.722 - prob: 1.000\n",
            "\n",
            "Interval 71894 (718930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.000 - mae: 7.232 - mean_q: 12.578 - prob: 1.000\n",
            "\n",
            "Interval 71895 (718940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.380 - mean_q: 12.772 - prob: 1.000\n",
            "\n",
            "Interval 71896 (718950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.000 - mae: 7.104 - mean_q: 12.492 - prob: 1.000\n",
            "\n",
            "Interval 71897 (718960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 71898 (718970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.565 - mean_q: 12.929 - prob: 1.000\n",
            "\n",
            "Interval 71899 (718980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.276 - mean_q: 12.568 - prob: 1.000\n",
            "\n",
            "Interval 71900 (718990 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 71901 (719000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.443 - mean_q: 12.815 - prob: 1.000\n",
            "\n",
            "Interval 71902 (719010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.300 - mean_q: 12.734 - prob: 1.000\n",
            "\n",
            "Interval 71903 (719020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 71904 (719030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.161 - mean_q: 12.474 - prob: 1.000\n",
            "\n",
            "Interval 71905 (719040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.498 - mean_q: 12.908 - prob: 1.000\n",
            "\n",
            "Interval 71906 (719050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.729 - mean_q: 13.320 - prob: 1.000\n",
            "\n",
            "Interval 71907 (719060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 71908 (719070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 71909 (719080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.001 - mae: 7.333 - mean_q: 12.561 - prob: 1.000\n",
            "\n",
            "Interval 71910 (719090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.406 - mean_q: 12.694 - prob: 1.000\n",
            "\n",
            "Interval 71911 (719100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.337 - mean_q: 12.711 - prob: 1.000\n",
            "\n",
            "Interval 71912 (719110 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 71913 (719120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.573 - mean_q: 13.052 - prob: 1.000\n",
            "\n",
            "Interval 71914 (719130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.376 - mean_q: 12.852 - prob: 1.000\n",
            "\n",
            "Interval 71915 (719140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 71916 (719150 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.232 - mean_q: 12.498 - prob: 1.000\n",
            "\n",
            "Interval 71917 (719160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.263 - mean_q: 12.572 - prob: 1.000\n",
            "\n",
            "Interval 71918 (719170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.551 - mean_q: 13.179 - prob: 1.000\n",
            "\n",
            "Interval 71919 (719180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.149 - mean_q: 12.340 - prob: 1.000\n",
            "\n",
            "Interval 71920 (719190 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 71921 (719200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.220 - mean_q: 12.450 - prob: 1.000\n",
            "\n",
            "Interval 71922 (719210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 71923 (719220 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.000 - mae: 7.642 - mean_q: 13.237 - prob: 1.000\n",
            "\n",
            "Interval 71924 (719230 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.778 - mean_q: 13.266 - prob: 1.000\n",
            "\n",
            "Interval 71925 (719240 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.302 - mean_q: 12.554 - prob: 1.000\n",
            "\n",
            "Interval 71926 (719250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 71927 (719260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.478 - mean_q: 12.750 - prob: 1.000\n",
            "\n",
            "Interval 71928 (719270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.000 - mae: 7.262 - mean_q: 12.583 - prob: 1.000\n",
            "\n",
            "Interval 71929 (719280 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 71930 (719290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.314 - mean_q: 12.631 - prob: 1.000\n",
            "\n",
            "Interval 71931 (719300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.377 - mean_q: 12.692 - prob: 1.000\n",
            "\n",
            "Interval 71932 (719310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.567 - mean_q: 13.059 - prob: 1.000\n",
            "\n",
            "Interval 71933 (719320 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 71934 (719330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.652 - mean_q: 13.234 - prob: 1.000\n",
            "\n",
            "Interval 71935 (719340 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 71936 (719350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.450 - mean_q: 12.646 - prob: 1.000\n",
            "\n",
            "Interval 71937 (719360 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.595 - mean_q: 13.016 - prob: 1.000\n",
            "\n",
            "Interval 71938 (719370 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 71939 (719380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -25.000 [-25.000, -25.000] - loss: 0.001 - mae: 7.214 - mean_q: 12.570 - prob: 1.000\n",
            "\n",
            "Interval 71940 (719390 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 71941 (719400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.491 - mean_q: 12.785 - prob: 1.000\n",
            "\n",
            "Interval 71942 (719410 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.464 - mean_q: 12.973 - prob: 1.000\n",
            "\n",
            "Interval 71943 (719420 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 71944 (719430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.233 - mean_q: 12.541 - prob: 1.000\n",
            "\n",
            "Interval 71945 (719440 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 71946 (719450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.000 - mae: 7.386 - mean_q: 12.759 - prob: 1.000\n",
            "\n",
            "Interval 71947 (719460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.410 - mean_q: 12.750 - prob: 1.000\n",
            "\n",
            "Interval 71948 (719470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.000 - mae: 7.542 - mean_q: 12.919 - prob: 1.000\n",
            "\n",
            "Interval 71949 (719480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.488 - mean_q: 12.918 - prob: 1.000\n",
            "\n",
            "Interval 71950 (719490 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 71951 (719500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.225 - mean_q: 12.565 - prob: 1.000\n",
            "\n",
            "Interval 71952 (719510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.226 - mean_q: 12.497 - prob: 1.000\n",
            "\n",
            "Interval 71953 (719520 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 71954 (719530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.154 - mean_q: 12.367 - prob: 1.000\n",
            "\n",
            "Interval 71955 (719540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.887 - mean_q: 13.448 - prob: 1.000\n",
            "\n",
            "Interval 71956 (719550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 71957 (719560 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: -1.000 [-14.000, 12.000] - loss: 0.000 - mae: 7.204 - mean_q: 12.518 - prob: 1.000\n",
            "\n",
            "Interval 71958 (719570 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 71959 (719580 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.6000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.110 - mean_q: 12.361 - prob: 1.000\n",
            "\n",
            "Interval 71960 (719590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 71961 (719600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.000 - mae: 7.557 - mean_q: 13.115 - prob: 1.000\n",
            "\n",
            "Interval 71962 (719610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.570 - mean_q: 13.013 - prob: 1.000\n",
            "\n",
            "Interval 71963 (719620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 71964 (719630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.000 - mae: 7.274 - mean_q: 12.610 - prob: 1.000\n",
            "\n",
            "Interval 71965 (719640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.447 - mean_q: 12.747 - prob: 1.000\n",
            "\n",
            "Interval 71966 (719650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 71967 (719660 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.204 - mean_q: 12.410 - prob: 1.000\n",
            "\n",
            "Interval 71968 (719670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.134 - mean_q: 12.440 - prob: 1.000\n",
            "\n",
            "Interval 71969 (719680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 71970 (719690 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 71971 (719700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.000 - mae: 7.417 - mean_q: 12.702 - prob: 1.000\n",
            "\n",
            "Interval 71972 (719710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.225 - mean_q: 12.571 - prob: 1.000\n",
            "\n",
            "Interval 71973 (719720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 71974 (719730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.000 - mae: 7.236 - mean_q: 12.510 - prob: 1.000\n",
            "\n",
            "Interval 71975 (719740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.494 - mean_q: 12.837 - prob: 1.000\n",
            "\n",
            "Interval 71976 (719750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.379 - mean_q: 12.705 - prob: 1.000\n",
            "\n",
            "Interval 71977 (719760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 71978 (719770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.149 - mean_q: 12.412 - prob: 1.000\n",
            "\n",
            "Interval 71979 (719780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.515 - mean_q: 12.868 - prob: 1.000\n",
            "\n",
            "Interval 71980 (719790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.000 - mae: 7.287 - mean_q: 12.579 - prob: 1.000\n",
            "\n",
            "Interval 71981 (719800 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -4.6000\n",
            "Interval 71982 (719810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -30.000 [-30.000, -30.000] - loss: 0.000 - mae: 7.763 - mean_q: 13.412 - prob: 1.000\n",
            "\n",
            "Interval 71983 (719820 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 71984 (719830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.000 - mae: 7.332 - mean_q: 12.663 - prob: 1.000\n",
            "\n",
            "Interval 71985 (719840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.771 - mean_q: 13.295 - prob: 1.000\n",
            "\n",
            "Interval 71986 (719850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.190 - mean_q: 12.440 - prob: 1.000\n",
            "\n",
            "Interval 71987 (719860 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 71988 (719870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.000 - mae: 7.586 - mean_q: 12.913 - prob: 1.000\n",
            "\n",
            "Interval 71989 (719880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.000 - mae: 7.481 - mean_q: 12.934 - prob: 1.000\n",
            "\n",
            "Interval 71990 (719890 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 71991 (719900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.000 - mae: 7.276 - mean_q: 12.600 - prob: 1.000\n",
            "\n",
            "Interval 71992 (719910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.288 - mean_q: 12.622 - prob: 1.000\n",
            "\n",
            "Interval 71993 (719920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 71994 (719930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.000 - mae: 7.332 - mean_q: 12.698 - prob: 1.000\n",
            "\n",
            "Interval 71995 (719940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.415 - mean_q: 12.654 - prob: 1.000\n",
            "\n",
            "Interval 71996 (719950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.296 - mean_q: 12.621 - prob: 1.000\n",
            "\n",
            "Interval 71997 (719960 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 71998 (719970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.606 - mean_q: 13.075 - prob: 1.000\n",
            "\n",
            "Interval 71999 (719980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.338 - mean_q: 12.687 - prob: 1.000\n",
            "\n",
            "Interval 72000 (719990 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 72001 (720000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.000 - mae: 7.506 - mean_q: 12.984 - prob: 1.000\n",
            "\n",
            "Interval 72002 (720010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.255 - mean_q: 12.477 - prob: 1.000\n",
            "\n",
            "Interval 72003 (720020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 72004 (720030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.6000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.358 - mean_q: 12.578 - prob: 1.000\n",
            "\n",
            "Interval 72005 (720040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 72006 (720050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.000 - mae: 7.513 - mean_q: 12.885 - prob: 1.000\n",
            "\n",
            "Interval 72007 (720060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.360 - mean_q: 12.772 - prob: 1.000\n",
            "\n",
            "Interval 72008 (720070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 72009 (720080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.437 - mean_q: 12.881 - prob: 1.000\n",
            "\n",
            "Interval 72010 (720090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.083 - mean_q: 12.218 - prob: 1.000\n",
            "\n",
            "Interval 72011 (720100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.365 - mean_q: 12.565 - prob: 1.000\n",
            "\n",
            "Interval 72012 (720110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.448 - mean_q: 12.885 - prob: 1.000\n",
            "\n",
            "Interval 72013 (720120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.000 - mae: 7.536 - mean_q: 13.007 - prob: 1.000\n",
            "\n",
            "Interval 72014 (720130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 72015 (720140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.172 - mean_q: 12.507 - prob: 1.000\n",
            "\n",
            "Interval 72016 (720150 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.262 - mean_q: 12.545 - prob: 1.000\n",
            "\n",
            "Interval 72017 (720160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 72018 (720170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.453 - mean_q: 12.885 - prob: 1.000\n",
            "\n",
            "Interval 72019 (720180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 72020 (720190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.249 - mean_q: 12.587 - prob: 1.000\n",
            "\n",
            "Interval 72021 (720200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 72022 (720210 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.315 - mean_q: 12.646 - prob: 1.000\n",
            "\n",
            "Interval 72023 (720220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.593 - mean_q: 13.062 - prob: 1.000\n",
            "\n",
            "Interval 72024 (720230 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 72025 (720240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.110 - mean_q: 12.426 - prob: 1.000\n",
            "\n",
            "Interval 72026 (720250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.698 - mean_q: 13.198 - prob: 1.000\n",
            "\n",
            "Interval 72027 (720260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.289 - mean_q: 12.462 - prob: 1.000\n",
            "\n",
            "Interval 72028 (720270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 72029 (720280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.001 - mae: 6.962 - mean_q: 12.174 - prob: 1.000\n",
            "\n",
            "Interval 72030 (720290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.492 - mean_q: 12.894 - prob: 1.000\n",
            "\n",
            "Interval 72031 (720300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 72032 (720310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.001 - mae: 7.615 - mean_q: 12.951 - prob: 1.000\n",
            "\n",
            "Interval 72033 (720320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 72034 (720330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.000 - mae: 7.171 - mean_q: 12.240 - prob: 1.000\n",
            "\n",
            "Interval 72035 (720340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.625 - mean_q: 13.159 - prob: 1.000\n",
            "\n",
            "Interval 72036 (720350 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 72037 (720360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.490 - mean_q: 12.808 - prob: 1.000\n",
            "\n",
            "Interval 72038 (720370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 72039 (720380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.571 - mean_q: 13.048 - prob: 1.000\n",
            "\n",
            "Interval 72040 (720390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.229 - mean_q: 12.603 - prob: 1.000\n",
            "\n",
            "Interval 72041 (720400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.550 - mean_q: 13.091 - prob: 1.000\n",
            "\n",
            "Interval 72042 (720410 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 72043 (720420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.561 - mean_q: 12.987 - prob: 1.000\n",
            "\n",
            "Interval 72044 (720430 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 72045 (720440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.000 - mae: 7.063 - mean_q: 12.391 - prob: 1.000\n",
            "\n",
            "Interval 72046 (720450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.379 - mean_q: 12.794 - prob: 1.000\n",
            "\n",
            "Interval 72047 (720460 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 72048 (720470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.440 - mean_q: 12.799 - prob: 1.000\n",
            "\n",
            "Interval 72049 (720480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.000 - mae: 7.261 - mean_q: 12.739 - prob: 1.000\n",
            "\n",
            "Interval 72050 (720490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 72051 (720500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.565 - mean_q: 12.966 - prob: 1.000\n",
            "\n",
            "Interval 72052 (720510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.463 - mean_q: 12.930 - prob: 1.000\n",
            "\n",
            "Interval 72053 (720520 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 72054 (720530 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.205 - mean_q: 12.394 - prob: 1.000\n",
            "\n",
            "Interval 79552 (795510 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.259 - mean_q: 12.754 - prob: 1.000\n",
            "\n",
            "Interval 79553 (795520 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 79554 (795530 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.000 - mae: 7.731 - mean_q: 13.365 - prob: 1.000\n",
            "\n",
            "Interval 79555 (795540 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 79556 (795550 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.000 - mae: 7.252 - mean_q: 12.574 - prob: 1.000\n",
            "\n",
            "Interval 79557 (795560 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.435 - mean_q: 12.983 - prob: 1.000\n",
            "\n",
            "Interval 79558 (795570 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.000 - mae: 7.350 - mean_q: 12.634 - prob: 1.000\n",
            "\n",
            "Interval 79559 (795580 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 79560 (795590 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 9.000 [6.000, 12.000] - loss: 0.001 - mae: 7.322 - mean_q: 12.711 - prob: 1.000\n",
            "\n",
            "Interval 79561 (795600 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -2.8000\n",
            "Interval 79562 (795610 steps performed)\n",
            "10/10 [==============================] - 0s 14ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.000 - mae: 7.405 - mean_q: 12.782 - prob: 1.000\n",
            "\n",
            "Interval 79563 (795620 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.531 - mean_q: 12.901 - prob: 1.000\n",
            "\n",
            "Interval 79564 (795630 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: -1.0000\n",
            "Interval 79565 (795640 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.112 - mean_q: 12.415 - prob: 1.000\n",
            "\n",
            "Interval 79566 (795650 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.000 - mae: 7.374 - mean_q: 12.892 - prob: 1.000\n",
            "\n",
            "Interval 79567 (795660 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: -1.0000\n",
            "Interval 79568 (795670 steps performed)\n",
            "10/10 [==============================] - 0s 16ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.534 - mean_q: 12.973 - prob: 1.000\n",
            "\n",
            "Interval 79569 (795680 steps performed)\n",
            "10/10 [==============================] - 0s 14ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.000 - mae: 7.080 - mean_q: 12.171 - prob: 1.000\n",
            "\n",
            "Interval 79570 (795690 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.264 - mean_q: 12.514 - prob: 1.000\n",
            "\n",
            "Interval 79571 (795700 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.474 - mean_q: 12.882 - prob: 1.000\n",
            "\n",
            "Interval 79572 (795710 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 79573 (795720 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.464 - mean_q: 12.656 - prob: 1.000\n",
            "\n",
            "Interval 79574 (795730 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.9000\n",
            "Interval 79575 (795740 steps performed)\n",
            "10/10 [==============================] - 0s 15ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -22.000 [-22.000, -22.000] - loss: 0.001 - mae: 7.232 - mean_q: 12.517 - prob: 1.000\n",
            "\n",
            "Interval 79576 (795750 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.260 - mean_q: 12.583 - prob: 1.000\n",
            "\n",
            "Interval 79577 (795760 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: -1.0000\n",
            "Interval 79578 (795770 steps performed)\n",
            "10/10 [==============================] - 0s 15ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.121 - mean_q: 12.307 - prob: 1.000\n",
            "\n",
            "Interval 79579 (795780 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.306 - mean_q: 12.619 - prob: 1.000\n",
            "\n",
            "Interval 79580 (795790 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.000 - mae: 7.125 - mean_q: 12.391 - prob: 1.000\n",
            "\n",
            "Interval 79581 (795800 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.602 - mean_q: 13.042 - prob: 1.000\n",
            "\n",
            "Interval 79582 (795810 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.126 - mean_q: 12.407 - prob: 1.000\n",
            "\n",
            "Interval 79583 (795820 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.9000\n",
            "Interval 79584 (795830 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.000 - mae: 7.918 - mean_q: 13.388 - prob: 1.000\n",
            "\n",
            "Interval 79585 (795840 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 79586 (795850 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.130 - mean_q: 12.426 - prob: 1.000\n",
            "\n",
            "Interval 79587 (795860 steps performed)\n",
            "10/10 [==============================] - 0s 15ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.000 - mae: 7.455 - mean_q: 12.819 - prob: 1.000\n",
            "\n",
            "Interval 79588 (795870 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.435 - mean_q: 12.820 - prob: 1.000\n",
            "\n",
            "Interval 79589 (795880 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: -2.8000\n",
            "Interval 79590 (795890 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -19.000 [-19.000, -19.000] - loss: 0.000 - mae: 7.110 - mean_q: 12.417 - prob: 1.000\n",
            "\n",
            "Interval 79591 (795900 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.9000\n",
            "Interval 79592 (795910 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.195 - mean_q: 12.412 - prob: 1.000\n",
            "\n",
            "Interval 79593 (795920 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.199 - mean_q: 12.483 - prob: 1.000\n",
            "\n",
            "Interval 79594 (795930 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 79595 (795940 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.000 - mae: 7.328 - mean_q: 12.783 - prob: 1.000\n",
            "\n",
            "Interval 79596 (795950 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.9000\n",
            "Interval 79597 (795960 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.000 - mae: 7.267 - mean_q: 12.538 - prob: 1.000\n",
            "\n",
            "Interval 79598 (795970 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 79599 (795980 steps performed)\n",
            "10/10 [==============================] - 0s 17ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.505 - mean_q: 12.918 - prob: 1.000\n",
            "\n",
            "Interval 79600 (795990 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.9000\n",
            "Interval 79601 (796000 steps performed)\n",
            "10/10 [==============================] - 0s 14ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.922 - mean_q: 13.441 - prob: 1.000\n",
            "\n",
            "Interval 79602 (796010 steps performed)\n",
            "10/10 [==============================] - 0s 14ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.000 - mae: 7.272 - mean_q: 12.657 - prob: 1.000\n",
            "\n",
            "Interval 79603 (796020 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.648 - mean_q: 13.141 - prob: 1.000\n",
            "\n",
            "Interval 79604 (796030 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 79605 (796040 steps performed)\n",
            "10/10 [==============================] - 0s 14ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.327 - mean_q: 12.656 - prob: 1.000\n",
            "\n",
            "Interval 79606 (796050 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: -1.0000\n",
            "Interval 79607 (796060 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.538 - mean_q: 13.123 - prob: 1.000\n",
            "\n",
            "Interval 79608 (796070 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.570 - mean_q: 13.046 - prob: 1.000\n",
            "\n",
            "Interval 79609 (796080 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -2.8000\n",
            "Interval 79610 (796090 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.001 - mae: 7.242 - mean_q: 12.601 - prob: 1.000\n",
            "\n",
            "Interval 79611 (796100 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.607 - mean_q: 12.967 - prob: 1.000\n",
            "\n",
            "Interval 79612 (796110 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 79613 (796120 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.000 - mae: 7.104 - mean_q: 12.394 - prob: 1.000\n",
            "\n",
            "Interval 79614 (796130 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 79615 (796140 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.441 - mean_q: 12.896 - prob: 1.000\n",
            "\n",
            "Interval 79616 (796150 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.002 - mae: 7.545 - mean_q: 12.948 - prob: 1.000\n",
            "\n",
            "Interval 79617 (796160 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 79618 (796170 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.476 - mean_q: 12.988 - prob: 1.000\n",
            "\n",
            "Interval 79619 (796180 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 79620 (796190 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -17.000 [-17.000, -17.000] - loss: 0.001 - mae: 7.282 - mean_q: 12.686 - prob: 1.000\n",
            "\n",
            "Interval 79621 (796200 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 79622 (796210 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.001 - mae: 7.088 - mean_q: 12.273 - prob: 1.000\n",
            "\n",
            "Interval 79623 (796220 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.445 - mean_q: 12.960 - prob: 1.000\n",
            "\n",
            "Interval 79624 (796230 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 79625 (796240 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.349 - mean_q: 12.761 - prob: 1.000\n",
            "\n",
            "Interval 79626 (796250 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.315 - mean_q: 12.745 - prob: 1.000\n",
            "\n",
            "Interval 79627 (796260 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.412 - mean_q: 12.764 - prob: 1.000\n",
            "\n",
            "Interval 79628 (796270 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 79629 (796280 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.451 - mean_q: 12.794 - prob: 1.000\n",
            "\n",
            "Interval 79630 (796290 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.188 - mean_q: 12.557 - prob: 1.000\n",
            "\n",
            "Interval 79631 (796300 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 79632 (796310 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.235 - mean_q: 12.601 - prob: 1.000\n",
            "\n",
            "Interval 79633 (796320 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.9000\n",
            "Interval 79634 (796330 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.000 - mae: 7.375 - mean_q: 12.753 - prob: 1.000\n",
            "\n",
            "Interval 79635 (796340 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 79636 (796350 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.002 - mae: 7.260 - mean_q: 12.536 - prob: 1.000\n",
            "\n",
            "Interval 79637 (796360 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 79638 (796370 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.000 - mae: 7.465 - mean_q: 12.785 - prob: 1.000\n",
            "\n",
            "Interval 79639 (796380 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.521 - mean_q: 13.124 - prob: 1.000\n",
            "\n",
            "Interval 79640 (796390 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 79641 (796400 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.319 - mean_q: 12.662 - prob: 1.000\n",
            "\n",
            "Interval 79642 (796410 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.117 - mean_q: 12.388 - prob: 1.000\n",
            "\n",
            "Interval 79643 (796420 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 79644 (796430 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.000 - mae: 7.387 - mean_q: 12.804 - prob: 1.000\n",
            "\n",
            "Interval 79645 (796440 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.338 - mean_q: 12.598 - prob: 1.000\n",
            "\n",
            "Interval 79646 (796450 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.492 - mean_q: 12.667 - prob: 1.000\n",
            "\n",
            "Interval 79647 (796460 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -2.8000\n",
            "Interval 79648 (796470 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 79649 (796480 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -22.000 [-22.000, -22.000] - loss: 0.000 - mae: 7.118 - mean_q: 12.296 - prob: 1.000\n",
            "\n",
            "Interval 79650 (796490 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.300 - mean_q: 12.600 - prob: 1.000\n",
            "\n",
            "Interval 79651 (796500 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.000 - mae: 7.423 - mean_q: 12.769 - prob: 1.000\n",
            "\n",
            "Interval 79652 (796510 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.334 - mean_q: 12.695 - prob: 1.000\n",
            "\n",
            "Interval 79653 (796520 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 79654 (796530 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.331 - mean_q: 12.514 - prob: 1.000\n",
            "\n",
            "Interval 79655 (796540 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.332 - mean_q: 12.742 - prob: 1.000\n",
            "\n",
            "Interval 79656 (796550 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 79657 (796560 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.000 - mae: 7.332 - mean_q: 12.752 - prob: 1.000\n",
            "\n",
            "Interval 79658 (796570 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.488 - mean_q: 12.980 - prob: 1.000\n",
            "\n",
            "Interval 79659 (796580 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.474 - mean_q: 12.839 - prob: 1.000\n",
            "\n",
            "Interval 79660 (796590 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.585 - mean_q: 13.060 - prob: 1.000\n",
            "\n",
            "Interval 79661 (796600 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 79662 (796610 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.000 - mae: 7.296 - mean_q: 12.728 - prob: 1.000\n",
            "\n",
            "Interval 79663 (796620 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 79664 (796630 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 5.500 [-3.000, 14.000] - loss: 0.000 - mae: 7.427 - mean_q: 12.981 - prob: 1.000\n",
            "\n",
            "Interval 79665 (796640 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 79666 (796650 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.147 - mean_q: 12.495 - prob: 1.000\n",
            "\n",
            "Interval 79667 (796660 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: -1.0000\n",
            "Interval 79668 (796670 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -17.000 [-17.000, -17.000] - loss: 0.000 - mae: 7.314 - mean_q: 12.658 - prob: 1.000\n",
            "\n",
            "Interval 79669 (796680 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.9000\n",
            "Interval 79670 (796690 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.000 - mae: 7.112 - mean_q: 12.414 - prob: 1.000\n",
            "\n",
            "Interval 79671 (796700 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.792 - mean_q: 13.292 - prob: 1.000\n",
            "\n",
            "Interval 79672 (796710 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.567 - mean_q: 13.024 - prob: 1.000\n",
            "\n",
            "Interval 79673 (796720 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.370 - mean_q: 12.720 - prob: 1.000\n",
            "\n",
            "Interval 79674 (796730 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.9000\n",
            "Interval 79675 (796740 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.000 - mae: 7.097 - mean_q: 12.512 - prob: 1.000\n",
            "\n",
            "Interval 79676 (796750 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 79677 (796760 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.084 - mean_q: 12.346 - prob: 1.000\n",
            "\n",
            "Interval 79678 (796770 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.197 - mean_q: 12.433 - prob: 1.000\n",
            "\n",
            "Interval 79679 (796780 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 79680 (796790 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 79681 (796800 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.379 - mean_q: 12.729 - prob: 1.000\n",
            "\n",
            "Interval 79682 (796810 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 79683 (796820 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.000 - mae: 7.285 - mean_q: 12.710 - prob: 1.000\n",
            "\n",
            "Interval 79684 (796830 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.000 - mae: 7.576 - mean_q: 13.112 - prob: 1.000\n",
            "\n",
            "Interval 79685 (796840 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 79686 (796850 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.112 - mean_q: 12.537 - prob: 1.000\n",
            "\n",
            "Interval 79687 (796860 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 79688 (796870 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.375 - mean_q: 12.805 - prob: 1.000\n",
            "\n",
            "Interval 79689 (796880 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 79690 (796890 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.000 - mae: 7.134 - mean_q: 12.526 - prob: 1.000\n",
            "\n",
            "Interval 79691 (796900 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.340 - mean_q: 12.735 - prob: 1.000\n",
            "\n",
            "Interval 79692 (796910 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 79693 (796920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.311 - mean_q: 12.667 - prob: 1.000\n",
            "\n",
            "Interval 86997 (869960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 86998 (869970 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.485 - mean_q: 12.814 - prob: 1.000\n",
            "\n",
            "Interval 86999 (869980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 87000 (869990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.280 - mean_q: 12.517 - prob: 1.000\n",
            "\n",
            "Interval 87001 (870000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.000 - mae: 7.359 - mean_q: 12.792 - prob: 1.000\n",
            "\n",
            "Interval 87002 (870010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 87003 (870020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.239 - mean_q: 12.531 - prob: 1.000\n",
            "\n",
            "Interval 87004 (870030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 87005 (870040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.432 - mean_q: 12.768 - prob: 1.000\n",
            "\n",
            "Interval 87006 (870050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.261 - mean_q: 12.584 - prob: 1.000\n",
            "\n",
            "Interval 87007 (870060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 87008 (870070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.000 - mae: 7.323 - mean_q: 12.588 - prob: 1.000\n",
            "\n",
            "Interval 87009 (870080 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 87010 (870090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.000 - mae: 7.321 - mean_q: 12.514 - prob: 1.000\n",
            "\n",
            "Interval 87011 (870100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.230 - mean_q: 12.670 - prob: 1.000\n",
            "\n",
            "Interval 87012 (870110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 87013 (870120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.446 - mean_q: 12.856 - prob: 1.000\n",
            "\n",
            "Interval 87014 (870130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.175 - mean_q: 12.319 - prob: 1.000\n",
            "\n",
            "Interval 87015 (870140 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.266 - mean_q: 12.643 - prob: 1.000\n",
            "\n",
            "Interval 87016 (870150 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 87017 (870160 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.416 - mean_q: 12.850 - prob: 1.000\n",
            "\n",
            "Interval 87018 (870170 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.000 - mae: 7.217 - mean_q: 12.446 - prob: 1.000\n",
            "\n",
            "Interval 87019 (870180 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 87020 (870190 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.000 - mae: 7.402 - mean_q: 12.779 - prob: 1.000\n",
            "\n",
            "Interval 87021 (870200 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.203 - mean_q: 12.574 - prob: 1.000\n",
            "\n",
            "Interval 87022 (870210 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 87023 (870220 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.701 - mean_q: 13.149 - prob: 1.000\n",
            "\n",
            "Interval 87024 (870230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 87025 (870240 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.000 - mae: 7.232 - mean_q: 12.642 - prob: 1.000\n",
            "\n",
            "Interval 87026 (870250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.000 - mae: 7.462 - mean_q: 12.766 - prob: 1.000\n",
            "\n",
            "Interval 87027 (870260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 87028 (870270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.367 - mean_q: 12.698 - prob: 1.000\n",
            "\n",
            "Interval 87029 (870280 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.078 - mean_q: 12.356 - prob: 1.000\n",
            "\n",
            "Interval 87030 (870290 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 87031 (870300 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.185 - mean_q: 12.328 - prob: 1.000\n",
            "\n",
            "Interval 87032 (870310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 87033 (870320 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.201 - mean_q: 12.580 - prob: 1.000\n",
            "\n",
            "Interval 87034 (870330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.281 - mean_q: 12.551 - prob: 1.000\n",
            "\n",
            "Interval 87035 (870340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 87036 (870350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 2.3000\n",
            "2 episodes - episode_reward: 5.500 [3.000, 8.000] - loss: 0.000 - mae: 7.283 - mean_q: 12.684 - prob: 1.000\n",
            "\n",
            "Interval 87037 (870360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 87038 (870370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.222 - mean_q: 12.489 - prob: 1.000\n",
            "\n",
            "Interval 87039 (870380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.657 - mean_q: 13.055 - prob: 1.000\n",
            "\n",
            "Interval 87040 (870390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.342 - mean_q: 12.678 - prob: 1.000\n",
            "\n",
            "Interval 87041 (870400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.242 - mean_q: 12.568 - prob: 1.000\n",
            "\n",
            "Interval 87042 (870410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 87043 (870420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.178 - mean_q: 12.496 - prob: 1.000\n",
            "\n",
            "Interval 87044 (870430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.325 - mean_q: 12.783 - prob: 1.000\n",
            "\n",
            "Interval 87045 (870440 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 87046 (870450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -26.000 [-26.000, -26.000] - loss: 0.001 - mae: 7.259 - mean_q: 12.618 - prob: 1.000\n",
            "\n",
            "Interval 87047 (870460 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 87048 (870470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.492 - mean_q: 12.921 - prob: 1.000\n",
            "\n",
            "Interval 87049 (870480 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.002 - mae: 7.565 - mean_q: 13.057 - prob: 1.000\n",
            "\n",
            "Interval 87050 (870490 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.476 - mean_q: 12.869 - prob: 1.000\n",
            "\n",
            "Interval 87051 (870500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 6.989 - mean_q: 12.334 - prob: 1.000\n",
            "\n",
            "Interval 87052 (870510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 87053 (870520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.620 - mean_q: 13.031 - prob: 1.000\n",
            "\n",
            "Interval 87054 (870530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.611 - mean_q: 13.065 - prob: 1.000\n",
            "\n",
            "Interval 87055 (870540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.478 - mean_q: 12.908 - prob: 1.000\n",
            "\n",
            "Interval 87056 (870550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 87057 (870560 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.548 - mean_q: 12.973 - prob: 1.000\n",
            "\n",
            "Interval 87058 (870570 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.422 - mean_q: 12.763 - prob: 1.000\n",
            "\n",
            "Interval 87059 (870580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 87060 (870590 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.792 - mean_q: 13.311 - prob: 1.000\n",
            "\n",
            "Interval 87061 (870600 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.434 - mean_q: 12.648 - prob: 1.000\n",
            "\n",
            "Interval 87062 (870610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 87063 (870620 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.395 - mean_q: 12.731 - prob: 1.000\n",
            "\n",
            "Interval 87064 (870630 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.182 - mean_q: 12.537 - prob: 1.000\n",
            "\n",
            "Interval 87065 (870640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 87066 (870650 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.241 - mean_q: 12.514 - prob: 1.000\n",
            "\n",
            "Interval 87067 (870660 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.418 - mean_q: 12.681 - prob: 1.000\n",
            "\n",
            "Interval 87068 (870670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 87069 (870680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 6.995 - mean_q: 12.376 - prob: 1.000\n",
            "\n",
            "Interval 87070 (870690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.524 - mean_q: 12.869 - prob: 1.000\n",
            "\n",
            "Interval 87071 (870700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.556 - mean_q: 12.882 - prob: 1.000\n",
            "\n",
            "Interval 87072 (870710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 87073 (870720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.600 - mean_q: 13.069 - prob: 1.000\n",
            "\n",
            "Interval 87074 (870730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 87075 (870740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.271 - mean_q: 12.628 - prob: 1.000\n",
            "\n",
            "Interval 87076 (870750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 87077 (870760 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.067 - mean_q: 12.205 - prob: 1.000\n",
            "\n",
            "Interval 87078 (870770 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.341 - mean_q: 12.662 - prob: 1.000\n",
            "\n",
            "Interval 87079 (870780 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 87080 (870790 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.000 - mae: 7.540 - mean_q: 12.893 - prob: 1.000\n",
            "\n",
            "Interval 87081 (870800 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.000 - mae: 7.620 - mean_q: 13.133 - prob: 1.000\n",
            "\n",
            "Interval 87082 (870810 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 87083 (870820 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.513 - mean_q: 13.039 - prob: 1.000\n",
            "\n",
            "Interval 87084 (870830 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.307 - mean_q: 12.757 - prob: 1.000\n",
            "\n",
            "Interval 87085 (870840 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 87086 (870850 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.186 - mean_q: 12.529 - prob: 1.000\n",
            "\n",
            "Interval 87087 (870860 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.254 - mean_q: 12.511 - prob: 1.000\n",
            "\n",
            "Interval 87088 (870870 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 87089 (870880 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.527 - mean_q: 12.802 - prob: 1.000\n",
            "\n",
            "Interval 87090 (870890 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.649 - mean_q: 13.096 - prob: 1.000\n",
            "\n",
            "Interval 87091 (870900 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.079 - mean_q: 12.108 - prob: 1.000\n",
            "\n",
            "Interval 87092 (870910 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.535 - mean_q: 12.874 - prob: 1.000\n",
            "\n",
            "Interval 87093 (870920 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 87094 (870930 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.093 - mean_q: 12.434 - prob: 1.000\n",
            "\n",
            "Interval 87095 (870940 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.411 - mean_q: 12.831 - prob: 1.000\n",
            "\n",
            "Interval 87096 (870950 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 87097 (870960 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.416 - mean_q: 12.862 - prob: 1.000\n",
            "\n",
            "Interval 87098 (870970 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.000 - mae: 6.945 - mean_q: 12.091 - prob: 1.000\n",
            "\n",
            "Interval 87099 (870980 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.259 - mean_q: 12.579 - prob: 1.000\n",
            "\n",
            "Interval 87100 (870990 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 87101 (871000 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.261 - mean_q: 12.511 - prob: 1.000\n",
            "\n",
            "Interval 87102 (871010 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.582 - mean_q: 13.000 - prob: 1.000\n",
            "\n",
            "Interval 87103 (871020 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.227 - mean_q: 12.462 - prob: 1.000\n",
            "\n",
            "Interval 87104 (871030 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 87105 (871040 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.000 - mae: 7.190 - mean_q: 12.492 - prob: 1.000\n",
            "\n",
            "Interval 87106 (871050 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.380 - mean_q: 12.739 - prob: 1.000\n",
            "\n",
            "Interval 87107 (871060 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 87108 (871070 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.575 - mean_q: 12.941 - prob: 1.000\n",
            "\n",
            "Interval 87109 (871080 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 87110 (871090 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -31.000 [-31.000, -31.000] - loss: 0.000 - mae: 7.500 - mean_q: 12.989 - prob: 1.000\n",
            "\n",
            "Interval 87111 (871100 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 87112 (871110 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.569 - mean_q: 13.017 - prob: 1.000\n",
            "\n",
            "Interval 87113 (871120 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.000 - mae: 7.328 - mean_q: 12.534 - prob: 1.000\n",
            "\n",
            "Interval 87114 (871130 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 87115 (871140 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.246 - mean_q: 12.573 - prob: 1.000\n",
            "\n",
            "Interval 87116 (871150 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 6.948 - mean_q: 12.151 - prob: 1.000\n",
            "\n",
            "Interval 87117 (871160 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.405 - mean_q: 12.675 - prob: 1.000\n",
            "\n",
            "Interval 87118 (871170 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 87119 (871180 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.424 - mean_q: 12.729 - prob: 1.000\n",
            "\n",
            "Interval 87120 (871190 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.746 - mean_q: 13.250 - prob: 1.000\n",
            "\n",
            "Interval 87121 (871200 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 87122 (871210 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.249 - mean_q: 12.671 - prob: 1.000\n",
            "\n",
            "Interval 87123 (871220 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.045 - mean_q: 12.458 - prob: 1.000\n",
            "\n",
            "Interval 87124 (871230 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.277 - mean_q: 12.549 - prob: 1.000\n",
            "\n",
            "Interval 87125 (871240 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.343 - mean_q: 12.615 - prob: 1.000\n",
            "\n",
            "Interval 87126 (871250 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 87127 (871260 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.472 - mean_q: 12.989 - prob: 1.000\n",
            "\n",
            "Interval 87128 (871270 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 87129 (871280 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.433 - mean_q: 12.813 - prob: 1.000\n",
            "\n",
            "Interval 87130 (871290 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.000 - mae: 7.373 - mean_q: 12.712 - prob: 1.000\n",
            "\n",
            "Interval 87131 (871300 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 87132 (871310 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.133 - mean_q: 12.392 - prob: 1.000\n",
            "\n",
            "Interval 87133 (871320 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.208 - mean_q: 12.401 - prob: 1.000\n",
            "\n",
            "Interval 87134 (871330 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 87135 (871340 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -24.000 [-24.000, -24.000] - loss: 0.000 - mae: 7.473 - mean_q: 12.935 - prob: 1.000\n",
            "\n",
            "Interval 87136 (871350 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 87137 (871360 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.248 - mean_q: 12.532 - prob: 1.000\n",
            "\n",
            "Interval 87138 (871370 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.432 - mean_q: 12.777 - prob: 1.000\n",
            "\n",
            "Interval 87139 (871380 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.248 - mean_q: 12.660 - prob: 1.000\n",
            "\n",
            "Interval 87140 (871390 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 87141 (871400 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.000 - mae: 7.336 - mean_q: 12.692 - prob: 1.000\n",
            "\n",
            "Interval 87142 (871410 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 87143 (871420 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.000 - mae: 7.166 - mean_q: 12.502 - prob: 1.000\n",
            "\n",
            "Interval 87144 (871430 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.194 - mean_q: 12.375 - prob: 1.000\n",
            "\n",
            "Interval 87145 (871440 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 87146 (871450 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 11.000 [8.000, 14.000] - loss: 0.000 - mae: 7.090 - mean_q: 12.270 - prob: 1.000\n",
            "\n",
            "Interval 87147 (871460 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 87148 (871470 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.385 - mean_q: 12.868 - prob: 1.000\n",
            "\n",
            "Interval 87149 (871480 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 87150 (871490 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.644 - mean_q: 13.137 - prob: 1.000\n",
            "\n",
            "Interval 87151 (871500 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.245 - mean_q: 12.569 - prob: 1.000\n",
            "\n",
            "Interval 87152 (871510 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 87153 (871520 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 10.000 [8.000, 12.000] - loss: 0.001 - mae: 7.378 - mean_q: 12.797 - prob: 1.000\n",
            "\n",
            "Interval 87154 (871530 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.9000\n",
            "Interval 87155 (871540 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.000 - mae: 7.451 - mean_q: 12.831 - prob: 1.000\n",
            "\n",
            "Interval 87156 (871550 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 87157 (871560 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.342 - mean_q: 12.448 - prob: 1.000\n",
            "\n",
            "Interval 87158 (871570 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 87159 (871580 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.468 - mean_q: 12.841 - prob: 1.000\n",
            "\n",
            "Interval 87160 (871590 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 87161 (871600 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.565 - mean_q: 12.879 - prob: 1.000\n",
            "\n",
            "Interval 87162 (871610 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 6.972 - mean_q: 12.053 - prob: 1.000\n",
            "\n",
            "Interval 87163 (871620 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 87164 (871630 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.000 - mae: 7.458 - mean_q: 12.914 - prob: 1.000\n",
            "\n",
            "Interval 87165 (871640 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.495 - mean_q: 12.806 - prob: 1.000\n",
            "\n",
            "Interval 87166 (871650 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 87167 (871660 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.245 - mean_q: 12.594 - prob: 1.000\n",
            "\n",
            "Interval 87168 (871670 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.545 - mean_q: 12.900 - prob: 1.000\n",
            "\n",
            "Interval 87169 (871680 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 87170 (871690 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.208 - mean_q: 12.616 - prob: 1.000\n",
            "\n",
            "Interval 87171 (871700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.000 - mae: 7.895 - mean_q: 13.403 - prob: 1.000\n",
            "\n",
            "Interval 94378 (943770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94379 (943780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.000 - mae: 7.329 - mean_q: 12.679 - prob: 1.000\n",
            "\n",
            "Interval 94380 (943790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94381 (943800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.215 - mean_q: 12.516 - prob: 1.000\n",
            "\n",
            "Interval 94382 (943810 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 94383 (943820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.149 - mean_q: 12.373 - prob: 1.000\n",
            "\n",
            "Interval 94384 (943830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.240 - mean_q: 12.596 - prob: 1.000\n",
            "\n",
            "Interval 94385 (943840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.350 - mean_q: 12.597 - prob: 1.000\n",
            "\n",
            "Interval 94386 (943850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.000 - mae: 7.162 - mean_q: 12.447 - prob: 1.000\n",
            "\n",
            "Interval 94387 (943860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.387 - mean_q: 12.795 - prob: 1.000\n",
            "\n",
            "Interval 94388 (943870 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.171 - mean_q: 12.314 - prob: 1.000\n",
            "\n",
            "Interval 94389 (943880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94390 (943890 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.510 - mean_q: 12.825 - prob: 1.000\n",
            "\n",
            "Interval 94391 (943900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.288 - mean_q: 12.655 - prob: 1.000\n",
            "\n",
            "Interval 94392 (943910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.260 - mean_q: 12.638 - prob: 1.000\n",
            "\n",
            "Interval 94393 (943920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94394 (943930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.000 - mae: 7.299 - mean_q: 12.692 - prob: 1.000\n",
            "\n",
            "Interval 94395 (943940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.423 - mean_q: 12.871 - prob: 1.000\n",
            "\n",
            "Interval 94396 (943950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.135 - mean_q: 12.343 - prob: 1.000\n",
            "\n",
            "Interval 94397 (943960 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 94398 (943970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.128 - mean_q: 12.456 - prob: 1.000\n",
            "\n",
            "Interval 94399 (943980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.496 - mean_q: 12.957 - prob: 1.000\n",
            "\n",
            "Interval 94400 (943990 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.429 - mean_q: 12.787 - prob: 1.000\n",
            "\n",
            "Interval 94401 (944000 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.449 - mean_q: 12.706 - prob: 1.000\n",
            "\n",
            "Interval 94402 (944010 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 94403 (944020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 6.960 - mean_q: 12.248 - prob: 1.000\n",
            "\n",
            "Interval 94404 (944030 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 94405 (944040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.394 - mean_q: 12.716 - prob: 1.000\n",
            "\n",
            "Interval 94406 (944050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.222 - mean_q: 12.614 - prob: 1.000\n",
            "\n",
            "Interval 94407 (944060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 94408 (944070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.561 - mean_q: 12.944 - prob: 1.000\n",
            "\n",
            "Interval 94409 (944080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94410 (944090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.612 - mean_q: 13.162 - prob: 1.000\n",
            "\n",
            "Interval 94411 (944100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.146 - mean_q: 12.404 - prob: 1.000\n",
            "\n",
            "Interval 94412 (944110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.170 - mean_q: 12.593 - prob: 1.000\n",
            "\n",
            "Interval 94413 (944120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94414 (944130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.007 - mean_q: 12.148 - prob: 1.000\n",
            "\n",
            "Interval 94415 (944140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.321 - mean_q: 12.642 - prob: 1.000\n",
            "\n",
            "Interval 94416 (944150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94417 (944160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.104 - mean_q: 12.566 - prob: 1.000\n",
            "\n",
            "Interval 94418 (944170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 6.987 - mean_q: 12.193 - prob: 1.000\n",
            "\n",
            "Interval 94419 (944180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.001 - mae: 7.325 - mean_q: 12.663 - prob: 1.000\n",
            "\n",
            "Interval 94420 (944190 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 94421 (944200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.000 - mae: 7.361 - mean_q: 12.646 - prob: 1.000\n",
            "\n",
            "Interval 94422 (944210 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 94423 (944220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.107 - mean_q: 12.358 - prob: 1.000\n",
            "\n",
            "Interval 94424 (944230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.220 - mean_q: 12.435 - prob: 1.000\n",
            "\n",
            "Interval 94425 (944240 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.359 - mean_q: 12.726 - prob: 1.000\n",
            "\n",
            "Interval 94426 (944250 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.000 - mae: 7.322 - mean_q: 12.587 - prob: 1.000\n",
            "\n",
            "Interval 94427 (944260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94428 (944270 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.406 - mean_q: 12.681 - prob: 1.000\n",
            "\n",
            "Interval 94429 (944280 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.211 - mean_q: 12.476 - prob: 1.000\n",
            "\n",
            "Interval 94430 (944290 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.224 - mean_q: 12.404 - prob: 1.000\n",
            "\n",
            "Interval 94431 (944300 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.000 - mae: 7.521 - mean_q: 12.860 - prob: 1.000\n",
            "\n",
            "Interval 94432 (944310 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.290 - mean_q: 12.745 - prob: 1.000\n",
            "\n",
            "Interval 94433 (944320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 94434 (944330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.000 - mae: 7.581 - mean_q: 13.119 - prob: 1.000\n",
            "\n",
            "Interval 94435 (944340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94436 (944350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.285 - mean_q: 12.724 - prob: 1.000\n",
            "\n",
            "Interval 94437 (944360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.171 - mean_q: 12.465 - prob: 1.000\n",
            "\n",
            "Interval 94438 (944370 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 94439 (944380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.424 - mean_q: 12.837 - prob: 1.000\n",
            "\n",
            "Interval 94440 (944390 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -2.8000\n",
            "Interval 94441 (944400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.001 - mae: 7.245 - mean_q: 12.589 - prob: 1.000\n",
            "\n",
            "Interval 94442 (944410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.083 - mean_q: 12.220 - prob: 1.000\n",
            "\n",
            "Interval 94443 (944420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94444 (944430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.580 - mean_q: 13.061 - prob: 1.000\n",
            "\n",
            "Interval 94445 (944440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 94446 (944450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.324 - mean_q: 12.667 - prob: 1.000\n",
            "\n",
            "Interval 94447 (944460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.340 - mean_q: 12.765 - prob: 1.000\n",
            "\n",
            "Interval 94448 (944470 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.000 - mae: 7.359 - mean_q: 12.742 - prob: 1.000\n",
            "\n",
            "Interval 94449 (944480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 94450 (944490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.000 - mae: 7.349 - mean_q: 12.836 - prob: 1.000\n",
            "\n",
            "Interval 94451 (944500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.387 - mean_q: 12.707 - prob: 1.000\n",
            "\n",
            "Interval 94452 (944510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94453 (944520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.347 - mean_q: 12.740 - prob: 1.000\n",
            "\n",
            "Interval 94454 (944530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94455 (944540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.000 - mae: 7.155 - mean_q: 12.380 - prob: 1.000\n",
            "\n",
            "Interval 94456 (944550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.138 - mean_q: 12.529 - prob: 1.000\n",
            "\n",
            "Interval 94457 (944560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.171 - mean_q: 12.297 - prob: 1.000\n",
            "\n",
            "Interval 94458 (944570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94459 (944580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.002 - mae: 7.433 - mean_q: 12.825 - prob: 1.000\n",
            "\n",
            "Interval 94460 (944590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.395 - mean_q: 12.604 - prob: 1.000\n",
            "\n",
            "Interval 94461 (944600 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 94462 (944610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.684 - mean_q: 13.192 - prob: 1.000\n",
            "\n",
            "Interval 94463 (944620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.488 - mean_q: 12.913 - prob: 1.000\n",
            "\n",
            "Interval 94464 (944630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94465 (944640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.389 - mean_q: 12.748 - prob: 1.000\n",
            "\n",
            "Interval 94466 (944650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94467 (944660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.001 - mae: 7.321 - mean_q: 12.670 - prob: 1.000\n",
            "\n",
            "Interval 94468 (944670 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.517 - mean_q: 12.925 - prob: 1.000\n",
            "\n",
            "Interval 94469 (944680 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 94470 (944690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.529 - mean_q: 12.887 - prob: 1.000\n",
            "\n",
            "Interval 94471 (944700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.624 - mean_q: 13.173 - prob: 1.000\n",
            "\n",
            "Interval 94472 (944710 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.296 - mean_q: 12.549 - prob: 1.000\n",
            "\n",
            "Interval 94473 (944720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 94474 (944730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.465 - mean_q: 12.749 - prob: 1.000\n",
            "\n",
            "Interval 94475 (944740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.323 - mean_q: 12.637 - prob: 1.000\n",
            "\n",
            "Interval 94476 (944750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94477 (944760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.376 - mean_q: 12.741 - prob: 1.000\n",
            "\n",
            "Interval 94478 (944770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 94479 (944780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -28.000 [-28.000, -28.000] - loss: 0.001 - mae: 7.306 - mean_q: 12.605 - prob: 1.000\n",
            "\n",
            "Interval 94480 (944790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.298 - mean_q: 12.685 - prob: 1.000\n",
            "\n",
            "Interval 94481 (944800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 6.967 - mean_q: 12.117 - prob: 1.000\n",
            "\n",
            "Interval 94482 (944810 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 94483 (944820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.109 - mean_q: 12.385 - prob: 1.000\n",
            "\n",
            "Interval 94484 (944830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.221 - mean_q: 12.481 - prob: 1.000\n",
            "\n",
            "Interval 94485 (944840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94486 (944850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.000 - mae: 7.631 - mean_q: 13.035 - prob: 1.000\n",
            "\n",
            "Interval 94487 (944860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94488 (944870 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.453 - mean_q: 12.956 - prob: 1.000\n",
            "\n",
            "Interval 94489 (944880 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.199 - mean_q: 12.562 - prob: 1.000\n",
            "\n",
            "Interval 94490 (944890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.229 - mean_q: 12.574 - prob: 1.000\n",
            "\n",
            "Interval 94491 (944900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94492 (944910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.310 - mean_q: 12.692 - prob: 1.000\n",
            "\n",
            "Interval 94493 (944920 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 94494 (944930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.000 - mae: 7.144 - mean_q: 12.419 - prob: 1.000\n",
            "\n",
            "Interval 94495 (944940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.452 - mean_q: 12.715 - prob: 1.000\n",
            "\n",
            "Interval 94496 (944950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94497 (944960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.202 - mean_q: 12.616 - prob: 1.000\n",
            "\n",
            "Interval 94498 (944970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.412 - mean_q: 12.838 - prob: 1.000\n",
            "\n",
            "Interval 94499 (944980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94500 (944990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.287 - mean_q: 12.775 - prob: 1.000\n",
            "\n",
            "Interval 94501 (945000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.456 - mean_q: 12.882 - prob: 1.000\n",
            "\n",
            "Interval 94502 (945010 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.438 - mean_q: 12.850 - prob: 1.000\n",
            "\n",
            "Interval 94503 (945020 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 94504 (945030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.000 - mae: 7.319 - mean_q: 12.788 - prob: 1.000\n",
            "\n",
            "Interval 94505 (945040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.258 - mean_q: 12.584 - prob: 1.000\n",
            "\n",
            "Interval 94506 (945050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.130 - mean_q: 12.287 - prob: 1.000\n",
            "\n",
            "Interval 94507 (945060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.068 - mean_q: 12.301 - prob: 1.000\n",
            "\n",
            "Interval 94508 (945070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94509 (945080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.295 - mean_q: 12.772 - prob: 1.000\n",
            "\n",
            "Interval 94510 (945090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94511 (945100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.628 - mean_q: 13.174 - prob: 1.000\n",
            "\n",
            "Interval 94512 (945110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94513 (945120 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.435 - mean_q: 12.854 - prob: 1.000\n",
            "\n",
            "Interval 94514 (945130 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.413 - mean_q: 12.768 - prob: 1.000\n",
            "\n",
            "Interval 94515 (945140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94516 (945150 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.300 - mean_q: 12.602 - prob: 1.000\n",
            "\n",
            "Interval 94517 (945160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 94518 (945170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.000 - mae: 7.319 - mean_q: 12.695 - prob: 1.000\n",
            "\n",
            "Interval 94519 (945180 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 94520 (945190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.000 - mae: 7.486 - mean_q: 12.900 - prob: 1.000\n",
            "\n",
            "Interval 94521 (945200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94522 (945210 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.001 - mae: 7.574 - mean_q: 13.082 - prob: 1.000\n",
            "\n",
            "Interval 94523 (945220 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 94524 (945230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.000 - mae: 7.458 - mean_q: 12.736 - prob: 1.000\n",
            "\n",
            "Interval 94525 (945240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94526 (945250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.512 - mean_q: 12.922 - prob: 1.000\n",
            "\n",
            "Interval 94527 (945260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.319 - mean_q: 12.736 - prob: 1.000\n",
            "\n",
            "Interval 94528 (945270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94529 (945280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.292 - mean_q: 12.728 - prob: 1.000\n",
            "\n",
            "Interval 94530 (945290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.181 - mean_q: 12.474 - prob: 1.000\n",
            "\n",
            "Interval 94531 (945300 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 94532 (945310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.229 - mean_q: 12.629 - prob: 1.000\n",
            "\n",
            "Interval 94533 (945320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.485 - mean_q: 12.824 - prob: 1.000\n",
            "\n",
            "Interval 94534 (945330 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 94535 (945340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -20.000 [-20.000, -20.000] - loss: 0.000 - mae: 6.993 - mean_q: 12.240 - prob: 1.000\n",
            "\n",
            "Interval 94536 (945350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94537 (945360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.284 - mean_q: 12.567 - prob: 1.000\n",
            "\n",
            "Interval 94538 (945370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.450 - mean_q: 12.805 - prob: 1.000\n",
            "\n",
            "Interval 94539 (945380 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.218 - mean_q: 12.464 - prob: 1.000\n",
            "\n",
            "Interval 94540 (945390 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 94541 (945400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.305 - mean_q: 12.615 - prob: 1.000\n",
            "\n",
            "Interval 94542 (945410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.403 - mean_q: 12.804 - prob: 1.000\n",
            "\n",
            "Interval 94543 (945420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.045 - mean_q: 12.415 - prob: 1.000\n",
            "\n",
            "Interval 94544 (945430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.323 - mean_q: 12.784 - prob: 1.000\n",
            "\n",
            "Interval 94545 (945440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.178 - mean_q: 12.385 - prob: 1.000\n",
            "\n",
            "Interval 94546 (945450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94547 (945460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.403 - mean_q: 12.820 - prob: 1.000\n",
            "\n",
            "Interval 94548 (945470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.579 - mean_q: 12.997 - prob: 1.000\n",
            "\n",
            "Interval 94549 (945480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94550 (945490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.314 - mean_q: 12.667 - prob: 1.000\n",
            "\n",
            "Interval 94551 (945500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 94552 (945510 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.000 - mae: 7.321 - mean_q: 12.834 - prob: 1.000\n",
            "\n",
            "Interval 94553 (945520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.413 - mean_q: 12.893 - prob: 1.000\n",
            "\n",
            "Interval 94554 (945530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.000 - mae: 7.352 - mean_q: 12.790 - prob: 1.000\n",
            "\n",
            "Interval 94555 (945540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.444 - mean_q: 12.755 - prob: 1.000\n",
            "\n",
            "Interval 94556 (945550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94557 (945560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.281 - mean_q: 12.556 - prob: 1.000\n",
            "\n",
            "Interval 94558 (945570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.424 - mean_q: 12.811 - prob: 1.000\n",
            "\n",
            "Interval 94559 (945580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.182 - mean_q: 12.287 - prob: 1.000\n",
            "\n",
            "Interval 94560 (945590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94561 (945600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.167 - mean_q: 12.569 - prob: 1.000\n",
            "\n",
            "Interval 94562 (945610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.256 - mean_q: 12.559 - prob: 1.000\n",
            "\n",
            "Interval 94563 (945620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94564 (945630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 6.910 - mean_q: 12.016 - prob: 1.000\n",
            "\n",
            "Interval 94565 (945640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.344 - mean_q: 12.598 - prob: 1.000\n",
            "\n",
            "Interval 94566 (945650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.000 - mae: 7.306 - mean_q: 12.798 - prob: 1.000\n",
            "\n",
            "Interval 94567 (945660 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 94568 (945670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.268 - mean_q: 12.547 - prob: 1.000\n",
            "\n",
            "Interval 94569 (945680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.616 - mean_q: 13.160 - prob: 1.000\n",
            "\n",
            "Interval 94570 (945690 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 94571 (945700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.381 - mean_q: 12.726 - prob: 1.000\n",
            "\n",
            "Interval 94572 (945710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.517 - mean_q: 12.823 - prob: 1.000\n",
            "\n",
            "Interval 94573 (945720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.390 - mean_q: 12.838 - prob: 1.000\n",
            "\n",
            "Interval 94574 (945730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.000 - mae: 7.275 - mean_q: 12.612 - prob: 1.000\n",
            "\n",
            "Interval 94575 (945740 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 94576 (945750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.480 - mean_q: 13.011 - prob: 1.000\n",
            "\n",
            "Interval 94577 (945760 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 94578 (945770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -19.000 [-19.000, -19.000] - loss: 0.000 - mae: 7.080 - mean_q: 12.350 - prob: 1.000\n",
            "\n",
            "Interval 94579 (945780 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -3.7000\n",
            "Interval 94580 (945790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -25.000 [-25.000, -25.000] - loss: 0.000 - mae: 7.434 - mean_q: 12.859 - prob: 1.000\n",
            "\n",
            "Interval 94581 (945800 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 94582 (945810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -24.000 [-24.000, -24.000] - loss: 0.000 - mae: 7.207 - mean_q: 12.463 - prob: 1.000\n",
            "\n",
            "Interval 94583 (945820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.535 - mean_q: 12.880 - prob: 1.000\n",
            "\n",
            "Interval 94584 (945830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -21.000 [-21.000, -21.000] - loss: 0.000 - mae: 7.401 - mean_q: 12.890 - prob: 1.000\n",
            "\n",
            "Interval 94585 (945840 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 94586 (945850 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.260 - mean_q: 12.681 - prob: 1.000\n",
            "\n",
            "Interval 94587 (945860 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.348 - mean_q: 12.749 - prob: 1.000\n",
            "\n",
            "Interval 94588 (945870 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -2.8000\n",
            "Interval 94589 (945880 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.000 - mae: 7.299 - mean_q: 12.700 - prob: 1.000\n",
            "\n",
            "Interval 94590 (945890 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.532 - mean_q: 12.884 - prob: 1.000\n",
            "\n",
            "Interval 94591 (945900 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.400 - mean_q: 12.637 - prob: 1.000\n",
            "\n",
            "Interval 94592 (945910 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.0000\n",
            "Interval 94593 (945920 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.482 - mean_q: 12.900 - prob: 1.000\n",
            "\n",
            "Interval 94594 (945930 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -2.8000\n",
            "Interval 94595 (945940 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.000 - mae: 7.494 - mean_q: 12.834 - prob: 1.000\n",
            "\n",
            "Interval 94596 (945950 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.347 - mean_q: 12.579 - prob: 1.000\n",
            "\n",
            "Interval 94597 (945960 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.147 - mean_q: 12.405 - prob: 1.000\n",
            "\n",
            "Interval 94598 (945970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94599 (945980 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.000 - mae: 7.044 - mean_q: 12.221 - prob: 1.000\n",
            "\n",
            "Interval 94600 (945990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.000 - mae: 7.230 - mean_q: 12.536 - prob: 1.000\n",
            "\n",
            "Interval 94601 (946000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.000 - mae: 6.983 - mean_q: 12.177 - prob: 1.000\n",
            "\n",
            "Interval 94602 (946010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.517 - mean_q: 12.866 - prob: 1.000\n",
            "\n",
            "Interval 94603 (946020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94604 (946030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.289 - mean_q: 12.696 - prob: 1.000\n",
            "\n",
            "Interval 94605 (946040 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.231 - mean_q: 12.572 - prob: 1.000\n",
            "\n",
            "Interval 94606 (946050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94607 (946060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 10.500 [7.000, 14.000] - loss: 0.000 - mae: 7.578 - mean_q: 13.139 - prob: 1.000\n",
            "\n",
            "Interval 94608 (946070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94609 (946080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.272 - mean_q: 12.446 - prob: 1.000\n",
            "\n",
            "Interval 94610 (946090 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.441 - mean_q: 12.783 - prob: 1.000\n",
            "\n",
            "Interval 94611 (946100 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.378 - mean_q: 12.883 - prob: 1.000\n",
            "\n",
            "Interval 94612 (946110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.199 - mean_q: 12.315 - prob: 1.000\n",
            "\n",
            "Interval 94613 (946120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 94614 (946130 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -23.000 [-23.000, -23.000] - loss: 0.000 - mae: 7.612 - mean_q: 13.104 - prob: 1.000\n",
            "\n",
            "Interval 94615 (946140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 94616 (946150 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.000 - mae: 7.468 - mean_q: 12.759 - prob: 1.000\n",
            "\n",
            "Interval 94617 (946160 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.451 - mean_q: 12.916 - prob: 1.000\n",
            "\n",
            "Interval 94618 (946170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 94619 (946180 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.062 - mean_q: 12.270 - prob: 1.000\n",
            "\n",
            "Interval 94620 (946190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.000 - mae: 7.431 - mean_q: 12.957 - prob: 1.000\n",
            "\n",
            "Interval 94621 (946200 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 94622 (946210 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.600 - mean_q: 13.039 - prob: 1.000\n",
            "\n",
            "Interval 94623 (946220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.555 - mean_q: 12.965 - prob: 1.000\n",
            "\n",
            "Interval 94624 (946230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94625 (946240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.000 - mae: 7.843 - mean_q: 13.241 - prob: 1.000\n",
            "\n",
            "Interval 94626 (946250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.602 - mean_q: 13.005 - prob: 1.000\n",
            "\n",
            "Interval 94627 (946260 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 94628 (946270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.236 - mean_q: 12.482 - prob: 1.000\n",
            "\n",
            "Interval 94629 (946280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.276 - mean_q: 12.668 - prob: 1.000\n",
            "\n",
            "Interval 94630 (946290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.314 - mean_q: 12.648 - prob: 1.000\n",
            "\n",
            "Interval 94631 (946300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.000 - mae: 7.315 - mean_q: 12.599 - prob: 1.000\n",
            "\n",
            "Interval 94632 (946310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94633 (946320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.543 - mean_q: 12.970 - prob: 1.000\n",
            "\n",
            "Interval 94634 (946330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.386 - mean_q: 12.606 - prob: 1.000\n",
            "\n",
            "Interval 94635 (946340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94636 (946350 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 6.956 - mean_q: 12.287 - prob: 1.000\n",
            "\n",
            "Interval 94637 (946360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.452 - mean_q: 12.984 - prob: 1.000\n",
            "\n",
            "Interval 94638 (946370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94639 (946380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.117 - mean_q: 12.348 - prob: 1.000\n",
            "\n",
            "Interval 94640 (946390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94641 (946400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.588 - mean_q: 12.935 - prob: 1.000\n",
            "\n",
            "Interval 94642 (946410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.401 - mean_q: 12.854 - prob: 1.000\n",
            "\n",
            "Interval 94643 (946420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.210 - mean_q: 12.537 - prob: 1.000\n",
            "\n",
            "Interval 94644 (946430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 94645 (946440 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.325 - mean_q: 12.672 - prob: 1.000\n",
            "\n",
            "Interval 94646 (946450 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.168 - mean_q: 12.391 - prob: 1.000\n",
            "\n",
            "Interval 94647 (946460 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 94648 (946470 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.000 - mae: 7.440 - mean_q: 12.934 - prob: 1.000\n",
            "\n",
            "Interval 94649 (946480 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.242 - mean_q: 12.655 - prob: 1.000\n",
            "\n",
            "Interval 94650 (946490 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 94651 (946500 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.439 - mean_q: 12.866 - prob: 1.000\n",
            "\n",
            "Interval 94652 (946510 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 94653 (946520 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.530 - mean_q: 13.041 - prob: 1.000\n",
            "\n",
            "Interval 94654 (946530 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 94655 (946540 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.000 - mae: 7.162 - mean_q: 12.472 - prob: 1.000\n",
            "\n",
            "Interval 94656 (946550 steps performed)\n",
            "10/10 [==============================] - 0s 16ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.013 - mean_q: 12.287 - prob: 1.000\n",
            "\n",
            "Interval 94657 (946560 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.000 - mae: 7.192 - mean_q: 12.473 - prob: 1.000\n",
            "\n",
            "Interval 94658 (946570 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.000 - mae: 7.213 - mean_q: 12.405 - prob: 1.000\n",
            "\n",
            "Interval 94659 (946580 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 94660 (946590 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.532 - mean_q: 13.017 - prob: 1.000\n",
            "\n",
            "Interval 94661 (946600 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.797 - mean_q: 13.170 - prob: 1.000\n",
            "\n",
            "Interval 94662 (946610 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: -1.0000\n",
            "Interval 94663 (946620 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.701 - mean_q: 13.138 - prob: 1.000\n",
            "\n",
            "Interval 94664 (946630 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.128 - mean_q: 12.387 - prob: 1.000\n",
            "\n",
            "Interval 94665 (946640 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 94666 (946650 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.000 - mae: 7.400 - mean_q: 12.772 - prob: 1.000\n",
            "\n",
            "Interval 94667 (946660 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.450 - mean_q: 12.878 - prob: 1.000\n",
            "\n",
            "Interval 94668 (946670 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 94669 (946680 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.000 - mae: 7.503 - mean_q: 12.806 - prob: 1.000\n",
            "\n",
            "Interval 94670 (946690 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 94671 (946700 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.124 - mean_q: 12.370 - prob: 1.000\n",
            "\n",
            "Interval 94672 (946710 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.527 - mean_q: 12.872 - prob: 1.000\n",
            "\n",
            "Interval 94673 (946720 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.9000\n",
            "Interval 94674 (946730 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.261 - mean_q: 12.685 - prob: 1.000\n",
            "\n",
            "Interval 94675 (946740 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 6.949 - mean_q: 12.126 - prob: 1.000\n",
            "\n",
            "Interval 94676 (946750 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 94677 (946760 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.000 - mae: 7.319 - mean_q: 12.784 - prob: 1.000\n",
            "\n",
            "Interval 94678 (946770 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.347 - mean_q: 12.567 - prob: 1.000\n",
            "\n",
            "Interval 94679 (946780 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 94680 (946790 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.515 - mean_q: 12.928 - prob: 1.000\n",
            "\n",
            "Interval 94681 (946800 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.336 - mean_q: 12.736 - prob: 1.000\n",
            "\n",
            "Interval 94682 (946810 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 94683 (946820 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.212 - mean_q: 12.451 - prob: 1.000\n",
            "\n",
            "Interval 94684 (946830 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.407 - mean_q: 12.768 - prob: 1.000\n",
            "\n",
            "Interval 94685 (946840 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 94686 (946850 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.497 - mean_q: 12.782 - prob: 1.000\n",
            "\n",
            "Interval 94687 (946860 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 94688 (946870 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.272 - mean_q: 12.601 - prob: 1.000\n",
            "\n",
            "Interval 94689 (946880 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.502 - mean_q: 12.823 - prob: 1.000\n",
            "\n",
            "Interval 94690 (946890 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.9000\n",
            "Interval 94691 (946900 steps performed)\n",
            "10/10 [==============================] - 0s 14ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.519 - mean_q: 13.081 - prob: 1.000\n",
            "\n",
            "Interval 94692 (946910 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.648 - mean_q: 13.097 - prob: 1.000\n",
            "\n",
            "Interval 94693 (946920 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.9000\n",
            "Interval 94694 (946930 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -17.000 [-17.000, -17.000] - loss: 0.000 - mae: 7.715 - mean_q: 13.135 - prob: 1.000\n",
            "\n",
            "Interval 94695 (946940 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: -1.0000\n",
            "Interval 94696 (946950 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.569 - mean_q: 12.917 - prob: 1.000\n",
            "\n",
            "Interval 94697 (946960 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.427 - mean_q: 12.774 - prob: 1.000\n",
            "\n",
            "Interval 94698 (946970 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 94699 (946980 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.535 - mean_q: 12.970 - prob: 1.000\n",
            "\n",
            "Interval 94700 (946990 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.0000\n",
            "Interval 94701 (947000 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.000 - mae: 7.451 - mean_q: 12.990 - prob: 1.000\n",
            "\n",
            "Interval 94702 (947010 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -23.000 [-23.000, -23.000] - loss: 0.001 - mae: 7.468 - mean_q: 12.832 - prob: 1.000\n",
            "\n",
            "Interval 94703 (947020 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 94704 (947030 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.705 - mean_q: 13.131 - prob: 1.000\n",
            "\n",
            "Interval 94705 (947040 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 94706 (947050 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.556 - mean_q: 12.934 - prob: 1.000\n",
            "\n",
            "Interval 94707 (947060 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.623 - mean_q: 13.107 - prob: 1.000\n",
            "\n",
            "Interval 94708 (947070 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.9000\n",
            "Interval 94709 (947080 steps performed)\n",
            "10/10 [==============================] - 0s 14ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.000 - mae: 7.312 - mean_q: 12.728 - prob: 1.000\n",
            "\n",
            "Interval 94710 (947090 steps performed)\n",
            "10/10 [==============================] - 0s 15ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.000 - mae: 7.503 - mean_q: 13.029 - prob: 1.000\n",
            "\n",
            "Interval 94711 (947100 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.000 - mae: 6.931 - mean_q: 12.029 - prob: 1.000\n",
            "\n",
            "Interval 94712 (947110 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 94713 (947120 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 11.000 [9.000, 13.000] - loss: 0.000 - mae: 7.185 - mean_q: 12.537 - prob: 1.000\n",
            "\n",
            "Interval 94714 (947130 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: -1.9000\n",
            "Interval 94715 (947140 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.000 - mae: 7.335 - mean_q: 12.675 - prob: 1.000\n",
            "\n",
            "Interval 94716 (947150 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 94717 (947160 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.000 - mae: 7.138 - mean_q: 12.398 - prob: 1.000\n",
            "\n",
            "Interval 94718 (947170 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 94719 (947180 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.000 - mae: 7.379 - mean_q: 12.699 - prob: 1.000\n",
            "\n",
            "Interval 94720 (947190 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: -1.0000\n",
            "Interval 94721 (947200 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.447 - mean_q: 12.749 - prob: 1.000\n",
            "\n",
            "Interval 94722 (947210 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.427 - mean_q: 12.797 - prob: 1.000\n",
            "\n",
            "Interval 94723 (947220 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: -1.9000\n",
            "Interval 94724 (947230 steps performed)\n",
            "10/10 [==============================] - 0s 14ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.493 - mean_q: 12.776 - prob: 1.000\n",
            "\n",
            "Interval 94725 (947240 steps performed)\n",
            "10/10 [==============================] - 0s 14ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.835 - mean_q: 13.367 - prob: 1.000\n",
            "\n",
            "Interval 94726 (947250 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: -1.9000\n",
            "Interval 94727 (947260 steps performed)\n",
            "10/10 [==============================] - 0s 14ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.253 - mean_q: 12.603 - prob: 1.000\n",
            "\n",
            "Interval 94728 (947270 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.000 - mae: 7.268 - mean_q: 12.679 - prob: 1.000\n",
            "\n",
            "Interval 94729 (947280 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.187 - mean_q: 12.420 - prob: 1.000\n",
            "\n",
            "Interval 94730 (947290 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.0000\n",
            "Interval 94731 (947300 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.382 - mean_q: 12.886 - prob: 1.000\n",
            "\n",
            "Interval 94732 (947310 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: -1.0000\n",
            "Interval 94733 (947320 steps performed)\n",
            "10/10 [==============================] - 0s 14ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.000 - mae: 7.452 - mean_q: 12.862 - prob: 1.000\n",
            "\n",
            "Interval 94734 (947330 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.219 - mean_q: 12.450 - prob: 1.000\n",
            "\n",
            "Interval 94735 (947340 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.094 - mean_q: 12.489 - prob: 1.000\n",
            "\n",
            "Interval 94736 (947350 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 94737 (947360 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.392 - mean_q: 12.673 - prob: 1.000\n",
            "\n",
            "Interval 94738 (947370 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.569 - mean_q: 12.949 - prob: 1.000\n",
            "\n",
            "Interval 94739 (947380 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 94740 (947390 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.457 - mean_q: 12.724 - prob: 1.000\n",
            "\n",
            "Interval 94741 (947400 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.9000\n",
            "Interval 94742 (947410 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.457 - mean_q: 12.899 - prob: 1.000\n",
            "\n",
            "Interval 94743 (947420 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.295 - mean_q: 12.714 - prob: 1.000\n",
            "\n",
            "Interval 94744 (947430 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -2.8000\n",
            "Interval 94745 (947440 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.549 - mean_q: 12.944 - prob: 1.000\n",
            "\n",
            "Interval 94746 (947450 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.617 - mean_q: 12.991 - prob: 1.000\n",
            "\n",
            "Interval 94747 (947460 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.262 - mean_q: 12.654 - prob: 1.000\n",
            "\n",
            "Interval 94748 (947470 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 94749 (947480 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.215 - mean_q: 12.426 - prob: 1.000\n",
            "\n",
            "Interval 94750 (947490 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.299 - mean_q: 12.699 - prob: 1.000\n",
            "\n",
            "Interval 94751 (947500 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: -1.9000\n",
            "Interval 94752 (947510 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 94753 (947520 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -20.000 [-20.000, -20.000] - loss: 0.000 - mae: 7.104 - mean_q: 12.456 - prob: 1.000\n",
            "\n",
            "Interval 94754 (947530 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.200 - mean_q: 12.557 - prob: 1.000\n",
            "\n",
            "Interval 94755 (947540 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 94756 (947550 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.191 - mean_q: 12.405 - prob: 1.000\n",
            "\n",
            "Interval 94757 (947560 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 94758 (947570 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.266 - mean_q: 12.698 - prob: 1.000\n",
            "\n",
            "Interval 94759 (947580 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.209 - mean_q: 12.488 - prob: 1.000\n",
            "\n",
            "Interval 94760 (947590 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 94761 (947600 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.000 - mae: 7.176 - mean_q: 12.442 - prob: 1.000\n",
            "\n",
            "Interval 94762 (947610 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.236 - mean_q: 12.408 - prob: 1.000\n",
            "\n",
            "Interval 94763 (947620 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.000 - mae: 7.667 - mean_q: 13.176 - prob: 1.000\n",
            "\n",
            "Interval 94764 (947630 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.337 - mean_q: 12.736 - prob: 1.000\n",
            "\n",
            "Interval 94765 (947640 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 94766 (947650 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.586 - mean_q: 12.950 - prob: 1.000\n",
            "\n",
            "Interval 94767 (947660 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.088 - mean_q: 12.350 - prob: 1.000\n",
            "\n",
            "Interval 94768 (947670 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 94769 (947680 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.071 - mean_q: 12.359 - prob: 1.000\n",
            "\n",
            "Interval 94770 (947690 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.291 - mean_q: 12.427 - prob: 1.000\n",
            "\n",
            "Interval 94771 (947700 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 94772 (947710 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.224 - mean_q: 12.556 - prob: 1.000\n",
            "\n",
            "Interval 94773 (947720 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 94774 (947730 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.381 - mean_q: 12.578 - prob: 1.000\n",
            "\n",
            "Interval 94775 (947740 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 94776 (947750 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.000 - mae: 7.197 - mean_q: 12.482 - prob: 1.000\n",
            "\n",
            "Interval 94777 (947760 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.396 - mean_q: 12.681 - prob: 1.000\n",
            "\n",
            "Interval 94778 (947770 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.204 - mean_q: 12.555 - prob: 1.000\n",
            "\n",
            "Interval 94779 (947780 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.0000\n",
            "Interval 94780 (947790 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.466 - mean_q: 12.633 - prob: 1.000\n",
            "\n",
            "Interval 94781 (947800 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.366 - mean_q: 12.809 - prob: 1.000\n",
            "\n",
            "Interval 94782 (947810 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.9000\n",
            "Interval 94783 (947820 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.000 - mae: 7.400 - mean_q: 12.651 - prob: 1.000\n",
            "\n",
            "Interval 94784 (947830 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.0000\n",
            "Interval 94785 (947840 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.360 - mean_q: 12.664 - prob: 1.000\n",
            "\n",
            "Interval 94786 (947850 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.298 - mean_q: 12.642 - prob: 1.000\n",
            "\n",
            "Interval 94787 (947860 steps performed)\n",
            "10/10 [==============================] - 0s 14ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.521 - mean_q: 12.850 - prob: 1.000\n",
            "\n",
            "Interval 94788 (947870 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 6.907 - mean_q: 11.950 - prob: 1.000\n",
            "\n",
            "Interval 94789 (947880 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -2.8000\n",
            "Interval 94790 (947890 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.000 - mae: 7.357 - mean_q: 12.613 - prob: 1.000\n",
            "\n",
            "Interval 94791 (947900 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.238 - mean_q: 12.451 - prob: 1.000\n",
            "\n",
            "Interval 94792 (947910 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: -1.9000\n",
            "Interval 94793 (947920 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.000 - mae: 7.597 - mean_q: 13.056 - prob: 1.000\n",
            "\n",
            "Interval 94794 (947930 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 94795 (947940 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.181 - mean_q: 12.409 - prob: 1.000\n",
            "\n",
            "Interval 94796 (947950 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.290 - mean_q: 12.547 - prob: 1.000\n",
            "\n",
            "Interval 94797 (947960 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 94798 (947970 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 94799 (947980 steps performed)\n",
            "10/10 [==============================] - 0s 13ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.264 - mean_q: 12.584 - prob: 1.000\n",
            "\n",
            "Interval 94800 (947990 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.370 - mean_q: 12.743 - prob: 1.000\n",
            "\n",
            "Interval 94801 (948000 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: -1.0000\n",
            "Interval 94802 (948010 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 10.000 [6.000, 14.000] - loss: 0.000 - mae: 7.016 - mean_q: 12.171 - prob: 1.000\n",
            "\n",
            "Interval 94803 (948020 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 94804 (948030 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.9000\n",
            "Interval 94805 (948040 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.001 - mae: 7.363 - mean_q: 12.714 - prob: 1.000\n",
            "\n",
            "Interval 94806 (948050 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: -1.9000\n",
            "Interval 94807 (948060 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -20.000 [-20.000, -20.000] - loss: 0.001 - mae: 7.829 - mean_q: 13.470 - prob: 1.000\n",
            "\n",
            "Interval 94808 (948070 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 94809 (948080 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.663 - mean_q: 13.117 - prob: 1.000\n",
            "\n",
            "Interval 94810 (948090 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.334 - mean_q: 12.656 - prob: 1.000\n",
            "\n",
            "Interval 94811 (948100 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 94812 (948110 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.000 - mae: 7.431 - mean_q: 12.893 - prob: 1.000\n",
            "\n",
            "Interval 94813 (948120 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.302 - mean_q: 12.649 - prob: 1.000\n",
            "\n",
            "Interval 94814 (948130 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.000 - mae: 7.289 - mean_q: 12.541 - prob: 1.000\n",
            "\n",
            "Interval 94815 (948140 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 94816 (948150 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.427 - mean_q: 12.853 - prob: 1.000\n",
            "\n",
            "Interval 94817 (948160 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.385 - mean_q: 12.685 - prob: 1.000\n",
            "\n",
            "Interval 94818 (948170 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.428 - mean_q: 12.882 - prob: 1.000\n",
            "\n",
            "Interval 94819 (948180 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.9000\n",
            "Interval 94820 (948190 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.023 - mean_q: 12.239 - prob: 1.000\n",
            "\n",
            "Interval 94821 (948200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 94822 (948210 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.085 - mean_q: 12.412 - prob: 1.000\n",
            "\n",
            "Interval 94823 (948220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.282 - mean_q: 12.674 - prob: 1.000\n",
            "\n",
            "Interval 94824 (948230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 94825 (948240 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.000 - mae: 7.525 - mean_q: 12.881 - prob: 1.000\n",
            "\n",
            "Interval 94826 (948250 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.157 - mean_q: 12.504 - prob: 1.000\n",
            "\n",
            "Interval 94827 (948260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94828 (948270 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.316 - mean_q: 12.744 - prob: 1.000\n",
            "\n",
            "Interval 94829 (948280 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.001 - mae: 7.282 - mean_q: 12.517 - prob: 1.000\n",
            "\n",
            "Interval 94830 (948290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.312 - mean_q: 12.629 - prob: 1.000\n",
            "\n",
            "Interval 94831 (948300 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 94832 (948310 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.579 - mean_q: 13.009 - prob: 1.000\n",
            "\n",
            "Interval 94833 (948320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94834 (948330 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.405 - mean_q: 12.667 - prob: 1.000\n",
            "\n",
            "Interval 94835 (948340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.573 - mean_q: 13.029 - prob: 1.000\n",
            "\n",
            "Interval 94836 (948350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94837 (948360 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.454 - mean_q: 12.870 - prob: 1.000\n",
            "\n",
            "Interval 94838 (948370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.474 - mean_q: 12.818 - prob: 1.000\n",
            "\n",
            "Interval 94839 (948380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.000 - mae: 7.573 - mean_q: 13.006 - prob: 1.000\n",
            "\n",
            "Interval 94840 (948390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94841 (948400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.307 - mean_q: 12.624 - prob: 1.000\n",
            "\n",
            "Interval 94842 (948410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.252 - mean_q: 12.513 - prob: 1.000\n",
            "\n",
            "Interval 94843 (948420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.152 - mean_q: 12.394 - prob: 1.000\n",
            "\n",
            "Interval 94844 (948430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94845 (948440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.314 - mean_q: 12.739 - prob: 1.000\n",
            "\n",
            "Interval 94846 (948450 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 12.000 [11.000, 13.000] - loss: 0.000 - mae: 7.427 - mean_q: 12.794 - prob: 1.000\n",
            "\n",
            "Interval 94847 (948460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94848 (948470 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.423 - mean_q: 12.931 - prob: 1.000\n",
            "\n",
            "Interval 94849 (948480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 94850 (948490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.001 - mae: 7.500 - mean_q: 12.807 - prob: 1.000\n",
            "\n",
            "Interval 94851 (948500 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 94852 (948510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -19.000 [-19.000, -19.000] - loss: 0.001 - mae: 7.428 - mean_q: 12.856 - prob: 1.000\n",
            "\n",
            "Interval 94853 (948520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.406 - mean_q: 12.707 - prob: 1.000\n",
            "\n",
            "Interval 94854 (948530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94855 (948540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.343 - mean_q: 12.488 - prob: 1.000\n",
            "\n",
            "Interval 94856 (948550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.445 - mean_q: 12.769 - prob: 1.000\n",
            "\n",
            "Interval 94857 (948560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94858 (948570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.340 - mean_q: 12.486 - prob: 1.000\n",
            "\n",
            "Interval 94859 (948580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94860 (948590 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.609 - mean_q: 13.135 - prob: 1.000\n",
            "\n",
            "Interval 94861 (948600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.750 - mean_q: 13.283 - prob: 1.000\n",
            "\n",
            "Interval 94862 (948610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.000 - mae: 7.196 - mean_q: 12.503 - prob: 1.000\n",
            "\n",
            "Interval 94863 (948620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.209 - mean_q: 12.441 - prob: 1.000\n",
            "\n",
            "Interval 94864 (948630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 94865 (948640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -11.000 [-11.000, -11.000] - loss: 0.000 - mae: 7.558 - mean_q: 13.035 - prob: 1.000\n",
            "\n",
            "Interval 94866 (948650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.494 - mean_q: 12.981 - prob: 1.000\n",
            "\n",
            "Interval 94867 (948660 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.482 - mean_q: 12.868 - prob: 1.000\n",
            "\n",
            "Interval 94868 (948670 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 94869 (948680 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.000 - mae: 7.431 - mean_q: 12.805 - prob: 1.000\n",
            "\n",
            "Interval 94870 (948690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94871 (948700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.500 - mean_q: 12.887 - prob: 1.000\n",
            "\n",
            "Interval 94872 (948710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.000 - mae: 7.669 - mean_q: 13.121 - prob: 1.000\n",
            "\n",
            "Interval 94873 (948720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.747 - mean_q: 13.329 - prob: 1.000\n",
            "\n",
            "Interval 94874 (948730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.213 - mean_q: 12.461 - prob: 1.000\n",
            "\n",
            "Interval 94875 (948740 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94876 (948750 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.446 - mean_q: 12.746 - prob: 1.000\n",
            "\n",
            "Interval 94877 (948760 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.000 - mae: 7.251 - mean_q: 12.527 - prob: 1.000\n",
            "\n",
            "Interval 94878 (948770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.073 - mean_q: 12.321 - prob: 1.000\n",
            "\n",
            "Interval 94879 (948780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94880 (948790 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.539 - mean_q: 13.014 - prob: 1.000\n",
            "\n",
            "Interval 94881 (948800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94882 (948810 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.336 - mean_q: 12.760 - prob: 1.000\n",
            "\n",
            "Interval 94883 (948820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.474 - mean_q: 12.875 - prob: 1.000\n",
            "\n",
            "Interval 94884 (948830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.361 - mean_q: 12.634 - prob: 1.000\n",
            "\n",
            "Interval 94885 (948840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94886 (948850 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.259 - mean_q: 12.555 - prob: 1.000\n",
            "\n",
            "Interval 94887 (948860 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.230 - mean_q: 12.489 - prob: 1.000\n",
            "\n",
            "Interval 94888 (948870 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 94889 (948880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.445 - mean_q: 12.844 - prob: 1.000\n",
            "\n",
            "Interval 94890 (948890 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.421 - mean_q: 12.905 - prob: 1.000\n",
            "\n",
            "Interval 94891 (948900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 94892 (948910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.000 - mae: 7.383 - mean_q: 12.727 - prob: 1.000\n",
            "\n",
            "Interval 94893 (948920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.281 - mean_q: 12.728 - prob: 1.000\n",
            "\n",
            "Interval 94894 (948930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.000 - mae: 7.674 - mean_q: 13.085 - prob: 1.000\n",
            "\n",
            "Interval 94895 (948940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94896 (948950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.266 - mean_q: 12.610 - prob: 1.000\n",
            "\n",
            "Interval 94897 (948960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.272 - mean_q: 12.623 - prob: 1.000\n",
            "\n",
            "Interval 94898 (948970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.327 - mean_q: 12.647 - prob: 1.000\n",
            "\n",
            "Interval 94899 (948980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.360 - mean_q: 12.681 - prob: 1.000\n",
            "\n",
            "Interval 94900 (948990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.511 - mean_q: 13.028 - prob: 1.000\n",
            "\n",
            "Interval 94901 (949000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 94902 (949010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.001 - mae: 7.786 - mean_q: 13.242 - prob: 1.000\n",
            "\n",
            "Interval 94903 (949020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.140 - mean_q: 12.429 - prob: 1.000\n",
            "\n",
            "Interval 94904 (949030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94905 (949040 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 11.000 [9.000, 13.000] - loss: 0.000 - mae: 7.566 - mean_q: 13.213 - prob: 1.000\n",
            "\n",
            "Interval 94906 (949050 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 94907 (949060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.437 - mean_q: 12.872 - prob: 1.000\n",
            "\n",
            "Interval 94908 (949070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94909 (949080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.633 - mean_q: 12.889 - prob: 1.000\n",
            "\n",
            "Interval 94910 (949090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94911 (949100 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.000 - mae: 7.220 - mean_q: 12.428 - prob: 1.000\n",
            "\n",
            "Interval 94912 (949110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.322 - mean_q: 12.577 - prob: 1.000\n",
            "\n",
            "Interval 94913 (949120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 94914 (949130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.000 - mae: 7.633 - mean_q: 13.033 - prob: 1.000\n",
            "\n",
            "Interval 94915 (949140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.533 - mean_q: 12.938 - prob: 1.000\n",
            "\n",
            "Interval 94916 (949150 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 94917 (949160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.104 - mean_q: 12.307 - prob: 1.000\n",
            "\n",
            "Interval 94918 (949170 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 94919 (949180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.313 - mean_q: 12.532 - prob: 1.000\n",
            "\n",
            "Interval 94920 (949190 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 94921 (949200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -17.000 [-17.000, -17.000] - loss: 0.000 - mae: 7.361 - mean_q: 12.632 - prob: 1.000\n",
            "\n",
            "Interval 94922 (949210 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.465 - mean_q: 12.849 - prob: 1.000\n",
            "\n",
            "Interval 94923 (949220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94924 (949230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.328 - mean_q: 12.652 - prob: 1.000\n",
            "\n",
            "Interval 94925 (949240 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 94926 (949250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.202 - mean_q: 12.521 - prob: 1.000\n",
            "\n",
            "Interval 94927 (949260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94928 (949270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.000 - mae: 7.077 - mean_q: 12.380 - prob: 1.000\n",
            "\n",
            "Interval 94929 (949280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 6.984 - mean_q: 12.205 - prob: 1.000\n",
            "\n",
            "Interval 94930 (949290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.000 - mae: 7.379 - mean_q: 12.773 - prob: 1.000\n",
            "\n",
            "Interval 94931 (949300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.690 - mean_q: 13.274 - prob: 1.000\n",
            "\n",
            "Interval 94932 (949310 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 94933 (949320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -23.000 [-23.000, -23.000] - loss: 0.000 - mae: 7.376 - mean_q: 12.790 - prob: 1.000\n",
            "\n",
            "Interval 94934 (949330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.260 - mean_q: 12.524 - prob: 1.000\n",
            "\n",
            "Interval 94935 (949340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94936 (949350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.633 - mean_q: 12.967 - prob: 1.000\n",
            "\n",
            "Interval 94937 (949360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94938 (949370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.000 - mae: 7.124 - mean_q: 12.332 - prob: 1.000\n",
            "\n",
            "Interval 94939 (949380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 94940 (949390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.000 - mae: 7.172 - mean_q: 12.484 - prob: 1.000\n",
            "\n",
            "Interval 94941 (949400 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 94942 (949410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.548 - mean_q: 13.081 - prob: 1.000\n",
            "\n",
            "Interval 94943 (949420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.615 - mean_q: 13.062 - prob: 1.000\n",
            "\n",
            "Interval 94944 (949430 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 94945 (949440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.600 - mean_q: 13.068 - prob: 1.000\n",
            "\n",
            "Interval 94946 (949450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.229 - mean_q: 12.493 - prob: 1.000\n",
            "\n",
            "Interval 94947 (949460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 6.941 - mean_q: 12.148 - prob: 1.000\n",
            "\n",
            "Interval 94948 (949470 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 94949 (949480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.331 - mean_q: 12.620 - prob: 1.000\n",
            "\n",
            "Interval 94950 (949490 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.193 - mean_q: 12.510 - prob: 1.000\n",
            "\n",
            "Interval 94951 (949500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94952 (949510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.415 - mean_q: 12.889 - prob: 1.000\n",
            "\n",
            "Interval 94953 (949520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.249 - mean_q: 12.511 - prob: 1.000\n",
            "\n",
            "Interval 94954 (949530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.764 - mean_q: 13.382 - prob: 1.000\n",
            "\n",
            "Interval 94955 (949540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94956 (949550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.000 - mae: 7.372 - mean_q: 12.727 - prob: 1.000\n",
            "\n",
            "Interval 94957 (949560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.450 - mean_q: 12.977 - prob: 1.000\n",
            "\n",
            "Interval 94958 (949570 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 94959 (949580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.273 - mean_q: 12.529 - prob: 1.000\n",
            "\n",
            "Interval 94960 (949590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 94961 (949600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.000 - mae: 7.551 - mean_q: 13.033 - prob: 1.000\n",
            "\n",
            "Interval 94962 (949610 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.000 - mae: 6.992 - mean_q: 12.210 - prob: 1.000\n",
            "\n",
            "Interval 94963 (949620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94964 (949630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.348 - mean_q: 12.719 - prob: 1.000\n",
            "\n",
            "Interval 94965 (949640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.462 - mean_q: 12.843 - prob: 1.000\n",
            "\n",
            "Interval 94966 (949650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.453 - mean_q: 12.774 - prob: 1.000\n",
            "\n",
            "Interval 94967 (949660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.257 - mean_q: 12.544 - prob: 1.000\n",
            "\n",
            "Interval 94968 (949670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94969 (949680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.457 - mean_q: 12.865 - prob: 1.000\n",
            "\n",
            "Interval 94970 (949690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.239 - mean_q: 12.597 - prob: 1.000\n",
            "\n",
            "Interval 94971 (949700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.000 - mae: 7.388 - mean_q: 12.792 - prob: 1.000\n",
            "\n",
            "Interval 94972 (949710 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 94973 (949720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.078 - mean_q: 12.216 - prob: 1.000\n",
            "\n",
            "Interval 94974 (949730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.558 - mean_q: 13.035 - prob: 1.000\n",
            "\n",
            "Interval 94975 (949740 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 94976 (949750 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.182 - mean_q: 12.497 - prob: 1.000\n",
            "\n",
            "Interval 94977 (949760 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.417 - mean_q: 12.925 - prob: 1.000\n",
            "\n",
            "Interval 94978 (949770 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.850 - mean_q: 13.247 - prob: 1.000\n",
            "\n",
            "Interval 94979 (949780 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 94980 (949790 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.418 - mean_q: 12.630 - prob: 1.000\n",
            "\n",
            "Interval 94981 (949800 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.148 - mean_q: 12.465 - prob: 1.000\n",
            "\n",
            "Interval 94982 (949810 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 94983 (949820 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.295 - mean_q: 12.577 - prob: 1.000\n",
            "\n",
            "Interval 94984 (949830 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.000 - mae: 7.467 - mean_q: 12.894 - prob: 1.000\n",
            "\n",
            "Interval 94985 (949840 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 94986 (949850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.038 - mean_q: 12.274 - prob: 1.000\n",
            "\n",
            "Interval 94987 (949860 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.302 - mean_q: 12.653 - prob: 1.000\n",
            "\n",
            "Interval 94988 (949870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94989 (949880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.277 - mean_q: 12.594 - prob: 1.000\n",
            "\n",
            "Interval 94990 (949890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.437 - mean_q: 12.808 - prob: 1.000\n",
            "\n",
            "Interval 94991 (949900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94992 (949910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.570 - mean_q: 13.039 - prob: 1.000\n",
            "\n",
            "Interval 94993 (949920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.663 - mean_q: 13.095 - prob: 1.000\n",
            "\n",
            "Interval 94994 (949930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.329 - mean_q: 12.685 - prob: 1.000\n",
            "\n",
            "Interval 94995 (949940 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.326 - mean_q: 12.610 - prob: 1.000\n",
            "\n",
            "Interval 94996 (949950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 94997 (949960 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.603 - mean_q: 12.990 - prob: 1.000\n",
            "\n",
            "Interval 94998 (949970 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.297 - mean_q: 12.613 - prob: 1.000\n",
            "\n",
            "Interval 94999 (949980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.320 - mean_q: 12.777 - prob: 1.000\n",
            "\n",
            "Interval 95000 (949990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 95001 (950000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.396 - mean_q: 12.710 - prob: 1.000\n",
            "\n",
            "Interval 95002 (950010 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.438 - mean_q: 12.938 - prob: 1.000\n",
            "\n",
            "Interval 95003 (950020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.113 - mean_q: 12.440 - prob: 1.000\n",
            "\n",
            "Interval 95004 (950030 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 95005 (950040 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.472 - mean_q: 12.868 - prob: 1.000\n",
            "\n",
            "Interval 95006 (950050 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.556 - mean_q: 12.919 - prob: 1.000\n",
            "\n",
            "Interval 95007 (950060 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 95008 (950070 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.391 - mean_q: 12.734 - prob: 1.000\n",
            "\n",
            "Interval 95009 (950080 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.134 - mean_q: 12.392 - prob: 1.000\n",
            "\n",
            "Interval 95010 (950090 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.468 - mean_q: 12.900 - prob: 1.000\n",
            "\n",
            "Interval 95011 (950100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 95012 (950110 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.518 - mean_q: 12.934 - prob: 1.000\n",
            "\n",
            "Interval 95013 (950120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.002 - mean_q: 12.268 - prob: 1.000\n",
            "\n",
            "Interval 95014 (950130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 95015 (950140 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.389 - mean_q: 12.789 - prob: 1.000\n",
            "\n",
            "Interval 95016 (950150 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.579 - mean_q: 13.095 - prob: 1.000\n",
            "\n",
            "Interval 95017 (950160 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.542 - mean_q: 12.897 - prob: 1.000\n",
            "\n",
            "Interval 95018 (950170 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 95019 (950180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 6.802 - mean_q: 11.901 - prob: 1.000\n",
            "\n",
            "Interval 95020 (950190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.427 - mean_q: 12.815 - prob: 1.000\n",
            "\n",
            "Interval 95021 (950200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.528 - mean_q: 12.896 - prob: 1.000\n",
            "\n",
            "Interval 95022 (950210 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 95023 (950220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.511 - mean_q: 12.846 - prob: 1.000\n",
            "\n",
            "Interval 95024 (950230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.301 - mean_q: 12.607 - prob: 1.000\n",
            "\n",
            "Interval 95025 (950240 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 95026 (950250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.600 - mean_q: 13.139 - prob: 1.000\n",
            "\n",
            "Interval 95027 (950260 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.000 - mae: 7.166 - mean_q: 12.357 - prob: 1.000\n",
            "\n",
            "Interval 95028 (950270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.562 - mean_q: 13.049 - prob: 1.000\n",
            "\n",
            "Interval 95029 (950280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 95030 (950290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.081 - mean_q: 12.502 - prob: 1.000\n",
            "\n",
            "Interval 95031 (950300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.288 - mean_q: 12.554 - prob: 1.000\n",
            "\n",
            "Interval 95032 (950310 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 95033 (950320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -16.000 [-16.000, -16.000] - loss: 0.000 - mae: 7.454 - mean_q: 12.905 - prob: 1.000\n",
            "\n",
            "Interval 95034 (950330 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.250 - mean_q: 12.519 - prob: 1.000\n",
            "\n",
            "Interval 95035 (950340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 95036 (950350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.566 - mean_q: 13.027 - prob: 1.000\n",
            "\n",
            "Interval 95037 (950360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 95038 (950370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.001 - mae: 7.324 - mean_q: 12.586 - prob: 1.000\n",
            "\n",
            "Interval 95039 (950380 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.229 - mean_q: 12.598 - prob: 1.000\n",
            "\n",
            "Interval 95040 (950390 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.591 - mean_q: 13.083 - prob: 1.000\n",
            "\n",
            "Interval 95041 (950400 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.270 - mean_q: 12.500 - prob: 1.000\n",
            "\n",
            "Interval 95042 (950410 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 95043 (950420 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.113 - mean_q: 12.338 - prob: 1.000\n",
            "\n",
            "Interval 95044 (950430 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.202 - mean_q: 12.542 - prob: 1.000\n",
            "\n",
            "Interval 95045 (950440 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 95046 (950450 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.045 - mean_q: 12.198 - prob: 1.000\n",
            "\n",
            "Interval 95047 (950460 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.497 - mean_q: 12.826 - prob: 1.000\n",
            "\n",
            "Interval 95048 (950470 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 95049 (950480 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.546 - mean_q: 13.031 - prob: 1.000\n",
            "\n",
            "Interval 95050 (950490 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.138 - mean_q: 12.621 - prob: 1.000\n",
            "\n",
            "Interval 95051 (950500 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -2.8000\n",
            "Interval 95052 (950510 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.000 - mae: 7.318 - mean_q: 12.757 - prob: 1.000\n",
            "\n",
            "Interval 95053 (950520 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 95054 (950530 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.402 - mean_q: 12.902 - prob: 1.000\n",
            "\n",
            "Interval 95055 (950540 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.189 - mean_q: 12.477 - prob: 1.000\n",
            "\n",
            "Interval 95056 (950550 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.585 - mean_q: 12.786 - prob: 1.000\n",
            "\n",
            "Interval 95057 (950560 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.502 - mean_q: 12.946 - prob: 1.000\n",
            "\n",
            "Interval 95058 (950570 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.489 - mean_q: 12.797 - prob: 1.000\n",
            "\n",
            "Interval 95059 (950580 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.437 - mean_q: 12.843 - prob: 1.000\n",
            "\n",
            "Interval 95060 (950590 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 95061 (950600 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.000 - mae: 7.442 - mean_q: 12.920 - prob: 1.000\n",
            "\n",
            "Interval 95062 (950610 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.000 - mae: 7.274 - mean_q: 12.509 - prob: 1.000\n",
            "\n",
            "Interval 95063 (950620 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.828 - mean_q: 13.277 - prob: 1.000\n",
            "\n",
            "Interval 95064 (950630 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 95065 (950640 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.000 - mae: 7.566 - mean_q: 12.984 - prob: 1.000\n",
            "\n",
            "Interval 95066 (950650 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 95067 (950660 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.250 - mean_q: 12.409 - prob: 1.000\n",
            "\n",
            "Interval 95068 (950670 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.367 - mean_q: 12.603 - prob: 1.000\n",
            "\n",
            "Interval 95069 (950680 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 95070 (950690 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.000 - mae: 7.310 - mean_q: 12.598 - prob: 1.000\n",
            "\n",
            "Interval 95071 (950700 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.410 - mean_q: 12.634 - prob: 1.000\n",
            "\n",
            "Interval 95072 (950710 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 95073 (950720 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.507 - mean_q: 13.021 - prob: 1.000\n",
            "\n",
            "Interval 95074 (950730 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 95075 (950740 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.123 - mean_q: 12.351 - prob: 1.000\n",
            "\n",
            "Interval 95076 (950750 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 95077 (950760 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.422 - mean_q: 12.766 - prob: 1.000\n",
            "\n",
            "Interval 95078 (950770 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.349 - mean_q: 12.680 - prob: 1.000\n",
            "\n",
            "Interval 95079 (950780 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 95080 (950790 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.321 - mean_q: 12.638 - prob: 1.000\n",
            "\n",
            "Interval 95081 (950800 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 95082 (950810 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.352 - mean_q: 12.761 - prob: 1.000\n",
            "\n",
            "Interval 95083 (950820 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.280 - mean_q: 12.624 - prob: 1.000\n",
            "\n",
            "Interval 95084 (950830 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 95085 (950840 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.357 - mean_q: 12.689 - prob: 1.000\n",
            "\n",
            "Interval 95086 (950850 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.450 - mean_q: 12.897 - prob: 1.000\n",
            "\n",
            "Interval 95087 (950860 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.000 - mae: 7.265 - mean_q: 12.579 - prob: 1.000\n",
            "\n",
            "Interval 95088 (950870 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.477 - mean_q: 13.001 - prob: 1.000\n",
            "\n",
            "Interval 95089 (950880 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 95090 (950890 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.239 - mean_q: 12.549 - prob: 1.000\n",
            "\n",
            "Interval 95091 (950900 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.243 - mean_q: 12.538 - prob: 1.000\n",
            "\n",
            "Interval 95092 (950910 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 95093 (950920 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.483 - mean_q: 12.888 - prob: 1.000\n",
            "\n",
            "Interval 95094 (950930 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 95095 (950940 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.553 - mean_q: 12.984 - prob: 1.000\n",
            "\n",
            "Interval 95096 (950950 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.248 - mean_q: 12.456 - prob: 1.000\n",
            "\n",
            "Interval 95097 (950960 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.323 - mean_q: 12.573 - prob: 1.000\n",
            "\n",
            "Interval 95098 (950970 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.000 - mae: 7.486 - mean_q: 12.976 - prob: 1.000\n",
            "\n",
            "Interval 95099 (950980 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 95100 (950990 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.375 - mean_q: 12.691 - prob: 1.000\n",
            "\n",
            "Interval 95101 (951000 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.449 - mean_q: 12.887 - prob: 1.000\n",
            "\n",
            "Interval 95102 (951010 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.271 - mean_q: 12.517 - prob: 1.000\n",
            "\n",
            "Interval 95103 (951020 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.393 - mean_q: 12.727 - prob: 1.000\n",
            "\n",
            "Interval 95104 (951030 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 95105 (951040 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.703 - mean_q: 13.171 - prob: 1.000\n",
            "\n",
            "Interval 95106 (951050 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.267 - mean_q: 12.523 - prob: 1.000\n",
            "\n",
            "Interval 95107 (951060 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 95108 (951070 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.397 - mean_q: 12.705 - prob: 1.000\n",
            "\n",
            "Interval 95109 (951080 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.000 - mae: 7.369 - mean_q: 12.700 - prob: 1.000\n",
            "\n",
            "Interval 95110 (951090 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.244 - mean_q: 12.499 - prob: 1.000\n",
            "\n",
            "Interval 95111 (951100 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.567 - mean_q: 12.950 - prob: 1.000\n",
            "\n",
            "Interval 95112 (951110 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 95113 (951120 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.302 - mean_q: 12.640 - prob: 1.000\n",
            "\n",
            "Interval 95114 (951130 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.387 - mean_q: 12.641 - prob: 1.000\n",
            "\n",
            "Interval 95115 (951140 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 95116 (951150 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 95117 (951160 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -19.000 [-19.000, -19.000] - loss: 0.000 - mae: 7.584 - mean_q: 13.080 - prob: 1.000\n",
            "\n",
            "Interval 95118 (951170 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.000 - mae: 7.111 - mean_q: 12.443 - prob: 1.000\n",
            "\n",
            "Interval 95119 (951180 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.411 - mean_q: 12.945 - prob: 1.000\n",
            "\n",
            "Interval 95120 (951190 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 95121 (951200 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.558 - mean_q: 12.935 - prob: 1.000\n",
            "\n",
            "Interval 95122 (951210 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 95123 (951220 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.6000\n",
            "1 episodes - episode_reward: -17.000 [-17.000, -17.000] - loss: 0.000 - mae: 7.180 - mean_q: 12.288 - prob: 1.000\n",
            "\n",
            "Interval 95124 (951230 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.000 - mae: 7.202 - mean_q: 12.430 - prob: 1.000\n",
            "\n",
            "Interval 95125 (951240 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 95126 (951250 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.351 - mean_q: 12.739 - prob: 1.000\n",
            "\n",
            "Interval 95127 (951260 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.415 - mean_q: 12.862 - prob: 1.000\n",
            "\n",
            "Interval 95128 (951270 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 95129 (951280 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.117 - mean_q: 12.161 - prob: 1.000\n",
            "\n",
            "Interval 95130 (951290 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.000 - mae: 7.676 - mean_q: 13.194 - prob: 1.000\n",
            "\n",
            "Interval 95131 (951300 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.349 - mean_q: 12.522 - prob: 1.000\n",
            "\n",
            "Interval 95132 (951310 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.657 - mean_q: 13.125 - prob: 1.000\n",
            "\n",
            "Interval 95133 (951320 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 95134 (951330 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.131 - mean_q: 12.363 - prob: 1.000\n",
            "\n",
            "Interval 95135 (951340 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.375 - mean_q: 12.829 - prob: 1.000\n",
            "\n",
            "Interval 95136 (951350 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.044 - mean_q: 12.314 - prob: 1.000\n",
            "\n",
            "Interval 95137 (951360 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 95138 (951370 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.587 - mean_q: 13.001 - prob: 1.000\n",
            "\n",
            "Interval 95139 (951380 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.447 - mean_q: 12.851 - prob: 1.000\n",
            "\n",
            "Interval 95140 (951390 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.9000\n",
            "Interval 95141 (951400 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.223 - mean_q: 12.555 - prob: 1.000\n",
            "\n",
            "Interval 95142 (951410 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 95143 (951420 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.378 - mean_q: 12.728 - prob: 1.000\n",
            "\n",
            "Interval 95144 (951430 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.517 - mean_q: 12.867 - prob: 1.000\n",
            "\n",
            "Interval 95145 (951440 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -2.8000\n",
            "Interval 95146 (951450 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.001 - mae: 7.519 - mean_q: 13.040 - prob: 1.000\n",
            "\n",
            "Interval 95147 (951460 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.063 - mean_q: 12.326 - prob: 1.000\n",
            "\n",
            "Interval 95148 (951470 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.347 - mean_q: 12.699 - prob: 1.000\n",
            "\n",
            "Interval 95149 (951480 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 95150 (951490 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.319 - mean_q: 12.760 - prob: 1.000\n",
            "\n",
            "Interval 95151 (951500 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.9000\n",
            "Interval 95152 (951510 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.352 - mean_q: 12.588 - prob: 1.000\n",
            "\n",
            "Interval 95153 (951520 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.000 - mae: 7.267 - mean_q: 12.615 - prob: 1.000\n",
            "\n",
            "Interval 95154 (951530 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 95155 (951540 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.692 - mean_q: 12.965 - prob: 1.000\n",
            "\n",
            "Interval 95156 (951550 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.219 - mean_q: 12.684 - prob: 1.000\n",
            "\n",
            "Interval 95157 (951560 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.9000\n",
            "Interval 95158 (951570 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.001 - mae: 7.509 - mean_q: 12.985 - prob: 1.000\n",
            "\n",
            "Interval 95159 (951580 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 95160 (951590 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 10.500 [7.000, 14.000] - loss: 0.000 - mae: 7.167 - mean_q: 12.341 - prob: 1.000\n",
            "\n",
            "Interval 95161 (951600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 95162 (951610 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 95163 (951620 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.436 - mean_q: 12.858 - prob: 1.000\n",
            "\n",
            "Interval 95164 (951630 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.510 - mean_q: 12.953 - prob: 1.000\n",
            "\n",
            "Interval 95165 (951640 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.350 - mean_q: 12.777 - prob: 1.000\n",
            "\n",
            "Interval 95166 (951650 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 95167 (951660 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.000 - mae: 7.447 - mean_q: 12.808 - prob: 1.000\n",
            "\n",
            "Interval 95168 (951670 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 95169 (951680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 6.986 - mean_q: 12.101 - prob: 1.000\n",
            "\n",
            "Interval 95170 (951690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 95171 (951700 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.357 - mean_q: 12.733 - prob: 1.000\n",
            "\n",
            "Interval 95172 (951710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.498 - mean_q: 12.889 - prob: 1.000\n",
            "\n",
            "Interval 95173 (951720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 95174 (951730 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.413 - mean_q: 12.779 - prob: 1.000\n",
            "\n",
            "Interval 95175 (951740 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 95176 (951750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.626 - mean_q: 13.060 - prob: 1.000\n",
            "\n",
            "Interval 95177 (951760 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.000 - mae: 6.991 - mean_q: 12.320 - prob: 1.000\n",
            "\n",
            "Interval 95178 (951770 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.709 - mean_q: 13.240 - prob: 1.000\n",
            "\n",
            "Interval 95179 (951780 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 95180 (951790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.000 - mae: 7.517 - mean_q: 13.033 - prob: 1.000\n",
            "\n",
            "Interval 95181 (951800 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 95182 (951810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.937 - mean_q: 13.580 - prob: 1.000\n",
            "\n",
            "Interval 95183 (951820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.429 - mean_q: 12.838 - prob: 1.000\n",
            "\n",
            "Interval 95184 (951830 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 95185 (951840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.307 - mean_q: 12.528 - prob: 1.000\n",
            "\n",
            "Interval 95186 (951850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.373 - mean_q: 12.638 - prob: 1.000\n",
            "\n",
            "Interval 95187 (951860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.383 - mean_q: 12.751 - prob: 1.000\n",
            "\n",
            "Interval 95188 (951870 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 95189 (951880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.640 - mean_q: 13.157 - prob: 1.000\n",
            "\n",
            "Interval 95190 (951890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 95191 (951900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.000 - mae: 7.545 - mean_q: 12.971 - prob: 1.000\n",
            "\n",
            "Interval 95192 (951910 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 95193 (951920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.625 - mean_q: 13.058 - prob: 1.000\n",
            "\n",
            "Interval 95194 (951930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.365 - mean_q: 12.702 - prob: 1.000\n",
            "\n",
            "Interval 95195 (951940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 95196 (951950 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.768 - mean_q: 13.223 - prob: 1.000\n",
            "\n",
            "Interval 95197 (951960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 95198 (951970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.000 - mae: 7.533 - mean_q: 13.011 - prob: 1.000\n",
            "\n",
            "Interval 95199 (951980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 95200 (951990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.000 - mae: 7.188 - mean_q: 12.402 - prob: 1.000\n",
            "\n",
            "Interval 95201 (952000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.673 - mean_q: 12.977 - prob: 1.000\n",
            "\n",
            "Interval 95202 (952010 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 95203 (952020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.462 - mean_q: 12.840 - prob: 1.000\n",
            "\n",
            "Interval 95204 (952030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.101 - mean_q: 12.344 - prob: 1.000\n",
            "\n",
            "Interval 95205 (952040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.342 - mean_q: 12.724 - prob: 1.000\n",
            "\n",
            "Interval 95206 (952050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.415 - mean_q: 12.793 - prob: 1.000\n",
            "\n",
            "Interval 95207 (952060 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 95208 (952070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.405 - mean_q: 12.662 - prob: 1.000\n",
            "\n",
            "Interval 95209 (952080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.424 - mean_q: 12.873 - prob: 1.000\n",
            "\n",
            "Interval 95210 (952090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.268 - mean_q: 12.476 - prob: 1.000\n",
            "\n",
            "Interval 95211 (952100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 95212 (952110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.412 - mean_q: 12.832 - prob: 1.000\n",
            "\n",
            "Interval 95213 (952120 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 95214 (952130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.709 - mean_q: 13.136 - prob: 1.000\n",
            "\n",
            "Interval 95215 (952140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.301 - mean_q: 12.607 - prob: 1.000\n",
            "\n",
            "Interval 95216 (952150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.000 - mae: 7.637 - mean_q: 13.100 - prob: 1.000\n",
            "\n",
            "Interval 95217 (952160 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.449 - mean_q: 12.815 - prob: 1.000\n",
            "\n",
            "Interval 95218 (952170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.000 - mae: 7.705 - mean_q: 13.112 - prob: 1.000\n",
            "\n",
            "Interval 95219 (952180 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.162 - mean_q: 12.440 - prob: 1.000\n",
            "\n",
            "Interval 95220 (952190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 95221 (952200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.067 - mean_q: 12.133 - prob: 1.000\n",
            "\n",
            "Interval 95222 (952210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.451 - mean_q: 12.734 - prob: 1.000\n",
            "\n",
            "Interval 95223 (952220 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 95224 (952230 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.466 - mean_q: 12.950 - prob: 1.000\n",
            "\n",
            "Interval 95225 (952240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.323 - mean_q: 12.722 - prob: 1.000\n",
            "\n",
            "Interval 95226 (952250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.001 - mae: 7.310 - mean_q: 12.632 - prob: 1.000\n",
            "\n",
            "Interval 95227 (952260 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 95228 (952270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.201 - mean_q: 12.473 - prob: 1.000\n",
            "\n",
            "Interval 95229 (952280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 95230 (952290 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.442 - mean_q: 12.897 - prob: 1.000\n",
            "\n",
            "Interval 95231 (952300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 95232 (952310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.000 - mae: 7.404 - mean_q: 12.791 - prob: 1.000\n",
            "\n",
            "Interval 95233 (952320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.070 - mean_q: 12.452 - prob: 1.000\n",
            "\n",
            "Interval 95234 (952330 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 95235 (952340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -10.000 [-10.000, -10.000] - loss: 0.000 - mae: 7.505 - mean_q: 12.777 - prob: 1.000\n",
            "\n",
            "Interval 95236 (952350 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 95237 (952360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.000 - mae: 7.298 - mean_q: 12.733 - prob: 1.000\n",
            "\n",
            "Interval 95238 (952370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 95239 (952380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.000 - mae: 7.418 - mean_q: 12.832 - prob: 1.000\n",
            "\n",
            "Interval 95240 (952390 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 95241 (952400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.215 - mean_q: 12.555 - prob: 1.000\n",
            "\n",
            "Interval 95242 (952410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.240 - mean_q: 12.546 - prob: 1.000\n",
            "\n",
            "Interval 95243 (952420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.209 - mean_q: 12.548 - prob: 1.000\n",
            "\n",
            "Interval 95244 (952430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 95245 (952440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.197 - mean_q: 12.588 - prob: 1.000\n",
            "\n",
            "Interval 95246 (952450 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.456 - mean_q: 12.918 - prob: 1.000\n",
            "\n",
            "Interval 95247 (952460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 95248 (952470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.347 - mean_q: 12.711 - prob: 1.000\n",
            "\n",
            "Interval 95249 (952480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.110 - mean_q: 12.429 - prob: 1.000\n",
            "\n",
            "Interval 95250 (952490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 95251 (952500 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.323 - mean_q: 12.672 - prob: 1.000\n",
            "\n",
            "Interval 95252 (952510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.000 - mae: 7.292 - mean_q: 12.641 - prob: 1.000\n",
            "\n",
            "Interval 95253 (952520 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 95254 (952530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.574 - mean_q: 13.002 - prob: 1.000\n",
            "\n",
            "Interval 95255 (952540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.153 - mean_q: 12.607 - prob: 1.000\n",
            "\n",
            "Interval 95256 (952550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.023 - mean_q: 12.258 - prob: 1.000\n",
            "\n",
            "Interval 95257 (952560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.487 - mean_q: 12.987 - prob: 1.000\n",
            "\n",
            "Interval 95258 (952570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.417 - mean_q: 12.793 - prob: 1.000\n",
            "\n",
            "Interval 95259 (952580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 95260 (952590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.307 - mean_q: 12.597 - prob: 1.000\n",
            "\n",
            "Interval 95261 (952600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.220 - mean_q: 12.487 - prob: 1.000\n",
            "\n",
            "Interval 95262 (952610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 95263 (952620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.001 - mae: 7.309 - mean_q: 12.672 - prob: 1.000\n",
            "\n",
            "Interval 95264 (952630 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.064 - mean_q: 12.223 - prob: 1.000\n",
            "\n",
            "Interval 95265 (952640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.000 - mae: 7.255 - mean_q: 12.632 - prob: 1.000\n",
            "\n",
            "Interval 95266 (952650 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 95267 (952660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.000 - mae: 7.381 - mean_q: 12.722 - prob: 1.000\n",
            "\n",
            "Interval 95268 (952670 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.000 - mae: 7.154 - mean_q: 12.467 - prob: 1.000\n",
            "\n",
            "Interval 95269 (952680 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 95270 (952690 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.482 - mean_q: 12.954 - prob: 1.000\n",
            "\n",
            "Interval 95271 (952700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.096 - mean_q: 12.254 - prob: 1.000\n",
            "\n",
            "Interval 95272 (952710 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 95273 (952720 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.349 - mean_q: 12.727 - prob: 1.000\n",
            "\n",
            "Interval 95274 (952730 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.383 - mean_q: 12.657 - prob: 1.000\n",
            "\n",
            "Interval 95275 (952740 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 95276 (952750 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.611 - mean_q: 13.083 - prob: 1.000\n",
            "\n",
            "Interval 95277 (952760 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.267 - mean_q: 12.530 - prob: 1.000\n",
            "\n",
            "Interval 95278 (952770 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 95279 (952780 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.000 - mae: 7.570 - mean_q: 12.936 - prob: 1.000\n",
            "\n",
            "Interval 95280 (952790 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.001 - mae: 7.392 - mean_q: 12.722 - prob: 1.000\n",
            "\n",
            "Interval 95281 (952800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.346 - mean_q: 12.734 - prob: 1.000\n",
            "\n",
            "Interval 95282 (952810 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 95283 (952820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.214 - mean_q: 12.528 - prob: 1.000\n",
            "\n",
            "Interval 95284 (952830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.001 - mae: 7.484 - mean_q: 12.790 - prob: 1.000\n",
            "\n",
            "Interval 95285 (952840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.260 - mean_q: 12.540 - prob: 1.000\n",
            "\n",
            "Interval 95286 (952850 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 95287 (952860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.168 - mean_q: 12.404 - prob: 1.000\n",
            "\n",
            "Interval 95288 (952870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.188 - mean_q: 12.622 - prob: 1.000\n",
            "\n",
            "Interval 95289 (952880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.390 - mean_q: 12.874 - prob: 1.000\n",
            "\n",
            "Interval 95290 (952890 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 95291 (952900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.479 - mean_q: 12.828 - prob: 1.000\n",
            "\n",
            "Interval 95292 (952910 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.000 - mae: 7.596 - mean_q: 12.943 - prob: 1.000\n",
            "\n",
            "Interval 95293 (952920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 95294 (952930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.000 - mae: 6.995 - mean_q: 12.148 - prob: 1.000\n",
            "\n",
            "Interval 95295 (952940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.313 - mean_q: 12.628 - prob: 1.000\n",
            "\n",
            "Interval 95296 (952950 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.336 - mean_q: 12.617 - prob: 1.000\n",
            "\n",
            "Interval 95297 (952960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.000 - mae: 7.173 - mean_q: 12.403 - prob: 1.000\n",
            "\n",
            "Interval 95298 (952970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 95299 (952980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.000 - mae: 7.274 - mean_q: 12.593 - prob: 1.000\n",
            "\n",
            "Interval 95300 (952990 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.263 - mean_q: 12.508 - prob: 1.000\n",
            "\n",
            "Interval 95301 (953000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 95302 (953010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.000 - mae: 7.354 - mean_q: 12.750 - prob: 1.000\n",
            "\n",
            "Interval 95303 (953020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 95304 (953030 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.233 - mean_q: 12.491 - prob: 1.000\n",
            "\n",
            "Interval 95305 (953040 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.373 - mean_q: 12.727 - prob: 1.000\n",
            "\n",
            "Interval 95306 (953050 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 95307 (953060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.302 - mean_q: 12.519 - prob: 1.000\n",
            "\n",
            "Interval 95308 (953070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.159 - mean_q: 12.469 - prob: 1.000\n",
            "\n",
            "Interval 95309 (953080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -13.000 [-13.000, -13.000] - loss: 0.000 - mae: 7.436 - mean_q: 12.615 - prob: 1.000\n",
            "\n",
            "Interval 95310 (953090 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 95311 (953100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.553 - mean_q: 13.032 - prob: 1.000\n",
            "\n",
            "Interval 95312 (953110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.416 - mean_q: 12.731 - prob: 1.000\n",
            "\n",
            "Interval 95313 (953120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 95314 (953130 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.055 - mean_q: 12.266 - prob: 1.000\n",
            "\n",
            "Interval 95315 (953140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 95316 (953150 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -9.000 [-9.000, -9.000] - loss: 0.000 - mae: 7.212 - mean_q: 12.483 - prob: 1.000\n",
            "\n",
            "Interval 95317 (953160 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.388 - mean_q: 12.656 - prob: 1.000\n",
            "\n",
            "Interval 95318 (953170 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 95319 (953180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.429 - mean_q: 12.839 - prob: 1.000\n",
            "\n",
            "Interval 95320 (953190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.464 - mean_q: 12.903 - prob: 1.000\n",
            "\n",
            "Interval 95321 (953200 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 95322 (953210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.508 - mean_q: 12.886 - prob: 1.000\n",
            "\n",
            "Interval 95323 (953220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 95324 (953230 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.000 - mae: 7.475 - mean_q: 12.883 - prob: 1.000\n",
            "\n",
            "Interval 95325 (953240 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.129 - mean_q: 12.371 - prob: 1.000\n",
            "\n",
            "Interval 95326 (953250 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.000 - mae: 7.514 - mean_q: 13.006 - prob: 1.000\n",
            "\n",
            "Interval 95327 (953260 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 6.865 - mean_q: 12.028 - prob: 1.000\n",
            "\n",
            "Interval 95328 (953270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 95329 (953280 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.387 - mean_q: 12.960 - prob: 1.000\n",
            "\n",
            "Interval 95330 (953290 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.107 - mean_q: 12.326 - prob: 1.000\n",
            "\n",
            "Interval 95331 (953300 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 95332 (953310 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.045 - mean_q: 12.269 - prob: 1.000\n",
            "\n",
            "Interval 95333 (953320 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 95334 (953330 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.000 - mae: 7.560 - mean_q: 13.052 - prob: 1.000\n",
            "\n",
            "Interval 95335 (953340 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.387 - mean_q: 12.803 - prob: 1.000\n",
            "\n",
            "Interval 95336 (953350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.078 - mean_q: 12.305 - prob: 1.000\n",
            "\n",
            "Interval 95337 (953360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.636 - mean_q: 13.060 - prob: 1.000\n",
            "\n",
            "Interval 95338 (953370 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 95339 (953380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.215 - mean_q: 12.391 - prob: 1.000\n",
            "\n",
            "Interval 95340 (953390 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.472 - mean_q: 13.018 - prob: 1.000\n",
            "\n",
            "Interval 95341 (953400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 95342 (953410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.369 - mean_q: 12.663 - prob: 1.000\n",
            "\n",
            "Interval 95343 (953420 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.678 - mean_q: 13.071 - prob: 1.000\n",
            "\n",
            "Interval 95344 (953430 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 95345 (953440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.000 - mae: 7.391 - mean_q: 12.634 - prob: 1.000\n",
            "\n",
            "Interval 95346 (953450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.096 - mean_q: 12.226 - prob: 1.000\n",
            "\n",
            "Interval 95347 (953460 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.441 - mean_q: 12.881 - prob: 1.000\n",
            "\n",
            "Interval 95348 (953470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 95349 (953480 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.330 - mean_q: 12.696 - prob: 1.000\n",
            "\n",
            "Interval 95350 (953490 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 95351 (953500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -6.000 [-6.000, -6.000] - loss: 0.000 - mae: 7.009 - mean_q: 12.222 - prob: 1.000\n",
            "\n",
            "Interval 95352 (953510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.527 - mean_q: 12.979 - prob: 1.000\n",
            "\n",
            "Interval 95353 (953520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 95354 (953530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.343 - mean_q: 12.695 - prob: 1.000\n",
            "\n",
            "Interval 95355 (953540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.226 - mean_q: 12.544 - prob: 1.000\n",
            "\n",
            "Interval 95356 (953550 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 95357 (953560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.325 - mean_q: 12.681 - prob: 1.000\n",
            "\n",
            "Interval 95358 (953570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.301 - mean_q: 12.708 - prob: 1.000\n",
            "\n",
            "Interval 95359 (953580 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.587 - mean_q: 13.088 - prob: 1.000\n",
            "\n",
            "Interval 95360 (953590 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 95361 (953600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.743 - mean_q: 13.268 - prob: 1.000\n",
            "\n",
            "Interval 95362 (953610 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.175 - mean_q: 12.371 - prob: 1.000\n",
            "\n",
            "Interval 95363 (953620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 95364 (953630 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.388 - mean_q: 12.816 - prob: 1.000\n",
            "\n",
            "Interval 95365 (953640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.748 - mean_q: 13.239 - prob: 1.000\n",
            "\n",
            "Interval 95366 (953650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 95367 (953660 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.546 - mean_q: 12.995 - prob: 1.000\n",
            "\n",
            "Interval 95368 (953670 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.000 - mae: 7.024 - mean_q: 12.195 - prob: 1.000\n",
            "\n",
            "Interval 95369 (953680 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 95370 (953690 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.684 - mean_q: 13.268 - prob: 1.000\n",
            "\n",
            "Interval 95371 (953700 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 95372 (953710 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.000 - mae: 7.150 - mean_q: 12.513 - prob: 1.000\n",
            "\n",
            "Interval 95373 (953720 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.643 - mean_q: 13.175 - prob: 1.000\n",
            "\n",
            "Interval 95374 (953730 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.263 - mean_q: 12.450 - prob: 1.000\n",
            "\n",
            "Interval 95375 (953740 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.384 - mean_q: 12.781 - prob: 1.000\n",
            "\n",
            "Interval 95376 (953750 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 95377 (953760 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.467 - mean_q: 12.943 - prob: 1.000\n",
            "\n",
            "Interval 95378 (953770 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 95379 (953780 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.642 - mean_q: 13.027 - prob: 1.000\n",
            "\n",
            "Interval 95380 (953790 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 7.566 - mean_q: 13.087 - prob: 1.000\n",
            "\n",
            "Interval 95381 (953800 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.306 - mean_q: 12.565 - prob: 1.000\n",
            "\n",
            "Interval 95382 (953810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 95383 (953820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.092 - mean_q: 12.464 - prob: 1.000\n",
            "\n",
            "Interval 95384 (953830 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.567 - mean_q: 13.040 - prob: 1.000\n",
            "\n",
            "Interval 95385 (953840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 95386 (953850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.091 - mean_q: 12.303 - prob: 1.000\n",
            "\n",
            "Interval 95387 (953860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 95388 (953870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.247 - mean_q: 12.640 - prob: 1.000\n",
            "\n",
            "Interval 95389 (953880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.334 - mean_q: 12.773 - prob: 1.000\n",
            "\n",
            "Interval 95390 (953890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 95391 (953900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.430 - mean_q: 12.782 - prob: 1.000\n",
            "\n",
            "Interval 95392 (953910 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 95393 (953920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.392 - mean_q: 12.720 - prob: 1.000\n",
            "\n",
            "Interval 95394 (953930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.354 - mean_q: 12.777 - prob: 1.000\n",
            "\n",
            "Interval 95395 (953940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.128 - mean_q: 12.475 - prob: 1.000\n",
            "\n",
            "Interval 95396 (953950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 95397 (953960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.227 - mean_q: 12.578 - prob: 1.000\n",
            "\n",
            "Interval 95398 (953970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 95399 (953980 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.411 - mean_q: 12.830 - prob: 1.000\n",
            "\n",
            "Interval 95400 (953990 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.191 - mean_q: 12.473 - prob: 1.000\n",
            "\n",
            "Interval 95401 (954000 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.000 - mae: 7.192 - mean_q: 12.394 - prob: 1.000\n",
            "\n",
            "Interval 95402 (954010 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.243 - mean_q: 12.469 - prob: 1.000\n",
            "\n",
            "Interval 95403 (954020 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 95404 (954030 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.000 - mae: 7.492 - mean_q: 13.001 - prob: 1.000\n",
            "\n",
            "Interval 95405 (954040 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.9000\n",
            "Interval 95406 (954050 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.406 - mean_q: 12.809 - prob: 1.000\n",
            "\n",
            "Interval 95407 (954060 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.001 - mae: 7.289 - mean_q: 12.726 - prob: 1.000\n",
            "\n",
            "Interval 95408 (954070 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 95409 (954080 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.000 - mae: 7.242 - mean_q: 12.493 - prob: 1.000\n",
            "\n",
            "Interval 95410 (954090 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.568 - mean_q: 13.016 - prob: 1.000\n",
            "\n",
            "Interval 95411 (954100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.001 - mae: 8.025 - mean_q: 13.627 - prob: 1.000\n",
            "\n",
            "Interval 95412 (954110 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.393 - mean_q: 12.802 - prob: 1.000\n",
            "\n",
            "Interval 95413 (954120 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.644 - mean_q: 12.909 - prob: 1.000\n",
            "\n",
            "Interval 95414 (954130 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 95415 (954140 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.251 - mean_q: 12.510 - prob: 1.000\n",
            "\n",
            "Interval 95416 (954150 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.001 - mae: 7.515 - mean_q: 12.901 - prob: 1.000\n",
            "\n",
            "Interval 95417 (954160 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 95418 (954170 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.332 - mean_q: 12.583 - prob: 1.000\n",
            "\n",
            "Interval 95419 (954180 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.395 - mean_q: 12.840 - prob: 1.000\n",
            "\n",
            "Interval 95420 (954190 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.530 - mean_q: 12.891 - prob: 1.000\n",
            "\n",
            "Interval 95421 (954200 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -1.000 [-1.000, -1.000] - loss: 0.000 - mae: 7.302 - mean_q: 12.591 - prob: 1.000\n",
            "\n",
            "Interval 95422 (954210 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 95423 (954220 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.000 - mae: 7.406 - mean_q: 12.854 - prob: 1.000\n",
            "\n",
            "Interval 95424 (954230 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.001 - mae: 7.189 - mean_q: 12.462 - prob: 1.000\n",
            "\n",
            "Interval 95425 (954240 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.250 - mean_q: 12.549 - prob: 1.000\n",
            "\n",
            "Interval 95426 (954250 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 95427 (954260 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.000 - mae: 7.292 - mean_q: 12.655 - prob: 1.000\n",
            "\n",
            "Interval 95428 (954270 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 95429 (954280 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -8.000 [-8.000, -8.000] - loss: 0.001 - mae: 7.260 - mean_q: 12.556 - prob: 1.000\n",
            "\n",
            "Interval 95430 (954290 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 95431 (954300 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.213 - mean_q: 12.713 - prob: 1.000\n",
            "\n",
            "Interval 95432 (954310 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.000 - mae: 7.524 - mean_q: 12.922 - prob: 1.000\n",
            "\n",
            "Interval 95433 (954320 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -3.000 [-3.000, -3.000] - loss: 0.000 - mae: 7.399 - mean_q: 12.667 - prob: 1.000\n",
            "\n",
            "Interval 95434 (954330 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 95435 (954340 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.375 - mean_q: 12.708 - prob: 1.000\n",
            "\n",
            "Interval 95436 (954350 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 95437 (954360 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.166 - mean_q: 12.424 - prob: 1.000\n",
            "\n",
            "Interval 95438 (954370 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.568 - mean_q: 13.036 - prob: 1.000\n",
            "\n",
            "Interval 95439 (954380 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 95440 (954390 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.486 - mean_q: 12.845 - prob: 1.000\n",
            "\n",
            "Interval 95441 (954400 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.437 - mean_q: 12.871 - prob: 1.000\n",
            "\n",
            "Interval 95442 (954410 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 95443 (954420 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 3.2000\n",
            "2 episodes - episode_reward: 10.000 [6.000, 14.000] - loss: 0.001 - mae: 7.129 - mean_q: 12.382 - prob: 1.000\n",
            "\n",
            "Interval 95444 (954430 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 95445 (954440 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.427 - mean_q: 12.640 - prob: 1.000\n",
            "\n",
            "Interval 95446 (954450 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.000 - mae: 7.151 - mean_q: 12.532 - prob: 1.000\n",
            "\n",
            "Interval 95447 (954460 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -3.7000\n",
            "Interval 95448 (954470 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -22.000 [-22.000, -22.000] - loss: 0.001 - mae: 7.246 - mean_q: 12.464 - prob: 1.000\n",
            "\n",
            "Interval 95449 (954480 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.505 - mean_q: 12.971 - prob: 1.000\n",
            "\n",
            "Interval 95450 (954490 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.001 - mae: 7.021 - mean_q: 12.228 - prob: 1.000\n",
            "\n",
            "Interval 95451 (954500 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.000 - mae: 7.168 - mean_q: 12.430 - prob: 1.000\n",
            "\n",
            "Interval 95452 (954510 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 95453 (954520 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.540 - mean_q: 12.914 - prob: 1.000\n",
            "\n",
            "Interval 95454 (954530 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.329 - mean_q: 12.844 - prob: 1.000\n",
            "\n",
            "Interval 95455 (954540 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 95456 (954550 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.437 - mean_q: 12.878 - prob: 1.000\n",
            "\n",
            "Interval 95457 (954560 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.591 - mean_q: 13.017 - prob: 1.000\n",
            "\n",
            "Interval 95458 (954570 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 95459 (954580 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.419 - mean_q: 12.804 - prob: 1.000\n",
            "\n",
            "Interval 95460 (954590 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.101 - mean_q: 12.481 - prob: 1.000\n",
            "\n",
            "Interval 95461 (954600 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 95462 (954610 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.250 - mean_q: 12.540 - prob: 1.000\n",
            "\n",
            "Interval 95463 (954620 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 95464 (954630 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.001 - mae: 7.566 - mean_q: 12.955 - prob: 1.000\n",
            "\n",
            "Interval 95465 (954640 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 95466 (954650 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mae: 7.124 - mean_q: 12.379 - prob: 1.000\n",
            "\n",
            "Interval 95467 (954660 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.001 - mae: 7.441 - mean_q: 12.837 - prob: 1.000\n",
            "\n",
            "Interval 95468 (954670 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.702 - mean_q: 13.167 - prob: 1.000\n",
            "\n",
            "Interval 95469 (954680 steps performed)\n",
            "10/10 [==============================] - 0s 12ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.570 - mean_q: 13.011 - prob: 1.000\n",
            "\n",
            "Interval 95470 (954690 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 95471 (954700 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.001 - mae: 7.186 - mean_q: 12.402 - prob: 1.000\n",
            "\n",
            "Interval 95472 (954710 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 95473 (954720 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.000 - mae: 7.531 - mean_q: 12.928 - prob: 1.000\n",
            "\n",
            "Interval 95474 (954730 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.044 - mean_q: 12.311 - prob: 1.000\n",
            "\n",
            "Interval 95475 (954740 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 7.388 - mean_q: 12.663 - prob: 1.000\n",
            "\n",
            "Interval 95476 (954750 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: -1.0000\n",
            "Interval 95477 (954760 steps performed)\n",
            "10/10 [==============================] - 0s 10ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 1.000 [1.000, 1.000] - loss: 0.001 - mae: 7.104 - mean_q: 12.360 - prob: 1.000\n",
            "\n",
            "Interval 95478 (954770 steps performed)\n",
            "10/10 [==============================] - 0s 15ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.001 - mae: 7.296 - mean_q: 12.677 - prob: 1.000\n",
            "\n",
            "Interval 95479 (954780 steps performed)\n",
            "10/10 [==============================] - 0s 11ms/step - reward: -1.0000\n",
            "Interval 95480 (954790 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.294 - mean_q: 12.738 - prob: 1.000\n",
            "\n",
            "Interval 95481 (954800 steps performed)\n",
            "10/10 [==============================] - 0s 9ms/step - reward: -1.0000\n",
            "Interval 95482 (954810 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.345 - mean_q: 12.612 - prob: 1.000\n",
            "\n",
            "Interval 95483 (954820 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.001 - mae: 7.156 - mean_q: 12.338 - prob: 1.000\n",
            "\n",
            "Interval 95484 (954830 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: -1.0000\n",
            "Interval 95485 (954840 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.641 - mean_q: 13.050 - prob: 1.000\n",
            "\n",
            "Interval 95486 (954850 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.001 - mae: 7.280 - mean_q: 12.702 - prob: 1.000\n",
            "\n",
            "Interval 95487 (954860 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 95488 (954870 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.012 - mean_q: 12.170 - prob: 1.000\n",
            "\n",
            "Interval 95489 (954880 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 95490 (954890 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -0.7000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.614 - mean_q: 13.120 - prob: 1.000\n",
            "\n",
            "Interval 95491 (954900 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -12.000 [-12.000, -12.000] - loss: 0.000 - mae: 7.516 - mean_q: 12.997 - prob: 1.000\n",
            "\n",
            "Interval 95492 (954910 steps performed)\n",
            "10/10 [==============================] - 0s 4ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.277 - mean_q: 12.616 - prob: 1.000\n",
            "\n",
            "Interval 95493 (954920 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 95494 (954930 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -2.000 [-2.000, -2.000] - loss: 0.001 - mae: 7.402 - mean_q: 12.904 - prob: 1.000\n",
            "\n",
            "Interval 95495 (954940 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -2.8000\n",
            "Interval 95496 (954950 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -14.000 [-14.000, -14.000] - loss: 0.000 - mae: 7.567 - mean_q: 13.056 - prob: 1.000\n",
            "\n",
            "Interval 95497 (954960 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.000 - mae: 7.188 - mean_q: 12.350 - prob: 1.000\n",
            "\n",
            "Interval 95498 (954970 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.000 - mae: 7.304 - mean_q: 12.510 - prob: 1.000\n",
            "\n",
            "Interval 95499 (954980 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.0000\n",
            "Interval 95500 (954990 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.000 - mae: 6.971 - mean_q: 12.175 - prob: 1.000\n",
            "\n",
            "Interval 95501 (955000 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -4.000 [-4.000, -4.000] - loss: 0.000 - mae: 7.100 - mean_q: 12.241 - prob: 1.000\n",
            "\n",
            "Interval 95502 (955010 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 95503 (955020 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.325 - mean_q: 12.682 - prob: 1.000\n",
            "\n",
            "Interval 95504 (955030 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.433 - mean_q: 12.751 - prob: 1.000\n",
            "\n",
            "Interval 95505 (955040 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.9000\n",
            "Interval 95506 (955050 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -7.000 [-7.000, -7.000] - loss: 0.001 - mae: 7.570 - mean_q: 13.024 - prob: 1.000\n",
            "\n",
            "Interval 95507 (955060 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.000 - mae: 7.499 - mean_q: 12.794 - prob: 1.000\n",
            "\n",
            "Interval 95508 (955070 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: -1.0000\n",
            "Interval 95509 (955080 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.188 - mean_q: 12.493 - prob: 1.000\n",
            "\n",
            "Interval 95510 (955090 steps performed)\n",
            "10/10 [==============================] - 0s 6ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.000 - mae: 7.169 - mean_q: 12.418 - prob: 1.000\n",
            "\n",
            "Interval 95511 (955100 steps performed)\n",
            "10/10 [==============================] - 0s 5ms/step - reward: -1.9000\n",
            "Interval 95512 (955110 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: -5.000 [-5.000, -5.000] - loss: 0.000 - mae: 7.130 - mean_q: 12.504 - prob: 1.000\n",
            "\n",
            "Interval 95513 (955120 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.001 - mae: 7.626 - mean_q: 13.106 - prob: 1.000\n",
            "\n",
            "Interval 95514 (955130 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 95515 (955140 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.001 - mae: 7.380 - mean_q: 12.782 - prob: 1.000\n",
            "\n",
            "Interval 95516 (955150 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 0.2000\n",
            "1 episodes - episode_reward: -15.000 [-15.000, -15.000] - loss: 0.001 - mae: 7.036 - mean_q: 12.106 - prob: 1.000\n",
            "\n",
            "Interval 95517 (955160 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 95518 (955170 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.001 - mae: 7.509 - mean_q: 12.903 - prob: 1.000\n",
            "\n",
            "Interval 95519 (955180 steps performed)\n",
            "10/10 [==============================] - 0s 7ms/step - reward: -1.0000\n",
            "Interval 95520 (955190 steps performed)\n",
            "10/10 [==============================] - 0s 8ms/step - reward: 1.1000\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.001 - mae: 7.635 - mean_q: 13.181 - prob: 1.000\n",
            "\n",
            "Interval 95521 (955200 steps performed)\n",
            " 1/10 [==>...........................] - ETA: 0s - reward: -1.0000"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ptQbvQLClBxj",
        "colab": {},
        "outputId": "cd28f203-64f2-4aec-cd7f-20fb45175440"
      },
      "source": [
        "dqn.test(env, nb_episodes=5, visualize=True, nb_max_episode_steps=99)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing for 5 episodes ...\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | :\u001b[43m \u001b[0m|\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : :\u001b[43m \u001b[0m|\n",
            "| | : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : :\u001b[43m \u001b[0m: |\n",
            "| | : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : :\u001b[43m \u001b[0m: : |\n",
            "| | : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| :\u001b[43m \u001b[0m: : : |\n",
            "| | : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "|\u001b[43m \u001b[0m: : : : |\n",
            "| | : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "|\u001b[43m \u001b[0m| : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[42mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "|\u001b[42m_\u001b[0m| : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "|\u001b[42m_\u001b[0m: : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "|\u001b[42m_\u001b[0m: | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|\u001b[35m\u001b[42mR\u001b[0m\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "Episode 1: reward: 7.000, steps: 14\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | :\u001b[43m \u001b[0m| : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : :\u001b[43m \u001b[0m: : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : :\u001b[43m \u001b[0m: |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : |\u001b[43m \u001b[0m: |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[42mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : |\u001b[42m_\u001b[0m: |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : :\u001b[42m_\u001b[0m: |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : :\u001b[42m_\u001b[0m: : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| :\u001b[42m_\u001b[0m: : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| :\u001b[42m_\u001b[0m| : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m:\u001b[42m_\u001b[0m| : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|\u001b[35m\u001b[42mR\u001b[0m\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "+---------+\n",
            "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "Episode 2: reward: 7.000, steps: 14\n",
            "+---------+\n",
            "|R: | : :\u001b[42mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : :\u001b[42m_\u001b[0m|\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | :\u001b[42m_\u001b[0m: |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : :\u001b[42m_\u001b[0m: |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : |\u001b[42m_\u001b[0m: |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35m\u001b[42mB\u001b[0m\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35m\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "Episode 3: reward: 14.000, steps: 7\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: |\u001b[43m \u001b[0m: :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : |\u001b[43m \u001b[0m: : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : :\u001b[43m \u001b[0m: : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| :\u001b[43m \u001b[0m: : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "|\u001b[43m \u001b[0m: : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "|\u001b[43m \u001b[0m: | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|\u001b[42mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "|\u001b[42m_\u001b[0m: | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "|\u001b[42m_\u001b[0m: : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "|\u001b[42m_\u001b[0m| : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35m\u001b[42mY\u001b[0m\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35m\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "Episode 4: reward: 8.000, steps: 13\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "|\u001b[43m \u001b[0m| : | : |\n",
            "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "|\u001b[43m \u001b[0m: : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| :\u001b[43m \u001b[0m: : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : :\u001b[43m \u001b[0m: : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : :\u001b[43m \u001b[0m: |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : |\u001b[43m \u001b[0m: |\n",
            "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |\u001b[42mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : |\u001b[42m_\u001b[0m: |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : :\u001b[42m_\u001b[0m: |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : :\u001b[42m_\u001b[0m: : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| :\u001b[42m_\u001b[0m: : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "|\u001b[42m_\u001b[0m: : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "|\u001b[42m_\u001b[0m| : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35m\u001b[42mY\u001b[0m\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35m\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "Episode 5: reward: 5.000, steps: 16\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f67494b7908>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    }
  ]
}